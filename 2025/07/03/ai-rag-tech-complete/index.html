
<!DOCTYPE html><html lang="zh-CN">

<head>
  <meta charset="utf-8">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.29.1" theme-name="Stellar" theme-version="1.29.1">
  
  <meta name="generator" content="Hexo 6.3.0">
  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel="preconnect" href="https://gcore.jsdelivr.net" crossorigin><link rel="preconnect" href="https://unpkg.com" crossorigin><link rel="preconnect" href="https://cdn.bootcdn.net" crossorigin>
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#000">
  <meta name="theme-color" content="#f9fafb">
  
  <title>RAG 全栈技术 - HedonWang</title>

  
    <meta name="description" content="RAG 全栈技术">
<meta property="og:type" content="article">
<meta property="og:title" content="RAG 全栈技术">
<meta property="og:url" content="https://hedon.top/2025/07/03/ai-rag-tech-complete/index.html">
<meta property="og:site_name" content="HedonWang">
<meta property="og:description" content="RAG 全栈技术">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image.png">
<meta property="og:image" content="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/output.png">
<meta property="article:published_time" content="2025-07-03T00:00:00.000Z">
<meta property="article:modified_time" content="2025-07-08T02:38:47.739Z">
<meta property="article:author" content="Hedon Wang">
<meta property="article:tag" content="ai">
<meta property="article:tag" content="rag">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image.png">
  
  
  
  <meta name="keywords" content="ai,rag">

  <!-- feed -->
  
    <link rel="alternate" href="/atom.xml" title="HedonWang" type="application/atom+xml">
  

  <link rel="stylesheet" href="/css/main.css?v=1.29.1">


  
    <link rel="shortcut icon" href="/assets/favicon.png">
  

  

  <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon.png"><link rel="shortcut icon" href="/assets/favicon.png"><meta name="theme-color" content="#f8f8f8"><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC&display=swap" rel="stylesheet"><link rel="stylesheet" href="/css/markmap.css"><script src="https://cdn.jsdelivr.net/npm/markmap-autoloader@0.18"></script>
</head>
<body>

<div class="l_body s:aa content tech" id="start" layout="post" ><aside class="l_left"><div class="leftbar-container">


<header class="header"><div class="logo-wrap"><div class="icon"><img no-lazy class="icon" src="/assets/favicon.png" onerror="javascript:this.classList.add('error');this.src='https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/image/2659360.svg';"></div><a class="title" href="/"><div class="main" ff="title">HedonWang</div><div class="sub cap">君子求诸己，律己则安。</div></a></div></header>

<div class="nav-area">
<div class="search-wrapper" id="search-wrapper"><form class="search-form"><a class="search-button" onclick="document.getElementById(&quot;search-input&quot;).focus();"><svg t="1705074644177" viewBox="0 0 1025 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1560" width="200" height="200"><path d="M1008.839137 935.96571L792.364903 719.491476a56.783488 56.783488 0 0 0-80.152866 0 358.53545 358.53545 0 1 1 100.857314-335.166073 362.840335 362.840335 0 0 1-3.689902 170.145468 51.248635 51.248635 0 1 0 99.217358 26.444296 462.057693 462.057693 0 1 0-158.255785 242.303546l185.930047 185.725053a51.248635 51.248635 0 0 0 72.568068 0 51.248635 51.248635 0 0 0 0-72.978056z" p-id="1561"></path><path d="M616.479587 615.969233a50.428657 50.428657 0 0 0-61.498362-5.534852 174.655348 174.655348 0 0 1-177.525271 3.484907 49.403684 49.403684 0 0 0-58.833433 6.76482l-3.074918 2.869923a49.403684 49.403684 0 0 0 8.609771 78.10292 277.767601 277.767601 0 0 0 286.992355-5.739847 49.403684 49.403684 0 0 0 8.404776-76.667958z" p-id="1562"></path></svg></a><input type="text" class="search-input" id="search-input" placeholder="站内搜索"></form><div id="search-result"></div><div class="search-no-result">没有找到内容！</div></div>


<nav class="menu dis-select"><a class="nav-item active" title="博客" href="/" style="color:#1BCDFC"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M5.879 2.879C5 3.757 5 5.172 5 8v8c0 2.828 0 4.243.879 5.121C6.757 22 8.172 22 11 22h2c2.828 0 4.243 0 5.121-.879C19 20.243 19 18.828 19 16V8c0-2.828 0-4.243-.879-5.121C17.243 2 15.828 2 13 2h-2c-2.828 0-4.243 0-5.121.879M8.25 17a.75.75 0 0 1 .75-.75h3a.75.75 0 0 1 0 1.5H9a.75.75 0 0 1-.75-.75M9 12.25a.75.75 0 0 0 0 1.5h6a.75.75 0 0 0 0-1.5zM8.25 9A.75.75 0 0 1 9 8.25h6a.75.75 0 0 1 0 1.5H9A.75.75 0 0 1 8.25 9" clip-rule="evenodd"/><path fill="currentColor" d="M5.235 4.058C5 4.941 5 6.177 5 8v8c0 1.823 0 3.058.235 3.942L5 19.924c-.975-.096-1.631-.313-2.121-.803C2 18.243 2 16.828 2 14v-4c0-2.829 0-4.243.879-5.121c.49-.49 1.146-.707 2.121-.803zm13.53 15.884C19 19.058 19 17.822 19 16V8c0-1.823 0-3.059-.235-3.942l.235.018c.975.096 1.631.313 2.121.803C22 5.757 22 7.17 22 9.999v4c0 2.83 0 4.243-.879 5.122c-.49.49-1.146.707-2.121.803z" opacity=".5"/></svg></a><a class="nav-item" title="专栏" href="/topic/" style="color:#3DC550"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M14.25 4.48v3.057c0 .111 0 .27.02.406a.936.936 0 0 0 .445.683a.96.96 0 0 0 .783.072c.13-.04.272-.108.378-.159L17 8.005l1.124.534c.106.05.248.119.378.16a.958.958 0 0 0 .783-.073a.936.936 0 0 0 .444-.683c.021-.136.021-.295.021-.406V3.031c.113-.005.224-.01.332-.013C21.154 2.98 22 3.86 22 4.933v11.21c0 1.112-.906 2.01-2.015 2.08c-.97.06-2.108.179-2.985.41c-1.082.286-1.99 1.068-3.373 1.436c-.626.167-1.324.257-1.627.323V5.174c.32-.079 1.382-.203 1.674-.371c.184-.107.377-.216.576-.323m5.478 8.338a.75.75 0 0 1-.546.91l-4 1a.75.75 0 0 1-.364-1.456l4-1a.75.75 0 0 1 .91.546" clip-rule="evenodd"/><path fill="currentColor" d="M18.25 3.151c-.62.073-1.23.18-1.75.336a8.2 8.2 0 0 0-.75.27v3.182l.75-.356l.008-.005a1.13 1.13 0 0 1 .492-.13c.047 0 .094.004.138.01c.175.029.315.1.354.12l.009.005l.749.356V3.647z"/><path fill="currentColor" d="M12 5.214c-.334-.064-1.057-.161-1.718-.339C8.938 4.515 8.05 3.765 7 3.487c-.887-.234-2.041-.352-3.018-.412C2.886 3.007 2 3.9 2 4.998v11.146c0 1.11.906 2.01 2.015 2.079c.97.06 2.108.179 2.985.41c.486.129 1.216.431 1.873.726c1.005.451 2.052.797 3.127 1.034z" opacity=".5"/><path fill="currentColor" d="M4.273 12.818a.75.75 0 0 1 .91-.545l4 1a.75.75 0 1 1-.365 1.455l-4-1a.75.75 0 0 1-.545-.91m.909-4.545a.75.75 0 1 0-.364 1.455l4 1a.75.75 0 0 0 .364-1.455z"/></svg></a><a class="nav-item" title="关于我" href="/about/" style="color:#F44336"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="m13.629 20.472l-.542.916c-.483.816-1.69.816-2.174 0l-.542-.916c-.42-.71-.63-1.066-.968-1.262c-.338-.197-.763-.204-1.613-.219c-1.256-.021-2.043-.098-2.703-.372a5 5 0 0 1-2.706-2.706C2 14.995 2 13.83 2 11.5v-1c0-3.273 0-4.91.737-6.112a5 5 0 0 1 1.65-1.651C5.59 2 7.228 2 10.5 2h3c3.273 0 4.91 0 6.113.737a5 5 0 0 1 1.65 1.65C22 5.59 22 7.228 22 10.5v1c0 2.33 0 3.495-.38 4.413a5 5 0 0 1-2.707 2.706c-.66.274-1.447.35-2.703.372c-.85.015-1.275.022-1.613.219c-.338.196-.548.551-.968 1.262" opacity=".5"/><path fill="currentColor" d="M10.99 14.308c-1.327-.978-3.49-2.84-3.49-4.593c0-2.677 2.475-3.677 4.5-1.609c2.025-2.068 4.5-1.068 4.5 1.609c0 1.752-2.163 3.615-3.49 4.593c-.454.335-.681.502-1.01.502c-.329 0-.556-.167-1.01-.502"/></svg></a><a class="nav-item" title="思维导图" href="/html/mindmap.html" style="color:#F44336"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="m13.629 20.472l-.542.916c-.483.816-1.69.816-2.174 0l-.542-.916c-.42-.71-.63-1.066-.968-1.262c-.338-.197-.763-.204-1.613-.219c-1.256-.021-2.043-.098-2.703-.372a5 5 0 0 1-2.706-2.706C2 14.995 2 13.83 2 11.5v-1c0-3.273 0-4.91.737-6.112a5 5 0 0 1 1.65-1.651C5.59 2 7.228 2 10.5 2h3c3.273 0 4.91 0 6.113.737a5 5 0 0 1 1.65 1.65C22 5.59 22 7.228 22 10.5v1c0 2.33 0 3.495-.38 4.413a5 5 0 0 1-2.707 2.706c-.66.274-1.447.35-2.703.372c-.85.015-1.275.022-1.613.219c-.338.196-.548.551-.968 1.262" opacity=".5"/><path fill="currentColor" d="M10.99 14.308c-1.327-.978-3.49-2.84-3.49-4.593c0-2.677 2.475-3.677 4.5-1.609c2.025-2.068 4.5-1.068 4.5 1.609c0 1.752-2.163 3.615-3.49 4.593c-.454.335-.681.502-1.01.502c-.329 0-.556-.167-1.01-.502"/></svg></a></nav>
</div>
<div class="widgets">


<widget class="widget-wrapper post-list"><div class="widget-header dis-select"><span class="name">最近更新</span></div><div class="widget-body fs14"><a class="item title" href="/2025/07/08/rust-02-closure/"><span class="title">Rust 入门丨02 闭包</span></a><a class="item title" href="/2025/07/08/fosa-ch7/"><span class="title">FOSA丨07丨架构特性范围</span></a><a class="item title" href="/2025/07/03/ai-rag-tech-complete/"><span class="title">RAG 全栈技术</span></a><a class="item title" href="/2025/07/07/note-write-a-web-framework/"><span class="title">课程笔记丨《手把手带你写一个 Web 框架》</span></a><a class="item title" href="/2025/06/26/fosa-ch1/"><span class="title">FOSA丨01丨软件架构概述</span></a><a class="item title" href="/2025/07/01/fosa-ch2/"><span class="title">FOSA丨02丨架构思维</span></a><a class="item title" href="/2025/07/02/fosa-ch3/"><span class="title">FOSA丨03丨模块化</span></a><a class="item title" href="/2025/07/03/fosa-ch4/"><span class="title">FOSA丨04丨架构特性定义</span></a><a class="item title" href="/2025/07/04/fosa-ch5/"><span class="title">FOSA丨05丨识别架构特征</span></a><a class="item title" href="/2025/07/07/fosa-ch6/"><span class="title">FOSA丨06丨评估和管理架构特性</span></a></div></widget>
</div>

</div></aside><div class="l_main" id="main">





<div class="article banner top"><img class="bg lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/banner/ai-rag-tech-complete.jpg">
  <div class="content">
    <div class="top bread-nav footnote"><div class="left"><div class="flex-row" id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a>
<span class="sep"></span><a class="cap breadcrumb" href="/">文章</a><span class="sep"></span><a class="cap breadcrumb-link" href="/categories/ai/">ai</a> <span class="sep"></span> <a class="cap breadcrumb-link" href="/categories/ai/rag/">rag</a></div>
<div class="flex-row" id="post-meta"><span class="text created">发布于：<time datetime="2025-07-03T00:00:00.000Z">2025-07-03</time></span><span class="sep updated"></span><span class="text updated">更新于：<time datetime="2025-07-08T02:38:47.739Z">2025-07-08</time></span></div></div></div>
    
    <div class="bottom only-title">
      
      <div class="text-area">
        <h1 class="text title"><span>RAG 全栈技术</span></h1>
        
      </div>
    </div>
    
  </div>
  </div><article class="md-text content"><h1 id="文档加载">1. 文档加载</h1>
<ul>
<li><a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/document_loaders/">langchain-document_loaders</a></li>
</ul>
<h2 id="langchian-document">langchian Document</h2>
<ul>
<li><code>page_content</code>: 文档内容</li>
<li><code>metadata</code>: 文档元信息</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.schema <span class="keyword">import</span> Document</span><br><span class="line"></span><br><span class="line">document = Document(</span><br><span class="line">    page_content=<span class="string">&quot;Hello, world!&quot;</span>,</span><br><span class="line">    metadata=&#123;<span class="string">&quot;source&quot;</span>: <span class="string">&quot;https://example.com&quot;</span>&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="html">html</h2>
<ul>
<li><p>在线网页：<em>from</em> langchain<em>community.document_loaders
_import</em> WebBaseLoader</p></li>
<li><p>本地文件：<em>from</em> langchain<em>community.document_loaders
_import</em> BSHTMLLoader</p></li>
<li><p>解析代码：<em>from</em> bs4 <em>import</em> BeautifulSoup</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取 HTML 文件内容</span></span><br><span class="line">html_txt = <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./file_load/test.html&quot;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">        html_txt += line</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析 HTML</span></span><br><span class="line">soup = BeautifulSoup(html_txt, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 代码块 td class=&quot;code&quot;</span></span><br><span class="line">code_content = soup.find_all(<span class="string">&#x27;td&#x27;</span>, class_=<span class="string">&quot;code&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> ele <span class="keyword">in</span> code_content:</span><br><span class="line">    <span class="built_in">print</span>(ele.text)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;+&quot;</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>这里对代码块解析时的 <code>class</code>
需要根据具体网页的元素定义进行更换，不过大体思路都一样（也不局限于代码块）。</p></li>
</ul>
<h2 id="pdf">PDF</h2>
<ul>
<li><p>加载文件：<em>from</em> langchain<em>community.document_loaders
_import</em> PyMuPDFLoader</p></li>
<li><p>解析表格：<em>import</em> fitz</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> fitz</span><br><span class="line"></span><br><span class="line">doc = fitz.<span class="built_in">open</span>(<span class="string">&quot;./file_load/fixtures/zhidu_travel.pdf&quot;</span>)</span><br><span class="line"></span><br><span class="line">table_data = []</span><br><span class="line">text_data = []</span><br><span class="line"></span><br><span class="line">doc_tables = []</span><br><span class="line"><span class="keyword">for</span> idx, page <span class="keyword">in</span> <span class="built_in">enumerate</span>(doc):</span><br><span class="line">    text = page.get_text()</span><br><span class="line">    text_data.append(text)</span><br><span class="line">    tabs = page.find_tables()</span><br><span class="line">    <span class="keyword">for</span> i, tab <span class="keyword">in</span> <span class="built_in">enumerate</span>(tabs):</span><br><span class="line">        ds = tab.to_pandas()</span><br><span class="line">        table_data.append(ds.to_markdown())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> tab <span class="keyword">in</span> table_data:</span><br><span class="line">    <span class="built_in">print</span>(tab)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="unstructured">Unstructured</h2>
<p><a
target="_blank" rel="noopener" href="https://github.com/Unstructured-IO/unstructured">Unstructured</a>
是由 Unstructured.IO 开发的开源 Python 库，专为处理非结构化数据（如
PDF、Word、HTML、XML 等）设计。在 LangChain
中，它作为文档加载的核心工具，实现以下功能：</p>
<ol type="1">
<li>格式支持广泛：解析 PDF、DOCX、PPTX、HTML、XML、CSV
等格式，甚至支持扫描件中的 OCR 文本提取。</li>
<li>元素分区（Partitioning）：将文档拆分为结构化元素（标题、段落、表格、列表），保留原始布局和元数据。</li>
<li>数据清洗：自动清理文档中的无关符号、页眉页脚，生成纯净文本。</li>
</ol>
<p>使用 langchain_unstructured 需要安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">uv add unstructured</span><br><span class="line">uv add langchain_unstructured</span><br><span class="line">uv add unstructured_inference</span><br><span class="line">uv add unstructured_pytesseract</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">系统依赖（macOS）</span></span><br><span class="line">brew install poppler</span><br><span class="line">brew install tesseract</span><br><span class="line">brew install libmagic</span><br><span class="line">brew install ghostscript</span><br><span class="line">brew install pandoc</span><br></pre></td></tr></table></figure>
<h3 id="pdf-1">PDF</h3>
<p>需要额外安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">uv remove camelot-py # 如果有 camelot 需要先移出，在一些版本上存在冲突</span><br><span class="line"></span><br><span class="line">uv add &quot;unstructured[pdf]&quot;</span><br></pre></td></tr></table></figure>
<p>使用时导入包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_unstructured <span class="keyword">import</span> UnstructuredLoader</span><br></pre></td></tr></table></figure>
<h3 id="ppt">PPT</h3>
<p>需要安装额外依赖：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv add python-pptx</span><br></pre></td></tr></table></figure>
<p>使用时导入包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> UnstructuredPowerPointLoader</span><br></pre></td></tr></table></figure>
<p>解析 PPT 中的表格及其他特殊类型，可以使用原始的
<code>python-pptx</code> 库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pptx <span class="keyword">import</span> Presentation</span><br><span class="line"><span class="keyword">from</span> pptx.enum.shapes <span class="keyword">import</span> MSO_SHAPE_TYPE</span><br><span class="line"></span><br><span class="line">ppt = Presentation(<span class="string">&quot;./file_load/fixtures/test_ppt.pptx&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> slide_number, slide <span class="keyword">in</span> <span class="built_in">enumerate</span>(ppt.slides, start=<span class="number">1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Slide <span class="subst">&#123;slide_number&#125;</span>:&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> shape <span class="keyword">in</span> slide.shapes:</span><br><span class="line">        <span class="keyword">if</span> shape.has_text_frame:  <span class="comment"># 文本信息</span></span><br><span class="line">            <span class="built_in">print</span>(shape.text)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> shape.has_table:  <span class="comment"># 表格信息</span></span><br><span class="line">            table = shape.table</span><br><span class="line">            <span class="keyword">for</span> row_idx, row <span class="keyword">in</span> <span class="built_in">enumerate</span>(table.rows):</span><br><span class="line">                <span class="keyword">for</span> col_idx, cell <span class="keyword">in</span> <span class="built_in">enumerate</span>(row.cells):</span><br><span class="line">                    cell_text = cell.text</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f&quot;Row <span class="subst">&#123;row_idx + <span class="number">1</span>&#125;</span>, Column <span class="subst">&#123;col_idx + <span class="number">1</span>&#125;</span>: <span class="subst">&#123;cell_text&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> shape.shape_type == MSO_SHAPE_TYPE.PICTURE: <span class="comment"># 图片信息</span></span><br><span class="line">            imgae = shape.image</span><br><span class="line">            image_filename = <span class="string">&quot;./file_load/fixtures/pic_from_ppt.jpg&quot;</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(image_filename, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(imgae.blob)</span><br></pre></td></tr></table></figure>
<h3 id="word">Word</h3>
<p>需要安装额外依赖：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">uv add docx2txt</span><br><span class="line">uv add python-docx</span><br></pre></td></tr></table></figure>
<p>使用时导入包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> Docx2txtLoader</span><br></pre></td></tr></table></figure>
<p>解析 Word 中的表格及其他特殊类型，可以使用原始的
<code>python-docx</code> 库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> docx <span class="keyword">import</span> Document</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_docx</span>(<span class="params">file_path</span>):</span><br><span class="line">    doc = Document(file_path)</span><br><span class="line">    <span class="keyword">for</span> para <span class="keyword">in</span> doc.paragraphs:</span><br><span class="line">        <span class="built_in">print</span>(para.text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> table <span class="keyword">in</span> doc.tables:</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> table.rows:</span><br><span class="line">            <span class="keyword">for</span> cell <span class="keyword">in</span> row.cells:</span><br><span class="line">                <span class="built_in">print</span>(cell.text, end=<span class="string">&#x27; | &#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">file_path = <span class="string">&quot;./file_load/fixtures/test_word.docx&quot;</span></span><br><span class="line">read_docx(file_path=file_path)</span><br></pre></td></tr></table></figure>
<h3 id="excel">Excel</h3>
<p>需要安装额外依赖：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv add openpyxl</span><br></pre></td></tr></table></figure>
<h2 id="ragflow.deepdoc">ragflow.deepdoc</h2>
<p>RAGFlow 是一个开源的、基于"深度文档理解"的 RAG 引擎。</p>
<p><strong>RAGFlow 的主要特点：</strong></p>
<ol type="1">
<li><strong>开箱即用：</strong> 提供 Web UI
界面，用户可以通过简单的几次点击，无需编写代码，就能完成知识库的建立和问答测试。通过
<code>Docker</code> 可以一键部署，非常方便。</li>
<li><strong>工作流自动化 (Automated Workflow)：</strong> RAGFlow
将复杂的 RAG
流程（文档解析、切块、向量化、存储、检索、生成）模板化。用户可以选择不同的模板来适应不同的数据和任务需求，整个过程高度自动化。</li>
<li><strong>可视化与可解释性：</strong> 在处理文档时，RAGFlow
会生成一个可视化的解析结果图，让用户能清晰地看到文档是如何被理解和切分的，大大增强了系统的透明度和可调试性。</li>
<li><strong>企业级特性：</strong>
它支持多种文档格式，能够生成可溯源的答案（即答案会附上来源出处），并且兼容多种
LLM 和向量数据库，易于集成到现有企业环境中。</li>
</ol>
<p>如果说 RAGFlow 是一个高效的问答“工厂”，那么 DeepDoc
就是这个工厂里最核心、最先进的“原材料加工车间”。所有外部文档在进入知识库之前，都必须经过
DeepDoc 的精细处理。</p>
<p>DeepDoc 的全称是 <strong>Deep Document
Understanding</strong>（深度文档理解），它是 RAGFlow
实现高质量检索的基石。它并非简单地提取文本，而是试图像人一样“看”和“理解”文档的版面布局和内在逻辑。</p>
<p><strong>DeepDoc 的工作原理与核心能力：</strong></p>
<ol type="1">
<li><strong>视觉版面分析 (Vision-based Layout Analysis)：</strong>
<ul>
<li><strong>理论：</strong> DeepDoc
首先会利用计算机视觉（CV）模型，像人眼一样扫描整个文档页面。它不是逐行读取字符，而是先识别出页面上的宏观结构，例如：这是标题、那是段落、这是一个表格、这是一张图片、这是一个页眉/页脚。</li>
<li><strong>实践：</strong> 对于一个两栏布局的 <code>PDF</code>
报告，传统的文本提取工具可能会把左边一行的结尾和右边一行的开头错误地拼在一起。而
DeepDoc
的视觉分析能准确识别出两个独立的栏目，并按照正确的阅读顺序（先读完左栏，再读右栏）来处理文本。</li>
</ul></li>
<li><strong>智能分块 (Intelligent Chunking)：</strong>
<ul>
<li><strong>理论：</strong> 这是 DeepDoc
最具价值的一点。在理解了文档布局之后，它会进行“语义分块”而非“物理分块”。传统的
RAG 会把文档切成固定长度（如 500
个字符）的块，这常常会将一个完整的表格或一段逻辑连贯的话拦腰截断。</li>
<li><strong>实践：</strong> DeepDoc
会将一个完整的表格识别出来并视为一个独立的“块”（Chunk）。一个标题和它紧随其后的段落也会被智能地划分在一起。这样做的好处是，当用户提问与表格相关的问题时，系统检索到的就是这个包含完整上下文的表格块，而不是表格的某几行碎片。这极大地保证了提供给
LLM 的上下文信息的完整性和逻辑性。</li>
</ul></li>
<li><strong>高质量光学字符识别 (OCR)：</strong>
<ul>
<li><strong>理论：</strong> 对于扫描的 <code>PDF</code>
文件或者文档中嵌入的图片，DeepDoc 内置了高质量的 OCR 引擎。</li>
<li><strong>实践：</strong>
即便文档是扫描的复印件，它也能尽可能准确地提取出其中的文字内容，并将其融入到上述的版面分析中，确保信息不丢失。</li>
</ul></li>
<li><strong>表格解析与转译：</strong>
<ul>
<li><strong>理论：</strong> 识别出表格只是第一步，更关键的是让 LLM
能“读懂”表格。</li>
<li><strong>实践：</strong> DeepDoc
能够提取出表格的结构化数据，并将其转换为 LLM 更容易理解的格式，例如
Markdown 格式。一个复杂的表格图片，在经过 DeepDoc 处理后，可能会变成一个
Markdown 文本表格，这样 LLM
就能轻松地理解其行列关系，并回答诸如“请总结一下表格中第三季度销售额最高的产品是哪个？”这类的问题。</li>
</ul></li>
</ol>
<p>笔者在实践过程中，通过精简 ragflow.deepdoc 中的
pdfparser，抽出了一个组件 <a
target="_blank" rel="noopener" href="https://github.com/hedon-ai-road/deepdoc_pdfparser">deepdoc_pdfparser</a>.</p>
<h1 id="分块策略">2. 分块策略</h1>
<ul>
<li>可视化工具：<a
target="_blank" rel="noopener" href="https://chunkviz.up.railway.app/">ChunkViz</a></li>
</ul>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/https%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F92c70184-ba0f-4877-9a55-e4add0e311ad_870x1116.gif"
alt="RAG 五大分块策略" />
<figcaption aria-hidden="true">RAG 五大分块策略</figcaption>
</figure>
<blockquote>
<p>参考：<a
target="_blank" rel="noopener" href="https://blog.dailydoseofds.com/p/5-chunking-strategies-for-rag-f8b?ref=dailydev">5-chunking-strategies-for-rag</a></p>
</blockquote>
<h1 id="向量嵌入">3. 向量嵌入</h1>
<h2 id="嵌入模型评测">3.1 嵌入模型评测</h2>
<p>Hugging Face 的 <a
target="_blank" rel="noopener" href="https://huggingface.co/spaces/mteb/leaderboard">MTEB</a> (Massive
Text Embedding Benchmark)
是一个大规模的文本嵌入模型评测基准。它的核心作用是<strong>为各种文本嵌入模型提供一个统一、全面、客观的性能衡量标准</strong>。</p>
<p>涵盖了文本嵌入在现实世界中最常见的 8 种应用场景，共计 58 个数据集和
112 种语言。这 8 大任务分别是：</p>
<ul>
<li><strong>Bitext Mining (双语文本挖掘):</strong>
在不同语言的句子中找出翻译对。</li>
<li><strong>Classification (分类):</strong>
将文本划分到预定义的类别中。</li>
<li><strong>Clustering (聚类):</strong> 将相似的文本分组在一起。</li>
<li><strong>Pair Classification (句子对分类):</strong>
判断两个句子是否具有某种关系 (如释义、矛盾等)。</li>
<li><strong>Reranking (重排序):</strong> 对一个已经排好序的列表
(如搜索结果) 进行重新排序，以提升质量。</li>
<li><strong>Retrieval (检索):</strong>
从一个大规模的文档语料库中找出与查询最相关的文档。这是目前文本嵌入最核心和最热门的应用之一。</li>
<li><strong>Semantic Textual Similarity (STS, 语义文本相似度):</strong>
判断两个句子的语义相似程度，通常给出一个从 0 到 5 的分数。</li>
<li><strong>Summarization (摘要):</strong>
评估生成的摘要与原文的语义相似度。</li>
</ul>
<figure>
<img src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image.png"
alt="MTEB" />
<figcaption aria-hidden="true">MTEB</figcaption>
</figure>
<h2 id="稀疏嵌入sparse-embedding">3.2 稀疏嵌入（Sparse Embedding）</h2>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 87%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>特征</strong></th>
<th style="text-align: left;"><strong>说明</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>维度</strong></td>
<td style="text-align: left;">通常等于完整词表或特征集合的大小，可达 10⁵
– 10⁶；大多数维度为 0，只有少数位置有权重</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>构造方式</strong></td>
<td style="text-align: left;">基于词频或词频-逆文档频率（TF-IDF）、BM25
等统计方法，不依赖深度学习</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>权重含义</strong></td>
<td
style="text-align: left;">每个非零维可直观解释为某个词或特征的重要度，具有高度可解释性</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>检索/存储</strong></td>
<td style="text-align: left;">用倒排索引即可实现 O(1)
级精确匹配；在线增量更新代价低</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>优势</strong></td>
<td style="text-align: left;">对长文档、术语精确匹配友好
易于调参（停用词、词根化） 资源消耗小、无推理延迟</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>劣势</strong></td>
<td style="text-align: left;">维度极高，逐向量暴力计算代价大
只捕获词面共现，无法理解语义或同义词 对拼写/语序变化鲁棒性差</td>
</tr>
</tbody>
</table>
<h3
id="tf-idfterm-frequency---inverse-document-frequency词频-逆文档频率">TF-IDF(Term
Frequency - Inverse Document Frequency，词频-逆文档频率)</h3>
<blockquote>
<p><em>一种经典的加权方案，用来衡量 词语 t 对 文档 d 在 语料库 D
中的重要程度。</em></p>
</blockquote>
<ul>
<li>一句话：词在整个语料库中出现得越少，但在本篇文档中出现得越多，那它就越重要。</li>
</ul>
<p>公式：<span class="math inline">\(TF-IDF(t,d,D) = TF(t,d) ×
IDF(t,D)\)</span></p>
<p>TF（局部权重）：</p>
<ul>
<li>计数：<code>tf = #t 出现次数</code></li>
<li>频率：<code>tf = #t / |d|</code></li>
<li>对数平滑：<code>tf = 1 + log(#t)</code></li>
</ul>
<p>ID（全局权重）：<span class="math display">\[IDF(t) =
log\frac{N-df(t)+0.5}{df(t)+0.5} \]</span></p>
<ul>
<li><em>N</em> = 语料中文档总数</li>
<li><em>df(t)</em> = 含词 <em>t</em> 的文档数</li>
<li>加 1 或 0.5 可以避免分母为 0，并抑制长尾噪声。</li>
</ul>
<h3 id="bm25best-matching-25">BM25(Best Matching 25)</h3>
<p>可视为 TF-IDF 的扩展版，进一步引入：</p>
<ul>
<li><em>k₁</em> 控制 TF 饱和：TF 越大，增益递减。</li>
<li><em>b</em> 长度归一化：文档越长，单词 TF 权重被抑制。</li>
</ul>
<p>公式：<span class="math display">\[w(t,d)=IDF(t)⋅\frac{TF(k_{1}
+1)}{TF+k_{1}·（1-b+b·\frac{文档长度}{平均文档长度} ）} \]</span></p>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 31%" />
<col style="width: 5%" />
<col style="width: 24%" />
<col style="width: 29%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>角色</strong></th>
<th style="text-align: left;"><strong>控制对象</strong></th>
<th style="text-align: left;"><strong>常见区间</strong></th>
<th style="text-align: left;"><strong>极值行为</strong></th>
<th style="text-align: left;"><strong>直觉比喻</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">k₁ (saturation factor)</td>
<td style="text-align: left;">TF
饱和曲线斜率——同一个词在同一文档中重复出现到第 n
次时，还能再加多少分</td>
<td style="text-align: left;">1.0 – 2.0</td>
<td style="text-align: left;">k₁ → 0：完全不计重复词；k₁ →
∞：线性计数，退化为 TF-IDF</td>
<td style="text-align: left;">沾一滴酱油 vs.
倒一瓶酱油：味道总有极限，不会永远 1 → 2 → 3 倍变浓</td>
</tr>
<tr class="even">
<td style="text-align: left;">b (length normalizer)</td>
<td
style="text-align: left;">文档长度惩罚强度——长文能否用“大块头”刷分</td>
<td style="text-align: left;">0.3 – 0.9</td>
<td style="text-align: left;">b = 0：不考虑长度（BM15）b =
1：长度全量归一化（BM11）</td>
<td style="text-align: left;">打篮球按身高加分：b=0 不管身高；b=1
按身高严格扣分；中间值折中</td>
</tr>
</tbody>
</table>
<h2 id="密集嵌入dense-embedding">3.3 密集嵌入（Dense Embedding）</h2>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 87%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>特征</strong></th>
<th style="text-align: left;"><strong>说明</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>维度</strong></td>
<td style="text-align: left;">兼顾效率与表达力，常见 128 –
1536；每一维几乎都非零。</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>构造方式</strong></td>
<td style="text-align: left;">由深度模型（BERT、Sentence-BERT、OpenAI
text-embedding-3-small 等）端对端学习，捕获上下文语义</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>权重含义</strong></td>
<td
style="text-align: left;">单维难以直观解释，但整体向量在低维空间中编码了丰富的语义相似度</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>检索/存储</strong></td>
<td style="text-align: left;">需专门的 ANN（HNSW、Faiss IVF-PQ
等）索引；向量更新需重新编码</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>优势</strong></td>
<td
style="text-align: left;">具备语义泛化能力，能跨同义词、拼写、语序可跨语言、跨模态（图文）在
RAG/问答场景提升召回率</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>劣势</strong></td>
<td style="text-align: left;">训练与推理成本高（GPU/CPU
向量化计算）结果可解释性弱 在线增量写入需再编码、重建索引</td>
</tr>
</tbody>
</table>
<h2 id="colbert">3.4 ColBERT</h2>
<p>ColBERT 是一种让 BERT 用“词级小向量”做快速、精准文本检索的方法 ——
既不像传统 TF-IDF 那样粗糙，也不像跨编码器那样慢。</p>
<p>ColBERT = “把 BERT 的句向量拆成 token 向量，再用 Late Interaction
重新拼起来做检索”的工程化改造版 BERT。</p>
<p><strong>Late Interaction</strong>
就是把查询（Q）和文档（D）<strong>先独立编码</strong>，等到最后打分时再让它们在
<strong>token 级别</strong> 做一次“小范围、轻量级”的互动——既不像
Cross-Encoder 那样“一上来就深度交互”，也不像 Bi-Encoder
那样“全程零交互”。</p>
<blockquote>
<p>换言之，BERT 提供<strong>语言理解底座</strong>，ColBERT
在此之上加了<strong>面向检索的输出格式与打分逻辑</strong>，二者既同宗又分工明确。</p>
</blockquote>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">           预训练阶段（同一个 BERT 权重）</span></span><br><span class="line"><span class="code"></span></span><br><span class="line">┌───────────────┐</span><br><span class="line">│ Google BERT │ ← 海量文本上做 MLM/NSP</span><br><span class="line">└───────────────┘</span><br><span class="line">│ （加载相同参数）</span><br><span class="line">╭───────────┴───────────╮</span><br><span class="line">│ │</span><br><span class="line">│ ↓ 普通微调 │ ↓ ColBERT 微调</span><br><span class="line">│ （分类、NER…） │ （稠密检索）</span><br><span class="line">│ │</span><br><span class="line">│ 取 [CLS] 整句向量 │ 保留 每个 token 向量</span><br><span class="line">│ + 任务特定头 │ + Late-Interaction 打分</span><br><span class="line">│ │</span><br><span class="line">╰───────────┬───────────╯</span><br><span class="line">│</span><br><span class="line">下游推理/检索</span><br></pre></td></tr></table></figure>
<h2 id="bge-m3">3.5 BGE-M3</h2>
<p>BGE-M3
是由智源研究院（BAAI）开发的新一代旗舰文本嵌入模型，它开创性地在单一模型内集成了<strong>多语言</strong>（支持超过
100 种语言）、<strong>长文本</strong> （支持 8192
词符）和<strong>多功能检索</strong>（同时支持稠密、稀疏和多向量检索）的强大能力。</p>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 3%" />
<col style="width: 52%" />
<col style="width: 34%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>M</strong></th>
<th style="text-align: left;"><strong>含义</strong></th>
<th style="text-align: left;"><strong>具体能力</strong></th>
<th style="text-align: left;"><strong>参考</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Multi-Functionality</td>
<td style="text-align: left;">多功能</td>
<td style="text-align: left;">同时产出
稠密向量（dense）、多向量/ColBERT（colbert） 和
稀疏向量（sparse），一套模型即可覆盖混合检索需求。</td>
<td style="text-align: left;"><a
target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-m3">huggingface.bge-m3</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">Multi-Linguality</td>
<td style="text-align: left;">多语种</td>
<td style="text-align: left;">覆盖 100+
语言，是目前公开数据集中多语检索任务的 SOTA。</td>
<td style="text-align: left;"><a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.03216?utm_source=chatgpt.com">arXiv.bge-m3</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Multi-Granularity</td>
<td style="text-align: left;">多粒度</td>
<td style="text-align: left;">最长输入 8 192
token，既能编码短句也能处理长文档。</td>
<td style="text-align: left;"><a
target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-m3">huggingface.bge-m3</a></td>
</tr>
</tbody>
</table>
<h1 id="查询增强技术">4. 查询增强技术</h1>
<h2 id="查询构建">4.1 查询构建</h2>
<h3 id="text-to-sql">4.1.1 Text-to-SQL</h3>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/1*wDKR6-ToiX5UgsUYN41JiQ.png"
alt="Text-to-SQL" />
<figcaption aria-hidden="true">Text-to-SQL</figcaption>
</figure>
<ol type="1">
<li>构建 DDL 知识库：schema 提取与切片；</li>
<li>构建 Q-SQL 知识库：示例对注入；</li>
<li>构建 DB 描述知识库：业务描述补充；</li>
<li>提供 RAG 检索上下文；</li>
<li>调用 LLM 进行 SQL 生成；</li>
<li>执行 SQL 并反馈结果；</li>
<li>迭代直到正确解决问题。</li>
</ol>
<p>常用框架：</p>
<ul>
<li>vanna</li>
<li>Chat2DB</li>
<li>DB-GPT</li>
</ul>
<h3 id="text-to-cypher">4.1.2 Text-to-Cypher</h3>
<p>跟 Text-to-SQL 一样，只不过是生成图数据库（neo4j）查询语句。</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/1*lhHfQGKOkZGNm40QIqTNIQ.png"
alt="Text-to-Cypher" />
<figcaption aria-hidden="true">Text-to-Cypher</figcaption>
</figure>
<ol type="1">
<li>构建图元模型（Graph Metamodel）知识库；</li>
<li>构建 Q-Cypher 知识库（示例对注入）；</li>
<li>构建图描述（Graph Description）知识库；</li>
<li>提供 RAG 检索上下文；</li>
<li>调用 LLM 进行 SQL 生成；</li>
<li>执行 SQL 并反馈结果；</li>
<li>迭代直到正确解决问题。</li>
</ol>
<h3 id="从查询中提取元数据构建过滤器">4.1.3
从查询中提取元数据构建过滤器</h3>
<figure>
<img src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/output.png"
alt="从查询中提取元数据构建过滤器" />
<figcaption aria-hidden="true">从查询中提取元数据构建过滤器</figcaption>
</figure>
<ol type="1">
<li>将自然语言转为向量查询语句；</li>
<li>利用 LLM 推断出元数据过滤条件；</li>
<li>在查询检索时，根据过滤条件进行文档过滤；</li>
<li>返回过滤后的相似文档；</li>
</ol>
<p>实战案例：</p>
<ul>
<li>https://ragflow.io/blog/implementing-text2sql-with-ragflow</li>
<li>https://medium.com/neo4j/generating-cypher-queries-with-chatgpt-4-on-any-graph-schema-a57d7082a7e7</li>
</ul>
<h2 id="查询翻译">4.2 查询翻译</h2>
<p>通过对用户查询进行改造和扩展，使其更加清晰、具体，从而提高检索精度。</p>
<p>常用工具：</p>
<table>
<colgroup>
<col style="width: 6%" />
<col style="width: 78%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>方案</strong></th>
<th style="text-align: left;"><strong>链接</strong></th>
<th style="text-align: left;"><strong>说明</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">ragbear</td>
<td style="text-align: left;"><a
target="_blank" rel="noopener" href="https://github.com/lexiforest/ragbear">GitHub -
lexiforest/ragbear</a></td>
<td style="text-align: left;">rewrite= 参数多种改写模式</td>
</tr>
<tr class="even">
<td style="text-align: left;">LangChain</td>
<td style="text-align: left;"><a
target="_blank" rel="noopener" href="https://blog.langchain.dev/query-transformations/">Query
Transformations</a></td>
<td style="text-align: left;">内置链式改写</td>
</tr>
<tr class="odd">
<td style="text-align: left;">LlamaIndex</td>
<td style="text-align: left;"><a
target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/examples/query_transformations/query_transform_cookbook/">Query
Transform Cookbook ¶</a></td>
<td style="text-align: left;">多策略组合</td>
</tr>
<tr class="even">
<td style="text-align: left;">Haystack</td>
<td style="text-align: left;"><a
target="_blank" rel="noopener" href="https://haystack.deepset.ai/cookbook/metadata_enrichment">Advanced
RAG: Automated Structured Metadata Enrichment | Haystack</a></td>
<td style="text-align: left;">pipeline node</td>
</tr>
</tbody>
</table>
<h3 id="query2doc">4.2.1 Query2Doc</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.07678">Query2Doc</a> 是指将 query
直接交给 LLM 去生成一份相关文档，然后将 query
和生成的文档一起去进行检索。虽然 LLM
生成的文档可能不对，但是提供了更丰富的信息、丰富了问题的语义，有助于提高检索时的精度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">query2doc</span>(<span class="params">query</span>):</span><br><span class="line">    prompt = <span class="string">f&quot;你是一名公司员工制度的问答助手，熟悉公司规章制度，请简短回答以下问题：<span class="subst">&#123;query&#125;</span>&quot;</span></span><br><span class="line">    doc_info = llm(prompt)</span><br><span class="line">    context_query = <span class="string">f&quot;<span class="subst">&#123;query&#125;</span>, <span class="subst">&#123;doc_info&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">return</span> context_query</span><br></pre></td></tr></table></figure>
<h3 id="hyde">4.2.2 HyDE</h3>
<p><a
target="_blank" rel="noopener" href="https://docs.haystack.deepset.ai/docs/hypothetical-document-embeddings-hyde">HyDE</a>（Hypothetical
Document Embeddings，假设文档向量）让 LLM 根据 query
去生成一系列假设性文档，然后将这些文档跟 query
一起做向量化，取向量均值去进行检索。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hyde</span>(<span class="params">query, include_query=<span class="literal">True</span></span>):</span><br><span class="line">    prompt_template = <span class="string">&quot;&quot;&quot;你是一名公司员工制度的问答助手，熟悉公司规章制度，请简短回答以下问题：</span></span><br><span class="line"><span class="string">    Question: &#123;question&#125;</span></span><br><span class="line"><span class="string">    Answer:&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    prompt = PromptTemplate(input_variables=[<span class="string">&quot;question&quot;</span>], template=prompt_template)</span><br><span class="line">    embeddings = HypotheticalDocumentEmbedder(llm_chain= prompt | llm,</span><br><span class="line">                                 base_embeddings=embedding_model.get_embedding_fun())</span><br><span class="line">    hyde_embedding = embeddings.embed_query(query)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> include_query:</span><br><span class="line">        query_embeddings = embedding_model.get_embedding_fun().embed_query(query)</span><br><span class="line">        result = (np.array(query_embeddings) + np.array(hyde_embedding)) / <span class="number">2</span></span><br><span class="line">        result = <span class="built_in">list</span>(result)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = hyde_embedding</span><br><span class="line">    result = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">float</span>, result))</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h3 id="子问题查询">4.2.3 子问题查询</h3>
<p>当问题比较复杂时，可以利用 LLM
将问题拆解成子问题，每个子问题都生成检索上下文，可以根据合并后总的上下文回答，也可以每个上下文独立回答后汇总。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sun_question</span>(<span class="params">query</span>):</span><br><span class="line">    prompt_template = <span class="string">&quot;&quot;&quot;你是一名公司员工制度的问答助手，熟悉公司规章制度。</span></span><br><span class="line"><span class="string">    你的任务是对复杂问题继续拆解，以便理解员工的意图。</span></span><br><span class="line"><span class="string">    请根据以下问题创建一个子问题列表：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    复杂问题：&#123;question&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    请执行以下步骤：</span></span><br><span class="line"><span class="string">    1. 识别主要问题：找出问题中的核心概念或主题。</span></span><br><span class="line"><span class="string">    2. 分解成子问题：将主要问题分解成可以独立理解和解决的多个子问题。</span></span><br><span class="line"><span class="string">    3. 只返回子问题列表，不包含其他解释信息，格式为：</span></span><br><span class="line"><span class="string">        1. 子问题1</span></span><br><span class="line"><span class="string">        2. 子问题2</span></span><br><span class="line"><span class="string">        3. 子问题3</span></span><br><span class="line"><span class="string">        ...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    prompt = PromptTemplate(input_variables=[<span class="string">&quot;question&quot;</span>], template=prompt_template)</span><br><span class="line"></span><br><span class="line">    llm_chain = prompt | llm</span><br><span class="line">    sub_queries = llm_chain.invoke(query).split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> sub_queries</span><br></pre></td></tr></table></figure>
<h3 id="查询改写">4.2.4 查询改写</h3>
<p>当问题表达不清、措辞差、缺少关键信息时，使用 LLM
根据用户问题<strong>多角度</strong>重写问题，增加额外的信息，提高检索质量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">question_rewrite</span>(<span class="params">query</span>):</span><br><span class="line">    prompt_template = <span class="string">&quot;&quot;&quot;你是一名公司员工制度的问答助手，熟悉公司规章制度。</span></span><br><span class="line"><span class="string">    你的任务是需要为给定的问题，从不同层次生成这个问题的转述版本，使其更易于检索，转述的版本增加一些公司规章制度的关键词。</span></span><br><span class="line"><span class="string">    问题：&#123;question&#125;</span></span><br><span class="line"><span class="string">    请直接给出转述后的问题列表，不包含其他解释信息，格式为：</span></span><br><span class="line"><span class="string">        1. 转述问题1</span></span><br><span class="line"><span class="string">        2. 转述问题2</span></span><br><span class="line"><span class="string">        3. 转述问题3</span></span><br><span class="line"><span class="string">        ...&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    prompt = PromptTemplate(input_variables=[<span class="string">&quot;question&quot;</span>], template=prompt_template)</span><br><span class="line"></span><br><span class="line">    llmchain = prompt | llm</span><br><span class="line">    rewrote_question = llmchain.invoke(query)</span><br><span class="line">    <span class="keyword">return</span> rewrote_question</span><br></pre></td></tr></table></figure>
<h3 id="查询抽象">4.2.5 查询抽象</h3>
<p>查询抽象（Take a Step
Back）是指当问题包含太多的细节，可能导致检索时忽略了关键的信息，降低检索质量。可以将用户的具体问题转化为一个更高层次的抽象问题，一个更广泛的问题，关注于高级概念或原则，从而提高检索质量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts.chat <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts.few_shot <span class="keyword">import</span> FewShotChatMessagePromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将复杂问题抽象化，使其更聚焦在本质问题上</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">take_step_back</span>(<span class="params">query</span>):</span><br><span class="line">    examples = [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;input&quot;</span>: <span class="string">&quot;我祖父去世了，我要回去几天&quot;</span>,</span><br><span class="line">            <span class="string">&quot;output&quot;</span>: <span class="string">&quot;公司丧葬假有什么规定？&quot;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;input&quot;</span>: <span class="string">&quot;我去北京出差，北京的消费高，有什么额外的补助？&quot;</span>,</span><br><span class="line">            <span class="string">&quot;output&quot;</span>: <span class="string">&quot;员工出差的交通费、住宿费、伙食补助费的规定是什么？&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    example_prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">        [</span><br><span class="line">            (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;input&#125;&quot;</span>),</span><br><span class="line">            (<span class="string">&quot;ai&quot;</span>, <span class="string">&quot;&#123;output&#125;&quot;</span>),</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    few_shot_prompt = FewShotChatMessagePromptTemplate(</span><br><span class="line">        example_prompt=example_prompt,</span><br><span class="line">        examples=examples,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">        [</span><br><span class="line">            (</span><br><span class="line">                <span class="string">&quot;system&quot;</span>,</span><br><span class="line">                <span class="string">&quot;&quot;&quot;你是一名公司员工制度的问答助手，熟悉公司规章制度。</span></span><br><span class="line"><span class="string">                你的任务是将输入的问题通过归纳、提炼，转换为关于公司规章制度制定相关的一般性问题，使得问题更容易捕捉问题的意图。</span></span><br><span class="line"><span class="string">                请参考下面的例子，按照同样的风格直接返回一个转述后的问题：&quot;&quot;&quot;</span></span><br><span class="line">            ),</span><br><span class="line">            <span class="comment"># few shot exmaples,</span></span><br><span class="line">            few_shot_prompt,</span><br><span class="line">            <span class="comment"># new question</span></span><br><span class="line">            (<span class="string">&quot;user&quot;</span>, <span class="string">&quot;&#123;question&#125;&quot;</span>)</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    question_gen = prompt | llm | StrOutputParser()</span><br><span class="line">    res = question_gen.invoke(&#123;<span class="string">&quot;question&quot;</span>: query&#125;).removeprefix(<span class="string">&quot;AI: &quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="查询路由">4.3 查询路由</h2>
<p>查询路由（Query
Routing）是指根据用户问题的具体意图，自动判断应该将该问题导向最合适的数据源（例如向量知识库、SQL
数据库、图数据库或特定 API）以获取最精准信息的决策过程。</p>
<table>
<colgroup>
<col style="width: 7%" />
<col style="width: 92%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">思路</th>
<th style="text-align: left;">图示</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">逻辑路由</td>
<td style="text-align: left;"><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/1831ccf2-5d1b-4a7c-b094-6251d4aa61f5.png"
alt="逻辑路由" /></td>
</tr>
<tr class="even">
<td style="text-align: left;">语义路由</td>
<td style="text-align: left;"><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/3b10f6dc-c598-498a-90c2-c9085c7d1b27.png"
alt="语义路由" /></td>
</tr>
</tbody>
</table>
<h1 id="索引优化技术">5. 索引优化技术</h1>
<p>基本思路：</p>
<ul>
<li>父子文档索引</li>
<li>分层索引</li>
<li>多表示索引</li>
</ul>
<h2 id="从小块到大上下文">5.1 从小块到大上下文</h2>
<blockquote>
<p>向量检索的时候，检索到的是一个小文档，但是通过小文档的
metadata，返回给 LLM 的是查询出来的大文档。</p>
</blockquote>
<p>节点-句子滑动窗口检索 SentenceWindowNodeParser</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/output%20(1).png"
alt="SentenceWindowNodeParser" />
<figcaption aria-hidden="true">SentenceWindowNodeParser</figcaption>
</figure>
<p>父子文本块检索：ParentDocumentRetriever</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image%20(1).png"
alt="ParentDocumentRetriever" />
<figcaption aria-hidden="true">ParentDocumentRetriever</figcaption>
</figure>
<p>前后串连、自动扩展上下文：PrevNextNodePostprocessor、AutoPrevNextPostprocessor</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image%20(2).png"
alt="PrevNextNodePostprocessor" />
<figcaption aria-hidden="true">PrevNextNodePostprocessor</figcaption>
</figure>
<h2 id="构建有层次的索引">5.2 构建有层次的索引</h2>
<ul>
<li>构建两个向量数据库（Summary 和 Details），通过 Metadata
进行连接；</li>
<li>通过 <code>llamaindex</code> 的 <code>indexnode</code> 和
<code>PandasQueryEngine</code>；</li>
<li>通过查询先检索相关表名，然后做 <code>Text2SQL</code>。</li>
</ul>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image%20(3).png"
alt="Summary &amp; Detail" />
<figcaption aria-hidden="true">Summary &amp; Detail</figcaption>
</figure>
<h2 id="构建多表示索引">5.3 构建多表示索引</h2>
<p>构建混合索引：EnsembleRetriever</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image%20(4).png"
alt="EnsembleRetriever" />
<figcaption aria-hidden="true">EnsembleRetriever</figcaption>
</figure>
<p>构建多表示索引：MultiVectorRetriever</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image%20(5).png"
alt="MultiVectorRetriever" />
<figcaption aria-hidden="true">MultiVectorRetriever</figcaption>
</figure>
<h1 id="检索后优化技术">6. 检索后优化技术</h1>
<h2 id="重排-rerank">6.1 重排 rerank</h2>
<p>传统的搜索或推荐系统通常分为两步：</p>
<ol type="1">
<li><strong>召回（Recall）</strong>:
从海量的候选集中，快速、粗略地筛选出几百或几千个可能相关的项目。此阶段追求<strong>速度</strong>和<strong>查全率</strong>，常用技术包括基于关键词的搜索（如
BM25）或向量相似度搜索（ANN）。</li>
<li><strong>排序/重排（Ranking/Reranking）</strong>:
对召回的结果进行更精细、更复杂的计算，以确定最终呈现给用户的顺序。此阶段追求<strong>精准度</strong>和<strong>查准率</strong>。我们今天讨论的技术就属于这个范畴。</li>
</ol>
<p>可以把这个过程比作“海选”和“决赛”。召回是海选，快速淘汰掉明显不相关的选手；重排是决赛，评委（重排模型）对入围选手进行全方位的严格评审，最终给出排名。</p>
<h3 id="rrf-重排">6.1.1 RRF 重排</h3>
<blockquote>
<p>民主投票式的融合</p>
</blockquote>
<p>RRF
是一种简单、高效、无需训练的“结果融合”策略。想象一下，你有多个独立的搜索系统（比如一个关键词搜索，一个向量搜索），它们各自对同一批文档给出了自己的排名。RRF
的作用就是将这些不同的排名列表“民主地”融合成一个最终的、可能更优的排名列表。</p>
<p>RRF
的核心思想是：<strong>一个文档在多个列表中的排名越靠前，它在最终列表中的排名就应该越靠前。</strong></p>
<p>与简单地将不同系统的得分相加不同，RRF 采用“倒数排名”（Reciprocal
Rank）来计算每个文档的最终分数。</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image%20(6).png"
alt="RRF Rerank" />
<figcaption aria-hidden="true">RRF Rerank</figcaption>
</figure>
<p>优势：</p>
<ul>
<li><strong>无需训练</strong>: 即插即用，非常方便。</li>
<li><strong>性能稳健</strong>:
在不同召回源质量参差不齐时，表现通常比简单的分数相加（如
<code>sum fusion</code>）更稳定。</li>
<li><strong>计算开销极低</strong>: 几乎不增加额外的计算负担。</li>
</ul>
<p>缺点：</p>
<ul>
<li><strong>效果上限不高</strong>:
它只利用了“排名”这一个信息，忽略了不同系统给出的原始“分数”中蕴含的置信度信息。</li>
<li><strong>依赖召回质量</strong>: 如果所有召回源的质量都很差，RRF
也无力回天。</li>
</ul>
<p>适应场景：</p>
<ul>
<li>混合搜索（Hybrid
Search）：融合关键词搜索（BM25）和向量搜索的结果。</li>
<li>多模态搜索：融合文本、图片等不同模态的搜索结果。</li>
</ul>
<h3 id="cross-encoder-重排">6.1.2 Cross-Encoder 重排</h3>
<blockquote>
<p>query 和文档的"深度阅读理解"</p>
</blockquote>
<p>如果说召回阶段的向量搜索是"看标题识文章"，那么 Cross-Encoder
重排就是"把 query
和每篇文档放在一起，逐字逐句地做一篇完整的阅读理解"。</p>
<p>它通过深度学习模型（通常是 Transformer 架构，如 BERT）来判断一个
query 和一个文档之间的相关性到底有多强。</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/1*hkV7rQlN6OuEin7qPxyafA.jpeg"
alt="Cross-Encoder Rerank" />
<figcaption aria-hidden="true">Cross-Encoder Rerank</figcaption>
</figure>
<p>基本流程：</p>
<ol type="1">
<li><strong>输入构建</strong>:
将查询（Query）和待排序的文档（Document）用一个特殊的分隔符（如
<code>[SEP]</code>）拼接在一起，形成一个单一的输入序列。例如：<code>[CLS] 我的问题是什么？[SEP] 这是候选文档的内容... [SEP]</code></li>
<li><strong>模型计算</strong>: 将这个拼接后的序列输入到一个预训练好的
Transformer 模型（如
BERT）中。模型内部的自注意力机制（Self-Attention）会让 Query
中的每个词和 Document 中的每个词都进行充分的交互和信息比对。</li>
<li><strong>分数输出</strong>:
模型在处理完整个序列后，通常会利用起始位置 <code>[CLS]</code> Token
对应的输出向量，接一个简单的线性层，最终输出一个单一的分数（logit），这个分数就代表了
Query 和 Document 之间的相关性得分。</li>
<li><strong>排序</strong>:
根据所有文档得到的相关性得分，从高到低进行排序，得到最终结果。</li>
</ol>
<p>优点：</p>
<ul>
<li><strong>效果极佳</strong>: 由于对 query
和文档进行了深度的、非对称的交互分析，其精度通常是所有重排方法中最高的。</li>
</ul>
<p>缺点：</p>
<ul>
<li><strong>计算成本极高</strong>: 对于每个
<code>(query, document)</code>
对，都需要进行一次完整的、重量级的模型前向传播。如果召回了 500
个文档，就需要进行 500 次 BERT
模型的计算，这在实时性要求高的场景下是巨大的挑战。</li>
<li><strong>无法提前索引</strong>: 文档的表示不是独立的，必须在查询时与
query 结合才能计算，因此无法像向量搜索那样提前为所有文档建立索引。</li>
</ul>
<p>适用场景：</p>
<ul>
<li>对精度要求极高，且可以容忍较高延迟的场景，如某些法律或医疗文献的精确查找。</li>
<li>作为“黄金标准”来生成高质量的标注数据，用于训练更轻量的召回模型（如
Bi-Encoder）。</li>
</ul>
<h3 id="colbert-重排">6.1.3 ColBERT 重排</h3>
<blockquote>
<p>Contextualized Late Interaction over BERT。</p>
<p>介于“看标题”和“深度阅读”之间的“划重点式”阅读</p>
</blockquote>
<p>ColBERT 试图在 Cross-Encoder 的高精度和 Bi-Encoder (召回阶段常用)
的高效率之间找到一个平衡点。它的核心思想是：<strong>不需要逐字逐句地进行完整对比，而是先把
query
和文档各自的"重点"（关键词向量）划出来，然后再计算这些重点之间的匹配程度。</strong></p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/colbert.png"
alt="ColBERT Rerank" />
<figcaption aria-hidden="true">ColBERT Rerank</figcaption>
</figure>
<p>基本流程：</p>
<ol type="1">
<li><strong>独立编码</strong>: 首先，使用一个 BERT
类的模型（但稍作修改）分别独立地处理 Query 和
Document。它不再是输出一个单一的 <code>[CLS]</code> 向量，而是为 Query
和 Document 中的<strong>每一个 Token</strong>
都生成一个上下文相关的向量。</li>
<li><strong>Query 端向量</strong>: 对于 Query，我们保留所有 Token
的输出向量。</li>
<li><strong>Document 端向量</strong>: 对于 Document，我们也保留所有
Token 的输出向量。这些向量可以<strong>提前计算并存储</strong>，这是它比
Cross-Encoder 高效的关键。</li>
<li><strong>延迟交互计算</strong>:
在查询时，进行“延迟交互”。具体来说，对于 Query
中的<strong>每一个</strong> Token 向量，我们都去 Document 的所有 Token
向量中寻找一个最相似的（通过最大内积<code>MaxSim</code>操作）。</li>
<li><strong>分数聚合</strong>: 最后，将 Query 中每个 Token
找到的最大相似度分数<strong>相加</strong>，得到最终的相关性总分。</li>
</ol>
<blockquote>
<p>这个过程好比：</p>
<ul>
<li><strong>Query</strong>: "best deep learning framework"</li>
<li><strong>Document</strong>: "PyTorch is a popular framework for deep
learning..."</li>
</ul>
<p>ColBERT 会分别计算：</p>
<ul>
<li>"best" 和文档中所有词向量的最大相似度。</li>
<li>"deep" 和文档中所有词向量的最大相似度。</li>
<li>"learning" 和文档中所有词向量的最大相似度。</li>
<li>"framework" 和文档中所有词向量的最大相似度。</li>
<li>然后把这四个最大相似度值加起来作为总分。</li>
</ul>
</blockquote>
<p>优点：</p>
<ul>
<li><strong>性能优越</strong>: 精度远超传统的
Bi-Encoder，并且在很多任务上能逼近 Cross-Encoder。</li>
<li><strong>效率较高</strong>:
由于文档向量可以预计算和索引，查询时的计算开销远低于
Cross-Encoder，只涉及向量的相似度计算。</li>
</ul>
<p>缺点：</p>
<ul>
<li><strong>存储开销大</strong>: 需要为文档中的每个 Token
都存储一个高维向量，存储成本远高于只存一个文档向量的 Bi-Encoder。</li>
<li><strong>实现相对复杂</strong>:
其索引和查询逻辑比标准向量搜索更复杂。</li>
</ul>
<p>适用场景：</p>
<ul>
<li>需要高精度但又对延迟有一定要求的现代搜索引擎，如微软的 Bing
就在使用类似的技术。</li>
<li>作为 RAG 系统中的高质量重排器。</li>
</ul>
<h3 id="cohere-和-jina-重排">6.1.4 Cohere 和 Jina 重排</h3>
<blockquote>
<p>商业化的"重排即服务"（Reranking-as-a-Service）</p>
</blockquote>
<p>Cohere 和 Jina AI 都是提供 AI
模型和服务的公司。它们都将高质量的重排模型封装成了简单易用的 API
服务。本质上，它们提供的重排器很可能就是基于类似 Cross-Encoder
架构的、在海量高质量数据上训练和优化的专有模型。</p>
<p>优点：</p>
<ul>
<li><strong>使用简单</strong>: 只需几行代码调用 API
即可，无需关心模型训练、部署和维护。</li>
<li><strong>效果保证</strong>:
通常能获得非常好的开箱即用效果，因为这些模型经过了大量数据的锤炼。</li>
</ul>
<p>缺点：</p>
<ul>
<li><strong>成本</strong>: 按调用量或 token
数量计费，对于大流量应用可能是一笔不小的开销。</li>
<li><strong>数据隐私</strong>: 需要将你的 query
和文档数据发送给第三方服务商，对于数据敏感的应用需要仔细评估其隐私政策。</li>
<li><strong>灵活性受限</strong>:
无法像自建模型那样进行深度定制或调优。</li>
</ul>
<p>适用场景：</p>
<ul>
<li>快速原型验证（MVP）。</li>
<li>中小型企业或开发者，希望以最小的工程代价获得最好的排序质量。</li>
<li>大型企业中非核心但又需要高质量排序的业务场景。</li>
</ul>
<h3 id="rankgpt-和-rankllm">6.1.5 RankGPT 和 RankLLM</h3>
<p>这是最新的重排范式，直接利用 LLM
强大的语言理解和推理能力来进行排序。它的思路是：<strong>不再让模型输出一个简单的相关性分数，而是让
LLM
直接对召回的文档列表进行"思考"和"比较"，然后输出一个排序好的列表。</strong></p>
<p>基本思路：</p>
<ol type="1">
<li><strong>构建 Prompt</strong>: 将 query
和召回的文档列表（通常是文档的标题和摘要）格式化成一个复杂的
Prompt。这个 Prompt 会明确指示 LLM
作为一个排序专家，对给定的文档列表根据与 query
的相关性进行排序，并按指定的格式输出结果。</li>
<li><strong>LLM 推理</strong>: 将这个 Prompt 发送给 LLM。LLM
会利用其强大的上下文理解能力，分析 query
的深层意图，并比较不同文档之间的细微差别（例如，一个内容更全面，另一个更新颖）。</li>
<li><strong>解析输出</strong>: LLM
会返回一个文本结果，比如一个重新排序好的文档 ID
列表。程序需要解析这个文本输出来获取最终的排序。</li>
</ol>
<p>优点：</p>
<ul>
<li><strong>理解复杂意图</strong>: LLM 能够理解非常复杂和模糊的
query，并能进行一定程度的推理，这是传统模型难以做到的。</li>
<li><strong>零样本/少样本能力强</strong>:
无需针对特定任务进行微调，就能在很多场景下取得惊人的效果。</li>
<li><strong>可解释性</strong>: 有时可以引导 LLM
给出排序的理由，增加了透明度。</li>
</ul>
<p>缺点：</p>
<ul>
<li><strong>成本和延迟极高</strong>: 调用大型 LLM API
的成本和时间开销是目前所有方法中最高的，通常只能用于非实时或小批量任务。</li>
<li><strong>上下文长度限制</strong>: LLM
的上下文窗口大小有限，一次能处理的文档数量和文档长度都受限。</li>
<li><strong>稳定性问题</strong>:
输出格式可能不稳定，需要设计鲁棒的解析逻辑。结果也可能有一定的随机性。</li>
</ul>
<p>适用场景：</p>
<ul>
<li>对召回结果的“最后一公里”进行精加工，例如对前 10
名结果进行最终排序。</li>
<li>作为生成高质量排序标注数据的强大工具。</li>
<li>对成本不敏感、但对排序质量有极致要求的特定应用。</li>
</ul>
<h3 id="时效加权重排">6.1.6 时效加权重排</h3>
<p>这是一种业务逻辑驱动的重排策略，而非特定的模型或算法。其核心思想是：<strong>对于某些类型的查询，最新的信息比旧的信息更有价值。</strong></p>
<p>基本思路：</p>
<ol type="1">
<li><strong>时间衰减函数 (Time Decay Function)</strong>:
设计一个函数，使得文档的分数随着其发布时间的流逝而衰减。最常用的函数是指数衰减或高斯衰减。</li>
<li><strong>分桶加权</strong>: 将文档按发布时间分到不同的桶里，如"24
小时内"、"一周内"、"一月内"、"更早"。为每个桶设置一个固定的权重或加分项。例如，"24
小时内"的文档分数乘以 1.5，"一周内"的乘以 1.2 等。</li>
</ol>
<p>优点：</p>
<ul>
<li><strong>实现简单</strong>: 逻辑清晰，容易实现和调整。</li>
<li><strong>效果显著</strong>:
对于新闻、社交媒体、产品更新等时效性强的查询，能极大提升用户体验。</li>
</ul>
<p>缺点：</p>
<ul>
<li><strong>"一刀切"风险</strong>:
如果不加区分地对所有查询都增强时效性，可能会伤害那些寻求""永恒"知识的查询（如"什么是牛顿第一定律"）。</li>
<li><strong>参数难调</strong>: 衰减函数的形状、权重 w
等参数需要根据经验和 A/B 测试来仔细调整。</li>
</ul>
<p>适用场景：</p>
<ul>
<li><strong>新闻搜索</strong>: 用户总是想看最新的报道。</li>
<li><strong>电商新品</strong>:
用户搜索"手机"时，可能更想看到最新款。</li>
<li><strong>社交媒体 Feed</strong>: 最新的帖子通常排在最前面。</li>
<li><strong>需要与查询意图识别结合</strong>:
一个优秀的系统应该能识别出哪些 query
是具有时效性意图的，然后动态地应用时效性加权。</li>
</ul>
<h2 id="压缩-compression">6.2 压缩 compression</h2>
<p>传统的 RAG
流程是“检索-增强-生成”。系统首先根据用户问题从知识库中检索出若干相关文档片段（Chunks），然后将这些片段作为上下文（Context）连同用户问题一起提交给大语言模型（LLM），由
LLM 生成最终答案。</p>
<p>这里面潜藏着几个挑战：</p>
<ol type="1">
<li><strong>上下文窗口限制 (Context Window Limit)</strong>：每个 LLM
都有其上下文长度上限（如 GPT-4 是 128k
tokens）。如果检索出的文档过多，会超出窗口限制，导致无法处理。</li>
<li><strong>成本与延迟 (Cost &amp; Latency)</strong>：LLM 的 API
调用费用通常与输入的 Token
数量成正比。上下文越长，费用越高，同时模型的推理时间也越长，导致用户等待时间增加。</li>
<li><strong>“大海捞针”问题 (Lost in the Middle)</strong>：研究表明，当
LLM
的上下文中包含大量信息时，它对位于上下文中间部分信息的注意力会下降。如果关键信息被大量无关或次要信息包围，LLM
可能无法有效利用它，从而影响生成答案的准确性。</li>
<li><strong>噪声干扰 (Noise
Interference)</strong>：检索出的文档片段虽然“相关”，但并非每个字、每句话都对回答当前问题至关重要。这些无关信息就是“噪声”，会干扰
LLM 的判断。</li>
</ol>
<p>因此，<strong>RAG
压缩技术的核心目标</strong>，就是在将检索到的信息送入 LLM
之前，对其进行“精炼”——去除无关信息、保留核心内容，从而在<strong>降低成本、提升效率</strong>的同时，<strong>提高最终答案的质量</strong>。</p>
<h3 id="上下文压缩检索器">6.2.1 上下文压缩检索器</h3>
<p>上下文压缩检索器是指 LangChain 提供的
<code>Contextual Compression Retriever</code>。</p>
<p>它不是一个独立的检索器，而是一个"包装器"（Wrapper）。它首先使用一个常规的检索器（如
<code>VectorStoreRetriever</code>）获取一批文档，然后通过一个嵌入的"文档压缩器"（Document
Compressor）对这些文档进行筛选或重写，最后只返回那些真正重要的信息。</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/0*SL1A95gsICIB5IDA.png"
alt="Contextual Compression Retriever" />
<figcaption aria-hidden="true">Contextual Compression
Retriever</figcaption>
</figure>
<p>LangChain 提供了两种主流的压缩器：</p>
<ul>
<li><strong><code>LLMChainExtractor</code></strong>：这个压缩器内部会运行一个
LLM（通常是一个小模型）。它会遍历每个检索到的文档，并向 LLM
提出一个问题，例如："请从以下文档中抽取出与'[用户原始问题]'相关的句子。"
LLM
会根据指令抽取出关键句子，丢弃无关部分，从而实现压缩。这是一种<strong>基于
LLM 的抽取式压缩</strong></li>
<li><strong><code>EmbeddingsFilter</code></strong>：这个压缩器不依赖
LLM。它会计算用户问题和每个检索文档（或文档内更小的句子片段）的嵌入向量（Embedding）之间的相似度。只有当相似度超过预设的阈值（e.g.,
<code>similarity_threshold=0.8</code>）时，该文档或句子才会被保留。这是一种<strong>基于嵌入相似度的过滤式压缩</strong>。</li>
</ul>
<p>优势：</p>
<ul>
<li><strong>提升信噪比</strong>：直接过滤掉与问题无关的整个文档或文档中的无关部分。</li>
<li><strong>灵活性高</strong>：可以根据需求选择计算成本低但效果略粗糙的<code>EmbeddingsFilter</code>，或选择成本高但更智能的<code>LLMChainExtractor</code>。</li>
<li><strong>模块化</strong>：与 LangChain 生态无缝集成，易于实现。</li>
</ul>
<h3 id="句子嵌入优化器">6.2.2 句子嵌入优化器</h3>
<p>句子嵌入优化器是指 LlamaIndex 提供的
<code>Sentence Embedding Optimizer</code>。</p>
<p>与 LangChain 的 <code>EmbeddingsFilter</code> 思想非常相似，但它在
LlamaIndex 的生态系统内，并专注于句子级别的精细化过滤。</p>
<p>在检索到相关的文档块（Node）之后，不是将整个文档块都丢给
LLM，而是深入到文档块内部，逐一分析每个句子，只保留与用户问题最相关的句子。</p>
<p>基本原理：</p>
<ol type="1">
<li><strong>初始节点检索</strong>：查询引擎首先从索引中检索出 Top-K
个最相关的节点（Nodes，相当于 LangChain 的 Documents）。</li>
<li><strong>句子级分析</strong>：<code>SentenceEmbeddingOptimizer</code>（或类似功能的<code>SimilarityPostprocessor</code>）接收这些节点。它会：
<ul>
<li>将每个节点分解成单独的句子。</li>
<li>为每个句子计算一个嵌入向量。</li>
<li>计算每个句子的嵌入向量与用户原始问题嵌入向量之间的相似度得分。</li>
</ul></li>
<li><strong>阈值过滤</strong>：它会根据一个预设的相似度阈值（<code>similarity_cutoff</code>）来决定保留哪些句子。只有得分高于阈值的句子才会被保留下来，组合成新的、更精简的节点内容。</li>
<li><strong>合成响应</strong>：最后，只有这些经过精炼的、包含高相关度句子的节点才会被送入响应合成器（Response
Synthesizer），由 LLM 生成最终答案。</li>
</ol>
<p>优势：</p>
<ul>
<li><strong>粒度极细</strong>：相比于过滤整个文档，句子级过滤能最大程度地保留一个文档块中的相关信息，同时剔除无关句子，精度更高。</li>
<li><strong>减少上下文割裂</strong>：有时一个文档块整体相关度可能不高，但其中有一两句关键信息。这种方法可以精准地把这两句"捞"出来，避免整个文档块被丢弃。</li>
</ul>
<h3 id="llmlingua">6.2.3 LLMLingua</h3>
<p>在将包含检索文档的冗长提示词（Prompt）发送给昂贵的大模型（如
GPT-4）之前，先用一个更小、更便宜的语言模型（如 GPT-2 或一个微调过的
Llama）来对这个提示词进行"有损压缩"。这个压缩过程会识别并删除那些对 LLM
理解问题和生成答案不太重要的词语或句子。</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/LLMLinguafigure1.png"
alt="LLMLingua: Innovating LLM efficiency with prompt compression - Microsoft Research" />
<figcaption aria-hidden="true">LLMLingua: Innovating LLM efficiency with
prompt compression - Microsoft Research</figcaption>
</figure>
<p>基本原理：</p>
<ol type="1">
<li><strong>构建完整提示词</strong>：将用户问题和所有检索到的文档拼接成一个完整的、非常长的提示词。</li>
<li><strong>小模型介入</strong>：LLMLingua
使用一个小模型来分析这个长提示词。它会评估如果从提示词中删除某个词或某段话，对大模型理解原始提示词的“困惑度”会产生多大影响。</li>
<li><strong>智能删除</strong>：它会优先删除那些对困惑度影响最小的词语和句子，因为这些内容被认为是信息量较低或冗余的。这个过程被设计得非常精巧，旨在保留关键的实体、术语和逻辑关系。</li>
<li><strong>生成压缩提示词</strong>：经过这个过程，原始的长提示词被压缩成一个更短的版本，其中包含了原始上下文的"精华"。</li>
<li><strong>提交大模型</strong>：最后，这个压缩后的、短小精悍的提示词被发送给目标大模型进行处理。</li>
</ol>
<h3 id="recomp-压缩">6.2.4 RECOMP 压缩</h3>
<p>RECOMP (REtrieval-and-COMPression)
是一种面向<strong>复杂问题</strong>的、多步骤的 RAG
策略，它将压缩思想融入到了一个更宏大的框架中。</p>
<p>当面对一个需要综合多个信息源才能回答的复杂问题时，传统的 RAG
一次性检索出的文档可能包含大量不相关细节。RECOMP
通过"分而治之"和"先抽取再合成"的方式来创建高度浓缩和相关的上下文。</p>
<p>基本原理：</p>
<ol type="1">
<li><strong>问题分解（可选）</strong>：对于一个非常复杂的问题，可能首先会将其分解为几个更简单的子问题。</li>
<li><strong>检索与抽取 (Retrieve and
Extract)</strong>：针对（每个子）问题，执行以下操作：
<ol type="1">
<li><strong>检索</strong>：从知识库中检索相关文档。</li>
<li><strong>抽取</strong>：<strong>这是关键步骤</strong>。它不是直接使用这些文档，而是向
LLM 发出指令，要求 LLM
阅读每个文档，并从中<strong>抽取</strong>出与当前（子）问题直接相关的<strong>简明摘要或关键事实点</strong>。例如："请阅读以下关于
A 公司的财报，并抽取出其 2023 年第四季度的收入和利润数字。"</li>
</ol></li>
<li><strong>压缩与合成 (Compress and Synthesize)</strong>：
<ol type="1">
<li>将从所有文档中抽取出的摘要或事实点收集起来。</li>
<li>再次调用
LLM，将这些零散但高度相关的信息点<strong>合成</strong>成一段连贯、流畅、无冗余的文本。这段文本就是最终为原始复杂问题量身定制的“完美上下文”。</li>
</ol></li>
<li><strong>最终生成</strong>：将这个合成好的、高度浓缩的上下文连同原始问题一起提交给
LLM，生成最终答案。</li>
</ol>
<p>优势：</p>
<ul>
<li><strong>极高的信息密度</strong>：最终生成的上下文几乎不含任何与问题无关的噪声，每一句话都是为了回答问题而存在的。</li>
<li><strong>处理复杂问题的能力强</strong>：非常适合需要整合来自不同文档、不同主题信息的“多跳（multi-hop）”问题。</li>
<li><strong>可解释性</strong>：由于中间步骤生成了摘要和事实点，这个过程比黑盒方法更易于调试和理解。</li>
</ul>
<h3 id="prompt-caching-记忆上下文">6.2.5 Prompt Caching 记忆上下文</h3>
<p>它是一种<strong>性能优化</strong>技术，而非内容压缩技术，但常在处理长上下文时被提及。</p>
<p>在 Transformer 模型（所有现代 LLM
的基础）中，当模型处理一个序列时，它会为每个 Token
计算一个键（Key）和值（Value）向量，这个计算过程非常耗时。Prompt
Caching（或称 KV Cache）技术的核心就是：<strong>将已经处理过的 Prompt
部分的 KV 向量缓存起来，下次请求时如果 Prompt
前缀相同，则直接复用缓存，无需重新计算。</strong></p>
<p>基本原理：</p>
<ol type="1">
<li><strong>首次请求</strong>：用户发送一个长
Prompt（例如，一篇需要总结的文章）。模型在处理这个长 Prompt
时，会计算其中每个 Token 的 KV 向量，并将它们存储在 GPU 的内存中（即 KV
Cache）。</li>
<li><strong>后续交互</strong>：现在，用户基于这篇文章提问（例如，“文章的作者是谁？”）。这个新的请求实际上是
<code>[原始长Prompt] + [新问题]</code>。</li>
<li><strong>缓存命中</strong>：当模型收到这个新请求时，它会发现请求的前半部分（<code>[原始长Prompt]</code>）与上一次完全相同。它会立即从
KV Cache 中加载这部分的 KV
向量，而<strong>只需为新的部分（<code>[新问题]</code>）计算 KV
向量</strong>。</li>
<li><strong>加速生成</strong>：这样一来，模型省去了重复计算长 Prompt
部分的巨大开销，从而极大地加快了对新问题的响应速度。</li>
</ol>
<p>优势：</p>
<ul>
<li><strong>大幅提升多轮对话或连续查询的性能</strong>：对于聊天机器人、文档问答等需要保持长上下文的场景，效果极其显著。</li>
<li><strong>降低总计算成本</strong>：虽然不减少送入的 Token
数，但通过复用计算结果，降低了处理相同前缀的实际计算成本和时间。</li>
</ul>
<h2 id="校正-correction">6.3 校正 correction</h2>
<p>C-RAG
的核心思想是在检索模块和生成模块之间，引入一个<strong>轻量级的“检索评估器”
(Retrieval
Evaluator)</strong>，并根据评估结果采取不同的<strong>校正措施</strong>。这项技术主要在学术论文
<em><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.15884">arXiv:2401.15884</a></em>
中被系统性地提出和阐述。</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image%20(7).png"
alt="C-RAG" />
<figcaption aria-hidden="true">C-RAG</figcaption>
</figure>
<p>C-RAG 的精髓在于其动态的、差异化的处理策略。</p>
<ul>
<li><strong>当评估为“不正确”时</strong>：C-RAG
会果断地<strong>抛弃</strong>所有从内部知识库检索到的文档。因为它判断这些文档只会误导
LLM。取而代之，它会<strong>重写 (Rewrite)</strong>
用户的查询，使其更适合通用搜索引擎，然后<strong>触发网络搜索 (Web
Search)</strong>，从更广阔的、实时更新的互联网中获取信息。这极大地扩展了
RAG
系统的知识边界，尤其适用于回答关于近期事件或内部知识库未覆盖领域的问题。</li>
<li><strong>当评估为“正确”时</strong>：即便文档是相关的，也可能包含大量与问题无关的“噪音”段落。为了让
LLM 更专注于核心信息，C-RAG 采用了一种“分解-再重组”
(Decompose-then-Recompose)*的知识精炼算法。
<ul>
<li><strong>分解
(Decompose)</strong>：将相关的文档分解成更小的、独立的知识片段
(Knowledge Strips)。</li>
<li><strong>重组
(Recompose)</strong>：再次使用评估器对每个知识片段进行打分，过滤掉无关的片段，只保留最核心、最相关的知识点，然后将这些精华片段“重组”起来，作为最终的上下文。</li>
</ul></li>
<li><strong>当评估为“模糊”时</strong>：C-RAG
会采取一种混合策略。它会<strong>同时</strong>对内部检索到的模糊文档进行上述的“分解-再重组”精炼，并启动网络搜索获取外部信息。最后，将两方面的信息<strong>合并</strong>，为
LLM 提供一个更全面、更鲁棒的上下文。</li>
</ul>
<p>实战案例：<a
target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag/">LangGraph-CRAG</a></p>
<h1 id="响应生成">7. 响应生成</h1>
<ul>
<li><a
target="_blank" rel="noopener" href="https://python.langchain.com/docs/concepts/output_parsers/">Output
parsers | 🦜️🔗 LangChain</a></li>
<li><a
target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/output_parser/">LlamaIndex
丨 Output Parsing Modules</a></li>
<li><a
target="_blank" rel="noopener" href="https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses">OpenAI
丨 Structured Outputs</a></li>
</ul>
<h1 id="系统性优化">8. 系统性优化</h1>
<p>系统性优化指的是从系统层面上，通过优化整个 RAG
流程来达到一个更好的检索效果。</p>
<h2 id="自我修正与反思型-rag">8.1 自我修正与反思型 RAG</h2>
<ul>
<li>业界标杆：<a
target="_blank" rel="noopener" href="https://github.com/AkariAsai/self-rag">self-rag</a></li>
<li>笔者实践：<a
target="_blank" rel="noopener" href="https://github.com/hedon-ai-road/regulation_rag/blob/main/self_rag.py">self-rag.py</a></li>
</ul>
<p>此架构模拟了人类“先思考、再审视、后修正”的决策过程。系统首先生成一个初步答案，然后启动一个内部的"批评家"来评估这个答案的质量。如果发现问题（如信息不完整、逻辑不通顺），系统会生成修正指令，并基于新指令进行迭代优化，直到产出高质量的最终答案。</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/d9b68194-3676-4c00-88c2-12f4e1020e64.png"
alt="Self-RAG 思想简化" />
<figcaption aria-hidden="true">Self-RAG 思想简化</figcaption>
</figure>
<h2 id="迭代式检索-rag">8.2 迭代式检索 RAG</h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.06840">RA-ISF</a></li>
</ul>
<p>此架构专门应对信息不足的问题。当一次检索无法获取回答复杂问题所需的全部信息时，系统会进入一个迭代循环。它会分析已获取的内容，智能地生成新的、更深入的查询，然后再次进行检索。这个过程不断重复，直到收集到足够全面的上下文，最后再进行综合生成。</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image%20(8).png"
alt="Iterative Retrieval RAG 原理简化" />
<figcaption aria-hidden="true">Iterative Retrieval RAG
原理简化</figcaption>
</figure>
<h2 id="自适应智能体-rag">8.3 自适应/智能体 RAG</h2>
<ul>
<li><a
target="_blank" rel="noopener" href="https://github.com/asinghcsu/AgenticRAG-Survey">AgenticRAG-Survey</a></li>
</ul>
<p>此架构将 RAG 提升到了一个智能体
（Agent）的高度。系统核心是一个作为大脑的
LLM，它能自主分析用户问题，并决策采取何种行动：是进行知识库检索、上网搜索、调用计算器，还是直接回答。它能制定多步计划并调用不同工具，展现出更高的灵活性和解决复杂问题的能力。</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image%20(9).png"
alt="Agentic RAG 原理简化" />
<figcaption aria-hidden="true">Agentic RAG 原理简化</figcaption>
</figure>
<h1 id="评估">9. 评估</h1>
<ul>
<li>实践案例：<a
target="_blank" rel="noopener" href="https://github.com/hedon-ai-road/regulation_rag/blob/main/eval.ipynb">eval.ipynb</a></li>
</ul>
<h2 id="三大标准">9.1 三大标准</h2>
<ul>
<li><strong>Context
Relevance</strong>：系统检索到的上下文是否紧密围绕用户的问题展开，是否包含了解答问题所需的关键信息。</li>
<li><strong>Faithfulness</strong>：生成的答案与给定的上下文之间的事实一致性。</li>
<li><strong>Answer
Relevance</strong>：关注答案是否直接回答了问题，还关注答案是否完整、是否包含冗余信息。</li>
</ul>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250704094042400.png"
alt="RAG 评估三大标准" />
<figcaption aria-hidden="true">RAG 评估三大标准</figcaption>
</figure>
<h2 id="三大步骤">9.2 三大步骤</h2>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250704094056404.png"
alt="RAG 评估三大步骤" />
<figcaption aria-hidden="true">RAG 评估三大步骤</figcaption>
</figure>
<h2 id="ragas">9.3 Ragas</h2>
<p><a target="_blank" rel="noopener" href="https://docs.ragas.io/en/stable/">Ragas</a> 评估指标：</p>
<ul>
<li><strong>Faithfulness</strong>:
生成的答案与给定的上下文之间的事实一致性。</li>
<li><strong>Answer relevancy</strong>:
关注答案是否直接回答了问题，还关注答案是否完整、是否包含冗余信息。</li>
<li><strong>Context Precision</strong>: 衡量检索上下文的信噪比。</li>
<li><strong>Context Recall</strong>:
判断是否能检索到回答问题所需的全部相关信息。</li>
</ul>
<p>优点：</p>
<p>优点：</p>
<ol type="1">
<li>轻量易用。</li>
<li>指标专业性：专为 RAG 设计四大核心指标：上下文相关性（Context
Relevance）、上下文召回率（Context
Recall）、答案忠实度（Faithfulness）、答案相关性（Answer
Relevance）。</li>
<li>无参考标签评估：不依赖参考答案即可完成评估，降低标注成本。</li>
</ol>
<p>缺点：</p>
<ol type="1">
<li>结果可解释性弱：仅输出分数，不提供得分原因。</li>
<li>本地化支持不足：主要优化英文场景，对中文等语言支持有限。</li>
<li>功能扩展性弱：不支持自定义指标，灵活性较。</li>
</ol>
<p>代码示例：</p>
<p><strong>1. 构建数据集</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line">questions = [</span><br><span class="line">    <span class="string">&quot;伙食补助费标准是什么?&quot;</span>,</span><br><span class="line">    <span class="string">&quot;出差可以买意外保险吗？需要自己购买吗&quot;</span>,</span><br><span class="line">]</span><br><span class="line">ground_truths = [</span><br><span class="line">    <span class="string">&quot;伙食补助费标准: 西藏、青海、新疆 120元/人、天 其他省份 100元/人、天&quot;</span>,</span><br><span class="line">    <span class="string">&quot;出差可以购买交通意外保险，由单位统一购买，不再重复购买&quot;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">answers = []</span><br><span class="line">contexts = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> query <span class="keyword">in</span> questions:</span><br><span class="line">    response, context_list = run_rag_pipeline_without_stream(query=query, k=<span class="number">3</span>)</span><br><span class="line">    answers.append(response)</span><br><span class="line">    contexts.append(context_list)</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&quot;question&quot;</span>: questions,</span><br><span class="line">    <span class="string">&quot;answer&quot;</span>: answers,</span><br><span class="line">    <span class="string">&quot;contexts&quot;</span>: contexts,</span><br><span class="line">    <span class="string">&quot;ground_truth&quot;</span>: ground_truths</span><br><span class="line">&#125;</span><br><span class="line">dataset = Dataset.from_dict(data)</span><br></pre></td></tr></table></figure>
<p><strong>2. 定义评估指标</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ragas.metrics <span class="keyword">import</span>(</span><br><span class="line">    faithfulness,</span><br><span class="line">    answer_relevancy,</span><br><span class="line">    context_recall,</span><br><span class="line">    context_precision,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>3. 执行评估</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ragas <span class="keyword">import</span> evaluate</span><br><span class="line"><span class="keyword">from</span> ragas <span class="keyword">import</span> RunConfig</span><br><span class="line"></span><br><span class="line">eval_llm = RagLLM()</span><br><span class="line">embedding_model = RagEmbedding()</span><br><span class="line">eval_embedding_fn = embedding_model.get_embedding_fun()</span><br><span class="line"></span><br><span class="line">result = evaluate(</span><br><span class="line">    dataset=dataset,</span><br><span class="line">    llm=eval_llm,</span><br><span class="line">    embeddings=eval_embedding_fn,</span><br><span class="line">    metrics=[</span><br><span class="line">        context_precision,</span><br><span class="line">        context_recall,</span><br><span class="line">        faithfulness,</span><br><span class="line">        answer_relevancy,</span><br><span class="line">    ],</span><br><span class="line">    raise_exceptions=<span class="literal">True</span>,</span><br><span class="line">    run_config=config</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">df = result.to_pandas()</span><br></pre></td></tr></table></figure>
<p><strong>4. 评估结果</strong></p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image%20(10).png"
alt="Ragas 评估结果示例" />
<figcaption aria-hidden="true">Ragas 评估结果示例</figcaption>
</figure>
<h2 id="trulens">9.4 TruLens</h2>
<p>提供一个交互式的仪表板（Dashboard），用于可视化评估结果、比较不同版本的实验并追踪性能变化。它不仅支持
LangChain 和 LlamaIndex 等主流框架，还支持对完全自定义的 RAG
应用进行封装和评估。</p>
<p>典型流程：</p>
<ol type="1">
<li>定义反馈函数（如
<code>Groundedness</code>，<code>AnswerRelevance</code>，<code>ContextRelevance</code>）；</li>
<li>然后用 <code>TruApp</code> 包装 RAG 应用；</li>
<li>再一个 <code>with</code> 上下文管理器中运行查询；</li>
<li><code>run_dashboard</code> 启动仪表盘查看结果。</li>
</ol>
<p>优点：</p>
<ol type="1">
<li>全链路追踪：记录 RAG
全流程（检索、上下文、生成），支持根本原因分析，精准定位故障点（如检索错误或生成偏差）。</li>
<li>可视化与集成：内置 Web 仪表盘，实时展示评估结果；深度集成 LangChain
和 LlamaIndex。</li>
<li>反馈函数组合：支持自定义反馈函数（如毒性检测、语言匹配），灵活适配业务需求。</li>
</ol>
<p>缺点：</p>
<ol type="1">
<li>指标覆盖面窄：核心仅三大指标（上下文相关性、答案忠实度、答案相关性），缺乏上下文召回率等关键维度。</li>
<li>依赖人工标注：答案正确性等指标需参考答案（Ground
Truth），增加标注成本。</li>
<li>调试门槛高：全链路追踪需额外配置，对新手不够友好。</li>
</ol>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trulens_eval <span class="keyword">import</span> TruApp, Feedback, OpenAI, Select</span><br><span class="line"><span class="keyword">from</span> trulens_eval.app <span class="keyword">import</span> App</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化反馈函数提供者</span></span><br><span class="line">provider = OpenAI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 RAG 三元组反馈函数</span></span><br><span class="line">f_groundedness = Feedback(provider.groundedness_measure_with_cot_reasons).on(Select.RecordCalls.retrieve.rets.collect()).on_output()</span><br><span class="line">f_answer_relevance = Feedback(provider.relevance_with_cot_reasons).on_input().on_output()</span><br><span class="line">f_context_relevance = Feedback(provider.context_relevance_with_cot_reasons).on_input().on(Select.RecordCalls.retrieve.rets[:]).aggregate(np.mean)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 包装 RAG 应用</span></span><br><span class="line">tru_rag_app = TruApp(rag_query_engine, app_id=<span class="string">&quot;RAG_v1&quot;</span>, feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行并记录评估</span></span><br><span class="line"><span class="keyword">with</span> tru_rag_app <span class="keyword">as</span> recording:</span><br><span class="line">    rag_query_engine.query(<span class="string">&quot;What did the author do growing up?&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动仪表板</span></span><br><span class="line">tru.run_dashboard()</span><br></pre></td></tr></table></figure>
<h2 id="deepeval">9.5 DeepEval</h2>
<p>将自身定位为 LLM 应用的"单元测试"框架，理念非常现代化。提供超过 14
种评估指标，不仅覆盖
RAG，还包括微调等场景。其一大亮点是指标具有<strong>自我解释</strong>能力，即在给出分数的同时，会提供具体的理由来解释为何得分不高，极大地便利了调试过程。此外，它与流行的测试框架
Pytest 深度集成，可以无缝地融入 CI/CD 流程。</p>
<p>优点：</p>
<ol type="1">
<li>工程化与自动化：原生支持 pytest，可集成 CI/CD
流水线，实现自动化测试与报告生成。</li>
<li>指标丰富且可定制：内置 30+ 指标（如忠实度、毒性、偏见检测），支持
DAG
自定义指标（决策树结构）满足复杂逻辑。独创上下文召回率计算（基于关键陈述覆盖比例）。</li>
<li>结果可解释性强：提供分数原因及改进建议，支持与 RAGAS
结果联动分析</li>
</ol>
<p>缺点：</p>
<ol type="1">
<li>部分指标非 RAG
专属：如摘要质量、知识保留等指标更通用，需筛选适用场景。</li>
<li>依赖评估模型：默认使用 OpenAI 模型，替换自定义模型需额外开发。</li>
<li>配置复杂：DAG
指标需设计节点逻辑（任务节点、裁决节点等），学习曲线陡峭。</li>
</ol>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> deepeval</span><br><span class="line"><span class="keyword">import</span> deepeval.evaluate</span><br><span class="line"><span class="keyword">from</span> deepeval.metrics <span class="keyword">import</span> (</span><br><span class="line">    FaithfulnessMetric,</span><br><span class="line">    AnswerRelevancyMetric,</span><br><span class="line">    ContextualPrecisionMetric,</span><br><span class="line">    ContextualRecallMetric</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> deepeval.test_case <span class="keyword">import</span> LLMTestCase</span><br><span class="line"></span><br><span class="line">test_cases = []</span><br><span class="line"><span class="keyword">for</span> i, question <span class="keyword">in</span> <span class="built_in">enumerate</span>(questions):</span><br><span class="line">    test_cases.append(LLMTestCase(</span><br><span class="line">        <span class="built_in">input</span>=question,</span><br><span class="line">        actual_output=answers[i],</span><br><span class="line">        retrieval_context=contexts[i],</span><br><span class="line">        expected_output=ground_truths[i],</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">evaluation_metrics = [</span><br><span class="line">    FaithfulnessMetric(threshold=<span class="number">0.7</span>),</span><br><span class="line">    AnswerRelevancyMetric(threshold=<span class="number">0.8</span>),</span><br><span class="line">    ContextualPrecisionMetric(threshold=<span class="number">0.7</span>),</span><br><span class="line">    ContextualRecallMetric(threshold=<span class="number">0.9</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">results = deepeval.evaluate(</span><br><span class="line">    test_cases=test_cases,</span><br><span class="line">    metrics=evaluation_metrics</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, result <span class="keyword">in</span> <span class="built_in">enumerate</span>(results.test_results):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;--- TestCase <span class="subst">&#123;i+<span class="number">1</span>&#125;</span> ---&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Query: <span class="subst">&#123;result.<span class="built_in">input</span>&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> result.success:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;✅ Overall Result: Passed\n&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;❌ Overall Result: Failed\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印每个指标的详细得分和原因</span></span><br><span class="line">    <span class="keyword">for</span> metric_result <span class="keyword">in</span> result.metrics_data:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;  📊 Metric: <span class="subst">&#123;metric_result.__class__.__name__&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;     - Score: <span class="subst">&#123;metric_result.score:<span class="number">.2</span>f&#125;</span> (Threshold: <span class="subst">&#123;metric_result.threshold&#125;</span>)&quot;</span>)</span><br><span class="line">        reason = <span class="built_in">getattr</span>(metric_result, <span class="string">&#x27;reason&#x27;</span>, <span class="string">&#x27;N/A&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;     - Reason: <span class="subst">&#123;reason&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">25</span> + <span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>结合单元测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pytest</span><br><span class="line"><span class="keyword">from</span> deepeval <span class="keyword">import</span> assert_test</span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&quot;test_case&quot;</span>, test_cases</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">regualation_rag_eval</span>(<span class="params">test_case: LLMTestCase</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Testing Input: <span class="subst">&#123;test_case.<span class="built_in">input</span>&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    assert_test(</span><br><span class="line">        test_case=test_case,</span><br><span class="line">        metrics=[</span><br><span class="line">            FaithfulnessMetric(threshold=<span class="number">0.7</span>),</span><br><span class="line">            AnswerRelevancyMetric(threshold=<span class="number">0.8</span>),</span><br><span class="line">            ContextualPrecisionMetric(threshold=<span class="number">0.7</span>),</span><br><span class="line">            ContextualRecallMetric(threshold=<span class="number">0.9</span>)</span><br><span class="line">        ]</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>输出示例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">--- TestCase 1 ---</span><br><span class="line">Query: 伙食补助费标准是什么?</span><br><span class="line">✅ Overall Result: Passed</span><br><span class="line"></span><br><span class="line">  📊 Metric: MetricData</span><br><span class="line">     - Score: 1.00 (Threshold: 0.7)</span><br><span class="line">     - Reason: The score is 1.00 because there are no contradictions, indicating a perfect alignment between the actual output and the retrieval context. Great job maintaining accuracy and consistency!</span><br><span class="line">  📊 Metric: MetricData</span><br><span class="line">     - Score: 1.00 (Threshold: 0.8)</span><br><span class="line">     - Reason: The score is 1.00 because the response perfectly addresses the question about the standard for meal allowances without any irrelevant information. Great job!</span><br><span class="line">  📊 Metric: MetricData</span><br><span class="line">     - Score: 1.00 (Threshold: 0.7)</span><br><span class="line">     - Reason: The score is 1.00 because the relevant nodes in the retrieval contexts are perfectly ranked above the irrelevant node. The first node provides a clear table with the &#x27;伙食补助费标准&#x27; for different regions, directly answering the input question. The second node further explains the concept and provides the same standards, reinforcing the relevance. The third node, which discusses hotel recommendations and accommodation fees, is unrelated and correctly ranked last.</span><br><span class="line">  📊 Metric: MetricData</span><br><span class="line">     - Score: 1.00 (Threshold: 0.9)</span><br><span class="line">     - Reason: The score is 1.00 because the expected output perfectly aligns with the information in the nodes in the retrieval context, showcasing a flawless match. Great job!</span><br></pre></td></tr></table></figure>
<h1 id="graph-rag">10. Graph RAG</h1>
<h2 id="图数据库">10.1 图数据库</h2>
<h3 id="neo4j">10.1.1 neo4j</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/neo4j/neo4j">neo4j</a> 使用的是语言是 <a
target="_blank" rel="noopener" href="https://neo4j.com/docs/getting-started/cypher/">cypher</a>。Cypher
的核心是 <code>MATCH</code>（模式匹配） +
<code>RETURN</code>（结果返回），辅以
<code>CREATE</code>/<code>MERGE</code>（数据操作）、<code>WHERE</code>（过滤）、<code>WITH</code>（管道传递）。</p>
<p><strong>1. 节点与关系语法</strong></p>
<ul>
<li><strong>节点</strong>：用圆括号 <code>()</code>
表示，可包含变量、标签和属性。
<ul>
<li><code>()</code>：匿名节点</li>
<li><code>(p:Person)</code>：变量 <code>p</code> + 标签
<code>Person</code></li>
<li><code>(p:Person &#123;name: 'Alice', age: 30&#125;)</code>：带属性的节点。</li>
</ul></li>
<li><strong>关系</strong>：用方括号 <code>[]</code>
表示，放在两个短横线中间（<code>--</code>），方向用箭头（<code>→</code>
或 <code>←</code>）指定。
<ul>
<li><code>-[:KNOWS]-</code>：无变量、类型为 <code>KNOWS</code>
的无向关系</li>
<li><code>-[r:ACTED_IN &#123;roles: ['Neo']&#125;]-→</code>：变量 <code>r</code> +
类型 <code>ACTED_IN</code> + 属性</li>
</ul></li>
</ul>
<p><strong>2. 模式匹配（MATCH）</strong></p>
<p>核心是通过路径模式描述图结构：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person)-[r:ACTED_IN]-&gt;(m:Movie &#123;title: &#x27;The Matrix&#x27;&#125;)</span><br><span class="line">RETURN p, r.roles</span><br></pre></td></tr></table></figure>
<ul>
<li>可选匹配：<code>OPTIONAL MATCH</code> 处理可能不存在的关系。</li>
</ul>
<p><strong>3. 数据操作语句</strong></p>
<ul>
<li><p>创建：</p>
<ul>
<li><code>CREATE (p:Person &#123;name: 'Alice'&#125;)</code>：创建节点。</li>
<li><code>CREATE (a)-[:FRIEND]-&gt;(b)</code>：创建关系（需先匹配
<code>a</code>, <code>b</code>）</li>
</ul></li>
<li><p>更新：<code>SET</code> 修改属性</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person) SET p.age = 31</span><br></pre></td></tr></table></figure></li>
<li><p>合并：<code>MERGE</code> 存在则匹配，不存在则创建</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MERGE (p:Person &#123;name: &#x27;Alice&#x27;&#125;)</span><br><span class="line">ON CREATE SET p.created_at = timestamp()</span><br></pre></td></tr></table></figure></li>
<li><p>删除：</p>
<ul>
<li><code>DELETE n</code>：删除节点（需先断开关系）</li>
<li><code>DETACH DELETE n</code>：删除节点及关联关系</li>
</ul></li>
</ul>
<p><strong>4. 查询控制条件</strong></p>
<ul>
<li><p>过滤：<code>WHERE</code> 条件筛选</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p:Person) WHERE p.age &gt; 30 OR p.name STARTS WITH &#x27;A&#x27;</span><br></pre></td></tr></table></figure></li>
<li><p>返回：<code>RETURN</code> 指定输出</p></li>
<li><p>连接查询：<code>WITH</code> 传递中间结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MATCH (p)-[:FRIEND]-&gt;(f)</span><br><span class="line">WITH p, count(f) AS friendCount</span><br><span class="line">WHERE friendCount &gt; 10</span><br><span class="line">RETURN p.name</span><br></pre></td></tr></table></figure></li>
<li><p>聚合与排序：</p>
<ul>
<li><code>COUNT()</code>, <code>COLLECT()</code>：聚合函数</li>
<li><code>ORDER BY p.age DESC LIMIT 10</code>：排序和分页</li>
</ul></li>
</ul>
<p><strong>5. 索引与约束</strong></p>
<ul>
<li><p>索引：加速节点查找</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE INDEX FOR (p:Person) ON (p.name)</span><br></pre></td></tr></table></figure></li>
<li><p>约束：确保数据唯一性</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE CONSTRAINT ON (m:Movie) ASSERT m.title IS UNIQUE</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="nebula-graph">10.1.2 nebula graph</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/vesoft-inc/nebula">nebula graph</a>
使用的语言是 <a
target="_blank" rel="noopener" href="https://docs.nebula-graph.io/3.8.0/3.ngql-guide/1.nGQL-overview/1.overview/">nGQL</a>。</p>
<h2 id="典型流程">10.2 典型流程</h2>
<ol type="1">
<li>将问题提交给 LLM，让其提取（总结）关键词；</li>
<li>通过关键词来地毯式查询节点，尝试命中图数据库中定义的节点；</li>
<li>如果有命中的，则通过节点来查询关联的关系和节点信息；</li>
<li>将查询到的信息组织上上下文提交给 LLM，解答最初的问题。</li>
</ol>
<h2 id="实战案例">10.3 实战案例</h2>
<ul>
<li><a
target="_blank" rel="noopener" href="https://github.com/hedon-py-road/learn-neo4j/blob/main/neo4j.ipynb">learn-neo4j</a></li>
<li><a
target="_blank" rel="noopener" href="https://github.com/hedon-ai-road/ftt_rag/blob/main/graph-rag.ipynb">ftt_graph_rag</a></li>
</ul>
<h1 id="react-rag">11. ReAct RAG</h1>
<p>ReAct = Reasoning + Acting = 推理 + 行动</p>
<ul>
<li>核心理念：让大型语言模型像人一样，在解决复杂问题时，能够先思考分析（推理），然后根据思考结果采取行动（行动），再观察行动结果，接着进行新一轮的思考，如此循环，直到问题解决。</li>
<li>核心流程：
<ul>
<li>思考（Thought）</li>
<li>行动（Action）</li>
<li>观察（Observation）</li>
<li>思考（Thought）</li>
<li>...</li>
<li>最终答案（Final Answer）</li>
</ul></li>
</ul>
<h2 id="prompt">11.1 Prompt</h2>
<p><strong>1. 明确的规则制定（Rule Formulation）</strong></p>
<ol type="1">
<li>循环结构：强制模型遵循 "Thought -&gt; Action -&gt; Observation"
的循环。</li>
<li>输出格式：严格规定每一个环节的输出格式，便于程序解析。</li>
<li>终止条件：明确告诉模型何时任务算完成，以及如何提交最终答</li>
</ol>
<p><strong>2. 精确的工具授权（Tool Granting）</strong></p>
<ol type="1">
<li>功能单一：每个工具最好只做一件事，这让模型更容易选择。</li>
<li>描述清晰：工具的描述 (description)
是模型决定使用哪个工具的唯一依据。描述要用自然语言写得清晰、准确，说明白“这个工具能干什么”。</li>
<li>参数明确：工具的输入参数 (parameters)
必须定义清楚，包括名称、类型和用途。</li>
<li>有了 MCP 后，这一步可以用 MCP 来替代。</li>
</ol>
<p><strong>3. 高质量的示例引导（Example Guidance）</strong></p>
<ol type="1">
<li>展示思维链：清晰地展示从问题到第一个思考，再到行动的逻辑。</li>
<li>覆盖典型场景：展示如何使用不同的工具，甚至是如何组合使用工具。</li>
<li>处理异常情况：最好能包含一个处理错误的示例
(比如搜索不到结果时该怎么办)，这能极大地提升模型的鲁棒性。</li>
</ol>
<p>Prompt 示例骨架：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">You are an expert assistant capable of solving complex problems by breaking them down into a sequence of thought and action. You must strictly follow the format of &quot;Thought, Action, Observation&quot; to solve the problem.</span><br><span class="line"></span><br><span class="line">You have access to the following tools:</span><br><span class="line"></span><br><span class="line">[tool<span class="emphasis">_definitions]</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">Use the following format:</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">Question: The user&#x27;s question you need to answer.</span></span><br><span class="line"><span class="emphasis">Thought: Your internal reasoning and plan for the next action.</span></span><br><span class="line"><span class="emphasis">Action: The tool you will use. It must be one of [list_</span>of<span class="emphasis">_tool_</span>names].</span><br><span class="line">Observation: The result returned by the action.</span><br><span class="line">... (this Thought/Action/Observation cycle can repeat N times)</span><br><span class="line">Thought: I now have the final answer.</span><br><span class="line">Action: finish(the final answer)</span><br></pre></td></tr></table></figure>
<p>Few Shot 示例：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Question: 苹果公司的现任 CEO 是谁？他的前任是谁？</span><br><span class="line"></span><br><span class="line">Thought: 我需要分两步解决这个问题。第一，找出苹果的现任 CEO。第二，找出他的前任。我先执行第一步。</span><br><span class="line">Action: search(&quot;苹果公司现任 CEO&quot;)</span><br><span class="line">Observation: 苹果公司现任 CEO 是蒂姆·库克 (Tim Cook)。</span><br><span class="line"></span><br><span class="line">Thought: 我已经知道了现任 CEO 是蒂姆·库克。现在我需要找出他的前任是谁。</span><br><span class="line">Action: search(&quot;蒂姆·库克的前任是谁&quot;)</span><br><span class="line">Observation: 蒂姆·库克的前任是苹果公司的创始人史蒂夫·乔布斯 (Steve Jobs)。</span><br><span class="line"></span><br><span class="line">Thought: 我已经获得了所有需要的信息：现任 CEO 是蒂姆·库克，前任是史蒂夫·乔布斯。我可以给出最终答案了。</span><br><span class="line">Action: finish(&quot;苹果公司的现任 CEO 是蒂姆·库克，他的前任是史蒂夫·乔布斯。&quot;)</span><br></pre></td></tr></table></figure>
<h2 id="实战案例-1">11.2 实战案例</h2>
<ul>
<li><a
target="_blank" rel="noopener" href="https://github.com/hedon-ai-road/react_rag">react-rag</a></li>
</ul>
<h1 id="rag-相关思考">12. RAG 相关思考</h1>
<ul>
<li><a
target="_blank" rel="noopener" href="https://www.woshipm.com/ai/6235363.html">企业大模型落地的现实解法：为什么
RAG 是绕不开的技术路径？</a></li>
<li><a
target="_blank" rel="noopener" href="https://blog.csdn.net/2401_84495872/article/details/148831083">不需要
RAG！手把手教你构建问答 Agent</a></li>
</ul>

<div class="article-footer fs14">
    <section id="license">
      <div class="header"><span>许可协议</span></div>
      <div class="body"><p>本文采用 <a
target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享
4.0 国际</a> 许可协议，转载请注明出处。</p>
</div>
    </section>
    
    <section id="share">
      <div class="header"><span>分享文章</span></div>
      <div class="body">
        <div class="link"><input class="copy-area" readonly="true" id="copy-link" value="https://hedon.top/2025/07/03/ai-rag-tech-complete/" /></div>
        <div class="social-wrap dis-select"><a class="social share-item wechat" onclick="util.toggle(&quot;qrcode-wechat&quot;)"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/b32ef3da1162a.svg" /></a><a class="social share-item weibo" target="_blank" rel="external nofollow noopener noreferrer" href="https://service.weibo.com/share/share.php?url=https://hedon.top/2025/07/03/ai-rag-tech-complete/&title=RAG 全栈技术 - HedonWang&pics=/banner/ai-rag-tech-complete.jpg&summary=RAG 全栈技术"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/80c07e4dbb303.svg" /></a><a class="social share-item email" href="mailto:?subject=RAG 全栈技术 - HedonWang&amp;body=https://hedon.top/2025/07/03/ai-rag-tech-complete/"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/a1b00e20f425d.svg" /></a><a class="social share-item link" onclick="util.copy(&quot;copy-link&quot;, &quot;复制成功&quot;)"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/8411ed322ced6.svg" /></a></div>
        
        <div class="qrcode" id="qrcode-wechat" style="opacity:0;height:0">
          <img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://api.qrserver.com/v1/create-qr-code/?size=256x256&data=https://hedon.top/2025/07/03/ai-rag-tech-complete/"/>
        </div>
        
      </div>
    </section>
    </div>
</article>
<div class="related-wrap" id="read-next"><section class="body"><div class="item" id="prev"><div class="note">较新文章</div><a href="/2025/07/03/fosa-ch4/">FOSA丨04丨架构特性定义</a></div><div class="item" id="next"><div class="note">较早文章</div><a href="/2025/07/02/fosa-ch3/">FOSA丨03丨模块化</a></div></section></div>




  <div class="related-wrap md-text" id="comments">
    <section class='header cmt-title cap theme'>
      <p>快来参与讨论吧~</p>

    </section>
    <section class='body cmt-body giscus'>
      

<svg class="loading" style="vertical-align:middle;fill:currentColor;overflow:hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2709"><path d="M832 512c0-176-144-320-320-320V128c211.2 0 384 172.8 384 384h-64zM192 512c0 176 144 320 320 320v64C300.8 896 128 723.2 128 512h64z" p-id="2710"></path></svg>

<div id="giscus" src="https://giscus.app/client.js" data-repo="hedon954/hedonspace" data-repo-id="R_kgDOKt17sQ" data-category="Q&A" data-category-id="DIC_kwDOKt17sc4CbAt-" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="preferred_color_scheme" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous"></div>

    </section>
  </div>



<footer class="page-footer footnote"><hr><div class="text"><p>本站由 <a href="/">Hedon Wang</a> 使用 <a
target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.29.1">Stellar
1.29.1</a> 主题创建。 本博客所有文章除特别声明外，均采用 <a
target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA
4.0</a> 许可协议，转载请注明出处。</p>
</div></footer>
<div class="main-mask" onclick="sidebar.dismiss()"></div></div><aside class="l_right">
<div class="widgets">



<widget class="widget-wrapper toc" id="data-toc" collapse="false"><div class="widget-header dis-select"><span class="name">本文目录</span><a class="cap-action" onclick="sidebar.toggleTOC()" ><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg></a></div><div class="widget-body"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%87%E6%A1%A3%E5%8A%A0%E8%BD%BD"><span class="toc-text">1. 文档加载</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#langchian-document"><span class="toc-text">langchian Document</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#html"><span class="toc-text">html</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pdf"><span class="toc-text">PDF</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#unstructured"><span class="toc-text">Unstructured</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#pdf-1"><span class="toc-text">PDF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ppt"><span class="toc-text">PPT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#word"><span class="toc-text">Word</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#excel"><span class="toc-text">Excel</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ragflow.deepdoc"><span class="toc-text">ragflow.deepdoc</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5"><span class="toc-text">2. 分块策略</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%B5%8C%E5%85%A5"><span class="toc-text">3. 向量嵌入</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B"><span class="toc-text">3.1 嵌入模型评测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A8%80%E7%96%8F%E5%B5%8C%E5%85%A5sparse-embedding"><span class="toc-text">3.2 稀疏嵌入（Sparse Embedding）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-idfterm-frequency---inverse-document-frequency%E8%AF%8D%E9%A2%91-%E9%80%86%E6%96%87%E6%A1%A3%E9%A2%91%E7%8E%87"><span class="toc-text">TF-IDF(Term
Frequency - Inverse Document Frequency，词频-逆文档频率)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bm25best-matching-25"><span class="toc-text">BM25(Best Matching 25)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%86%E9%9B%86%E5%B5%8C%E5%85%A5dense-embedding"><span class="toc-text">3.3 密集嵌入（Dense Embedding）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#colbert"><span class="toc-text">3.4 ColBERT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bge-m3"><span class="toc-text">3.5 BGE-M3</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF"><span class="toc-text">4. 查询增强技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E6%9E%84%E5%BB%BA"><span class="toc-text">4.1 查询构建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#text-to-sql"><span class="toc-text">4.1.1 Text-to-SQL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#text-to-cypher"><span class="toc-text">4.1.2 Text-to-Cypher</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E6%9F%A5%E8%AF%A2%E4%B8%AD%E6%8F%90%E5%8F%96%E5%85%83%E6%95%B0%E6%8D%AE%E6%9E%84%E5%BB%BA%E8%BF%87%E6%BB%A4%E5%99%A8"><span class="toc-text">4.1.3
从查询中提取元数据构建过滤器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E7%BF%BB%E8%AF%91"><span class="toc-text">4.2 查询翻译</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#query2doc"><span class="toc-text">4.2.1 Query2Doc</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hyde"><span class="toc-text">4.2.2 HyDE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%90%E9%97%AE%E9%A2%98%E6%9F%A5%E8%AF%A2"><span class="toc-text">4.2.3 子问题查询</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E6%94%B9%E5%86%99"><span class="toc-text">4.2.4 查询改写</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E6%8A%BD%E8%B1%A1"><span class="toc-text">4.2.5 查询抽象</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E8%B7%AF%E7%94%B1"><span class="toc-text">4.3 查询路由</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-text">5. 索引优化技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E5%B0%8F%E5%9D%97%E5%88%B0%E5%A4%A7%E4%B8%8A%E4%B8%8B%E6%96%87"><span class="toc-text">5.1 从小块到大上下文</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E6%9C%89%E5%B1%82%E6%AC%A1%E7%9A%84%E7%B4%A2%E5%BC%95"><span class="toc-text">5.2 构建有层次的索引</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E5%A4%9A%E8%A1%A8%E7%A4%BA%E7%B4%A2%E5%BC%95"><span class="toc-text">5.3 构建多表示索引</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A3%80%E7%B4%A2%E5%90%8E%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-text">6. 检索后优化技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8D%E6%8E%92-rerank"><span class="toc-text">6.1 重排 rerank</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#rrf-%E9%87%8D%E6%8E%92"><span class="toc-text">6.1.1 RRF 重排</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cross-encoder-%E9%87%8D%E6%8E%92"><span class="toc-text">6.1.2 Cross-Encoder 重排</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#colbert-%E9%87%8D%E6%8E%92"><span class="toc-text">6.1.3 ColBERT 重排</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cohere-%E5%92%8C-jina-%E9%87%8D%E6%8E%92"><span class="toc-text">6.1.4 Cohere 和 Jina 重排</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rankgpt-%E5%92%8C-rankllm"><span class="toc-text">6.1.5 RankGPT 和 RankLLM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%B6%E6%95%88%E5%8A%A0%E6%9D%83%E9%87%8D%E6%8E%92"><span class="toc-text">6.1.6 时效加权重排</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9-compression"><span class="toc-text">6.2 压缩 compression</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8E%8B%E7%BC%A9%E6%A3%80%E7%B4%A2%E5%99%A8"><span class="toc-text">6.2.1 上下文压缩检索器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%A5%E5%AD%90%E5%B5%8C%E5%85%A5%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">6.2.2 句子嵌入优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#llmlingua"><span class="toc-text">6.2.3 LLMLingua</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#recomp-%E5%8E%8B%E7%BC%A9"><span class="toc-text">6.2.4 RECOMP 压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#prompt-caching-%E8%AE%B0%E5%BF%86%E4%B8%8A%E4%B8%8B%E6%96%87"><span class="toc-text">6.2.5 Prompt Caching 记忆上下文</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%A1%E6%AD%A3-correction"><span class="toc-text">6.3 校正 correction</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%93%8D%E5%BA%94%E7%94%9F%E6%88%90"><span class="toc-text">7. 响应生成</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E6%80%A7%E4%BC%98%E5%8C%96"><span class="toc-text">8. 系统性优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%88%91%E4%BF%AE%E6%AD%A3%E4%B8%8E%E5%8F%8D%E6%80%9D%E5%9E%8B-rag"><span class="toc-text">8.1 自我修正与反思型 RAG</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%AD%E4%BB%A3%E5%BC%8F%E6%A3%80%E7%B4%A2-rag"><span class="toc-text">8.2 迭代式检索 RAG</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E9%80%82%E5%BA%94%E6%99%BA%E8%83%BD%E4%BD%93-rag"><span class="toc-text">8.3 自适应&#x2F;智能体 RAG</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0"><span class="toc-text">9. 评估</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E5%A4%A7%E6%A0%87%E5%87%86"><span class="toc-text">9.1 三大标准</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E5%A4%A7%E6%AD%A5%E9%AA%A4"><span class="toc-text">9.2 三大步骤</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ragas"><span class="toc-text">9.3 Ragas</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#trulens"><span class="toc-text">9.4 TruLens</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#deepeval"><span class="toc-text">9.5 DeepEval</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#graph-rag"><span class="toc-text">10. Graph RAG</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">10.1 图数据库</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#neo4j"><span class="toc-text">10.1.1 neo4j</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nebula-graph"><span class="toc-text">10.1.2 nebula graph</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B8%E5%9E%8B%E6%B5%81%E7%A8%8B"><span class="toc-text">10.2 典型流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B"><span class="toc-text">10.3 实战案例</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#react-rag"><span class="toc-text">11. ReAct RAG</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#prompt"><span class="toc-text">11.1 Prompt</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-1"><span class="toc-text">11.2 实战案例</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#rag-%E7%9B%B8%E5%85%B3%E6%80%9D%E8%80%83"><span class="toc-text">12. RAG 相关思考</span></a></li></ol></div><div class="widget-footer">

<a class="top" onclick="util.scrollTop()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 12c0-4.714 0-7.071 1.464-8.536C4.93 2 7.286 2 12 2c4.714 0 7.071 0 8.535 1.464C22 4.93 22 7.286 22 12c0 4.714 0 7.071-1.465 8.535C19.072 22 16.714 22 12 22s-7.071 0-8.536-1.465C2 19.072 2 16.714 2 12Z"/><path stroke-linecap="round" stroke-linejoin="round" d="m9 15.5l3-3l3 3m-6-4l3-3l3 3"/></g></svg><span>回到顶部</span></a><a class="buttom" onclick="util.scrollComment()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M10.46 1.25h3.08c1.603 0 2.86 0 3.864.095c1.023.098 1.861.3 2.6.752a5.75 5.75 0 0 1 1.899 1.899c.452.738.654 1.577.752 2.6c.095 1.004.095 2.261.095 3.865v1.067c0 1.141 0 2.036-.05 2.759c-.05.735-.153 1.347-.388 1.913a5.75 5.75 0 0 1-3.112 3.112c-.805.334-1.721.408-2.977.43a10.81 10.81 0 0 0-.929.036c-.198.022-.275.054-.32.08c-.047.028-.112.078-.224.232c-.121.166-.258.396-.476.764l-.542.916c-.773 1.307-2.69 1.307-3.464 0l-.542-.916a10.605 10.605 0 0 0-.476-.764c-.112-.154-.177-.204-.224-.232c-.045-.026-.122-.058-.32-.08c-.212-.023-.49-.03-.93-.037c-1.255-.021-2.171-.095-2.976-.429A5.75 5.75 0 0 1 1.688 16.2c-.235-.566-.338-1.178-.389-1.913c-.049-.723-.049-1.618-.049-2.76v-1.066c0-1.604 0-2.86.095-3.865c.098-1.023.3-1.862.752-2.6a5.75 5.75 0 0 1 1.899-1.899c.738-.452 1.577-.654 2.6-.752C7.6 1.25 8.857 1.25 10.461 1.25M6.739 2.839c-.914.087-1.495.253-1.959.537A4.25 4.25 0 0 0 3.376 4.78c-.284.464-.45 1.045-.537 1.96c-.088.924-.089 2.11-.089 3.761v1c0 1.175 0 2.019.046 2.685c.045.659.131 1.089.278 1.441a4.25 4.25 0 0 0 2.3 2.3c.515.214 1.173.294 2.429.316h.031c.398.007.747.013 1.037.045c.311.035.616.104.909.274c.29.17.5.395.682.645c.169.232.342.525.538.856l.559.944a.52.52 0 0 0 .882 0l.559-.944c.196-.331.37-.624.538-.856c.182-.25.392-.476.682-.645c.293-.17.598-.24.909-.274c.29-.032.639-.038 1.037-.045h.032c1.255-.022 1.913-.102 2.428-.316a4.25 4.25 0 0 0 2.3-2.3c.147-.352.233-.782.278-1.441c.046-.666.046-1.51.046-2.685v-1c0-1.651 0-2.837-.089-3.762c-.087-.914-.253-1.495-.537-1.959a4.25 4.25 0 0 0-1.403-1.403c-.464-.284-1.045-.45-1.96-.537c-.924-.088-2.11-.089-3.761-.089h-3c-1.651 0-2.837 0-3.762.089" clip-rule="evenodd"/><path fill="currentColor" d="M9 11a1 1 0 1 1-2 0a1 1 0 0 1 2 0m4 0a1 1 0 1 1-2 0a1 1 0 0 1 2 0m4 0a1 1 0 1 1-2 0a1 1 0 0 1 2 0"/></svg><span>参与讨论</span></a></div></widget>
</div></aside><div class='float-panel blur'>
  <button type='button' style='display:none' class='laptop-only rightbar-toggle mobile' onclick='sidebar.rightbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg>
  </button>
  <button type='button' style='display:none' class='mobile-only leftbar-toggle mobile' onclick='sidebar.leftbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 11c0-3.771 0-5.657 1.172-6.828C4.343 3 6.229 3 10 3h4c3.771 0 5.657 0 6.828 1.172C22 5.343 22 7.229 22 11v2c0 3.771 0 5.657-1.172 6.828C19.657 21 17.771 21 14 21h-4c-3.771 0-5.657 0-6.828-1.172C2 18.657 2 16.771 2 13z"/><path id="sep" stroke-linecap="round" d="M5.5 10h6m-5 4h4m4.5 7V3"/></g></svg>
  </button>
</div>
</div><div class="scripts">
<script type="text/javascript">
  const ctx = {
    date_suffix: {
      just: `刚刚`,
      min: `分钟前`,
      hour: `小时前`,
      day: `天前`,
    },
    root : `/`,
  };

  // required plugins (only load if needs)
  if (`local_search`) {
    ctx.search = {};
    ctx.search.service = `local_search`;
    if (ctx.search.service == 'local_search') {
      let service_obj = Object.assign({}, `{"field":"all","path":"/search.json","content":true,"skip_search":null,"sort":"-date"}`);
      ctx.search[ctx.search.service] = service_obj;
    }
  }
  const def = {
    avatar: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/3442075.svg`,
    cover: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/cover/76b86c0226ffd.svg`,
  };
  const deps = {
    jquery: `https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js`,
    marked: `https://cdn.jsdelivr.net/npm/marked@13.0.1/lib/marked.umd.min.js`
  }
  

</script>

<script type="text/javascript">
  const utils = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    css: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    js: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      if (src.startsWith('/')){
        src = ctx.root + src.substring(1);
      }
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    jq: (fn) => {
      if (typeof jQuery === 'undefined') {
        utils.js(deps.jquery).then(fn)
      } else {
        fn()
      }
    },
    
    onLoading: (el) => {
      if (el) {
        $(el).append('<div class="loading-wrap"><svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" stroke-opacity=".3" d="M12 3C16.9706 3 21 7.02944 21 12C21 16.9706 16.9706 21 12 21C7.02944 21 3 16.9706 3 12C3 7.02944 7.02944 3 12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="1.3s" values="60;0"/></path><path stroke-dasharray="15" stroke-dashoffset="15" d="M12 3C16.9706 3 21 7.02944 21 12"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.3s" values="15;0"/><animateTransform attributeName="transform" dur="1.5s" repeatCount="indefinite" type="rotate" values="0 12 12;360 12 12"/></path></g></svg></div>');
      }
    },
    onLoadSuccess: (el) => {
      if (el) {
        $(el).find('.loading-wrap').remove();
      }
    },
    onLoadFailure: (el) => {
      if (el) {
        $(el).find('.loading-wrap svg').remove();
        $(el).find('.loading-wrap').append('<svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" d="M12 3L21 20H3L12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.5s" values="60;0"/></path><path stroke-dasharray="6" stroke-dashoffset="6" d="M12 10V14"><animate fill="freeze" attributeName="stroke-dashoffset" begin="0.6s" dur="0.2s" values="6;0"/></path></g><circle cx="12" cy="17" r="1" fill="currentColor" fill-opacity="0"><animate fill="freeze" attributeName="fill-opacity" begin="0.8s" dur="0.4s" values="0;1"/></circle></svg>');
        $(el).find('.loading-wrap').addClass('error');
      }
    },
    request: (el, url, callback, onFailure) => {
      let retryTimes = 3;
      utils.onLoading(el);
      function req() {
        return new Promise((resolve, reject) => {
          let status = 0; // 0 等待 1 完成 2 超时
          let timer = setTimeout(() => {
            if (status === 0) {
              status = 2;
              timer = null;
              reject('请求超时');
              if (retryTimes == 0) {
                onFailure();
              }
            }
          }, 5000);
          fetch(url).then(function(response) {
            if (status !== 2) {
              clearTimeout(timer);
              resolve(response);
              timer = null;
              status = 1;
            }
            if (response.ok) {
              return response.json();
            }
            throw new Error('Network response was not ok.');
          }).then(function(data) {
            retryTimes = 0;
            utils.onLoadSuccess(el);
            callback(data);
          }).catch(function(error) {
            if (retryTimes > 0) {
              retryTimes -= 1;
              setTimeout(() => {
                req();
              }, 5000);
            } else {
              utils.onLoadFailure(el);
              onFailure();
            }
          });
        });
      }
      req();
    },
  };
</script>

<script>
  const sidebar = {
    leftbar: () => {
      if (l_body) {
        l_body.toggleAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    rightbar: () => {
      if (l_body) {
        l_body.toggleAttribute('rightbar');
        l_body.removeAttribute('leftbar');
      }
    },
    dismiss: () => {
      if (l_body) {
        l_body.removeAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    toggleTOC: () => {
      document.querySelector('#data-toc').classList.toggle('collapse');
    }
  }
</script>
<script type="text/javascript">
  (() => {
    const tagSwitchers = document.querySelectorAll('.tag-subtree.parent-tag > a > .tag-switcher-wrapper')
    for (const tagSwitcher of tagSwitchers) {
      tagSwitcher.addEventListener('click', (e) => {
        const parent = e.target.closest('.tag-subtree.parent-tag')
        parent.classList.toggle('expanded')
        e.preventDefault()
      })
    }

    // Get active tag from query string, then activate it.
    const urlParams = new URLSearchParams(window.location.search)
    const activeTag = urlParams.get('tag')
    if (activeTag) {
      let tag = document.querySelector(`.tag-subtree[data-tag="${activeTag}"]`)
      if (tag) {
        tag.querySelector('a').classList.add('active')
        
        while (tag) {
          tag.classList.add('expanded')
          tag = tag.parentElement.closest('.tag-subtree.parent-tag')
        }
      }
    }
  })()
</script>


<!-- required -->
<script src="/js/main.js?v=1.29.1" defer></script>

<script type="text/javascript">
  const applyTheme = (theme) => {
    if (theme === 'auto') {
      document.documentElement.removeAttribute('data-theme')
    } else {
      document.documentElement.setAttribute('data-theme', theme)
    }

    applyThemeToGiscus(theme)
  }

  const applyThemeToGiscus = (theme) => {
    theme = theme === 'auto' ? 'preferred_color_scheme' : theme

    const cmt = document.getElementById('giscus')
    if (cmt) {
      // This works before giscus load.
      cmt.setAttribute('data-theme', theme)
    }

    const iframe = document.querySelector('#comments > section.giscus > iframe')
    if (iframe) {
      // This works after giscus loaded.
      const src = iframe.src
      const newSrc = src.replace(/theme=[\w]+/, `theme=${theme}`)
      iframe.src = newSrc
    }
  }

  const switchTheme = () => {
    // light -> dark -> auto -> light -> ...
    const currentTheme = document.documentElement.getAttribute('data-theme')
    let newTheme;
    switch (currentTheme) {
      case 'light':
        newTheme = 'dark'
        break
      case 'dark':
        newTheme = 'auto'
        break
      default:
        newTheme = 'light'
    }
    applyTheme(newTheme)
    window.localStorage.setItem('Stellar.theme', newTheme)

    const messages = {
      light: `切换到浅色模式`,
      dark: `切换到深色模式`,
      auto: `切换到跟随系统配色`,
    }
    hud?.toast?.(messages[newTheme])
  }

  (() => {
    // Apply user's preferred theme, if any.
    const theme = window.localStorage.getItem('Stellar.theme')
    if (theme !== null) {
      applyTheme(theme)
    }
  })()
</script>


<!-- optional -->

  <script type="module">
  const el = document.querySelector('#comments #giscus');
  util.viewportLazyload(el, load_discus, false);

  function load_discus() {
    if (!el) return;
    try {
        el.innerHTML = '';
      } catch (error) {
        console.error(error);
      }
      const script = document.createElement('script');
      script.async = true;
      for (const key of Object.keys(el.attributes)) {
        const attr = el.attributes[key];
        if (['class', 'id'].includes(attr.name) === false) {
          script.setAttribute(attr.name, attr.value);
        }
      }
      el.appendChild(script);
  }
</script>




<script defer>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.services = Object.assign({}, JSON.parse(`{"mdrender":{"js":"/js/services/mdrender.js"},"siteinfo":{"js":"/js/services/siteinfo.js","api":"https://site-info-api-hedon.vercel.app/api/v1?url={href}"},"ghinfo":{"js":"/js/services/ghinfo.js"},"sites":{"js":"/js/services/sites.js"},"friends":{"js":"/js/services/friends.js"},"timeline":{"js":"/js/services/timeline.js"},"fcircle":{"js":"/js/services/fcircle.js"},"weibo":{"js":"/js/services/weibo.js"},"memos":{"js":"/js/services/memos.js"},"twikoo":{"js":"/js/services/twikoo_latest_comment.js"},"waline":{"js":"/js/services/waline_latest_comment.js"},"artalk":{"js":"/js/services/artalk_latest_comment.js"},"giscus":{"js":"/js/services/giscus_latest_comment.js"}}`));
    for (let id of Object.keys(ctx.services)) {
      const js = ctx.services[id].js;
      if (id == 'siteinfo') {
        ctx.cardlinks = document.querySelectorAll('a.link-card[cardlink]');
        if (ctx.cardlinks?.length > 0) {
          utils.js(js, { defer: true }).then(function () {
            setCardLink(ctx.cardlinks);
          });
        }
      } else {
        const els = document.getElementsByClassName(`ds-${id}`);
        if (els?.length > 0) {
          utils.jq(() => {
            if (id == 'timeline' || 'memos' || 'marked') {
              utils.js(deps.marked).then(function () {
                utils.js(js, { defer: true });
              });
            } else {
              utils.js(js, { defer: true });
            }
          });
        }
      }
    }
  });
</script>

<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.search = {
      path: `/search.json`,
    }
    utils.js('/js/search/local-search.js', { defer: true });
  });
</script><script>
  window.FPConfig = {
    delay: 0,
    ignoreKeywords: [],
    maxRPS: 5,
    hoverDelay: 25
  };
</script>
<script defer src="https://cdn.jsdelivr.net/npm/flying-pages@2/flying-pages.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.min.js"></script>
<script>
  // https://www.npmjs.com/package/vanilla-lazyload
  // Set the options globally
  // to make LazyLoad self-initialize
  window.lazyLoadOptions = {
    elements_selector: ".lazy",
  };
  // Listen to the initialization event
  // and get the instance of LazyLoad
  window.addEventListener(
    "LazyLoad::Initialized",
    function (event) {
      window.lazyLoadInstance = event.detail.instance;
    },
    false
  );
  document.addEventListener('DOMContentLoaded', function () {
    window.lazyLoadInstance?.update();
  });
</script><script>
  ctx.fancybox = {
    selector: `.timenode p>img, .md-text img:not([class]), .md-text .image img`,
    css: `https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0/dist/fancybox/fancybox.css`,
    js: `https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0/dist/fancybox/fancybox.umd.js`
  };
  var selector = '[data-fancybox]:not(.error)';
  if (ctx.fancybox.selector) {
    selector += `, ${ctx.fancybox.selector}`
  }
  var needFancybox = document.querySelectorAll(selector).length !== 0;
  if (!needFancybox) {
    const els = document.getElementsByClassName('ds-memos');
    if (els != undefined && els.length > 0) {
      needFancybox = true;
    }
  }
  if (needFancybox) {
    utils.css(ctx.fancybox.css);
    utils.js(ctx.fancybox.js, { defer: true }).then(function () {
      Fancybox.bind(selector, {
        hideScrollbar: false,
        Thumbs: {
          autoStart: false,
        },
        caption: (fancybox, slide) => {
          return slide.triggerEl.alt || slide.triggerEl.dataset.caption || null
        }
      });
    })
  }
</script>
<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    const swiper_api = document.getElementById('swiper-api');
    if (swiper_api != undefined) {
      utils.css(`https://unpkg.com/swiper@10.3.1/swiper-bundle.min.css`);
      utils.js(`https://unpkg.com/swiper@10.3.1/swiper-bundle.min.js`, { defer: true }).then(function () {
        const effect = swiper_api.getAttribute('effect') || '';
        var swiper = new Swiper('.swiper#swiper-api', {
          slidesPerView: 'auto',
          spaceBetween: 8,
          centeredSlides: true,
          effect: effect,
          rewind: true,
          pagination: {
            el: '.swiper-pagination',
            clickable: true,
          },
          navigation: {
            nextEl: '.swiper-button-next',
            prevEl: '.swiper-button-prev',
          },
        });
      })
    }
  });
</script>
<script defer type="text/javascript" src="https://cdn.jsdelivr.net/npm/mermaid@v9/dist/mermaid.min.js"></script>
<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    var mermaid_config = {
      startOnLoad: true,
      theme:
        "" == "auto" &&
          window.matchMedia("(prefers-color-scheme: dark)").matches
          ? "dark"
          : "neutral",
      logLevel: 3,
      themeVariables: {
        darkMode: true
      },
      flowchart: {
        useMaxWidth: false,
        htmlLabels: true,
        curve: "linear"
      },
      gantt: {
        axisFormat: "%Y/%m/%d"
      },
      sequence: {
        actorMargin: 50
      }
    }
    mermaid.initialize(mermaid_config);
  });
</script><script>
  document.addEventListener('DOMContentLoaded', function () {
    window.codeElements = document.querySelectorAll('.code');
    if (window.codeElements.length > 0) {
      ctx.copycode = {
        default_text: `Copy`,
        success_text: `Copied`,
        toast: `复制成功`,
      };
      utils.js('/js/plugins/copycode.js');
    }
  });
</script>


<!-- inject -->

</div></body></html>
