
<!DOCTYPE html><html lang="zh-CN">

<head>
  <meta charset="utf-8">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.29.1" theme-name="Stellar" theme-version="1.29.1">
  
  <meta name="generator" content="Hexo 6.3.0">
  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel="preconnect" href="https://gcore.jsdelivr.net" crossorigin><link rel="preconnect" href="https://unpkg.com" crossorigin><link rel="preconnect" href="https://cdn.bootcdn.net" crossorigin>
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#000">
  <meta name="theme-color" content="#f9fafb">
  
  <title>读书笔记丨《从零构建大语言模型》 - HedonWang</title>

  
    <meta name="description" content="通过 500 行代码实现，深入解析从零构建 GPT 风格大语言模型的完整流程：从数据处理、Transformer 架构核心、训练循环到文本生成策略，带你理解大模型背后的工程实现原理。">
<meta property="og:type" content="article">
<meta property="og:title" content="读书笔记丨《从零构建大语言模型》">
<meta property="og:url" content="https://hedon.top/2025/08/30/llm/note-llm-from-scratch/index.html">
<meta property="og:site_name" content="HedonWang">
<meta property="og:description" content="通过 500 行代码实现，深入解析从零构建 GPT 风格大语言模型的完整流程：从数据处理、Transformer 架构核心、训练循环到文本生成策略，带你理解大模型背后的工程实现原理。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831134803291.png">
<meta property="og:image" content="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830172936403.png">
<meta property="og:image" content="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831134830004.png">
<meta property="og:image" content="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/1*iWQzxhVlvadk6VAJjsgXgg.png">
<meta property="og:image" content="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831134852813.png">
<meta property="og:image" content="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831135822616.png">
<meta property="article:published_time" content="2025-08-30T03:22:00.000Z">
<meta property="article:modified_time" content="2025-09-01T11:31:10.498Z">
<meta property="article:author" content="Hedon Wang">
<meta property="article:tag" content="读书笔记">
<meta property="article:tag" content="大模型">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831134803291.png">
  
  
  
  <meta name="keywords" content="读书笔记,大模型">

  <!-- feed -->
  
    <link rel="alternate" href="/atom.xml" title="HedonWang" type="application/atom+xml">
  

  <link rel="stylesheet" href="/css/main.css?v=1.29.1">


  
    <link rel="shortcut icon" href="/assets/favicon.png">
  

  

  <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon.png"><link rel="shortcut icon" href="/assets/favicon.png"><meta name="theme-color" content="#f8f8f8"><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC&display=swap" rel="stylesheet"><link rel="stylesheet" href="/css/markmap.css"><script src="https://cdn.jsdelivr.net/npm/markmap-autoloader@0.18"></script>
</head>
<body>

<div class="l_body s:aa content tech" id="start" layout="post" ><aside class="l_left"><div class="leftbar-container">


<header class="header"><div class="logo-wrap"><div class="icon"><img no-lazy class="icon" src="/assets/favicon.png" onerror="javascript:this.classList.add('error');this.src='https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/image/2659360.svg';"></div><a class="title" href="/"><div class="main" ff="title">HedonWang</div><div class="sub cap">君子求诸己，律己则安。</div></a></div></header>

<div class="nav-area">
<div class="search-wrapper" id="search-wrapper"><form class="search-form"><a class="search-button" onclick="document.getElementById(&quot;search-input&quot;).focus();"><svg t="1705074644177" viewBox="0 0 1025 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1560" width="200" height="200"><path d="M1008.839137 935.96571L792.364903 719.491476a56.783488 56.783488 0 0 0-80.152866 0 358.53545 358.53545 0 1 1 100.857314-335.166073 362.840335 362.840335 0 0 1-3.689902 170.145468 51.248635 51.248635 0 1 0 99.217358 26.444296 462.057693 462.057693 0 1 0-158.255785 242.303546l185.930047 185.725053a51.248635 51.248635 0 0 0 72.568068 0 51.248635 51.248635 0 0 0 0-72.978056z" p-id="1561"></path><path d="M616.479587 615.969233a50.428657 50.428657 0 0 0-61.498362-5.534852 174.655348 174.655348 0 0 1-177.525271 3.484907 49.403684 49.403684 0 0 0-58.833433 6.76482l-3.074918 2.869923a49.403684 49.403684 0 0 0 8.609771 78.10292 277.767601 277.767601 0 0 0 286.992355-5.739847 49.403684 49.403684 0 0 0 8.404776-76.667958z" p-id="1562"></path></svg></a><input type="text" class="search-input" id="search-input" placeholder="站内搜索"></form><div id="search-result"></div><div class="search-no-result">没有找到内容！</div></div>


<nav class="menu dis-select"><a class="nav-item active" title="博客" href="/" style="color:#1BCDFC"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M5.879 2.879C5 3.757 5 5.172 5 8v8c0 2.828 0 4.243.879 5.121C6.757 22 8.172 22 11 22h2c2.828 0 4.243 0 5.121-.879C19 20.243 19 18.828 19 16V8c0-2.828 0-4.243-.879-5.121C17.243 2 15.828 2 13 2h-2c-2.828 0-4.243 0-5.121.879M8.25 17a.75.75 0 0 1 .75-.75h3a.75.75 0 0 1 0 1.5H9a.75.75 0 0 1-.75-.75M9 12.25a.75.75 0 0 0 0 1.5h6a.75.75 0 0 0 0-1.5zM8.25 9A.75.75 0 0 1 9 8.25h6a.75.75 0 0 1 0 1.5H9A.75.75 0 0 1 8.25 9" clip-rule="evenodd"/><path fill="currentColor" d="M5.235 4.058C5 4.941 5 6.177 5 8v8c0 1.823 0 3.058.235 3.942L5 19.924c-.975-.096-1.631-.313-2.121-.803C2 18.243 2 16.828 2 14v-4c0-2.829 0-4.243.879-5.121c.49-.49 1.146-.707 2.121-.803zm13.53 15.884C19 19.058 19 17.822 19 16V8c0-1.823 0-3.059-.235-3.942l.235.018c.975.096 1.631.313 2.121.803C22 5.757 22 7.17 22 9.999v4c0 2.83 0 4.243-.879 5.122c-.49.49-1.146.707-2.121.803z" opacity=".5"/></svg></a><a class="nav-item" title="专栏" href="/topic/" style="color:#3DC550"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M14.25 4.48v3.057c0 .111 0 .27.02.406a.936.936 0 0 0 .445.683a.96.96 0 0 0 .783.072c.13-.04.272-.108.378-.159L17 8.005l1.124.534c.106.05.248.119.378.16a.958.958 0 0 0 .783-.073a.936.936 0 0 0 .444-.683c.021-.136.021-.295.021-.406V3.031c.113-.005.224-.01.332-.013C21.154 2.98 22 3.86 22 4.933v11.21c0 1.112-.906 2.01-2.015 2.08c-.97.06-2.108.179-2.985.41c-1.082.286-1.99 1.068-3.373 1.436c-.626.167-1.324.257-1.627.323V5.174c.32-.079 1.382-.203 1.674-.371c.184-.107.377-.216.576-.323m5.478 8.338a.75.75 0 0 1-.546.91l-4 1a.75.75 0 0 1-.364-1.456l4-1a.75.75 0 0 1 .91.546" clip-rule="evenodd"/><path fill="currentColor" d="M18.25 3.151c-.62.073-1.23.18-1.75.336a8.2 8.2 0 0 0-.75.27v3.182l.75-.356l.008-.005a1.13 1.13 0 0 1 .492-.13c.047 0 .094.004.138.01c.175.029.315.1.354.12l.009.005l.749.356V3.647z"/><path fill="currentColor" d="M12 5.214c-.334-.064-1.057-.161-1.718-.339C8.938 4.515 8.05 3.765 7 3.487c-.887-.234-2.041-.352-3.018-.412C2.886 3.007 2 3.9 2 4.998v11.146c0 1.11.906 2.01 2.015 2.079c.97.06 2.108.179 2.985.41c.486.129 1.216.431 1.873.726c1.005.451 2.052.797 3.127 1.034z" opacity=".5"/><path fill="currentColor" d="M4.273 12.818a.75.75 0 0 1 .91-.545l4 1a.75.75 0 1 1-.365 1.455l-4-1a.75.75 0 0 1-.545-.91m.909-4.545a.75.75 0 1 0-.364 1.455l4 1a.75.75 0 0 0 .364-1.455z"/></svg></a><a class="nav-item" title="关于我" href="/about/" style="color:#F44336"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="m13.629 20.472l-.542.916c-.483.816-1.69.816-2.174 0l-.542-.916c-.42-.71-.63-1.066-.968-1.262c-.338-.197-.763-.204-1.613-.219c-1.256-.021-2.043-.098-2.703-.372a5 5 0 0 1-2.706-2.706C2 14.995 2 13.83 2 11.5v-1c0-3.273 0-4.91.737-6.112a5 5 0 0 1 1.65-1.651C5.59 2 7.228 2 10.5 2h3c3.273 0 4.91 0 6.113.737a5 5 0 0 1 1.65 1.65C22 5.59 22 7.228 22 10.5v1c0 2.33 0 3.495-.38 4.413a5 5 0 0 1-2.707 2.706c-.66.274-1.447.35-2.703.372c-.85.015-1.275.022-1.613.219c-.338.196-.548.551-.968 1.262" opacity=".5"/><path fill="currentColor" d="M10.99 14.308c-1.327-.978-3.49-2.84-3.49-4.593c0-2.677 2.475-3.677 4.5-1.609c2.025-2.068 4.5-1.068 4.5 1.609c0 1.752-2.163 3.615-3.49 4.593c-.454.335-.681.502-1.01.502c-.329 0-.556-.167-1.01-.502"/></svg></a><a class="nav-item" title="思维导图" href="/html/mindmap.html" style="color:#F44336"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="m13.629 20.472l-.542.916c-.483.816-1.69.816-2.174 0l-.542-.916c-.42-.71-.63-1.066-.968-1.262c-.338-.197-.763-.204-1.613-.219c-1.256-.021-2.043-.098-2.703-.372a5 5 0 0 1-2.706-2.706C2 14.995 2 13.83 2 11.5v-1c0-3.273 0-4.91.737-6.112a5 5 0 0 1 1.65-1.651C5.59 2 7.228 2 10.5 2h3c3.273 0 4.91 0 6.113.737a5 5 0 0 1 1.65 1.65C22 5.59 22 7.228 22 10.5v1c0 2.33 0 3.495-.38 4.413a5 5 0 0 1-2.707 2.706c-.66.274-1.447.35-2.703.372c-.85.015-1.275.022-1.613.219c-.338.196-.548.551-.968 1.262" opacity=".5"/><path fill="currentColor" d="M10.99 14.308c-1.327-.978-3.49-2.84-3.49-4.593c0-2.677 2.475-3.677 4.5-1.609c2.025-2.068 4.5-1.068 4.5 1.609c0 1.752-2.163 3.615-3.49 4.593c-.454.335-.681.502-1.01.502c-.329 0-.556-.167-1.01-.502"/></svg></a></nav>
</div>
<div class="widgets">


<widget class="widget-wrapper post-list"><div class="widget-header dis-select"><span class="name">最近更新</span></div><div class="widget-body fs14"><a class="item title" href="/2025/09/08/redis/redis-datatype-hash-and-set/"><span class="title">Redis 数据类型丨Hash&Set</span></a><a class="item title" href="/2025/09/05/qa/qa-traditional-backend-to-ai-engineer/"><span class="title">Q&A丨AI 视角下的后端技术重塑</span></a><a class="item title" href="/2025/08/30/llm/note-llm-from-scratch/"><span class="title">读书笔记丨《从零构建大语言模型》</span></a><a class="item title" href="/2025/08/25/redis/redis-datatype-string/"><span class="title">Redis 数据类型丨String丨从第一性原理看 Redis 字符串的设计哲学 (基于 Redis 8.2.1 源码)</span></a><a class="item title" href="/2025/08/30/graceful-restart-from-tableflip-to-k8s/"><span class="title">优雅重启的范式转移：从 tableflip 到 Kubernetes 的 Go 服务升级终极指南</span></a><a class="item title" href="/2023/12/23/floating-point-number/"><span class="title">一文彻底掌握浮点数</span></a><a class="item title" href="/2025/08/21/llm/guide-to-lr-warmup-cosine-annealing-gradient-clipping/"><span class="title">模型训练核心技巧：学习率预热、余弦衰减与梯度裁剪</span></a><a class="item title" href="/2025/08/20/redis/redis-datatype-list/"><span class="title">Redis 数据类型丨List丨从双向链表到 Listpack 的演进之路 (基于 Redis 8.2.1 源码)</span></a><a class="item title" href="/2025/08/18/llm/pytorch/"><span class="title">告别死记硬背：一份真正理解 PyTorch 核心设计的指南</span></a><a class="item title" href="/2025/08/15/encryption-mode/"><span class="title">从 ECB 到 GCM：理解加密模式的演进</span></a></div></widget>
</div>

</div></aside><div class="l_main" id="main">





<div class="article banner top"><img class="bg lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/banner/note-llm-from-scratch.jpg">
  <div class="content">
    <div class="top bread-nav footnote"><div class="left"><div class="flex-row" id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a>
<span class="sep"></span><a class="cap breadcrumb" href="/">文章</a><span class="sep"></span><a class="cap breadcrumb-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></div>
<div class="flex-row" id="post-meta"><span class="text created">发布于：<time datetime="2025-08-30T03:22:00.000Z">2025-08-30</time></span><span class="sep updated"></span><span class="text updated">更新于：<time datetime="2025-09-01T11:31:10.498Z">2025-09-01</time></span></div></div></div>
    
    <div class="bottom only-title">
      
      <div class="text-area">
        <h1 class="text title"><span>读书笔记丨《从零构建大语言模型》</span></h1>
        
      </div>
    </div>
    
  </div>
  </div><article class="md-text content"><p>最近在看《从零构建大语言模型》这本书，跟着思路自己也动手码了一个基础款的
LLM，代码不多，拢共 500 来行，完整代码 <a
target="_blank" rel="noopener" href="https://github.com/hedon-ai-road/llm-from-scratch/blob/main/all_in_one.py">llm-from-scrtach</a>。</p>
<p>所以，这篇读书笔记不打算空谈理论，就想换个方式：试着把这 500
行代码的来龙去脉讲清楚。通过解说代码，把从零构建一个大模型需要经历哪些环节、每个环节的目标和大概原理给串起来。</p>
<p>笔者并非该方向的专业人士，很多东西不会讲得太深（我自己也不懂），所以很多时候都是点到即止。这篇文章更适合那些和我一样的开发者，希望能从一个接地气的工程角度，对大模型的构建原理有个宏观的了解。</p>
<p>我们将采用一种贴近软件开发者思维的<strong>代码执行流</strong>视角，从程序的入口
<code>main</code>
函数开始，顺着调用栈逐层深入，探究数据处理、模型构建、训练循环的每一个细节。当一个模块被调用时，我们便深入其中，直至最底层的实现。</p>
<p>这趟旅程将遵循以下路线图：</p>
<ul>
<li><strong>从 <code>main</code>
函数出发</strong>：探寻程序的入口与总调度中心。</li>
<li><strong>深入数据流水线</strong>：看原始文本如何被加工成模型能够消化的食粮。</li>
<li><strong>解构核心引擎</strong>：层层剖析
<code>GPTModel</code>、<code>TransformerBlock</code> 直至最深处的
<code>MultiHeadAttention</code> 机制。</li>
<li><strong>启动训练循环</strong>：见证模型如何通过损失计算和权重更新，从随机变得智能。</li>
<li><strong>见证文本生成</strong>：观察训练好的模型如何像我们一样，逐字逐句地"思考"和"创作"。</li>
</ul>
<p>让我们即刻出发，揭开大语言模型背后的代码之谜。</p>
<h1 id="数据准备与采样">1. 数据准备与采样</h1>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831134803291.png" style="zoom:33%;" /></p>
<p><code>prepare_train_and_val_data</code> 的具体实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_train_and_val_data</span>(<span class="params">file_path, tokenizer</span>) -&gt; <span class="built_in">tuple</span>[DataLoader, DataLoader]:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;准备训练和验证数据加载器&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 步骤 1: 读取原始文本</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        text_data = file.read()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 步骤 2: 按比例分割成训练和验证两部分：90%训练，10%验证</span></span><br><span class="line">    train_ratio = <span class="number">0.9</span></span><br><span class="line">    split_idx = <span class="built_in">int</span>(train_ratio * <span class="built_in">len</span>(text_data))</span><br><span class="line">    train_data = text_data[:split_idx]</span><br><span class="line">    val_data = text_data[split_idx:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 步骤 3: 分别为两部分文本创建 DataLoader</span></span><br><span class="line">    train_loader = create_dataloader(</span><br><span class="line">        tokenizer,</span><br><span class="line">        train_data,</span><br><span class="line">        batch_size=<span class="number">2</span>,</span><br><span class="line">        max_length=GPT_CONFIG_124M[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">        stride=GPT_CONFIG_124M[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">        drop_last=<span class="literal">True</span>,</span><br><span class="line">        shuffle=<span class="literal">True</span>,</span><br><span class="line">        num_workers=<span class="number">0</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val_loader = create_dataloader(</span><br><span class="line">        tokenizer,</span><br><span class="line">        val_data,</span><br><span class="line">        batch_size=<span class="number">2</span>,</span><br><span class="line">        max_length=GPT_CONFIG_124M[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">        stride=GPT_CONFIG_124M[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">        drop_last=<span class="literal">False</span>,</span><br><span class="line">        shuffle=<span class="literal">False</span>,</span><br><span class="line">        num_workers=<span class="number">0</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> train_loader, val_loader</span><br></pre></td></tr></table></figure>
<h2 id="划分数据集-prepare_train_and_val_data">1.1 划分数据集
prepare_train_and_val_data</h2>
<p><code>prepare_train_and_val_data</code>
的代码并不复杂，就是将数据集划分为训练集和验证集，那么第一个问题就来了：<strong><u>模型为何需要训练集和验证集？</u></strong></p>
<p>和人类一样，模型通过<strong>看例子</strong>来学习。我们给它一本"教科书"和配套的"练习册"（<strong>训练集</strong>），让它反复练习，寻找规律。但我们如何知道它是真的学会了，还是仅仅背下了答案（过拟合）呢？</p>
<p>答案是，我们需要一场它从未见过的<strong>模拟考试</strong>（<strong>验证集</strong>）。</p>
<ul>
<li><strong>训练集 (Training
Set)</strong>：这是模型学习的<strong>唯一资料</strong>。模型会尽全力去拟合训练集中的数据，目标是在这个数据集上获得尽可能低的出错率（损失）。这对应了代码中
90% 的文本数据 。</li>
<li><strong>验证集 (Validation
Set)</strong>：这是一份<strong>被隔离的数据</strong>，模型在训练过程中<strong>绝对不能</strong>用它来更新自己的权重。我们只在训练的特定阶段用它来“考”一下模型，看看模型在“新题型”上的表现如何。这对应了代码中
10% 的文本数据 。</li>
</ul>
<p>如果模型在训练集上表现优异（比如损失很低），但在验证集上表现糟糕，这就亮起了<strong>过拟合</strong>的红灯。这说明模型只是死记硬背了训练题，而没有学到普适的规律。<code>prepare_train_and_val_data</code>
函数的核心使命，就是为模型准备好这两份至关重要的数据集。</p>
<p>它调用了 <code>create_dataloader</code> 函数。我们必须注意
<code>train_loader</code> 和 <code>val_loader</code>
在配置上的一个<strong>关键区别</strong>：</p>
<ul>
<li><p><strong><code>train_loader</code> 的 <code>shuffle</code> 设置为
<code>True</code></strong> 。</p>
<p>这是为了<strong>保证训练的有效性</strong>。在每一轮 (epoch)
训练开始时，<code>DataLoader</code>
都会将训练样本的顺序完全打乱。这就像我们学习时会打乱单词卡片的顺序一样，可以防止模型学到样本的出场顺序这种无关信息，从而迫使它学习更具泛化性的语言规律。</p></li>
<li><p><strong><code>val_loader</code> 的 <code>shuffle</code> 设置为
<code>False</code></strong> 。</p>
<p>这是为了<strong>保证评估的客观性和一致性</strong>。验证集是我们的模拟考试，我们希望每次考试的卷子（题目顺序）都是一样的，这样才能客观地比较模型在不同训练阶段的得分，判断它是否真的在进步。</p></li>
</ul>
<h2 id="构建数据集-create_dataloader">1.2 构建数据集
create_dataloader</h2>
<p><code>prepare_train_and_val_data</code>
只是一个调度者，真正的数据加工发生在它调用的
<code>create_dataloader</code> 和 <code>GPTDataset</code> 中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataloader</span>(<span class="params">tokenizer, txt, batch_size=<span class="number">4</span>, max_length=<span class="number">256</span>, stride=<span class="number">128</span>, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>, num_workers=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建数据加载器&quot;&quot;&quot;</span></span><br><span class="line">    dataset = GPTDataset(txt, tokenizer, max_length, stride)</span><br><span class="line">    dataloader = DataLoader(</span><br><span class="line">        dataset,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=shuffle,</span><br><span class="line">        drop_last=drop_last,</span><br><span class="line">        num_workers=num_workers,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br></pre></td></tr></table></figure>
<p><code>create_dataloader</code>
是一个简单的封装，它的核心是做了两件事：</p>
<ol type="1">
<li><code>dataset = GPTDataset(txt, tokenizer, max_length, stride)</code>：实例化一个
<code>GPTDataset</code> 对象。</li>
<li><code>dataloader = DataLoader(dataset, ...)</code>：将这个
<code>dataset</code> 对象包装成一个 PyTorch 的
<code>DataLoader</code>。</li>
</ol>
<h3 id="dataset-dataloader">1.2.1 Dataset &amp; DataLoader</h3>
<p>在深入 <code>GPTDataset</code> 结构之前，这里我们先对 PyTorch 中的 2
个关键数据类型 <code>Dataset</code> 和 <code>DataLoader</code>
进行简要介绍，对于 Pytorch 更详细的介绍可参考笔者这篇 <a
href="https://hedon.top/2025/08/18/llm/pytorch/">告别死记硬背：一份真正理解
PyTorch 核心设计的指南</a>。</p>
<p>简而言之，<code>Dataset</code> 和 <code>DataLoader</code>
是为了解决"数据集是什么"和"如何使用数据集"这 2
个核心问题，更具体的来说，在数据准备阶段，我们可能会面临以下几个问题：</p>
<ol type="1">
<li>原始数据格式各异，如何统一读取？</li>
<li>数据集可能非常大，无法一次性载入内存，怎么办？</li>
<li>训练时需要对数据进行批量 (batching)、打乱 (shuffling) 和预处理
(preprocessing)，如何高效实现？</li>
<li>如何利用多核 CPU 来加速数据加载，避免 GPU 等待？</li>
</ol>
<p>PyTorch 的解决方案就是 <code>Dataset</code> 和
<code>DataLoader</code> ：</p>
<ul>
<li><code>Dataset</code>：<strong>它定义了"数据集"是什么</strong>。这是一个抽象类，你只需要继承它并实现两个方法：<code>__len__</code>(返回数据集大小)
和 <code>__getitem__</code> (根据索引 <code>idx</code>
返回一条数据)。它解决了如何获取单条数据的问题，将数据访问的逻辑封装起来。</li>
<li><code>DataLoader</code>：<strong>它定义了"如何使用数据集"</strong>。它接收一个
<code>Dataset</code>对象，并在此基础上，优雅地解决了所有工程问题：
<ul>
<li><code>batch_size</code>：自动将单条数据打包成一个 batch。</li>
<li><code>shuffle=True</code>：在每个 epoch
开始时自动打乱数据顺序。</li>
<li><code>num_workers</code>：启动多个子进程并行加载数据，极大地提高了数据供给效率。</li>
<li><code>collate_fn</code>：自定义如何将多条样本合并成一个
batch，对于处理非标准数据（如不同长度的句子）非常有用。</li>
</ul></li>
</ul>
<p>它们之间的关系如图所示：</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830165323656.png"
alt="Dataset 与 DataLoader" />
<figcaption aria-hidden="true">Dataset 与 DataLoader</figcaption>
</figure>
<h3 id="gptdataset">1.2.2 GPTDataset</h3>
<p>现在我们可以来看 <code>GPTDataset</code> 的具体逻辑了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;GPT训练数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, txt, tokenizer, max_length, stride</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="variable language_">self</span>.input_ids = []</span><br><span class="line">        <span class="variable language_">self</span>.target_ids = []</span><br><span class="line"></span><br><span class="line">        token_ids = tokenizer.encode(txt)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用滑动窗口创建训练样本</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(token_ids) - max_length, stride):</span><br><span class="line">            input_chunk = token_ids[i:i+max_length]</span><br><span class="line">            target_chunk = token_ids[i+<span class="number">1</span>:i+max_length+<span class="number">1</span>]</span><br><span class="line">            <span class="variable language_">self</span>.input_ids.append(torch.tensor(input_chunk))</span><br><span class="line">            <span class="variable language_">self</span>.target_ids.append(torch.tensor(target_chunk))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.input_ids)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.input_ids[idx], <span class="variable language_">self</span>.target_ids[idx]</span><br></pre></td></tr></table></figure>
<p><code>GPTDataset</code> 继承了 <code>PyTorch</code> 的
<code>Dataset</code> 类型，我们重点来看它的构造函数
<code>__init__()</code>，它分为 2 个步骤：</p>
<ol type="1">
<li>将文本数据转为词元 ID 列表；</li>
<li>使用滑动窗口逐个构建<strong>输入-目标对</strong>，构建整个数据集，分别置于
<code>input_ids</code> 和 <code>target_ids</code> 这 2 个字段中。</li>
</ol>
<h4 id="文本词元化">1.2.2.1 文本词元化</h4>
<p>现在到了本篇的第一个真正意义上的理论环节，我们需要先搞清楚词元化（即下面这一行代码）到底是在做什么？为什么要这样？有哪些具体的方式方法？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">token_ids = tokenizer.encode(txt)</span><br></pre></td></tr></table></figure>
<p>包括大语言模型在内的深度神经网络模型是无法直接处理原始文本的。由于文本数据是离散的，因此我们无法直接用它来执行神经网络训练所需的数学运算。我们需要一种将单词表示为连续值的向量格式的方法（通常是张量
Tensor）。</p>
<p>将数据转换为向量格式的过程通常称为嵌入（embedding），如下图所示：</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830170327803.png" /></p>
<p>要理解
<code>Tensor</code>，我们需要先建立一个最重要的心智模型：<strong>Tensor
的每一个维度 (dimension) 都有其特定的语义含义</strong>。</p>
<p>一个典型的 4D Tensor <code>(B, C, H, W)</code> 在计算机视觉中，其形状
<code>(16, 3, 224, 224)</code> 并不是一串孤立的数字，它的意思是：</p>
<ul>
<li><strong>B (Batch size) = 16</strong>: 这个 Tensor 里有 16
张独立的图像。</li>
<li><strong>C (Channels) = 3</strong>: 每张图像有 3 个通道（R, G,
B）。</li>
<li><strong>H (Height) = 224</strong>: 每张图像的高度是 224 像素。</li>
<li><strong>W (Width) = 224</strong>: 每张图像的宽度是 224 像素。</li>
</ul>
<p>在多个维度综合起来语义含义越接近的词，它们的词嵌入向量在空间表示中就越相近，也就越"相似"，如下图所示：</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830170432509.png" /></p>
<p>当把文本转为词嵌入向量之后，我们的训练模型就可以识别这些数据并利用它们进行学习了。</p>
<p>一个完整的文本处理步骤，大概如下图所示： <img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830170953531.png" /></p>
<ol type="1">
<li><p>输入文件
<code>This is an example.</code>：这是所有处理的起点，是我们希望模型去理解和回应的原始、非结构化的人类语言。</p></li>
<li><p>词元化：原始文本被切分成独立的单元：<code>This</code>,
<code>is</code>, <code>an</code>, <code>example</code>,
<code>.</code>。</p>
<p>计算机模型无法一次性理解一整个句子。它需要将句子分解成更小的、标准化的单元，这些单元被称为<strong>词元
(Token)</strong>。如图中的文字描述，词元既可以是单词，也可以是标点符号之类的特殊字符。</p></li>
<li><p>转换为词元 ID：每个词元被映射到一个唯一的整数：<code>This</code>
-&gt; <code>40134</code>, <code>is</code> -&gt; <code>2052</code>,
<code>an</code> -&gt; <code>133</code>, <code>example</code> -&gt;
<code>389</code>, <code>.</code> -&gt; <code>12</code>。</p>
<p>计算机不认识字符串 <code>This</code>，但它能高效地处理数字
<code>40134</code>。这一步是<strong>将语言世界映射到数字世界</strong>的关键。每一个
ID 都对应着模型词汇表（一个巨大的“字典”）中的一个条目。</p></li>
<li><p>生成词元嵌入：一串数字 ID
变成了多个向量（关于词嵌入的具体细节，我们会在后续进行展开）。</p>
<p>即我们前面提到的，单个数字 ID（如
<code>40134</code>）本身是孤立的，不包含任何语义信息，所以我们需要将其转为词嵌入向量，在训练过程中，模型会不断调整这些向量，使得<strong>意思相近的词元，其向量在空间中的位置也相互靠近</strong>。</p></li>
<li><p>模型处理与输出：这些嵌入向量组成的序列，最终被送入<strong>类 GPT
的纯解码器 Transformer</strong> 。这是模型的核心大脑。Transformer
模型会分析这些向量之间的关系，理解整个句子的上下文，然后进行计算。经过<strong>后续处理步骤</strong>（如选择概率最高的词元）后，模型会生成一个<strong>输出文本</strong>。</p></li>
</ol>
<blockquote>
<p>[!IMPORTANT]</p>
<p>小结一下，从<strong>人类语言 (字符串) -&gt; 语言单元 (词元) -&gt;
机器语言 (数字 ID) -&gt; 数学对象 (嵌入向量) -&gt;
模型输入</strong>，每一步都是为了让原始的、非结构化的文本，变得结构化、数值化，并富含语义信息，最终成为能够被神经网络高效处理的原料。</p>
</blockquote>
<p>一个简单的分词器实现如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleTokenizer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="variable language_">self</span>.str_to_int = vocab</span><br><span class="line">        <span class="variable language_">self</span>.int_to_str = &#123;i:s <span class="keyword">for</span> s, i <span class="keyword">in</span> vocab.items()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text</span>):</span><br><span class="line">        preprocessed = re.split(<span class="string">r&#x27;([,.:;?_!&quot;()\&#x27;]|--|\s)&#x27;</span>, text)</span><br><span class="line">        preprocessed = [item.strip() <span class="keyword">for</span> item <span class="keyword">in</span> preprocessed <span class="keyword">if</span> item.strip()]</span><br><span class="line">        preprocessed = [item <span class="keyword">if</span> item <span class="keyword">in</span> <span class="variable language_">self</span>.str_to_int <span class="keyword">else</span> <span class="string">&quot;&lt;|unk|&gt;&quot;</span> <span class="keyword">for</span> item <span class="keyword">in</span> preprocessed]</span><br><span class="line">        ids = [<span class="variable language_">self</span>.str_to_int[s] <span class="keyword">for</span> s <span class="keyword">in</span> preprocessed]</span><br><span class="line">        <span class="keyword">return</span> ids</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, ids</span>):</span><br><span class="line">        text = <span class="string">&quot; &quot;</span>.join([<span class="variable language_">self</span>.int_to_str[i] <span class="keyword">for</span> i <span class="keyword">in</span> ids])</span><br><span class="line">        <span class="comment"># Remove the spaces before specific punctuation marks.</span></span><br><span class="line">        text = re.sub(<span class="string">r&#x27;\s+([,.?!&quot;()\&#x27;])&#x27;</span>, <span class="string">r&#x27;\1&#x27;</span>, text)</span><br><span class="line">        <span class="keyword">return</span> text</span><br></pre></td></tr></table></figure>
<ol type="1">
<li><code>__init__</code> 初始化词典，里面每一个词元都唯一对应一个
ID；</li>
<li><code>encode</code> 原始文本转为一系列词元
ID，对于不识别的词元，会使用 <code>&lt;|unk|&gt;</code>
特殊标识进行占位，一般来说，还会使用诸如
<code>&lt;|endoftext|&gt;</code>
等特殊标识符来表示文本结束等特殊语义。</li>
<li><code>decode</code> 将词元 ID 列表转回原始文本。</li>
</ol>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830172025895.png" /></p>
<p>回到本篇的代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">token_ids = tokenizer.encode(txt)</span><br></pre></td></tr></table></figure>
<p>这里我们使用的是现有的 Python 开源库 <code>tiktoken</code>，它基于
Rust 的源代码非常高效地实现了 BPE（Byte Pair Encoder） 算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line"></span><br><span class="line">tokenizer = tiktoken.get_encoding(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">text1 = <span class="string">&quot;Hello, do you like tea?&quot;</span></span><br><span class="line">text2 = <span class="string">&quot;In the sunlit terraces of the palace.&quot;</span></span><br><span class="line">text = <span class="string">&quot; &lt;|endoftext|&gt; &quot;</span>.join((text1, text2))</span><br><span class="line">ids = tokenizer.encode(text, allowed_special=&#123;<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(ids)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(ids))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 20562, 13]</span><br><span class="line">Hello, do you like tea? &lt;|endoftext|&gt; In the sunlit terraces of the palace.</span><br></pre></td></tr></table></figure>
<p>通过输出，我们可以看到 <code>&lt;|endoftext|&gt;</code>
词元被分配了一个较大的词元 ID，即 <code>50256</code>。事实上，用于训练
GPT-2、GPT-3 和 ChatGPT 中使用的原始模型的 BPE 分词器的词汇总量为
<code>50257</code>，这意味着 <code>&lt;|endoftext|&gt;</code>
被分配了最大的词元 ID。</p>
<p>另外，BPE 分词器可以正确地编码和解码未知单词，比如
<code>someunknownPlace</code>。BPE
分词器是如何做到在不使用&lt;|unk|&gt;词元的前提下处理任何未知词汇的呢？</p>
<p>BPE
算法的原理是将不在预定义词汇表中的单词分解为更小的子词单元甚至单个字符，
从而能够处理词汇表之外的单词。因此，得益于 BPE
算法，如果分词器在分词过程中遇到不熟悉的单词，它可以将其表示为子词词元或字符序列，如下图所示。</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830172322896.png" /></p>
<p>将未知单词分解为单个字符的能力确保了分词器以及用其训练的大语言模型能够处理任何文本，即使文本中包含训练数据中不存在的单词。</p>
<h4 id="滑动窗口进行数据采样">1.2.2.2 滑动窗口进行数据采样</h4>
<p>分析完了词元化的背后底层逻辑后，我们来看这一部分的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, txt, tokenizer, max_length, stride</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">				<span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用滑动窗口创建训练样本</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(token_ids) - max_length, stride):</span><br><span class="line">            input_chunk = token_ids[i:i+max_length]</span><br><span class="line">            target_chunk = token_ids[i+<span class="number">1</span>:i+max_length+<span class="number">1</span>]</span><br><span class="line">            <span class="variable language_">self</span>.input_ids.append(torch.tensor(input_chunk))</span><br><span class="line">            <span class="variable language_">self</span>.target_ids.append(torch.tensor(target_chunk))</span><br></pre></td></tr></table></figure>
<p>要理解这段代码，我们需要回归到大语言模型（文本模型）是唯一任务：<strong><u>根据你给出的上文，猜出下一个词应该是什么</u></strong>。</p>
<p>例如，对于句子
<code>Time is an illusion</code>，我们可以为模型制作如下一系列的练习题：</p>
<ul>
<li><strong>问题</strong>：<code>Time</code> -&gt;
<strong>答案</strong>：<code>is</code></li>
<li><strong>问题</strong>：<code>Time is</code> -&gt;
<strong>答案</strong>：<code>an</code></li>
<li><strong>问题</strong>：<code>Time is an</code> -&gt;
<strong>答案</strong>：<code>illusion</code></li>
</ul>
<p>模型需要通过海量的这类"问答对"进行练习，才能逐渐掌握语言的规律。如果手动去制作上亿个这样的问答对，显然是不现实的。代码中的"滑动窗口"机制，就是为了解决这个问题。我们用一个具体的例子来解释这个
<code>for</code> 循环：</p>
<ul>
<li>假设 <code>max_length = 5</code></li>
<li>假设一段文本分词后的 <code>token_ids</code> 是
<code>[10, 20, 30, 40, 50, 60]</code></li>
</ul>
<p>当 <code>for</code> 循环第一次执行时 (<code>i=0</code>)：</p>
<ul>
<li><strong><code>input_chunk = token_ids[0:5]</code></strong> 会切出
<code>[10, 20, 30, 40, 50]</code>
<ul>
<li>这就是提供给模型的<strong>上下文</strong>，也就是<strong>问题</strong>。</li>
</ul></li>
<li><strong><code>target_chunk = token_ids[1:6]</code></strong> 会切出
<code>[20, 30, 40, 50, 60]</code>
<ul>
<li>这就是模型需要预测的<strong>正确答案</strong>。</li>
</ul></li>
</ul>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830172936403.png" alt="滑动窗口示意图" style="zoom:33%;" /></p>
<p>所以我们通过这样一个 for 循环，就可以根据传入的文本 <code>txt</code>
快速生成大量的<strong>输入-目标对</strong>供给模型进行训练和检验。</p>
<hr />
<blockquote>
<p>[!IMPORTANT]</p>
<p>回到 <code>prepare_train_and_val_data</code>
函数，现在我们可以用一句话概括它的全部工作：<strong>它是一个数据准备总管，负责将一本原始小说，严格划分为用于学习的训练集和用于考试的验证集，并最终将它们都加工成模型可以直接使用的、一批一批的、包含(输入-目标)对的标准化数据传送带。</strong></p>
</blockquote>
<h1 id="初始化模型与优化器">2. 初始化模型与优化器</h1>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831134830004.png" style="zoom:33%;" /></p>
<ol type="1">
<li><code>GPTModel()</code> 初始化一个 GPT 模型实例；</li>
<li><code>model.to(device)</code> 是 PyTorch
中用于将模型移动到指定设备（CPU 或 GPU）的方法，深度学习模型在 GPU
上训练速度比 CPU
快很多，通过这种方式可以确保模型和输入数据在同一个设备上。</li>
<li><code>torch.optim.AdamW()</code>
创建一个优化器（optimizer），用于训练神经网络模型。<code>AdamW</code>
是一种优化算法，在训练过程中，优化器会接收损失函数计算出的梯度、使用
AdamW
算法更新模型参数和帮助模型逐步收敛到最优解。在本篇中，我们不对这个进行过多的解释，因为这并不在我们的核心学习目标上。</li>
</ol>
<h2 id="模型配置">2.1 模型配置</h2>
<p>在深入代码细节之前，我们先看 <code>GPT_CONFIG_124M</code>
这个<strong>配置字典</strong>。它就像是建造 GPT
模型大厦的<strong>设计蓝图</strong>，定义了模型的规模和所有关键参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">GPT_CONFIG_124M = &#123;</span><br><span class="line">    <span class="string">&quot;vocab_size&quot;</span>: <span class="number">50257</span>,    <span class="comment"># 词汇表大小</span></span><br><span class="line">    <span class="string">&quot;context_length&quot;</span>: <span class="number">256</span>,  <span class="comment"># 上下文长度</span></span><br><span class="line">    <span class="string">&quot;emb_dim&quot;</span>: <span class="number">768</span>,         <span class="comment"># 嵌入维度</span></span><br><span class="line">    <span class="string">&quot;n_heads&quot;</span>: <span class="number">12</span>,          <span class="comment"># 注意力头数量</span></span><br><span class="line">    <span class="string">&quot;n_layers&quot;</span>: <span class="number">12</span>,         <span class="comment"># Transformer层数</span></span><br><span class="line">    <span class="string">&quot;drop_rate&quot;</span>: <span class="number">0.1</span>,       <span class="comment"># dropout率</span></span><br><span class="line">    <span class="string">&quot;qkv_bias&quot;</span>: <span class="literal">False</span>       <span class="comment"># QKV线性层是否使用偏置</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p><code>vocab_size</code>: 词汇表里有多少个不同的词元
(Token)。<code>50257</code> 是 GPT-2
使用的标准词汇表大小，即我们前面讨论的 BPE 分词器的词汇表大小。</p></li>
<li><p><code>context_length</code>:
模型一次能处理的<strong>最长文本长度</strong>（以词元计）。这里是
256，意味着模型一次最多能看 256 个词元。</p></li>
<li><p><code>emb_dim</code>:
<strong>嵌入维度</strong>。这是模型内部表示每个词元的向量长度。768
维意味着每个词都会被转换成一个包含 768
个数字的向量，这是模型理解语言的基础。</p></li>
<li><p><code>n_heads</code> 和 <code>n_layers</code>:
这两个参数共同决定了模型的<strong>深度和宽度</strong>。<code>n_layers=12</code>
表示我们的模型会堆叠 12 个 <code>TransformerBlock</code>，而
<code>n_heads=12</code> 表示在每个 Block 内部的注意力机制都有 12
个"头"，让模型能从多个角度分析文本。</p></li>
<li><p><code>drop_rate</code>: Dropout
比率。这是防止模型过拟合的重要技术。0.1 表示在训练时，每个神经元有 10%
的概率被临时"关闭"，迫使模型学习更鲁棒的特征表示。</p></li>
<li><p><code>qkv_bias</code>:
查询-键-值偏置。这个参数控制是否在注意力计算中添加偏置项。False
表示不使用偏置，这是 GPT-2 的设计选择，可能有助于模型的稳定性。</p></li>
</ul>
<blockquote>
<p>有些概念你可能还不认识，没关系，我们继续往下看，待会就懂了！· ·</p>
</blockquote>
<h2 id="模型结构总览">2.2 模型结构总览</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;完整的GPT模型实现&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.tok_emb = nn.Embedding(cfg[<span class="string">&quot;vocab_size&quot;</span>], cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.pos_emb = nn.Embedding(cfg[<span class="string">&quot;context_length&quot;</span>], cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.drop_emb = nn.Dropout(cfg[<span class="string">&quot;drop_rate&quot;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 堆叠多个Transformer块</span></span><br><span class="line">        <span class="variable language_">self</span>.trf_blocks = nn.Sequential(</span><br><span class="line">            *[TransformerBlock(cfg) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(cfg[<span class="string">&quot;n_layers&quot;</span>])],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.final_norm = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.out_head = nn.Linear(cfg[<span class="string">&quot;emb_dim&quot;</span>], cfg[<span class="string">&quot;vocab_size&quot;</span>], bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, in_idx</span>):</span><br><span class="line">        batch_size, seq_len = in_idx.shape</span><br><span class="line">        <span class="comment"># 词嵌入 + 位置嵌入</span></span><br><span class="line">        tok_embeds = <span class="variable language_">self</span>.tok_emb(in_idx)</span><br><span class="line">        pos_embeds = <span class="variable language_">self</span>.pos_emb(torch.arange(seq_len, device=in_idx.device))</span><br><span class="line">        x = tok_embeds + pos_embeds</span><br><span class="line">        x = <span class="variable language_">self</span>.drop_emb(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.trf_blocks(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.final_norm(x)</span><br><span class="line">        logits = <span class="variable language_">self</span>.out_head(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure>
<p>整个 GPT 模型的架构如下图所示：</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830175651213.png" /></p>
<p><code>GPTModel</code>
类是整个语言模型的顶层封装，其设计目标是构建一个<strong>端到端的、具备自回归（auto-regressive）生成能力</strong>的序列处理架构。从根本上说，任何一个此类模型都必须解决三个核心问题：</p>
<ol type="1">
<li><strong>输入表示 (Input
Representation)</strong>：如何将离散的、一维的词元 ID
序列，转化为模型能够处理的、富含信息的连续多维向量？</li>
<li><strong>上下文编码 (Contextual
Encoding)</strong>：如何对输入序列中的每个元素进行深度处理，使其向量表示能够充分融合整个序列（尤其是其上文）的上下文信息？</li>
<li><strong>输出投影 (Output
Projection)</strong>：如何将模型内部经过深度处理的上下文向量，重新映射回词汇表空间，以生成对下一个词元的概率预测？</li>
</ol>
<p><code>GPTModel</code>
的结构正是围绕这三个核心问题，划分成了三个逻辑清晰的功能区块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>):</span><br><span class="line">        <span class="comment"># 区块一：输入表示层</span></span><br><span class="line">        <span class="variable language_">self</span>.tok_emb = nn.Embedding(...)</span><br><span class="line">        <span class="variable language_">self</span>.pos_emb = nn.Embedding(...)</span><br><span class="line">        <span class="variable language_">self</span>.drop_emb = nn.Dropout(...)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 区块二：上下文编码器堆栈</span></span><br><span class="line">        <span class="variable language_">self</span>.trf_blocks = nn.Sequential(...)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 区块三：输出投影层</span></span><br><span class="line">        <span class="variable language_">self</span>.final_norm = LayerNorm(...)</span><br><span class="line">        <span class="variable language_">self</span>.out_head = nn.Linear(...)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, in_idx</span>):</span><br><span class="line">        <span class="comment"># 执行区块一的功能</span></span><br><span class="line">        tok_embeds = <span class="variable language_">self</span>.tok_emb(in_idx)</span><br><span class="line">        pos_embeds = <span class="variable language_">self</span>.pos_emb(...)</span><br><span class="line">        x = tok_embeds + pos_embeds</span><br><span class="line">        x = <span class="variable language_">self</span>.drop_emb(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 执行区块二的功能</span></span><br><span class="line">        x = <span class="variable language_">self</span>.trf_blocks(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 执行区块三的功能</span></span><br><span class="line">        x = <span class="variable language_">self</span>.final_norm(x)</span><br><span class="line">        logits = <span class="variable language_">self</span>.out_head(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure>
<h2 id="输入表示层从离散符号到情境化向量">2.3
输入表示层：从离散符号到情境化向量</h2>
<p>数据流的第一步是将输入的词元索引 <code>in_idx</code>
转换为包含位置信息的向量表示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GPTModel forward 方法的起始部分</span></span><br><span class="line">tok_embeds = <span class="variable language_">self</span>.tok_emb(in_idx)</span><br><span class="line">pos_embeds = <span class="variable language_">self</span>.pos_emb(torch.arange(seq_len, device=in_idx.device))</span><br><span class="line">x = tok_embeds + pos_embeds</span><br><span class="line">x = <span class="variable language_">self</span>.drop_emb(x)</span><br></pre></td></tr></table></figure>
<h3 id="词元嵌入">2.3.1 词元嵌入</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.tok_emb = nn.Embedding(cfg[<span class="string">&quot;vocab_size&quot;</span>], cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br></pre></td></tr></table></figure>
<p>正如前文所说的，计算机无法直接处理"单词"这样的符号。为了进行数学运算，必须将每个离散的词元映射到一个高维的连续向量空间中。这个过程被称为嵌入(Embedding)。<code>nn.Embedding</code>
是一个简单的查找表。它本质上是一个权重矩阵，维度为
<code>(vocab_size, emb_dim)</code>。输入一个词元的索引，它会返回该索引对应的行向量。这个向量是可训练的，模型在训练过程中会不断调整这些向量，使得在向量空间中语义相近的词元彼此靠近。</p>
<h3 id="位置嵌入">2.3.2 位置嵌入</h3>
<p>理论上，词元嵌入非常适合作为大语言模型的输入。然而，大语言模型存在一个小缺陷——它们的自注意力机制（见后文）无法感知词元在序列中的位置或顺序。嵌入层的工作机制是，无论词元
ID 在输入序列中的位置如何，相同的词元 ID
始终被映射到相同的向量表示，如下图所示。</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830193802429.png" /></p>
<p>举个最简单的例子："人咬狗"和"狗咬人"在上述机制看来，包含的词元集合是相同的。然而，顺序在自然语言中至关重要。</p>
<p>为了实现这一点，可以将位置信息进行嵌入，一般有以下 3
种位置嵌入方式：</p>
<ul>
<li><strong>绝对位置嵌入 (absolute positional
embedding)</strong>：直接与序列中的特定位置相关联。对于输入序列的每个位置，该方法都会向对应词元的嵌入向量中添加一个独特的位置嵌入，以明确指示其在序列中的确切位置。例如，序列中的第一个词元会有一个特定的位置嵌入，第二个词元则会有另一个不同的位置嵌入，以此类推。这种方式可以是可学习的，也可以是通过固定的数学函数（如正弦/余弦函数）生成的。</li>
<li><strong>相对位置嵌入 (relative positional
embedding)</strong>：关注的是词元之间的相对位置或距离，而非它们的绝对位置。该方法通常在计算注意力分数时，引入一个与词元间距离相关的偏置项，从而让模型学习的是词元之间的"间隔"关系，而不是它们在序列中的"具体坐标"。这种方法使得模型能够更好地适应不同长度（包括在训练过程中从未见过的长度）的序列。</li>
<li><strong>旋转位置嵌入 (Rotary Positional Embedding,
RoPE)</strong>：通过一种创新的方式将位置信息融入自注意力机制中。它并非将位置向量直接添加到词元嵌入上，而是根据词元的绝对位置，对其在注意力计算中使用的查询（Query）和键（Key）向量进行旋转。这种精妙的旋转操作使得任意两个词元之间的注意力分数，能够自然地表示出它们的相对位置关系，从而让模型在处理位置信息时既高效又具备强大的长度泛化能力。</li>
</ul>
<p>GPT-2
采用的是可学习的绝对位置嵌入，这种方式简单直接，模型可以在训练中自行学会每个位置的'坐标'信息，对于其设计的固定上下文长度（如
1024）来说已经足够有效。而像 RoPE
这样的相对位置嵌入，则在处理超长文本和提升长度泛化能力方面表现更优，因此被
Llama 等更新的模型所采用。</p>
<p>本书使用的是<strong>绝对位置嵌入</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;完整的GPT模型实现&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">				<span class="comment"># ...</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_emb = nn.Embedding(cfg[<span class="string">&quot;context_length&quot;</span>], cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, in_idx</span>):</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        pos_embeds = <span class="variable language_">self</span>.pos_emb(torch.arange(seq_len, device=in_idx.device))</span><br><span class="line">        x = tok_embeds + pos_embeds</span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<ol type="1">
<li><strong>参数初始化</strong>：<code>self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])</code>
这行代码会初始化一个权重矩阵，也称为查找表。该矩阵的维度是
<code>(context_length, emb_dim)</code>。矩阵的每一行都是一个向量，且每一行都唯一对应一个从
<code>0</code> 到 <code>context_length - 1</code>
的绝对位置索引。这些行向量是模型的可训练参数，其初始值通常是随机设定的。</li>
<li><strong>向量查找</strong>：当模型处理一个具体输入时，<code>torch.arange(seq_len)</code>
会首先生成一个包含该输入序列所有位置索引的张量 (tensor)，例如
<code>[0, 1, 2, ..., seq_len-1]</code>。随后，这个位置索引张量被传递给
<code>self.pos_emb</code>
层。该层会根据索引值，从第一步初始化的权重矩阵中，精确地查找并提取出每一行对应的位置向量，最终构成一个维度为
<code>(seq_len, emb_dim)</code> 的位置嵌入张量
<code>pos_embeds</code>。</li>
<li><strong>信息融合</strong>：<code>x = tok_embeds + pos_embeds</code>
执行向量的逐元素加法操作。此操作将代表词元语义信息的
<code>tok_embeds</code> 张量与上一步生成的位置信息
<code>pos_embeds</code> 张量合并。</li>
</ol>
<p>如下图所示：</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830195222938.png" /></p>
<h3 id="dropout-掩码">2.3.3 dropout 掩码</h3>
<p>至此，我们已经通过词元嵌入和位置嵌入的结合，得到了一个信息完备的输入向量。这个向量既包含了词元的语义信息，也明确了其在序列中的顺序，可以说是为模型准备了一份完美的"学习材料"。</p>
<p>然而，在将这份完美的材料送入 Transformer
的核心进行深度加工之前，我们还需要进行一个看似矛盾，却至关重要的操作——故意引入一些不确定性。为什么要这么做呢？<strong><u>这是为了防止模型在训练中变得过于依赖输入的每一个细节，从而陷入"死记硬背"的陷阱，也就是我们常说的"过拟合"。</u></strong>为了让模型学会从不完美的信息中也能提取核心规律，我们需要引入一种正则化技术。这正是我们接下来要讨论的
<strong>Dropout</strong>。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/1*iWQzxhVlvadk6VAJjsgXgg.png" alt="Dropout 技术示意图" style="zoom:50%;" /></p>
<p>它的主要目的是<strong>防止模型过拟合</strong>，提升模型的<strong>泛化能力</strong>。在<strong>训练期间</strong>，它会随机地将一部分输入数据置为零，迫使模型不能过度依赖于任何少数的特征，从而学习到更加鲁棒的模式。在<strong>评估和预测时</strong>，Dropout
会自动失效，不会对数据做任何改动。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;完整的GPT模型实现&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="variable language_">self</span>.drop_emb = nn.Dropout(cfg[<span class="string">&quot;drop_rate&quot;</span>])</span><br><span class="line">				<span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, in_idx</span>):</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        x = <span class="variable language_">self</span>.drop_emb(x)</span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<p>我们本次的实现总共会有 2 个地方应用到 dropout 技术：</p>
<ol type="1">
<li>在词元嵌入和位置嵌入相加之后，进入第一个 Transformer Block
之前，即上面的代码所做的事情。这是模型遇到的<strong>第一层正则化</strong>。它直接作用于融合了语义和位置信息的初始输入向量
<code>x</code>。通过随机将输入向量中的某些特征置为零，它迫使<strong>后续所有</strong>的
Transformer Block
都不能过度依赖输入向量中的任何单一维度。这相当于从源头上增加了训练难度，要求整个模型学习到对输入特征扰动不敏感的、更本质的规律。</li>
<li>在多头注意力模块内部，计算出注意力权重 <code>attn_weights</code>
并经过 softmax 归一化之后，在用它去加权 <code>Value</code>
向量之前。这种 dropout
<strong>不作用于输入向量本身，而是作用于注意力权重</strong>。注意力权重决定了在生成一个词的表示时，应该关注上下文中其他词的程度。在这里应用
dropout，会随机地将某些词与词之间的注意力连接切断（权重置为
0）。这可以防止模型在学习时走捷径，比如过度依赖于某个特定的前文词汇。它鼓励模型去考虑更广泛的上下文信息，而不是仅仅依赖几个最强的信号。（关于注意力模块，我们后面会详细讨论）</li>
</ol>
<hr />
<blockquote>
<p>[!IMPORTANT] 到目前为止，我们已经完整地剖析了 <code>GPTModel</code>
的<strong>输入表示层</strong>。我们从第一性原理出发，理解了为什么需要将离散的词元
ID，通过<strong>词元嵌入（Token
Embedding）</strong>和<strong>位置嵌入（Positional
Embedding）</strong>，转化为一个融合了<strong>语义</strong>与<strong>顺序</strong>信息的、信息完备的高维向量
<code>x</code>。</p>
<p>这个过程的本质，是将人类的符号语言，翻译成了神经网络能够进行数学运算的、结构化的内部语言。</p>
<p>我们还探讨了 <code>Dropout</code>
技术。它像一个严格的教练，通过在训练中随机遮盖部分信息，强迫模型不能死记硬背，必须学会从不完整的信息中提炼出更本质、更鲁棒的规律，从而提升其泛化能力。</p>
<p>现在，我们有了一批准备就绪、信息丰富且经过初步正则化处理的训练材料。然而，此时此刻，序列中的每一个向量虽然知道了自己是谁以及在哪，但它仍然是一个<strong>独立的、上下文无关的个体</strong>。它并不知道自己与其他词元之间存在着怎样复杂的句法和语义关联。</p>
<p>那么，模型是如何让这些孤立的向量开始"交流"，理解彼此之间的关系，并最终形成对整个序列的深度理解呢？</p>
<p>答案，就藏在 Transformer 架构的革命性核心——<strong>自注意力机制
(Self-Attention Mechanism)</strong> 之中。下面我们就来深入 LLM
中最关键的部分，探究 Transformer 架构的层层细节！</p>
</blockquote>
<h2 id="核心处理层transformer-块的堆叠">2.4 核心处理层：Transformer
块的堆叠</h2>
<p>在 <code>GPTModel</code> 中，我们共使用了 <code>n_layers</code> 个
<code>TransformerBlock</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">       	<span class="comment"># ...</span></span><br><span class="line">        <span class="comment"># 堆叠多个Transformer块</span></span><br><span class="line">        <span class="variable language_">self</span>.trf_blocks = nn.Sequential(</span><br><span class="line">            *[TransformerBlock(cfg) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(cfg[<span class="string">&quot;n_layers&quot;</span>])],</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>我们先来看 <code>TransformerBlock</code> 的结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer块：多头注意力 + 前馈网络 + 残差连接&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.att = MultiHeadAttention(</span><br><span class="line">            d_in=cfg[<span class="string">&quot;emb_dim&quot;</span>],</span><br><span class="line">            d_out=cfg[<span class="string">&quot;emb_dim&quot;</span>],</span><br><span class="line">            context_length=cfg[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">            num_heads=cfg[<span class="string">&quot;n_heads&quot;</span>],</span><br><span class="line">            dropout=cfg[<span class="string">&quot;drop_rate&quot;</span>],</span><br><span class="line">            qkv_bias=cfg[<span class="string">&quot;qkv_bias&quot;</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.ff = FeedForward(cfg)</span><br><span class="line">        <span class="variable language_">self</span>.norm1 = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.norm2 = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.drop_shortcut = nn.Dropout(cfg[<span class="string">&quot;drop_rate&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 第一个子层：多头注意力 + 残差连接</span></span><br><span class="line">        shortcut = x</span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.att(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.drop_shortcut(x)</span><br><span class="line">        x = x + shortcut</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二个子层：前馈网络 + 残差连接</span></span><br><span class="line">        shortcut = x</span><br><span class="line">        x = <span class="variable language_">self</span>.norm2(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.ff(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.drop_shortcut(x)</span><br><span class="line">        x = x + shortcut</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>它的结构示意图如下所示：</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830202334178.png" /></p>
<h3 id="注意力机制">2.4.1 注意力机制</h3>
<p>深入探讨大语言模型核心的自注意力机制之前，让我们考虑一下在大语言模型出现之前的没有注意力机制的架构中所存在的问题。假设我们想要开发一个将文本从一种语言翻译成另一种语言的语言翻译模型。如下图所示，由于源语言和目标语言的语法结构不同，我们无法简单地逐个单词进行翻译。</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830202911825.png" /></p>
<p>这正是传统的序列处理模型（如
RNN）一个根本缺陷的体现：<strong>信息瓶颈</strong>。它们通过一个循环结构顺序处理文本，导致序列末端的信息很难直接关联到序列开头的遥远信息。</p>
<blockquote>
<p>[!IMPORTANT]</p>
<p><strong>自注意力机制 (Self-Attention)</strong>
的提出，正是为了打破这种信息瓶颈。其根本思想是：<strong>为序列中的每个元素，建立与其他所有元素的直接连接，并动态计算这些连接的强度（即注意力权重）</strong>。这样，模型在处理任何一个词元时，都能拥有一个全局视野，直接审视并借鉴整个上下文。</p>
</blockquote>
<p>在 <code>TransformerBlock</code>
的内部，<code>MultiHeadAttention</code>
模块是其第一个、也是最为关键的子层。它是整个 GPT
模型"智能"的根本来源。要理解它，我们不能一蹴而就。很幸运的是，《从零构建大语言模型》的作者
Sebastian
Raschka，为我们提供了一条从简单到复杂的演进路径，如下图所示，这部分的内容非常精华且重要，所以笔者将尽可能将这部分的内容进行完整记录，以帮助读者们更好的理解自注意力机制。</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830203719560.png" /></p>
<h4 id="没有可训练权重的简单自注意力机制">2.4.1.1
没有可训练权重的简单自注意力机制</h4>
<p>在深入研究包含可训练权重的复杂版本之前，书中首先实现了一个不含任何可训练权重的简化自注意力机制，以便阐明其核心概念
。</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830204044917.png" /></p>
<p>如上图所示，这个机制的目标是为输入序列中的每一个词元（Token），计算出一个<strong>上下文向量（Context
Vector）</strong>
。这个上下文向量是一种增强版的嵌入，它不仅包含了当前词元自身的信息，还融合了序列中所有其他词元的信息
。这对于理解句子中单词间的关系至关重要 。</p>
<p>计算这个上下文向量的过程分为三步：</p>
<ol type="1">
<li><strong>计算注意力分数</strong>：衡量每个词对其他词的"相关性"或"相似度"。计算每对词之间的点积，得到相似度分数。点积在这里可以被看作是一种衡量相似度的方式：两个向量的点积越大，代表它们之间的对齐程度或相似度越高，注意力分数也越高
。</li>
<li><strong>归一化获取注意力权重</strong>：得到的注意力分数是一些原始的数值，它们的尺度不一。为了使其规范化并易于解释，我们使用
<strong>Softmax 函数</strong>对这些分数进行处理 。Softmax
函数能将一组任意实数转换为一个概率分布，确保所有输出值的和为
1，并且每个值都是正数。这样得到的数值就是"注意力权重"，代表了在当前查询下，序列中每个词元的重要性
。</li>
<li><strong>计算上下文向量</strong>：最后一步，将序列中的每一个词元嵌入向量与其对应的注意力权重相乘，然后将所有结果向量相加
。最终得到的向量就是我们想要的上下文向量，它是整个输入序列的加权和，权重由刚刚计算出的注意力权重决定
。</li>
</ol>
<p>这 3 个步骤实现了自注意力机制的核心思想：</p>
<ul>
<li>看：计算每个词对其他词的关注度</li>
<li>权衡：将关注度转换为权重</li>
<li>融合：根据权重融合所有词的信息</li>
</ul>
<p>最终效果：
每个词的向量表示都包含了整个序列的上下文信息，而不仅仅是自己的信息。这样模型就能理解词与词之间的关系，比如
<code>"journey starts"</code> 中的 <code>"starts"</code> 会更多地关注
<code>"journey"</code> 的信息。</p>
<p>现在我们用代码来演示一下，假设我们有以下输入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.tensor(</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">0.43</span>, <span class="number">0.15</span>, <span class="number">0.89</span>],   <span class="comment"># Your</span></span><br><span class="line">        [<span class="number">0.55</span>, <span class="number">0.87</span>, <span class="number">0.66</span>],   <span class="comment"># journey</span></span><br><span class="line">        [<span class="number">0.57</span>, <span class="number">0.85</span>, <span class="number">0.64</span>],   <span class="comment"># starts</span></span><br><span class="line">        [<span class="number">0.22</span>, <span class="number">0.58</span>, <span class="number">0.33</span>],   <span class="comment"># with</span></span><br><span class="line">        [<span class="number">0.77</span>, <span class="number">0.25</span>, <span class="number">0.10</span>],   <span class="comment"># one</span></span><br><span class="line">        [<span class="number">0.05</span>, <span class="number">0.80</span>, <span class="number">0.55</span>],   <span class="comment"># step</span></span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>笔者将尝试从第一性原理出发，对代码进行一步步拆解，说明白这个过程为什么能够实现我们期望的目标：<strong>为每个词元（Token）生成一个包含了上下文信息的向量</strong>。</p>
<p>这个问题的核心在于，我们要证明<strong>最终的上下文向量 (Context
Vector)
的确融合了其他词元的信息，并且是根据"相关性"来融合的</strong>。</p>
<p>我们将以你例子中的词 <code>starts</code> (第三个词元)
为例，来全程追踪它的变化。</p>
<p><code>starts</code> 的原始输入向量是:
<code>[0.57, 0.85, 0.64]</code>。这个向量只代表 <code>starts</code>
本身，它对句子中的其他词一无所知，是孤立的。<u>我们的目标是生成一个新的向量，让这个新的向量知道它前面有
<code>Your journey</code>，后面有 <code>with one step</code>。</u></p>
<p>第一步我们计算注意力分数，是为了发现"谁与我最相关"：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">attn_scores = torch.empty(<span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line"><span class="keyword">for</span> i, x_i <span class="keyword">in</span> <span class="built_in">enumerate</span>(inputs):</span><br><span class="line">    <span class="keyword">for</span> j, x_j <span class="keyword">in</span> <span class="built_in">enumerate</span>(inputs):</span><br><span class="line">        attn_scores[i, j] = torch.dot(x_i, x_j)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line"><span class="comment"># tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],</span></span><br><span class="line"><span class="comment">#         [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],</span></span><br><span class="line"><span class="comment">#         [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],</span></span><br><span class="line"><span class="comment">#         [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],</span></span><br><span class="line"><span class="comment">#         [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],</span></span><br><span class="line"><span class="comment">#         [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])</span></span><br></pre></td></tr></table></figure>
<p><code>attn_scores</code> 的第 3
行是：<code>[0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605]</code>。这行数字告诉我们，<code>starts</code>
这个词与句子中每个词的原始相关性得分分别是：</p>
<ul>
<li>与 <code>Your</code> 的相关性: <code>0.9422</code></li>
<li>与 <code>journey</code> 的相关性: <code>1.4754</code> &lt;--
<strong>非常高</strong></li>
<li>与 <code>starts</code> (自身) 的相关性: <code>1.4570</code> &lt;--
<strong>非常高</strong></li>
<li>与 <code>with</code> 的相关性: <code>0.8296</code></li>
<li>与 <code>one</code> 的相关性: <code>0.7154</code></li>
<li>与 <code>step</code> 的相关性: <code>1.0605</code></li>
</ul>
<p>仅从这一步看，这个机制已经成功地从数学上<strong>发现</strong>了
<code>starts</code> 与 <code>journey</code>
之间的紧密关系，因为它们的点积分数是最高的之一。这完全符合我们对语言的直觉（"旅程"和"开始"在语义上强相关）。</p>
<p>第一步得到的分数是原始的、未经缩放的数值，不易于作为权重使用。比如
<code>1.4754</code>
究竟代表多大的重要性？我们无法直接判断。所以第二步就是进行归一化获取注意力权重：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. 归一化获取注意力权重</span></span><br><span class="line">attn_weights = torch.softmax(attn_scores, dim=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(attn_weights)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -1 表示在最后一个维度进行归一化，因为 attn_scores 是一个 [行, 列]，所以这里是在列上进行归一化，使得每行的值（在列维度的总和）为 1</span></span><br><span class="line"><span class="comment"># &quot;沿着列的方向&quot; = 在每一行内部，从左到右（列 0 到列 5）进行归一化</span></span><br><span class="line"><span class="comment"># 每一行都独立进行这个过程，结果每一行的 6 个数字加起来都等于 1</span></span><br><span class="line">row_2_sum = <span class="built_in">sum</span>([<span class="number">0.1385</span>, <span class="number">0.2379</span>, <span class="number">0.2333</span>, <span class="number">0.1240</span>, <span class="number">0.1082</span>, <span class="number">0.1581</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Row 2 sum:&quot;</span>, row_2_sum)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;All row sums:&quot;</span>, attn_weights.<span class="built_in">sum</span>(dim=-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line"><span class="comment"># tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],</span></span><br><span class="line"><span class="comment">#         [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],</span></span><br><span class="line"><span class="comment">#         [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],</span></span><br><span class="line"><span class="comment">#         [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],</span></span><br><span class="line"><span class="comment">#         [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],</span></span><br><span class="line"><span class="comment">#         [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])</span></span><br><span class="line"><span class="comment"># Row 2 sum: 1.0</span></span><br><span class="line"><span class="comment"># All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])</span></span><br></pre></td></tr></table></figure>
<p>现在，这些数字的意义变得非常清晰了： 当模型在处理 <code>starts</code>
这个词时，它应该将它的注意力这样分配：</p>
<ul>
<li><code>13.90%</code> 的注意力给 <code>Your</code></li>
<li><code>23.69%</code> 的注意力给 <code>journey</code> &lt;--
<strong>权重最高</strong></li>
<li><code>23.26%</code> 的注意力给 <code>starts</code> (自身)</li>
<li><code>12.42%</code> 的注意力给 <code>with</code></li>
<li><code>11.08%</code> 的注意力给 <code>one</code></li>
<li><code>15.65%</code> 的注意力给 <code>step</code></li>
</ul>
<p>这一步将第一步发现的相关性，转化为了具体的、可操作的重要性权重。它明确地告诉我们，为了理解
<code>starts</code>，我们需要重点参考 <code>journey</code> 和
<code>starts</code> 自身的信息。</p>
<p>接下来，这是最关键的一步，我们终于要创造那个"增强版"的向量（上下文向量）了。我们的目标是为词
<code>i</code> (例如 <code>starts</code>)
创建一个<strong>新的表示</strong> <span
class="math inline">\(C_i\)</span>。这个新的表示 <span
class="math inline">\(C_i\)</span> 必须满足两个条件：</p>
<ol type="1">
<li>它必须包含<strong>所有</strong>其他词 <code>j</code> (从
<code>Your</code> 到 <code>step</code>) 的信息。</li>
<li>每个词 <code>j</code>
贡献的信息量，应该由我们刚刚算出的<strong>注意力权重</strong> <span
class="math inline">\({w_{ij}}\)</span>
来决定。权重越高的词，影响越大。</li>
</ol>
<p>现在，让我们思考一下，在数学上，特别是向量空间中，有什么运算可以同时满足这两个条件？<strong>答案就是加权平均
(Weighted Average) 或 加权和 (Weighted Sum)。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. 计算上下文向量</span></span><br><span class="line">all_contexts_vec = attn_weights @ inputs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line"><span class="comment"># tensor([[0.4421, 0.5931, 0.5790],</span></span><br><span class="line"><span class="comment">#         [0.4419, 0.6515, 0.5683],</span></span><br><span class="line"><span class="comment">#         [0.4431, 0.6496, 0.5671],</span></span><br><span class="line"><span class="comment">#         [0.4304, 0.6298, 0.5510],</span></span><br><span class="line"><span class="comment">#         [0.4671, 0.5910, 0.5266],</span></span><br><span class="line"><span class="comment">#         [0.4177, 0.6503, 0.5645]])</span></span><br></pre></td></tr></table></figure>
<p>矩阵乘法 <code>attn_weights @ inputs</code>
是一种非常高效的、一次性为所有词计算加权求和的方式。本质上是通过以下方式计算的：</p>
<p><span class="math display">\[
C_i = \sum_{j=1}^N w_{ij} \cdot V_j
\]</span></p>
<p>其中，<span class="math inline">\(w_{ij}\)</span> 是词 i 对词 j
的注意力权重，<span class="math inline">\(V_j\)</span> 是词 j
的原始向量。</p>
<p>现在我们来看这个公式的两个部分：</p>
<p><strong>第一部分：<span class="math inline">\(w_{ij}\cdot
V{j}\)</span>
（缩放）</strong>：通过这一步，我们为句子中的<strong>每一个词</strong>，都生成了一个<strong>待贡献</strong>的向量。这个向量的意义和原始词一样，但它的影响力已经被其对应的注意力权重精确地调整好了。</p>
<p><strong>第二部分：∑j=1N
（求和）</strong>：这一步是把所有这些待贡献的向量<strong>全部加起来</strong>：<span
class="math inline">\(C_{starts}=(w_{s,y}⋅V_y)+(w_{s,j}⋅V_j)+(w_{s,s}⋅V_s)+…\)</span>。这一步的几何意义是：在那个高维的意义空间里，我们从原点出发，先沿着加强版
<code>journey</code> 向量走一段，再接着走削弱版
<code>one</code>向量的方向，再走加强版 <code>starts</code>
自身的方向......
把所有词的贡献都走完，最终到达的那个<strong>新的位置</strong>，就是我们的上下文向量
<span class="math inline">\(C_{starts}\)</span>。</p>
<p>这个三步过程之所以能起到我们想要的作用，是因为它完美地模拟了人类理解语言的一个核心逻辑：</p>
<ol type="1">
<li><strong>关注焦点</strong>：当我们读到一个词时，我们会本能地寻找与它最相关的词。这个机制通过计算点积，<strong>找到了</strong>这些相关词。</li>
<li><strong>分配精力</strong>：我们不会对所有相关的词都投入相同的精力。这个机制通过
Softmax，将"相关性"转化为"重要性"权重，<strong>量化了</strong>应该投入多少精力。</li>
<li><strong>综合理解</strong>：我们基于这些焦点和精力分配，在大脑中形成对当前词的综合理解。这个机制通过加权求和，将所有词的信息根据重要性权重<strong>融合</strong>在一起，生成了最终的上下文向量。</li>
</ol>
<p>通过这个过程，输出的每一个向量都从"<u>我是谁</u>"的孤立状态，变成了"<u>在这样一个句子里，我是谁</u>"的上下文感知状态，从而实现了我们的最终目标。</p>
<h4 id="实现带可训练权重的自注意力机制">2.4.1.2
实现带可训练权重的自注意力机制</h4>
<p>在上个阶段，<strong>注意力机制本身没有自己独立的、可以在训练中被优化的参数</strong>。模型在训练时，虽然可以学习和调整输入的
<code>x</code>
向量（即词嵌入本身），但它<strong>无法学习如何更好地计算注意力</strong>。无论输入的向量如何变化，计算注意力的公式始终不变。而且这种方式过于僵化。一个词元的向量表示
<code>x</code>
需要同时承载多种信息，它既要代表自身的语义，又要能很好地跟其他词元的向量进行点积来判断相关性。这就像要求一个人同时扮演运动员和裁判员，角色发生了混淆，难以做到最优。</p>
<p>所以第二个版本的根本问题就是：<u>如何让模型<strong>学会</strong>去关注什么？如何让相似度的计算方式本身变得<strong>灵活和强大</strong>？</u></p>
<p>解决方案是引入角色分工，让专业的角色做专业的事。第二个版本中，我们不再直接使用原始的输入向量
<code>x</code>，而是引入三个独立的可训练线性变换层
(<code>nn.Linear</code>)：<strong>Query (Q)</strong>、<strong>Key
(K)</strong> 和 <strong>Value (V)</strong>。</p>
<ul>
<li><strong>查询向量
(Query)</strong>：代表当前这个词，主动去查询句子中其他词与自己的关系。可以理解为：我
(starts) 是谁？</li>
<li><strong>键向量
(Key)</strong>：代表句子中的每个词，用来被其他词查询的。可以理解为：我是
(journey)，你可以通过这个‘键’来了解我。</li>
<li><strong>值向量
(Value)</strong>：代表句子中每个词所携带的真正信息。一旦查询完毕，确定了关系密切度，我们就从这个值中提取信息。</li>
</ul>
<p>至关重要的是，这三个权重矩阵 <span
class="math inline">\(W_q\)</span>、<span
class="math inline">\(W_k\)</span>、<span
class="math inline">\(W_v\)</span>
是<strong>可训练的</strong>。这意味着在训练过程中，模型会不断优化它们，学会如何将原始输入
<code>x</code> 转换成最有效的 Q、K 和
V，从而学会<strong>如何更好地去关注</strong>，让注意力的计算本身变得灵活而强大
。（所以才称这个版本是带可训练权重的自注意力机制）</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/1*moKYjUdtx-uEyYMbhPWbIw.png"
alt="Linear transformation of the word embedding to obtain Query, Key, and Value vectors — From(https://epichka.com/blog/2023/qkv-transformer/)" />
<figcaption aria-hidden="true">Linear transformation of the word
embedding to obtain Query, Key, and Value vectors —
From(https://epichka.com/blog/2023/qkv-transformer/)</figcaption>
</figure>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttention_v2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out, qkv_bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        d_in (int): 输入向量的维度。</span></span><br><span class="line"><span class="string">        d_out (int): 查询(Query)、键(Key)和值(Value)向量的输出维度。</span></span><br><span class="line"><span class="string">        qkv_bias (bool): 是否为Q, K, V线性层添加偏置项。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 使用 nn.Linear 层来定义 Q, K, V 的线性变换。</span></span><br><span class="line">        <span class="comment"># 相比手动创建 nn.Parameter，nn.Linear 提供了更优的权重初始化，并可选择性地包含偏置项，</span></span><br><span class="line">        <span class="comment"># 是 PyTorch 中实现线性变换的标准做法。</span></span><br><span class="line">        <span class="variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x (torch.Tensor): 输入张量，形状为 [批量大小, 序列长度, d_in]。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. 线性投影：将输入 x 转换为 Q, K, V 表示</span></span><br><span class="line">        <span class="comment"># 每个输入词元的向量都会通过独立的线性层，生成其在查询、键、值三个空间中的新表示。</span></span><br><span class="line">        keys = <span class="variable language_">self</span>.W_key(x)      <span class="comment"># 形状: [批量大小, 序列长度, d_out]</span></span><br><span class="line">        queries = <span class="variable language_">self</span>.W_query(x)  <span class="comment"># 形状: [批量大小, 序列长度, d_out]</span></span><br><span class="line">        values = <span class="variable language_">self</span>.W_value(x)    <span class="comment"># 形状: [批量大小, 序列长度, d_out]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 计算注意力分数：通过点积衡量 Query 和 Key 的相似度</span></span><br><span class="line">        <span class="comment"># queries 与 keys 的转置 (.T) 进行矩阵相乘，得到一个注意力分数矩阵。</span></span><br><span class="line">        <span class="comment"># 矩阵中的每个元素 attn_scores[i, j] 代表第 i 个查询与第 j 个键之间的原始相关性。</span></span><br><span class="line">        attn_scores = queries @ keys.T</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 缩放与归一化：将分数转换为最终的注意力权重</span></span><br><span class="line">        <span class="comment"># a. 缩放(Scaling): 将分数除以 key 向量维度的平方根。这一步对于稳定训练至关重要，</span></span><br><span class="line">        <span class="comment">#    可以防止在维度过高时，点积结果过大导致 softmax 梯度消失。</span></span><br><span class="line">        <span class="comment"># b. 归一化(Normalization): 使用 softmax 函数将缩放后的分数转换为概率分布，</span></span><br><span class="line">        <span class="comment">#    确保每一行（代表每个查询）的注意力权重总和为1。</span></span><br><span class="line">        attn_weights = torch.softmax(</span><br><span class="line">            attn_scores / keys.shape[-<span class="number">1</span>] ** <span class="number">0.5</span>, dim=-<span class="number">1</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. 计算上下文向量：对 Value 向量进行加权求和</span></span><br><span class="line">        <span class="comment"># 将上一步得到的注意力权重矩阵与 values 矩阵相乘。</span></span><br><span class="line">        <span class="comment"># 这一步是根据注意力权重，对所有词元的 Value 信息进行加权聚合，</span></span><br><span class="line">        <span class="comment"># 最终为每个词元生成一个融合了全局上下文信息的新向量。</span></span><br><span class="line">        context_vec = attn_weights @ values</span><br><span class="line">        <span class="keyword">return</span> context_vec</span><br></pre></td></tr></table></figure>
<p>大体流程可参考下图理解：</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/1*6BEwO4jKy9UC7AzU6nIPPg.png"
alt="Dot-product attention procedure —From(https://epichka.com/blog/2023/qkv-transformer/)" />
<figcaption aria-hidden="true">Dot-product attention procedure
—From(https://epichka.com/blog/2023/qkv-transformer/)</figcaption>
</figure>
<blockquote>
<p>[!NOTE]</p>
<p><strong>缩放点积注意力的原理</strong>：对嵌入维度进行归一化是为了避免梯度过小，从而提升训练性能。例如，在类
GPT 大语言模型中，嵌入维度通常大于
1000，这可能导致点积非常大，从而在反向传播时由于 softmax
函数的作用导致梯度非常小。当点积增大时，softmax
函数会表现得更像阶跃函数，导致梯度接近零。这些小梯度可能会显著减慢学习速度或使训练停滞。</p>
<p>因此，通过嵌入维度的平方根进行缩放解释了为什么这种自注意力机制也被称为缩放点积注意力机制。</p>
</blockquote>
<h4 id="利用因果注意力隐藏未来词汇">2.4.1.3
利用因果注意力隐藏未来词汇</h4>
<p>对于像 GPT
这样用于文本生成的模型，有一个核心要求：<u>在预测序列中的下一个词元时，模型只能看到当前位置及之前的信息，绝不能偷看未来的词元。</u>标准的自注意力机制会一次性访问整个输入序列，这显然不符合要求。为了解决这个问题，我们引入了<strong>因果注意力（Causal
Attention）</strong>，也称为掩码注意力（Masked Attention）。</p>
<p>实现因果注意力的关键在于<strong>掩码（Masking）</strong>操作。具体做法是在计算出注意力分数之后、应用
Softmax 函数之前，对注意力分数矩阵进行修改
。我们会创建一个"上三角"掩码矩阵，其中主对角线及以下的元素为
0，而主对角线以上的元素为负无穷大 (<code>-inf</code>) 。</p>
<p>当这个掩码矩阵被加到注意力分数矩阵上时，所有代表"未来"位置的分数都会变成负无穷大
。经过 Softmax 函数处理后，这些负无穷大的值对应的概率会变为 0
。这样一来，任何词元在计算其上下文向量时，其注意力权重都只会分布在它自身及之前的位置上，从而有效地隐藏了未来的词汇
。</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830233836610.png" /></p>
<p>代码实现逻辑如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># diagonal=1 表示不包含主对角线，只包含上三角部分</span></span><br><span class="line">mask = torch.triu(torch.ones(context_length, context_length), diagonal=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(mask)</span><br><span class="line"><span class="comment"># masked_fill() 用指定值填充掩码为True的位置</span></span><br><span class="line">masked = attn_scores.masked_fill(mask.<span class="built_in">bool</span>(), -torch.inf)</span><br><span class="line"><span class="built_in">print</span>(masked)</span><br><span class="line"><span class="comment"># masked / keys.shape[-1]**0.5 进行缩放，torch.softmax 进行归一化</span></span><br><span class="line">attn_weights = torch.softmax(masked / keys.shape[-<span class="number">1</span>]**<span class="number">0.5</span>, dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(attn_weights)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line">tensor([[-<span class="number">0.0763</span>,    -inf,    -inf,    -inf,    -inf,    -inf],</span><br><span class="line">        [-<span class="number">0.0408</span>,  <span class="number">0.0038</span>,    -inf,    -inf,    -inf,    -inf],</span><br><span class="line">        [-<span class="number">0.0423</span>,  <span class="number">0.0025</span>,  <span class="number">0.0074</span>,    -inf,    -inf,    -inf],</span><br><span class="line">        [-<span class="number">0.0090</span>,  <span class="number">0.0404</span>,  <span class="number">0.0416</span>,  <span class="number">0.0233</span>,    -inf,    -inf],</span><br><span class="line">        [-<span class="number">0.0586</span>, -<span class="number">0.0207</span>, -<span class="number">0.0141</span>, -<span class="number">0.0227</span>,  <span class="number">0.1097</span>,    -inf],</span><br><span class="line">        [ <span class="number">0.0040</span>,  <span class="number">0.0504</span>,  <span class="number">0.0502</span>,  <span class="number">0.0317</span>,  <span class="number">0.0320</span>,  <span class="number">0.0354</span>]],</span><br><span class="line">       grad_fn=&lt;MaskedFillBackward0&gt;)</span><br><span class="line">tensor([[<span class="number">1.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.4921</span>, <span class="number">0.5079</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.3259</span>, <span class="number">0.3364</span>, <span class="number">0.3376</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.2442</span>, <span class="number">0.2529</span>, <span class="number">0.2531</span>, <span class="number">0.2498</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.1919</span>, <span class="number">0.1971</span>, <span class="number">0.1980</span>, <span class="number">0.1968</span>, <span class="number">0.2161</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.1632</span>, <span class="number">0.1686</span>, <span class="number">0.1686</span>, <span class="number">0.1664</span>, <span class="number">0.1664</span>, <span class="number">0.1668</span>]],</span><br><span class="line">       grad_fn=&lt;SoftmaxBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>此外，为了防止模型在训练中过拟合，还可以在注意力权重矩阵上应用我们前面提到的
<strong>Dropout</strong>
技术。即在训练过程中，随机地将一部分注意力权重置为零，这有助于增强模型的泛化能力
。</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830234226162.png" /></p>
<p>代码实现逻辑如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dropout = torch.nn.Dropout(<span class="number">0.5</span>) <span class="comment"># 使用 50% 的 dropout 率</span></span><br><span class="line">example = torch.ones(<span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(example)</span><br><span class="line"><span class="built_in">print</span>(dropout(example))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对权重矩阵进行 dropout</span></span><br><span class="line"><span class="built_in">print</span>(dropout(attn_weights))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line">tensor([[<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">0.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">2.</span>, <span class="number">0.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">0.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">2.</span>, <span class="number">0.</span>, <span class="number">2.</span>, <span class="number">0.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">0.</span>]])</span><br><span class="line">tensor([[<span class="number">2.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.0000</span>, <span class="number">0.6790</span>, <span class="number">0.6818</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.5059</span>, <span class="number">0.5040</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.4336</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.3242</span>, <span class="number">0.3345</span>, <span class="number">0.0000</span>, <span class="number">0.3339</span>, <span class="number">0.3433</span>, <span class="number">0.3292</span>]],</span><br><span class="line">       grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>可以看到大约一半的值被置为
0，且原来的值被放大了，用于位置权重的整体平衡。</p>
<p>一个完整的简单因果注意力类的参考实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CausalAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    一个实现了因果自注意力（Causal Self-Attention）的模块。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    因果特性确保在处理序列中的任何一个词元（token）时，</span></span><br><span class="line"><span class="string">    注意力机制只能关注到当前位置及之前位置的词元，而不能看到未来的词元。</span></span><br><span class="line"><span class="string">    这对于自回归（auto-regressive）的文本生成任务至关重要。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out, context_length, dropout, qkv_bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化方法。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">        d_in (int): 输入嵌入向量的维度。</span></span><br><span class="line"><span class="string">        d_out (int): 查询(Query)、键(Key)和值(Value)向量的输出维度。</span></span><br><span class="line"><span class="string">        context_length (int): 模型的最大序列长度，用于创建因果掩码。</span></span><br><span class="line"><span class="string">        dropout (float): 应用于注意力权重矩阵的 dropout 比率。</span></span><br><span class="line"><span class="string">        qkv_bias (bool): 是否为 Q, K, V 的线性层添加偏置项。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d_out = d_out</span><br><span class="line">        <span class="comment"># 定义用于生成 Query, Key, Value 的线性变换层</span></span><br><span class="line">        <span class="variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="comment"># 定义 Dropout 层，用于正则化，防止过拟合</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># 创建并注册因果掩码（causal mask）</span></span><br><span class="line">        <span class="comment"># register_buffer 将一个张量注册为模块的缓冲区，它不会被视为模型参数（即不会在训练中被更新），</span></span><br><span class="line">        <span class="comment"># 但会随着模型移动（例如，.to(device)）。</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(</span><br><span class="line">            <span class="string">&#x27;mask&#x27;</span>,</span><br><span class="line">            <span class="comment"># torch.triu 创建一个上三角矩阵。diagonal=1 表示主对角线（及以下）的元素都为0，</span></span><br><span class="line">            <span class="comment"># 只有主对角线上方的元素为1。这个矩阵用于屏蔽未来的位置。</span></span><br><span class="line">            torch.triu(torch.ones(context_length, context_length), diagonal=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        执行前向传播。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">        x (torch.Tensor): 输入张量，形状为 [批量大小, 序列长度, 输入嵌入维度 d_in]。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 获取输入的维度信息</span></span><br><span class="line">        b, num_tokens, d_in = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. 将输入 x 投影到 Query, Key, Value 空间</span></span><br><span class="line">        keys = <span class="variable language_">self</span>.W_key(x)      <span class="comment"># 输出形状: [b, num_tokens, d_out]</span></span><br><span class="line">        queries = <span class="variable language_">self</span>.W_query(x) <span class="comment"># 输出形状: [b, num_tokens, d_out]</span></span><br><span class="line">        values = <span class="variable language_">self</span>.W_value(x)  <span class="comment"># 输出形状: [b, num_tokens, d_out]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 计算注意力分数</span></span><br><span class="line">        <span class="comment"># 将 keys 的最后两个维度转置，以便进行矩阵乘法</span></span><br><span class="line">        <span class="comment"># 形状从 [b, num_tokens, d_out] 变为 [b, d_out, num_tokens]</span></span><br><span class="line">        <span class="comment"># queries @ keys.transpose(...) 计算每个查询与所有键的点积</span></span><br><span class="line">        attn_scores = queries @ keys.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 输出形状: [b, num_tokens, num_tokens]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 应用因果掩码</span></span><br><span class="line">        <span class="comment"># masked_fill_ 是一个原地操作（in-place），它会直接修改 attn_scores 张量</span></span><br><span class="line">        <span class="comment"># self.mask.bool()[:num_tokens, :num_tokens] 会选取与当前输入序列长度匹配的掩码部分</span></span><br><span class="line">        <span class="comment"># 并将所有需要屏蔽的“未来”位置的分数填充为负无穷大</span></span><br><span class="line">        attn_scores.masked_fill_(</span><br><span class="line">            <span class="variable language_">self</span>.mask.<span class="built_in">bool</span>()[:num_tokens, :num_tokens], -torch.inf</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. 缩放分数并应用 softmax 得到注意力权重</span></span><br><span class="line">        <span class="comment"># a. 缩放 (Scaling): 除以 key 维度的平方根，稳定梯度</span></span><br><span class="line">        <span class="comment"># b. Softmax: 将分数转换为概率分布。由于未来位置的分数是负无穷，</span></span><br><span class="line">        <span class="comment">#    经过 softmax 后它们的权重将变为0。</span></span><br><span class="line">        attn_weights = torch.softmax(</span><br><span class="line">            attn_scores / keys.shape[-<span class="number">1</span>]**<span class="number">0.5</span>, dim=-<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. 应用 Dropout</span></span><br><span class="line">        <span class="comment"># 在训练阶段，随机将一些注意力权重置为0，以防止过拟合</span></span><br><span class="line">        attn_weights = <span class="variable language_">self</span>.dropout(attn_weights)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 6. 计算上下文向量</span></span><br><span class="line">        <span class="comment"># 将注意力权重与 value 向量相乘，得到加权和</span></span><br><span class="line">        context_vec = attn_weights @ values <span class="comment"># 输出形状: [b, num_tokens, d_out]</span></span><br><span class="line">        <span class="keyword">return</span> context_vec</span><br></pre></td></tr></table></figure>
<h4 id="将单头注意力扩展到多头注意力">2.4.1.4
将单头注意力扩展到多头注意力</h4>
<p>虽然带有可训练权重的因果自注意力机制已经非常强大，但它仍然有局限性：<u>模型在某个位置只能学习到一种注意力模式</u>。为了让模型能够从不同角度、不同表示子空间共同关注信息，原始
Transformer 论文引入了<strong>多头注意力（Multi-Head
Attention）</strong>机制 。</p>
<p>"多头"的核心思想是并行地运行多次注意力计算，而不是只进行一次
。具体实现如下：</p>
<ol type="1">
<li><strong>分割成多个头</strong>：我们不再只有一组 <span
class="math inline">\(W_q\)</span>、<span
class="math inline">\(W_k\)</span>、<span
class="math inline">\(W_v\)</span>
权重矩阵，而是为每个头都创建一组独立的权重矩阵 。例如，如果我们有 12
个头，那我们就有 12 组这样的矩阵。</li>
<li><strong>并行计算注意力</strong>：每个头都独立地对输入执行缩放点积注意力计算（包含因果掩码）。由于每个头拥有不同的权重矩阵，它们会将输入投影到不同的表示子空间，从而学习到输入序列的不同方面特征
。例如，一个头可能关注语法结构，另一个头可能关注语义关联。</li>
<li><strong>拼接与投影</strong>：在所有头都完成计算后，我们会得到多个输出上下文向量。我们将这些向量<strong>拼接（concatenate）</strong>在一起，形成一个更长的向量
。</li>
<li><strong>最终线性投影</strong>：最后，这个拼接后的长向量会通过一个额外的线性层（<code>out_proj</code>）进行投影，将其维度恢复到模型期望的维度，并融合所有头学习到的信息
。</li>
</ol>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830235007200.png" /></p>
<p>在代码中，可以通过实现一个简单的
<code>MultiHeadAttentionWrapper</code>
类来达到这一目标，<code>MultiHeadAttentionWrapper</code>
类堆叠了多个之前实现的 <code>CausalAttention</code> 模块实例。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttentionWrapper</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;一个实现多头注意力的封装类&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out, context_length,</span></span><br><span class="line"><span class="params">                dropout, num_heads, qkv_bias=<span class="literal">False</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.heads = nn.ModuleList(  <span class="comment"># 堆叠多个 CausalAttention</span></span><br><span class="line">            [CausalAttention(</span><br><span class="line">                d_in, d_out, context_length, dropout, qkv_bias,</span><br><span class="line">            ) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> (num_heads)]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.cat([head(x) <span class="keyword">for</span> head <span class="keyword">in</span> <span class="variable language_">self</span>.heads], dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>书中还提到了一种更高效的实现方式：与其创建多组独立的权重矩阵，不如创建一个更大的权重矩阵，一次性完成对所有头的查询、键、值向量的计算，然后通过重塑（reshape）和转置（transpose）操作将结果分割成多个头
。这种方法在数学上是等价的，但利用了现代硬件进行大规模矩阵运算的优势，计算效率更高。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">输入: [2, 6, 3]</span></span><br><span class="line"><span class="string">    ↓</span></span><br><span class="line"><span class="string">Q,K,V: [2, 6, 2]</span></span><br><span class="line"><span class="string">    ↓</span></span><br><span class="line"><span class="string">重塑: [2, 6, 2, 1] (2个头，每个头1维)</span></span><br><span class="line"><span class="string">    ↓</span></span><br><span class="line"><span class="string">转置: [2, 2, 6, 1] (2个头并行计算)</span></span><br><span class="line"><span class="string">    ↓</span></span><br><span class="line"><span class="string">注意力: [2, 2, 6, 6] (每个头有自己的注意力矩阵)</span></span><br><span class="line"><span class="string">    ↓</span></span><br><span class="line"><span class="string">上下文: [2, 2, 6, 1] (每个头的结果)</span></span><br><span class="line"><span class="string">    ↓</span></span><br><span class="line"><span class="string">合并: [2, 6, 2] (所有头的结果合并)</span></span><br><span class="line"><span class="string">    ↓</span></span><br><span class="line"><span class="string">输出投影: [2, 6, 2]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;一个高效的多头注意力类&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in: <span class="built_in">int</span>, d_out: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                context_length: <span class="built_in">int</span>, dropout: <span class="built_in">float</span>, num_heads: <span class="built_in">int</span>, qkv_bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> (d_out % num_heads == <span class="number">0</span>), <span class="string">&quot;d_out must be divisible by num_heads&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.d_out = d_out  <span class="comment"># 输出维度</span></span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads <span class="comment"># 头数量</span></span><br><span class="line">        <span class="variable language_">self</span>.head_dim = d_out // num_heads <span class="comment"># 每个头的维度</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化可训练的权重矩阵，分别代表查询向量、键向量、值向量</span></span><br><span class="line">        <span class="variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用一个线性层来组合头的输出</span></span><br><span class="line">        <span class="variable language_">self</span>.out_proj = nn.Linear(d_out, d_out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 掩码 + dropout</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(</span><br><span class="line">            <span class="string">&quot;mask&quot;</span>,</span><br><span class="line">            torch.triu(torch.ones(context_length, context_length), diagonal=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># [batch_size, sequence_length, embedding_dim]</span></span><br><span class="line">        b, num_tokens, d_in = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算权重矩阵 Q/K/V</span></span><br><span class="line">        keys: Tensor = <span class="variable language_">self</span>.W_key(x)</span><br><span class="line">        queries: Tensor = <span class="variable language_">self</span>.W_query(x)</span><br><span class="line">        values: Tensor = <span class="variable language_">self</span>.W_value(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 重塑为多头格式</span></span><br><span class="line">        <span class="comment"># 将 [batch, seq_len, d_out] 重塑为 [batch, seq_len, num_heads, head_dim]</span></span><br><span class="line">        <span class="comment"># [2, 6, 4] -&gt; [2, 6, 2, 2]</span></span><br><span class="line">        keys = keys.view(b, num_tokens, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        values = values.view(b, num_tokens, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        queries = queries.view(b, num_tokens, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 调整维度顺序，让每个头独立计算注意力，便于批量处理所有头</span></span><br><span class="line">        <span class="comment"># 从形状 (b, num_tokens, num_heads, head_dim)</span></span><br><span class="line">        <span class="comment"># 转换到 (b, num_heads, num_tokens, head_dim)</span></span><br><span class="line">        keys = keys.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        values = values.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        queries = queries.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算注意力分数，这样每个批次(2)的每个头(2)都有了一个 6×6 的注意力分数矩阵</span></span><br><span class="line">        attn_scores = queries @ keys.transpose(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># [2, 2, 6, 2] @ [2, 2, 2, 6] = [2, 2, 6, 6]</span></span><br><span class="line">        mask_bool: Tensor = <span class="variable language_">self</span>.mask.<span class="built_in">bool</span>()[:num_tokens, :num_tokens]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用因果掩码</span></span><br><span class="line">        attn_scores.masked_fill_(mask_bool, -torch.inf)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 归一化权重</span></span><br><span class="line">        attn_weights = torch.softmax(attn_scores / keys.shape[-<span class="number">1</span>]**<span class="number">0.5</span>, dim=-<span class="number">1</span>) <span class="comment"># [2, 2, 6, 6]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用 dropout 掩码减少过拟合</span></span><br><span class="line">        attn_weights = <span class="variable language_">self</span>.dropout(attn_weights) <span class="comment"># [2, 2, 6, 6]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每个头计算自己的上下文向量</span></span><br><span class="line">        context_vec: Tensor = (attn_weights @ values).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># [2, 2, 6, 6] @ [2, 2, 6, 2] = [2, 2, 6, 2] -&gt; [2, 6, 2, 2]</span></span><br><span class="line">        <span class="comment"># 重塑回原始格式 [2, 6, 2, 2] -&gt; [2, 6, 4]</span></span><br><span class="line">        context_vec = context_vec.contiguous().view(b, num_tokens, <span class="variable language_">self</span>.d_out)</span><br><span class="line">        <span class="comment"># 通过输出投影层 [2, 6, 4]</span></span><br><span class="line">        context_vec = <span class="variable language_">self</span>.out_proj(context_vec)</span><br><span class="line">        <span class="keyword">return</span> context_vec</span><br></pre></td></tr></table></figure>
<p>第二个版本 <code>MultiHeadAttention</code>
之所以更好，根本原因在于它<strong>将多次小规模的独立计算，整合为一次大规模的并行计算</strong>，从而最大化地利用了现代硬件（尤其是
GPU）的并行处理能力。</p>
<p>第一个版本 <code>MultiHeadAttentionWrapper</code>
的根本问题是<strong>计算被拆散了</strong>。对于一个有 12
个头的模型，这意味着要执行 <strong>12 组</strong>独立的 Q, K, V
矩阵乘法。在 GPU 上，每次独立的矩阵乘法都需要一次内核启动（kernel
launch），这个启动本身是有开销的。执行 12
次小规模的计算，其总开销远大于执行 1
次等效的大规模计算。这就像让一个工人去搬 12
次箱子，每次只搬一个，远不如让他用推车一次性搬完 12 个箱子来得快。</p>
<p>这里面的向量变化可能有一些复杂，感兴趣的读者可以参考下图进行辅助理解。</p>
<figure>
<img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830235453791.png"
alt="多头注意力完整流程可视化" />
<figcaption aria-hidden="true">多头注意力完整流程可视化</figcaption>
</figure>
<blockquote>
<p>[!IMPORTANT]</p>
<p>到此为止，我们已经走完了一条从最简陋到最完备的注意力机制演进之路。我们从一个不带任何可训练参数的简单点积模型出发，理解了"看-权衡-融合"的核心思想；接着，通过引入可学习的
QKV
矩阵，赋予了模型<strong>学会如何去关注</strong>的能力；随后，我们用因果掩码为模型戴上了眼罩，强制它遵守时间顺序，只能回顾过去；最后，通过多头机制和高效的并行化实现，我们构建出了
GPT 模型真正的<strong>认知核心</strong>——<code>MultiHeadAttention</code>
模块。</p>
<p>这个模块是 Transformer
架构的灵魂。它为模型提供了一个动态的、可学习的机制，使其能够在处理每一个词元时，都能审视全局（或全局的过去），并精确地计算出上下文中每一个其他词元对当前词元的重要性，最终生成一个富含深度上下文信息的新表示。</p>
</blockquote>
<p>然而，一个强大的引擎（<code>MultiHeadAttention</code>）本身还不足以构成一辆性能优越的赛车（<code>TransformerBlock</code>）。我们还需要稳定系统、传动装置和进一步的加工环节。这就引出了我们接下来的问题：</p>
<ul>
<li>模型在通过注意力机制<strong>融合</strong>了上下文信息之后，如何对这些新信息进行进一步的<strong>加工和思考</strong>？</li>
<li>当我们把 12
个这样强大的计算层堆叠在一起时，如何保证训练过程的稳定，防止梯度消失或爆炸？</li>
<li>在经过如此复杂的变换后，如何确保原始的、未经处理的信息不会在层层传递中丢失？</li>
</ul>
<p>让我们先来回顾一下 <code>TransformerBlock</code> 的结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer块：多头注意力 + 前馈网络 + 残差连接&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.att = MultiHeadAttention(  <span class="comment"># &lt;----- 我们已经搞定了！</span></span><br><span class="line">            d_in=cfg[<span class="string">&quot;emb_dim&quot;</span>],</span><br><span class="line">            d_out=cfg[<span class="string">&quot;emb_dim&quot;</span>],</span><br><span class="line">            context_length=cfg[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">            num_heads=cfg[<span class="string">&quot;n_heads&quot;</span>],</span><br><span class="line">            dropout=cfg[<span class="string">&quot;drop_rate&quot;</span>],</span><br><span class="line">            qkv_bias=cfg[<span class="string">&quot;qkv_bias&quot;</span>],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># &lt;----- 接下来我们继续来解决后面的内容</span></span><br><span class="line">        <span class="variable language_">self</span>.ff = FeedForward(cfg)</span><br><span class="line">        <span class="variable language_">self</span>.norm1 = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.norm2 = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.drop_shortcut = nn.Dropout(cfg[<span class="string">&quot;drop_rate&quot;</span>])</span><br></pre></td></tr></table></figure>
<p>答案就藏在构成 <code>TransformerBlock</code>
的另外几个关键组件中。接下来，我们将把目光从注意力机制本身移开，去探索环绕在它周围的左膀右臂——<strong>前馈网络
(FeedForward Network)</strong>、<strong>层归一化 (Layer
Normalization)</strong> 和 <strong>残差连接 (Shortcut/Residual
Connections)</strong>，看看它们是如何协同工作，共同构成 Transformer
架构坚实可靠的核心处理单元的。</p>
<ul>
<li><strong>层归一化 (LayerNorm)</strong>:
它的根本作用是<strong>稳定训练过程</strong>。在数据经过复杂的注意力计算或前馈网络变换后，其数值分布可能会变得非常不稳定。层归一化就像一个调节器，在每个子层处理之前，都将数据拉回到一个标准的、易于处理的分布上，确保信息流的稳定。</li>
<li><strong>前馈神经网络 (FeedForward Network)</strong>:
如果说注意力机制负责<strong>融合</strong>来自上下文的信息，那么前馈网络则负责对这些融合后的信息进行<strong>加工和思考</strong>。它是一个小型的、独立处理每个词元位置的神经网络，用于提取更高级、更抽象的特征，增加模型的非线性表达能力。</li>
<li><strong>快捷连接 (Shortcut/Residual Connection)</strong>:
这是训练深度网络的关键技巧。它允许信息绕过某个处理层（如注意力或前馈网络），直接传递到下一层。这确保了即使在经过多达
12
层甚至更多的深度变换后，最原始的输入信息也不会完全丢失，同时极大地缓解了深度学习中的梯度消失问题，让深度堆叠成为可能。</li>
</ul>
<h3 id="使用层归一化进行归一化激活">2.4.2
使用层归一化进行归一化激活</h3>
<p>一个深度神经网络就像一个多级信息加工流水线。数据（信号）在每一层都会被权重矩阵进行复杂的数学变换。当层数很深时，每一层微小的变化都可能被逐层放大。这会导致两个极端问题
：</p>
<ol type="1">
<li><strong>信号爆炸</strong>：某些层的输出值变得非常大，导致后续计算溢出，训练过程崩溃。</li>
<li><strong>信号消失</strong>：某些层的输出值变得非常小，接近于零，导致信息无法有效传递到更深层，模型学不到东西。
这两种情况统称为<strong>内部协变量偏移 (Internal Covariate
Shift)</strong>，它使得训练过程极其不稳定，就像在一条崎岖不平的山路上开车，油门（学习率）稍有不慎就会冲出赛道。</li>
</ol>
<p><strong>第一性原理解决方案：强制信号标准化</strong></p>
<p>最直接的解决方案，就是在信息进入每个核心处理单元（如注意力和前馈网络）之前，强制进行一次校准或标准化。<strong>层归一化</strong>正是扮演了这个角色。
它的核心思想是，不管上一层传来的数据分布如何，它都强行将这批数据的均值调整为
0，方差调整为 1
。这相当于在流水线的每个关键工序前都安装了一个<strong>稳压器</strong>，确保无论输入信号如何波动，进入工序的信号始终是稳定、标准化的。</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831000639468.png" /></p>
<p>在 <code>TransformerBlock</code>
中，层归一化被放置在<strong>多头注意力和前馈网络之前</strong>
(<code>self.norm1</code> 和 <code>self.norm2</code>)
。这确保了这两个进行核心计算的模块接收到的输入始终处于一个稳定且易于处理的范围内，从而极大地稳定了整个深度模型的训练过程。</p>
<p><code>LayerNorm</code> 的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;层归一化实现&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, emb_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.eps = <span class="number">1e-5</span></span><br><span class="line">        <span class="variable language_">self</span>.scale = nn.Parameter(torch.ones(emb_dim))</span><br><span class="line">        <span class="variable language_">self</span>.shift = nn.Parameter(torch.zeros(emb_dim))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mean = x.mean(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        var = x.var(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>, unbiased=<span class="literal">False</span>)</span><br><span class="line">        norm_x = (x-mean) / torch.sqrt(var + <span class="variable language_">self</span>.eps)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.scale * norm_x + <span class="variable language_">self</span>.shift</span><br></pre></td></tr></table></figure>
<p>让我们从第一性原理出发，来根本性地解释 <code>LayerNorm</code>
的这份代码实现。它的每一行都服务于一个核心目的：<strong>在保持模型表达能力的同时，稳定深度网络的训练过程</strong>。我们可以将这个实现拆解为两个核心部分来理解：<strong>强制标准化</strong>
和 <strong>可学习的自适应调整</strong>。</p>
<h4 id="第一部分强制标准化---解决信号失控问题">2.4.2.1
第一部分：强制标准化 - 解决信号失控问题</h4>
<p>这是代码的核心计算部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># forward 方法中的核心计算</span></span><br><span class="line">mean = x.mean(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">var = x.var(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>, unbiased=<span class="literal">False</span>)</span><br><span class="line">norm_x = (x-mean) / torch.sqrt(var + <span class="variable language_">self</span>.eps)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>mean = x.mean(dim=-1, keepdim=True)</code> 和
<code>var = x.var(dim=-1, ...)</code>：这两行代码计算了<strong>每一个</strong>输入样本<strong>在其特征维度（<code>emb_dim</code>）上</strong>的均值和方差。<code>dim=-1</code>
是关键，它指定了归一化是沿着特征维度进行的，而不是像批归一化（Batch
Norm）那样跨批次进行。这使得 <code>LayerNorm</code>
的效果与批次大小无关，在处理可变长度序列时尤其稳定。</li>
<li><code>norm_x = (x-mean) / torch.sqrt(var + self.eps)</code>：这是标准的<strong>标准化公式</strong>（减去均值，再除以标准差）。它将原始输入
<code>x</code> 转换为了一个均值为 0、方差为 1 的新向量
<code>norm_x</code>。
<ul>
<li><code>self.eps = 1e-5</code>：<code>eps</code> (epsilon)
是一个极小的常数，它的唯一作用是<strong>防止分母为零</strong>。如果某个样本的方差恰好为
0，没有 <code>eps</code> 就会导致除零错误，<code>eps</code>
保证了计算的数值稳定性 1。</li>
<li><code>unbiased=False</code>：这是一个实现细节，表示在计算方差时分母是
<code>N</code> 而不是 <code>N-1</code>。选择 <code>False</code>
是为了<strong>与原始 GPT-2 模型的实现保持兼容</strong>，因为其最初是使用
TensorFlow 实现的，而这是 TensorFlow 的默认行为 2。</li>
</ul></li>
</ul>
<p>至此，我们已经强制将输入信号稳定在一个 <code>N(0, 1)</code>
的标准正态分布上。但这又带来了新的问题。</p>
<h4 id="第二部分可学习的自适应调整---恢复模型的表达能力">2.4.2.2
第二部分：可学习的自适应调整 - 恢复模型的表达能力</h4>
<p>这是在 <code>__init__</code> 中定义并在 <code>forward</code>
最后使用的部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># __init__ 中</span></span><br><span class="line"><span class="variable language_">self</span>.scale = nn.Parameter(torch.ones(emb_dim))</span><br><span class="line"><span class="variable language_">self</span>.shift = nn.Parameter(torch.zeros(emb_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment"># forward 的最后一步</span></span><br><span class="line"><span class="keyword">return</span> <span class="variable language_">self</span>.scale * norm_x + <span class="variable language_">self</span>.shift</span><br></pre></td></tr></table></figure>
<p><strong>根本问题</strong>：将每一层的输入都强制变为均值为 0、方差为 1
的分布，这种做法可能<strong>过于暴力和死板</strong>。它虽然稳定了训练，但也可能限制了模型的表达能力。也许对于某个特定层来说，一个均值为
10、方差为 5
的输入分布才是最优的。我们不希望因为追求稳定而扼杀了模型学习这种分布的可能性。</p>
<p><strong>解决方案</strong>：在强制标准化之后，再赋予模型<strong>撤销或重新调整</strong>这种标准化的能力。这是通过两个可学习的参数
<code>scale</code> 和 <code>shift</code> 来实现的。</p>
<ul>
<li><code>self.scale</code>
(增益)：这是一个与特征维度相同大小的可学习向量。它与标准化后的
<code>norm_x</code> 进行逐元素相乘。它被初始化为全
1，所以在训练刚开始时，它不起任何作用（乘以 1 等于不变）。</li>
<li><code>self.shift</code>
(偏置)：这也是一个可学习的向量。它被加到缩放后的结果上。它被初始化为全
0，所以在训练开始时，它也不起作用（加上 0 等于不变）。</li>
</ul>
<p><strong>这步的精髓在于</strong>：模型在训练过程中，可以通过反向传播自由地学习
<code>scale</code> 和 <code>shift</code> 的最佳值。</p>
<ul>
<li>如果模型发现强制标准化 <code>N(0, 1)</code>
对当前层来说是最好的，它就会让 <code>scale</code> 保持接近
1，<code>shift</code> 保持接近 0。</li>
<li>如果模型发现一个不同的分布更好，它就可以学会相应的
<code>scale</code> 和 <code>shift</code> 值，将 <code>norm_x</code>
线性变换到任何它认为最优的均值和方差。</li>
</ul>
<p>总的来说，<code>LayerNorm</code> 的实现是一个精妙的两步过程：</p>
<ol type="1">
<li><strong>先稳定</strong>：通过强制的标准化，将可能失控的输入信号拉回到一个稳定的
<code>N(0, 1)</code> 分布，解决了深度网络训练不稳定的根本问题。</li>
<li><strong>后放开</strong>：通过引入可学习的 <code>scale</code> 和
<code>shift</code>
参数，赋予模型恢复甚至创造全新分布的自由度，解决了强制标准化可能带来的表达能力受限的问题。</li>
</ol>
<p>最终，这个实现既保证了训练的<strong>稳定性</strong>，又保留了模型的<strong>灵活性和表达能力</strong>。</p>
<h3 id="实现具有-gelu-激活函数的前馈神经网络">2.4.3 实现具有 GELU
激活函数的前馈神经网络</h3>
<p>自注意力机制的核心是<strong>加权求和</strong>
(<code>attn_weights @ values</code>)。虽然计算权重时有
<code>softmax</code>
引入了非线性，但信息融合的最后一步本质上是一个线性组合。如果整个
<code>TransformerBlock</code>
只依赖于注意力机制来处理信息，那么模型的表达能力将受到限制。它擅长<strong>融合</strong>信息，但在对融合后的信息进行<strong>深度加工</strong>方面能力不足。</p>
<p><strong>第一性原理解决方案：为每个词元提供独立的非线性处理空间</strong></p>
<p>为了弥补这一不足，我们需要一个专门的组件来对注意力机制输出的上下文向量进行进一步的、更复杂的非线性变换。<strong>前馈网络
(FFN)</strong> 就是这个组件。 它通常由两个线性层和一个非线性激活函数（如
GELU）组成
。它会对序列中的<strong>每一个词元向量独立地</strong>进行一次"升维-非线性激活-降维"的操作
。</p>
<ol type="1">
<li><strong>升维</strong>：第一个线性层将向量维度扩大（例如从 768
维扩展到 3072
维），这为模型提供了更广阔的特征空间来表示和加工信息。</li>
<li><strong>非线性激活
(GELU)</strong>：这是关键一步，打破了线性变换的局限，允许模型学习输入和输出之间更复杂、更抽象的关系。</li>
<li><strong>降维</strong>：第二个线性层将维度恢复到原始大小，以便于下一层
<code>TransformerBlock</code> 处理。</li>
</ol>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831001916054.png" /></p>
<p>前馈神经网络 <code>FeedForward</code> 的实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;前馈神经网络&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layers = nn.Sequential(</span><br><span class="line">            nn.Linear(cfg[<span class="string">&quot;emb_dim&quot;</span>], <span class="number">4</span> * cfg[<span class="string">&quot;emb_dim&quot;</span>]),</span><br><span class="line">            GELU(),</span><br><span class="line">            nn.Linear(<span class="number">4</span> * cfg[<span class="string">&quot;emb_dim&quot;</span>], cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.layers(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GELU</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;GELU激活函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.5</span> * x * (<span class="number">1</span> + torch.tanh(</span><br><span class="line">            torch.sqrt(torch.tensor(<span class="number">2.0</span> / torch.pi)) *</span><br><span class="line">            (x + <span class="number">0.044715</span> * torch.<span class="built_in">pow</span>(x, <span class="number">3</span>))</span><br><span class="line">        ))</span><br></pre></td></tr></table></figure>
<ol type="1">
<li><strong>引入非线性</strong>：通过 <code>GELU</code>
激活函数，让模型有能力学习复杂的数据模式。</li>
<li><strong>深度加工信息</strong>：通过"升维-降维"的结构，为模型提供一个更广阔的计算空间来提取和转换特征，同时保持整个
<code>TransformerBlock</code>
输入输出维度的一致性，使其能够被方便地深度堆叠。</li>
</ol>
<h3 id="添加快捷连接">2.4.4 添加快捷连接</h3>
<p>当网络非常深时（例如堆叠 12 层 <code>Transformer</code>
块），会遇到两个致命问题：</p>
<ol type="1">
<li><strong>梯度消失 (Vanishing
Gradients)</strong>：在训练时，用于更新权重的梯度信号需要从最后一层反向传播到第一层。每经过一层，梯度都会被乘以该层的权重。在深层网络中，这些连乘操作很可能导致梯度信号迅速衰减，等传到浅层网络时已经微乎其微，导致浅层参数几乎不更新，模型无法有效训练
。</li>
<li><strong>信息退化 (Information Degradation)</strong>：输入向量
<code>x</code> 每经过一个 <code>TransformerBlock</code>
，都会被复杂的注意力机制和前馈网络完全重构。在经过多层变换后，最原始、最直接的语义和位置信息可能会被冲淡甚至丢失。</li>
</ol>
<p><strong>第一性原理解决方案：建立信息/梯度的"高速公路"</strong></p>
<p>解决方案出奇地简单而有效：在每个复杂处理单元（如注意力和前馈网络）旁边，建立一条<strong>直连通道</strong>，让输入可以直接跳过这个单元，与该单元的输出相加。这就是<strong>快捷连接</strong>或<strong>残差连接</strong>。</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831002424412.png" /></p>
<p>在 <code>TransformerBlock</code> 中，残差连接的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        shortcut = x      <span class="comment"># &lt; ------ 保存原始输入</span></span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.att(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.drop_shortcut(x)</span><br><span class="line">        x = x + shortcut  <span class="comment"># &lt; ------ 残差连接</span></span><br><span class="line"></span><br><span class="line">        shortcut = x      <span class="comment"># &lt; ------ 保存原始输入</span></span><br><span class="line">        x = <span class="variable language_">self</span>.norm2(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.ff(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.drop_shortcut(x)</span><br><span class="line">        x = x + shortcut  <span class="comment"># &lt; ------ 残差连接</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>残差连接在这段代码中体现在以下两个关键操作上：</p>
<ol type="1">
<li><code>shortcut = x</code>:
这是<strong>分叉路口</strong>，将原始信息备份到 <code>shortcut</code>
变量中，开辟了直连通道。</li>
<li><code>x = x + shortcut</code>:
这是<strong>十字路口汇合</strong>，将主干道上经过复杂处理的信息与旁路上的原始信息重新组合。</li>
</ol>
<p>具体如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一个子层：多头注意力 + 残差连接</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------- 残差连接的起点 -------------------</span></span><br><span class="line"><span class="comment"># 1. 保存原始输入：在进行任何变换之前，我们先把原始的输入 x 保存到一个名为 shortcut 的变量中。</span></span><br><span class="line"><span class="comment">#    这就相当于开辟了一条“快捷通道”或“旁路”，让原始信息可以绕过复杂的处理。</span></span><br><span class="line">shortcut = x</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------- 主处理路径 (F(x)) -------------------</span></span><br><span class="line"><span class="comment"># 2. 对输入进行复杂变换：</span></span><br><span class="line"><span class="comment">#    - 先进行层归一化</span></span><br><span class="line"><span class="comment">#    - 再通过多头注意力机制</span></span><br><span class="line"><span class="comment">#    - 最后应用 Dropout</span></span><br><span class="line"><span class="comment">#    这一系列操作的结果，更新了变量 x。现在的 x 已经不再是原始输入，</span></span><br><span class="line"><span class="comment">#    而是经过注意力模块深度加工后的“增量信息”或“残差”。</span></span><br><span class="line">x = <span class="variable language_">self</span>.norm1(x)</span><br><span class="line">x = <span class="variable language_">self</span>.att(x)</span><br><span class="line">x = <span class="variable language_">self</span>.drop_shortcut(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------- 残差连接的终点 -------------------</span></span><br><span class="line"><span class="comment"># 3. 将原始输入与变换后的结果相加：</span></span><br><span class="line"><span class="comment">#    这行代码是残差连接最关键的体现。我们将“快捷通道”中的原始信息 (shortcut)，</span></span><br><span class="line"><span class="comment">#    与“主处理路径”上经过复杂变换后的增量信息 (x) 进行逐元素相加。</span></span><br><span class="line"><span class="comment">#    这样，最终的输出既包含了新学到的上下文关系，又没有丢失最原始的输入信息。</span></span><br><span class="line">x = x + shortcut</span><br></pre></td></tr></table></figure>
<h2 id="输出层从向量到概率分布">2.5 输出层：从向量到概率分布</h2>
<p>我们从顶层的 <code>GPTModel</code> 容器开始，构建了其核心的可堆叠单元
<code>TransformerBlock</code>。在 <code>TransformerBlock</code>
内部，我们不仅实现了其进行上下文信息融合的核心模块——<code>MultiHeadAttention</code>，还集成了确保其稳定和高效运行的关键辅助组件：<code>LayerNorm</code>、<code>FeedForward</code>
网络和残差连接。</p>
<p>现在，我们回到 <code>GPTModel</code>
的结构上，看看还剩余哪些部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">       	<span class="comment"># 前面都已经介绍了...</span></span><br><span class="line">        <span class="variable language_">self</span>.final_norm = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.out_head = nn.Linear(cfg[<span class="string">&quot;emb_dim&quot;</span>], cfg[<span class="string">&quot;vocab_size&quot;</span>], bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, in_idx</span>):</span><br><span class="line">        <span class="comment"># 前面都已经介绍了...</span></span><br><span class="line">        x = <span class="variable language_">self</span>.final_norm(x)</span><br><span class="line">        logits = <span class="variable language_">self</span>.out_head(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure>
<p>经过 <code>n_layers</code> 层 <code>TransformerBlock</code>
的深度处理后，我们得到了一个张量 <code>x</code>，其维度为
<code>(batch_size, context_length, emb_dim)</code>。这个张量中的每一个向量都蕴含了丰富的上下文信息。然而，这仍然是模型的<strong>内部表示</strong>。模型的最终任务是<strong>预测下一个词元</strong>。</p>
<p><strong>根本问题</strong>：如何将这个 <code>emb_dim</code>
维度的、连续的内部状态向量，转换为一个覆盖整个词汇表（<code>vocab_size</code>）的、离散的预测结果？</p>
<p><strong>第一性原理解决方案：投影回词汇空间</strong></p>
<p>这个转换过程由输出层完成，它包含两个步骤：</p>
<ol type="1">
<li><strong>最终归一化 (<code>self.final_norm</code>)</strong>:
在进行最后的投影之前，对 <code>Transformer</code>
栈的输出再进行一次层归一化。这可以看作是进入最终决策阶段前的一次信号整理，确保输入到输出头的数值分布是稳定的，这有助于后续损失计算和梯度传播的稳定性。</li>
<li><strong>输出头投影 (<code>self.out_head</code>)</strong>:
这是至关重要的一步。<code>self.out_head</code>
是一个标准的线性层，其权重矩阵的维度是
<code>(emb_dim, vocab_size)</code>。
<ul>
<li><strong>它的作用</strong>：将每一个经过深度处理的、代表特定位置上下文信息的
<code>emb_dim</code> 维向量，<strong>线性投影</strong>到一个
<code>vocab_size</code> 维的空间中。</li>
<li><strong>输出的含义</strong>：这个 <code>vocab_size</code>
维的新向量被称为
<strong>logits</strong>。它的每一个维度都唯一对应词汇表中的一个词元。该维度上的数值，就代表模型预测该词元是下一个词的<strong>原始置信度分数</strong>（未经归一化的对数概率）。分数越高，模型认为该词元出现的可能性越大。</li>
</ul></li>
</ol>
<p>最终，<code>forward</code> 函数返回的 <code>logits</code>
张量，其维度为
<code>(batch_size, context_length, vocab_size)</code>，精确地包含了模型在每一个输入位置上，对词汇表中所有词元的预测分数。这是模型进行思考和计算后，给出的最终答卷。</p>
<h2 id="基础文本生成">2.6 基础文本生成</h2>
<p>至此，我们已经完成了 <code>GPTModel</code> 的全部架构代码实现。</p>
<p>当前，我们拥有一个结构上完整、参数可扩展的 GPT
模型蓝图。然而，必须明确的是，这个模型的所有可训练参数（<code>nn.Embedding</code>、<code>nn.Linear</code>、<code>LayerNorm</code>
中的权重和偏置）均由<strong>随机值</strong>初始化。因此，尽管模型结构已经完备，但它不具备任何语言知识，无法执行任何有意义的任务，其输出将是无意义的随机内容。</p>
<p>不过，在让我们的模型具备输出有意义的内容之前，我们还是先来实现模型文本生成的能力。GPT
模型将输出张量转化为生成文本的过程涉及多个步骤，如下图所示。这些步骤包括解码输出张量、根据概率分布选择词元，以及将这些词元转换为人类可读的文本。</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831005826451.png" /></p>
<p>下图更加详细地展示的下一词元生成过程说明了 GPT
模型如何在给定输入的情况下生成下一个词元。在每一步中，模型输出一个矩阵，其中的向量表示有可能的下一个词元。将与下一个词元对应的向量提取出来，并通过
softmax
函数转换为概率分布。在包含这些概率分数的向量中，找到最高值的索引，这个索引对应于词元
ID。然后将这个词元 ID
解码为文本，生成序列中的下一个词元。最后，将这个词元附加到之前的输入中，形成新的输入序列，供下一次迭代使用。这个逐步的过程使得模型能够按顺序生成文本，从最初的输入上下文中构建连贯的短语和句子。</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831005901397.png" /></p>
<p>让我们来实现一个文本生成工具，如下代码所示，<code>generate_and_print_sample</code>
是一个用于快速验证和展示的便捷工具，它封装了从编码、生成到解码的全过程，并妥善处理了模型的训练/评估模式切换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_and_print_sample</span>(<span class="params">model, tokenizer, device, start_context</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成文本样本&quot;&quot;&quot;</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    context_size = model.pos_emb.weight.shape[<span class="number">0</span>]</span><br><span class="line">    encoded = text_to_token_ids(start_context, tokenizer).to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        token_ids = generate_text_simple(</span><br><span class="line">            model=model, idx=encoded,</span><br><span class="line">            max_new_tokens=<span class="number">50</span>, context_length=context_size,</span><br><span class="line">        )</span><br><span class="line">    decoded_text = token_ids_to_text(token_ids, tokenizer)</span><br><span class="line">    <span class="built_in">print</span>(decoded_text.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot; &quot;</span>))</span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_text_simple</span>(<span class="params">model, idx, max_new_tokens, context_length</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用模型生成文本&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line">        idx_cond = idx[:, -context_length:]</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            logits = model(idx_cond)</span><br><span class="line"></span><br><span class="line">        logits = logits[:, -<span class="number">1</span>, :]</span><br><span class="line">        probas = torch.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">        idx_next = torch.argmax(probas, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> idx</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">text_to_token_ids</span>(<span class="params">text, tokenizer</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将文本转换为token ID&quot;&quot;&quot;</span></span><br><span class="line">    encoded = tokenizer.encode(text, allowed_special=&#123;<span class="string">&#x27;&lt;|endoftext|&gt;&#x27;</span>&#125;)</span><br><span class="line">    encoded_tensor = torch.tensor(encoded).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> encoded_tensor</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">token_ids_to_text</span>(<span class="params">token_ids, tokenizer</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将token ID转换回文本&quot;&quot;&quot;</span></span><br><span class="line">    flat = token_ids.squeeze(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> tokenizer.decode(flat.tolist())</span><br></pre></td></tr></table></figure>
<p>我们尝试调用一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">generate_and_print_sample(model, tokenizer, device, <span class="string">&quot;Every effort moves you&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>可以看到输出是毫无意义的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Every effort moves you Mexican rarity implementing NouPsychCle...&quot; Contributamong enable lacked complications tendon conclud Nearly oddly insign Champions senseless poopuclear shuts dove aspirinentionrous Miniasions fearsomeRanked adore disadvantages disregkeepvocensed eased museums William glovesople Palace shooters increases felony chops Batteryracuse Advertising cease</span><br></pre></td></tr></table></figure>
<p>那么，如何将这个参数随机化的架构，转变为一个能够理解和生成语言的功能性模型呢？答案是通过<strong>模型训练
(Model Training)</strong>。</p>
<h1 id="训练模型">3. 训练模型</h1>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831134852813.png" style="zoom:33%;" /></p>
<p>本篇我们将进入将架构赋予生命的核心环节：<strong>实现训练循环
(Training
Loop)</strong>。我们将详细介绍模型如何通过处理大量文本数据，在一个反复迭代的过程中，系统性地调整其内部数以亿计的参数。</p>
<p>我们将具体探讨<strong>损失函数 (Loss Function)</strong>
的计算、<strong>优化器 (Optimizer)</strong> 的作用以及<strong>反向传播
(Backpropagation)</strong>
的机制，这些是驱动模型从随机状态向智能状态收敛的根本动力。</p>
<h2 id="模型训练流程">3.1 模型训练流程</h2>
<p><strong>训练流程</strong>的根本目的，就是通过一个系统性的、迭代的优化过程，让初始化的
<code>GPTModel</code>
这个"空壳大脑"通过学习海量的数据样本，逐步调整其内部参数，最终掌握预测下一个词元的规律。</p>
<p>模型学习的数学基础是<strong>梯度下降 (Gradient
Descent)</strong>。其核心思想可以归结为：</p>
<ol type="1">
<li><strong>定义目标</strong>：我们需要一个<strong>损失函数 (Loss
Function)</strong>
来量化模型当前预测与真实答案之间的差距。差距越大，损失值越高。</li>
<li><strong>寻找方向</strong>：通过微积分计算损失函数对模型中每一个参数的<strong>梯度
(Gradient)</strong>。梯度指明了在该参数上，能让损失值<strong>上升最快</strong>的方向。</li>
<li><strong>进行修正</strong>：我们让参数朝着梯度的<strong>相反方向</strong>迈出一小步。这一小步的步长由<strong>学习率
(Learning Rate)</strong> 控制。</li>
<li><strong>反复迭代</strong>：不断重复"预测-&gt;计算损失-&gt;计算梯度-&gt;更新参数"的过程，模型的参数就会被逐步优化，使得损失值越来越小，预测越来越准。</li>
</ol>
<p><code>train_model_simple</code> 函数就是这一原理的精确代码实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model_simple</span>(<span class="params">model, train_loader, val_loader,</span></span><br><span class="line"><span class="params">                    optimizer, device, num_epochs,</span></span><br><span class="line"><span class="params">                    eval_freq, eval_iter, start_context, tokenizer</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型的主循环&quot;&quot;&quot;</span></span><br><span class="line">    train_losses, val_losses, track_tokens_seen = [], [], []</span><br><span class="line">    tokens_seen, global_step = <span class="number">0</span>, -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> input_batch, target_batch <span class="keyword">in</span> train_loader:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss = calc_loss_batch(input_batch, target_batch, model, device)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            tokens_seen += input_batch.numel()</span><br><span class="line">            global_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 定期评估</span></span><br><span class="line">            <span class="keyword">if</span> global_step % eval_freq == <span class="number">0</span>:</span><br><span class="line">                train_loss, val_loss = evaluate_model(</span><br><span class="line">                    model, train_loader, val_loader, device, eval_iter)</span><br><span class="line">                train_losses.append(train_loss)</span><br><span class="line">                val_losses.append(val_loss)</span><br><span class="line">                track_tokens_seen.append(tokens_seen)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Ep <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> (Step <span class="subst">&#123;global_step:06d&#125;</span>):&quot;</span></span><br><span class="line">                    <span class="string">f&quot;Train loss <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span>, &quot;</span></span><br><span class="line">                    <span class="string">f&quot;Val loss <span class="subst">&#123;val_loss:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每3个epoch生成样本</span></span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">3</span> == <span class="number">0</span>:</span><br><span class="line">            generate_and_print_sample(model, tokenizer, device, start_context)</span><br><span class="line">    <span class="keyword">return</span> train_losses, val_losses, track_tokens_seen</span><br></pre></td></tr></table></figure>
<p>我们可以将这个函数的结构分解为三个层次：<strong>外层循环</strong>、<strong>核心学习循环</strong>和<strong>监控系统</strong>。</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831011653589.png" /></p>
<h3 id="外层循环">3.1.1 外层循环</h3>
<p><code>for epoch in range(num_epochs):</code>
定义了模型需要完整地看几遍整个训练数据集。一个 <strong>Epoch</strong>
代表对所有训练数据的一次完整遍历。让模型反复看同样的数据，是为了使其有机会从不同的批次组合和随机顺序中，更深入地学习数据中的模式。</p>
<h3 id="核心学习循环">3.1.2 核心学习循环</h3>
<p><code>for input_batch, target_batch in train_loader:</code>
是学习发生的真正场所。对于从 <code>train_loader</code>
中取出的每一个数据批次，模型都会严格执行梯度下降的"四步曲"：</p>
<ol type="1">
<li><p>清空旧梯度<code>optimizer.zero_grad()</code></p>
<p>PyTorch
的梯度计算默认是<strong>累加</strong>的。如果不手动清零，当前批次计算出的梯度会和之前所有批次的梯度叠加在一起，导致错误的更新方向。因此，在每次计算新梯度前，必须先“清空缓存”。</p></li>
<li><p>前向传播与计算损失 <code>loss = calc_loss_batch(...)</code></p>
<p>我们需要知道模型在当前参数下的表现有多差。<code>calc_loss_batch</code>
函数内部会调用
<code>model(input_batch)</code>，完成一次<strong>前向传播</strong>，得到预测的
logits。然后，使用<strong>交叉熵损失函数</strong>
<code>cross_entropy</code> 来计算预测 logits 和真实
<code>target_batch</code> 之间的差距，得到一个量化误差的标量
<code>loss</code>。</p></li>
<li><p>反向传播计算梯度 <code>loss.backward()</code></p>
<p>知道了总误差（<code>loss</code>）后，我们需要将这个误差"分摊"到每一个导致误差的参数上，即计算损失对每一个模型参数的偏导数。这是
PyTorch <code>autograd</code>
引擎的核心功能。这一行代码会自动地、高效地完成整个<strong>反向传播</strong>过程，计算出模型中所有可训练参数的梯度，并存储在它们的
<code>.grad</code> 属性中。</p>
<blockquote>
<p>不熟悉反向传播概念的读者，可参考：<a
href="https://hedon.top/2025/07/27/llm/back-propagation/">大白话解释反向传播算法</a></p>
</blockquote></li>
<li><p>更新模型参数 <code>optimizer.step()</code></p>
<p>有了修正方向（梯度）后，需要一个执行者来实际地调整参数。优化器（如
<code>AdamW</code>）会根据 <code>loss.backward()</code>
计算出的梯度，以及自身的更新规则（如学习率），去更新模型中的每一个参数，完成一次学习和进化。</p></li>
</ol>
<h3 id="监控系统">3.1.3 监控系统</h3>
<p>仅仅闷头学习是不够的，我们还需要知道学得怎么样。这个函数内置了两套监控系统：</p>
<ul>
<li><strong>定量评估
(<code>if global_step % eval_freq == 0</code>)</strong>：每隔
<code>eval_freq</code> 步，就调用 <code>evaluate_model</code>
函数。该函数会暂停训练 (<code>model.eval()</code>)，在不计算梯度
(<code>torch.no_grad()</code>)
的模式下，快速计算模型在<strong>训练集</strong>和<strong>验证集</strong>上的损失。通过观察这两个损失的变化，我们可以清晰地了解模型的学习状态。</li>
<li><strong>定性观察 (<code>if epoch % 3 == 0</code>)</strong>：每隔几个
epoch，就调用 <code>generate_and_print_sample</code>
函数。它会给模型一个固定的开头
(<code>start_context</code>)，让模型在当前的学习状态下续写一段文本。通过观察从最初的“胡言乱语”到逐渐生成通顺句子的过程，我们可以获得最直观的反馈。</li>
</ul>
<h2 id="计算文本生成损失">3.2 计算文本生成损失</h2>
<p>在 <code>train_model_simple</code> 中，有两个关键的辅助函数：</p>
<ul>
<li><code>calc_loss_batch</code>
：对于给定的一个批次数据，计算出模型预测与真实答案之间的差距有多大。</li>
<li><code>evaluate_model</code>：在不更新模型参数的前提下，客观地评估模型在当前阶段的学习效果。</li>
</ul>
<h3 id="批量损失计算">3.2.1 批量损失计算</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calc_loss_batch</span>(<span class="params">input_batch, target_batch, model, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算单个批次的损失&quot;&quot;&quot;</span></span><br><span class="line">    input_batch = input_batch.to(device)</span><br><span class="line">    target_batch = target_batch.to(device)</span><br><span class="line">    logits = model(input_batch)</span><br><span class="line">    loss = torch.nn.functional.cross_entropy(</span><br><span class="line">        logits.flatten(<span class="number">0</span>, <span class="number">1</span>), target_batch.flatten(),</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p>这里我们首先需要弄清楚一个核心问题：<strong><u>什么叫做模型预测与真实答案之间的差距？这个差距怎么算？为什么能这么算？</u></strong></p>
<h4 id="差距的本质是什么">3.2.1.1 差距的本质是什么？</h4>
<ul>
<li><strong>模型的预测</strong>：<code>logits</code>。对于输入序列中的每一个位置，模型都会输出一个长度为
<code>vocab_size</code> (例如 50257)
的向量。这个向量里的每一个数值，代表模型认为对应的词元是正确答案的<strong>原始置信度分数</strong>。分数越高，代表模型越确信。</li>
<li><strong>真实答案</strong>：<code>target_batch</code>。这是一个具体的、唯一的词元
ID。例如，对于输入 <code>"Time is"</code>，正确的下一个词是
<code>"an"</code>，那么真实答案就是 <code>"an"</code> 对应的那个唯一的
ID。</li>
<li><strong>差距</strong>：差距就是<strong>模型赋予"真实答案"的那个置信度分数，与它本应达到的理想状态（绝对确信）之间的距离</strong>。如果模型给真实答案的置信度分数很高，而给其他所有错误答案的分数都很低，那么这个差距就很小。反之，如果模型给真实答案的分数很低，那么差距就很大。</li>
</ul>
<h4 id="这个差距怎么算">3.2.1.2 这个差距怎么算？</h4>
<p><code>torch.nn.functional.cross_entropy</code>
这个函数虽然只有一行，但它在内部完成了两个关键的数学步骤，来将我们上面描述的抽象差距转化为一个可量化的数值（损失）。</p>
<p><strong>第一步：使用 <code>Softmax</code>
将置信度分数转为概率</strong></p>
<p>模型的 <code>logits</code>
只是原始分数，有正有负，大小不一，不方便直接比较。我们需要将它转换成一个标准的<strong>概率分布</strong>，即所有可能答案的概率加起来等于
1。<code>Softmax</code> 函数就是做这个的。</p>
<p>例如，假设词汇表只有 5 个词，对于某个位置，模型输出的
<code>logits</code> 是 <code>[1.0, 4.0, 2.0, -1.0, 0.0]</code>。经过
<code>Softmax</code> 转换后，它会变成类似
<code>[0.02, 0.68, 0.06, 0.00, 0.24]</code> 的概率分布。</p>
<p>现在，模型的预测变得清晰了：它有 68%
的把握认为第二个词是正确答案。</p>
<p><strong>第二步：使用负对数似然 (Negative Log-Likelihood)
计算损失</strong></p>
<p>现在我们有了模型的概率预测，也知道了唯一的正确答案（比如就是第二个词）。我们如何量化这个预测的好坏呢？</p>
<p>我们只需要看模型<strong>赋予那个正确答案的概率值</strong>。在这个例子里，是
<code>0.68</code>。</p>
<ul>
<li><strong>理想情况</strong>：如果模型完美，它应该给正确答案 100%
的概率，即 <code>1.0</code>。</li>
<li><strong>我们希望</strong>：让模型赋予正确答案的概率尽可能接近
<code>1.0</code>。</li>
</ul>
<p>负对数似然就是实现这个目标的完美工具。它的公式是 <span
class="math inline">\(Loss=−log(p_{correct})\)</span>，其中 <span
class="math inline">\(p_{correct}\)</span>
是模型赋予正确答案的概率。</p>
<p>让我们看看它的特性：</p>
<ul>
<li>当 <span class="math inline">\(p_{correct}→1.0\)</span>
（模型预测很准）时，<span class="math inline">\(Loss=−log(1.0) →
0\)</span> 损失非常小。</li>
<li>当 <span class="math inline">\(p_{correct} →
0\)</span>（模型预测离谱）时，<span class="math inline">\(Loss=−log(0) →
infty\)</span> 损失会变得非常大。</li>
</ul>
<p>这个特性棒极了！它<strong>极大地惩罚了那些离谱的错误预测</strong>，从而在反向传播时产生巨大的梯度，迫使模型去修正这个严重的错误。</p>
<p><code>cross_entropy</code> 函数将 <code>Softmax</code>
和<strong>负对数似然</strong>这两步合并在了一起，不仅方便使用，而且在数值计算上更加稳定。</p>
<h4 id="为什么能这么算">3.2.1.3 为什么能这么算？</h4>
<p>从根本上说，这是因为我们将<strong>语言建模问题，转化为了一个序列性的多分类问题
(Multi-Class Classification Problem)</strong>。</p>
<p>在序列的每一个时间步，模型都在做一个分类任务：从
<code>vocab_size</code>
个可能的类别（词元）中，选出最有可能的那一个。</p>
<p><strong>交叉熵 (Cross-Entropy)</strong>
源自信息论，是衡量两个概率分布之间差异的标准方法。在这里，这两个分布是：</p>
<ol type="1">
<li><strong>模型的预测分布</strong>：经过 <code>Softmax</code>
后的那个概率向量。</li>
<li><strong>真实的理想分布</strong>：一个 <strong>one-hot</strong>
向量。即在正确答案的索引位置为 1，其他所有位置为 0 的向量（例如
<code>[0, 1, 0, 0, 0]</code>）。</li>
</ol>
<p>最小化交叉熵损失，就是在<strong>迫使模型的预测分布去无限逼近那个理想的、尖锐的真实分布</strong>。通过在海量文本上不断地做这件事，模型就不得不去学习语言的内在规律和模式，以便在任何给定的上下文后，都能生成一个最接近真实世界的下一个词的概率分布。</p>
<p>最后，代码中的 <code>.flatten(0, 1)</code> 操作，是将
<code>(batch_size, context_length)</code>
这两个维度压平。这相当于告诉损失函数："别把它们看作是一批句子，请把这批数据里<strong>所有位置的预测任务，都当作是独立的分类问题</strong>来同等对待"，从而高效地一次性计算出整个批次的总损失。</p>
<blockquote>
<p>[!NOTE]</p>
<p>对交叉熵损失概念依旧不是很熟悉的读者，可以参考：<a
href="https://hedon.top/2025/08/13/llm/cross-entropy-loss/">大白话解释交叉熵损失</a></p>
</blockquote>
<h3 id="模型评估">3.2.2 模型评估</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_model</span>(<span class="params">model, train_loader, val_loader, device, eval_iter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估模型性能&quot;&quot;&quot;</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)</span><br><span class="line">        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">return</span> train_loss, val_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_loss_loader</span>(<span class="params">data_loader, model, device, num_batches=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算数据加载器的平均损失&quot;&quot;&quot;</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data_loader) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">float</span>(<span class="string">&quot;nan&quot;</span>)</span><br><span class="line">    <span class="keyword">elif</span> num_batches <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        num_batches = <span class="built_in">len</span>(data_loader)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        num_batches = <span class="built_in">min</span>(num_batches, <span class="built_in">len</span>(data_loader))</span><br><span class="line">    <span class="keyword">for</span> i, (input_batch, target_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">        <span class="keyword">if</span> i &lt; num_batches:</span><br><span class="line">            loss = calc_loss_batch(input_batch, target_batch, model, device)</span><br><span class="line">            total_loss += loss.item()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> total_loss / num_batches</span><br></pre></td></tr></table></figure>
<p>这段代码的结构清晰地体现了评估的核心原则：</p>
<ol type="1">
<li><strong>状态切换 (<code>model.eval()</code> 和
<code>model.train()</code>)</strong>：这是评估流程的开关。<code>model.eval()</code>
会关闭 <code>Dropout</code>
等只在训练时使用的层，确保评估结果的稳定和可复现。评估结束后，<code>model.train()</code>
会重新打开它们，让训练继续。</li>
<li><strong>隔离环境
(<code>with torch.no_grad()</code>)</strong>：这是为了确保评估的纯粹性。在这个代码块中，PyTorch
不会跟踪计算图和梯度。这不仅能防止任何意外的参数更新，还能大幅减少内存占用和计算时间，让评估更高效。</li>
<li><strong>委托计算
(<code>calc_loss_loader</code>)</strong>：<code>evaluate_model</code>
将具体的计算任务委托给
<code>calc_loss_loader</code>。这个函数是一个通用的损失计算器，它迭代指定数量的批次，调用
<code>calc_loss_batch</code>
累加损失，最后返回平均损失。这种分层设计让代码更整洁。</li>
<li><strong>双重检验</strong>：同时计算<strong>训练损失</strong>和<strong>验证损失</strong>是至关重要的。
<ul>
<li>训练损失持续下降，说明模型在努力学习。</li>
<li>验证损失也随之下降，说明模型学到了普适的规律（泛化能力好）。</li>
<li>如果训练损失下降，但验证损失开始上升，这就是<strong>过拟合</strong>的信号，说明模型开始"死记硬背"训练题，而不是真正理解。</li>
</ul></li>
</ol>
<h2 id="保存和加载模型">3.3 保存和加载模型</h2>
<p>到目前为止，我们已经讨论了如何从数值上评估训练进展，并从头开始预训练了一个大语言模型。尽管样例中使用的大语言模型和数据集都相对较小，但这足以表明预训练大语言模型代价高昂。因此，保存大语言模型的参数非常重要，这样就不必每次使用它时都重新运行训练。</p>
<p>这部分很简单，可以直接参考 <a
target="_blank" rel="noopener" href="https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html">PyTorch
- Saving and Loading Models</a></p>
<ul>
<li><p>保存模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dump_model_file = <span class="string">&quot;model_and_optimizer.pth&quot;</span></span><br><span class="line">torch.save(&#123;</span><br><span class="line">    <span class="string">&quot;model_state_dict&quot;</span>: model.state_dict(),</span><br><span class="line">    <span class="string">&quot;optimizer_state_dict&quot;</span>: optimizer.state_dict(),</span><br><span class="line">&#125;, dump_model_file)</span><br></pre></td></tr></table></figure></li>
<li><p>加载模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">checkpoint = torch.load(dump_model_file, map_location=device)</span><br><span class="line">model = GPTModel(GPT_CONFIG_124M)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&quot;model_state_dict&quot;</span>])</span><br><span class="line">optimizer = torch.optim.AdamW(</span><br><span class="line">    model.parameters(),</span><br><span class="line">    lr=<span class="number">5e-4</span>, weight_decay=<span class="number">0.1</span>,</span><br><span class="line">)</span><br><span class="line">optimizer.load_state_dict(checkpoint[<span class="string">&quot;optimizer_state_dict&quot;</span>])</span><br><span class="line">model.train()</span><br></pre></td></tr></table></figure></li>
</ul>
<p>既然我们能加载自己之前训练的模型，那是不是可以加载别人训练的更好的模型呢？当然！幸运的是，OpenAI
公开分享了它们的 GPT-2
模型的权重，从而省去了我们自己在大型语料库上重新训练模型所需投入的数万到数十万美元。因此，我们可以将这些权重加载到
GPTModel 类中，并使用该模型进行文本生成。这里， 权重指的是存储在 PyTorch
的 Linear 层和 Embedding 层的 <code>.weight</code>
属性中的权重参数。前面在训练模型时，我们通过
<code>model.parameters()</code> 访问过。</p>
<p>这部分不在本篇的核心讨论目标之中，感兴趣的读者可以参考：<a
target="_blank" rel="noopener" href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/01_main-chapter-code">LLMs-from-scratch-ch05</a>。</p>
<h1 id="文本生成">4. 文本生成</h1>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831135822616.png" style="zoom:33%;" /></p>
<p>终于来到我们最后一个环节了：文本生成！</p>
<p>前面我们在 2.6 章节已经介绍了使用 <code>generate_text_simple</code>
进行最基本的文本生成了。本篇我们将介绍一个更具实际意义的文本生成函数
<code>generate</code>，它是 <code>generate_text_simple</code>
经过两种技术（温度缩放和 Top-k 采样）改进而来的。</p>
<p>还是回到第一性原理上，<strong><u>为什么我们需要采取额外的技术来优化文本生成？</u></strong></p>
<p>要回答这个问题，我们必须搞清楚 <code>generate_text_simple</code>
中使用的最基本生成方法——<strong>贪婪解码 (Greedy
Decoding)</strong>——的根本缺陷是什么。</p>
<p>在 <code>generate_text_simple</code> 函数中，核心决策步骤是
<code>idx_next = torch.argmax(probas, dim=-1, keepdim=True)</code>。这行代码的意思是：在模型预测的所有词元的概率中，永远选择那个<strong>概率最高</strong>的词元作为下一个词。</p>
<p>这种方法虽然简单直接，但存在三个致命的问题：</p>
<ol type="1">
<li><strong>重复和乏味</strong>：模型很容易陷入重复的循环中。例如，如果
"the" 是最常见的下一个词，模型可能会不断生成 "the the
the..."。因为它只看眼前概率最高的一步，缺乏全局视野，导致生成的文本非常单调和机械。</li>
<li><strong>确定性和可预测性</strong>：对于同一个输入，贪婪解码的输出永远是完全相同的。这对于需要创造力和多样性的任务（如写故事、回答开放性问题）来说是不可接受的。</li>
<li><strong>错失更优解</strong>：有时，概率第二或第三高的词，可能在长远来看会引导出一个更通顺、更有意义的句子。贪婪解码这种"短视"的策略，会因为眼前的"最优"选择而错失全局的"更优"路径。</li>
</ol>
<p><strong>根本问题在于，语言本身不是一个永远选择最常见单词的确定性过程，它充满了多样性和一定的随机性。</strong>
我们需要一种方法，既能让模型主要选择那些靠谱的、概率高的词，又能引入适度的随机性，让它偶尔能灵光一闪，选择一些不那么常见但同样合理的词，从而生成更自然、更有趣的文本。</p>
<p><strong>温度缩放 (Temperature Scaling)</strong> 和 <strong>Top-k 采样
(Top-k Sampling)</strong> 就是解决这个问题的两种强大技术。</p>
<h2 id="温度缩放">4.1 温度缩放</h2>
<p>温度缩放是一种在从 <code>logits</code>
计算最终概率时，调节模型"自信度"的技术。</p>
<p>它的核心公式是：</p>
<p><span class="math display">\[
probabilities = Softmax(\frac{logits}{temperature})
\]</span></p>
<blockquote>
<p>因为关键的 Softmax 函数是非线性的，它会将 logits
被温度缩放后<strong>减小的差值</strong>转换成更<strong>平缓</strong>的概率分布，或将<strong>放大的差值</strong>转换成更<strong>尖锐</strong>的概率分布，所以最终结果会改变。</p>
</blockquote>
<ul>
<li><strong>当 <code>temperature</code> &gt; 1 (例如
1.5)</strong>：<code>logits</code>
会被缩小，使得不同词元之间的分数差距变小。经过 <code>Softmax</code>
后，概率分布会变得更<strong>平缓</strong>。这意味着，模型会降低对高概率词的执念，同时提升对低概率词的关注度，从而有更大的机会选择不那么常见的词。这会增加生成文本的<strong>多样性和创造性</strong>，但过高则可能导致内容不连贯。</li>
<li><strong>当 <code>temperature</code> &lt; 1 (例如
0.7)</strong>：<code>logits</code>
会被放大，使得分数差距拉大。<code>Softmax</code>
后的概率分布会变得更<strong>陡峭</strong>。模型会更加确信那些它认为概率最高的词，降低选择其他词的可能性。这使得生成文本更<strong>稳定和保守</strong>，更贴近训练数据的模式。</li>
<li><strong>当 <code>temperature</code> 趋近于
0</strong>：这会极端地放大最高分数的 <code>logit</code>，使得
<code>Softmax</code> 的结果无限接近于
<code>argmax</code>，最终效果等同于贪婪解码。</li>
</ul>
<p>所以你现在应该知道 ChatGPT 接口中这个参数的来源了吧！</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831141218682.png" /></p>
<h2 id="top-k-采样">4.2 Top-k 采样</h2>
<p>即便我们用温度调节了想象力，仍然存在一个问题：词汇表非常大，有很多词元的概率虽然不为零，但实际上是完全不合理的。如果在采样时不幸选中了它们，就会产生无意义的文本。</p>
<p>Top-k
采样的思想非常直观：<strong>我们只在最靠谱的一小撮候选词中进行采样。</strong></p>
<p>它的步骤如下：</p>
<ol type="1">
<li><strong>筛选</strong>：在模型生成了所有词元的概率分布后，我们只保留其中概率最高的
<code>k</code> 个词元。</li>
<li><strong>重新分配概率</strong>：将这 <code>k</code>
个词元的概率进行归一化，使它们的概率之和为 1。</li>
<li><strong>采样</strong>：在这个小得多的、由靠谱候选词组成的集合中，根据新的概率分布进行随机采样。</li>
</ol>
<p>例如，如果设置
<code>k=5</code>，那么模型在决定下一个词时，只会从它认为最有可能的 5
个词中进行选择。这极大地<strong>降低了生成离谱或不相关词汇的风险</strong>，同时又通过在少数几个好的选项中进行随机抽样，保留了文本的多样性。它在模型的"创造力"和"连贯性"之间取得了绝佳的平衡。</p>
<p>值得一提的是，在 ChatGPT 的 API 中，并没有提供 <code>top_k</code>
这个参数，相反的，它提供的是 <code>top_p</code>。</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831142404381.png" /></p>
<p><code>top_k</code> 和 <code>top_p</code>
都是为了解决同样的问题：如何在一个合理的范围内进行随机采样，以避免模型生成无意义的词。但它们的实现方式决定了各自的优劣。</p>
<p><code>top_k</code> 强制模型只从概率最高的 <code>k</code>
个词中选择。问题在于，这个 <code>k</code>
是一个固定值，无法适应模型在不同情况下的自信度。</p>
<ul>
<li><strong>当模型非常确定时</strong>：比如在 "The capital of France is"
之后，模型对 "Paris" 的预测概率可能高达 99%。此时如果你的 <code>k</code>
设置为 10，你仍然会把 9 个几乎不可能的选项（比如 "London",
"Berlin"）纳入采样范围，这可能会引入不必要的噪声。</li>
<li><strong>当模型非常不确定时</strong>：比如在一个开放式创作的开头
"Once upon a time, there was
a"，可能有非常多合理的词，它们的概率分布可能非常平缓（例如，前 20
个词的概率都差不多）。此时如果你的 <code>k</code> 设置为
5，你就会武断地切掉很多同样合理的选项，限制了模型的创造力。</li>
</ul>
<p><code>top_p</code>
不限制候选词的数量，而是限制候选词的<strong>累积概率</strong>。例如，设置
<code>top_p: 0.9</code>，模型会从高到低选择词元，直到它们的概率总和达到
90%，然后只在这个动态生成的候选集里进行采样。</p>
<ul>
<li><strong>在模型非常确定的情况下</strong>： "Paris" 的概率是
99%，已经超过了 90% 的阈值。因此，候选集里<strong>只有 "Paris"
一个词</strong>。这完美地保留了模型的确定性。</li>
<li><strong>在模型非常不确定的情况下</strong>：为了凑够 90%
的概率，可能需要把<strong>前 20
个词</strong>都包含进来。这同样完美地适应了模型的不确定性，允许它在一个更广阔、更具创造力的空间里进行选择。</li>
</ul>
<h2 id="结合">4.3 结合</h2>
<p>将上述两种技术结合起来，就得到了我们最终实现的文本生成函数
<code>generate</code> 了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">model, idx, max_new_tokens, context_length,</span></span><br><span class="line"><span class="params">        temperature=<span class="number">0.0</span>, top_k=<span class="literal">None</span>, eos_id=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;带温度缩放、top_k 筛选的文本生成策略&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line">        idx_cond = idx[:, -context_length:]</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            logits = model(idx_cond)</span><br><span class="line">        logits = logits[:, -<span class="number">1</span>, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用 top_k 采样筛选 logits</span></span><br><span class="line">        <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            top_logits, _ = torch.topk(logits, top_k)</span><br><span class="line">            min_val = top_logits[:, -<span class="number">1</span>]</span><br><span class="line">            logits = torch.where(</span><br><span class="line">                logits &lt; min_val,</span><br><span class="line">                torch.tensor(<span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)).to(logits.device),</span><br><span class="line">                logits,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> temperature &gt; <span class="number">0.0</span>:</span><br><span class="line">            <span class="comment"># 使用温度缩放</span></span><br><span class="line">            logits = logits / temperature</span><br><span class="line">            probs = torch.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">            idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 当不使用温度缩放时，执行贪心解码，选取下一个词元</span></span><br><span class="line">            idx_next = torch.argmax(logits, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果遇到序列结束词元，则提前停止生成</span></span><br><span class="line">        <span class="keyword">if</span> idx_next == eos_id:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure>
<h2 id="其他思路">4.4 其他思路</h2>
<p>除了常见的温度采样和 top-k/top-p
采样，还有多种技术可以用来控制模型的文本生成策略，每种技术都有其独特的优缺点和适用场景。</p>
<p><strong>1. Beam Search (集束搜索)</strong></p>
<p>这是一种基于搜索的启发式算法，旨在找到一个整体概率最高的序列，而不是仅仅关注每一步的最优选择。</p>
<ul>
<li><strong>工作原理</strong>：在生成的每一步，它会保留 <code>k</code>
个（<code>k</code> 在这里被称为集束宽度或 beam
width）最可能的候选序列。在下一步，它会从这 <code>k</code>
个序列出发，生成所有可能的下一个词，并计算新序列的总概率，然后再次只保留总概率最高的
<code>k</code> 个序列。这个过程会一直持续到生成结束。</li>
<li><strong>优势</strong>：通过探索多种可能性，它通常能生成比贪婪搜索（Greedy
Search，即每步都选概率最高的词）更流畅、更全局最优的序列。</li>
<li><strong>劣势</strong>：它倾向于生成高频、安全的文本，可能会缺乏多样性和创造性。同时，计算开销比简单的采样方法要大。</li>
</ul>
<p><strong>2. Contrastive Search (对比搜索)</strong></p>
<p>这是一种较新的解码方法，旨在通过结合模型的概率和词元间的相似性来提升生成文本的连贯性和多样性，有效减少重复。</p>
<ul>
<li><strong>工作原理</strong>：在每一步选择下一个词元时，它会同时考虑两个因素：
<ol type="1">
<li><strong>模型置信度</strong>：下一个词元的概率要高。</li>
<li><strong>多样性/惩罚</strong>：下一个词元不应该和前文已经生成的词元过于相似。它通过计算候选词元与上文的相似性得分，并从模型概率中减去这个相似性得分作为惩罚项。</li>
</ol></li>
<li><strong>优势</strong>：在许多评测中，对比搜索被证明可以在不需要对模型进行任何额外训练的情况下，显著优于传统的解码方法，尤其在减少文本重复和提升连贯性方面表现突出。</li>
</ul>
<p><strong>3. Mirostat 采样</strong></p>
<p>这是一种自适应的采样算法，它的目标是让生成文本的"惊奇度"（Perplexity，一种衡量不确定性的指标）维持在一个预设的目标值附近。</p>
<ul>
<li><strong>工作原理</strong>：Mirostat
会在生成过程中持续监控输出文本的困惑度（Perplexity）。如果当前文本的困惑度低于目标值（意味着文本过于平淡、可预测），算法就会动态调整采样策略（如调整
<code>top-k</code> 的 <code>k</code>
值）来增加随机性。反之，如果困惑度太高（文本可能不连贯），它就会降低随机性。</li>
<li><strong>优势</strong>：它通过一个反馈循环来直接控制生成文本的统计特性，可以有效避免陷入无聊陷阱（过度重复）和困惑陷阱（内容不连贯）。</li>
</ul>
<p><strong>总结</strong></p>
<table style="width:100%;">
<colgroup>
<col style="width: 15%" />
<col style="width: 31%" />
<col style="width: 30%" />
<col style="width: 22%" />
</colgroup>
<thead>
<tr class="header">
<th>解码策略</th>
<th>核心思想</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Beam Search</strong></td>
<td>保留 <code>k</code> 个最可能的序列，追求全局最优。</td>
<td>连贯性好，适合翻译、摘要等任务。</td>
<td>多样性差，计算成本较高。</td>
</tr>
<tr class="even">
<td><strong>Contrastive Search</strong></td>
<td>结合模型概率和与上文的相异度来选择。</td>
<td>显著减少重复，提升连贯性。</td>
<td>算法相对复杂。</td>
</tr>
<tr class="odd">
<td><strong>Mirostat</strong></td>
<td>动态调整采样，使文本的困惑度维持在目标水平。</td>
<td>直接控制文本的统计特性，避免重复和不连贯。</td>
<td>需要设定一个合适的目标困惑度值。</td>
</tr>
</tbody>
</table>
<h1 id="总结">总结</h1>
<p>至此，我们 500
行代码的旅程也接近了尾声。本文完整记录了从零开始构建一个 GPT
风格语言模型的全过程，旨在将一个复杂的系统拆解为一系列清晰、可执行的步骤。</p>
<p>首先，从数据处理入手，阐述了如何将原始文本语料通过词元化、滑动窗口采样等方法，构建成模型训练所需的、包含输入-目标对的批量化张量（batched
tensors）。</p>
<p>接着，深入剖析了 <strong>Transformer
模型的核心架构</strong>。从输入层的词元与位置嵌入，到作为核心处理单元的
TransformerBlock
堆叠。在此过程中，详细解释了多头因果自注意力机制、前馈网络、层归一化和残差连接等关键组件的原理与作用，展示了它们如何协同工作以融合上下文信息并稳定深度网络的训练。</p>
<p>在模型结构之后，文章介绍了完整的<strong>训练循环</strong>。这包括前向传播、交叉熵损失计算、反向传播和优化器更新参数的完整流程，并展示了如何通过验证集监控训练状态，以评估模型的学习效果和泛化能力。</p>
<p>最后，文章探讨了<strong>文本生成阶段的解码策略</strong>，分析了从基础的贪婪解码到更高级的温度采样、Top-k
和 Top-p
等方法的原理，以及它们如何被用于控制生成文本的多样性与连贯性。</p>
<p>本文的核心主线是展示一个复杂的大语言模型系统，实际上可以被拆解为一系列目标明确、逻辑清晰的子问题和对应的工程实现。通过逐一解决从数据表示、上下文融合、深度网络训练稳定性到高质量文本生成等一系列挑战，我们最终将这些独立的模块化解决方案组合成一个功能完备的系统。</p>
<p>通过这种从零开始的构建过程，我们不仅能理解各个技术点的作用，更能把握它们之间如何相互关联、协同工作，从而对整个大语言模型的工作原理形成一个结构化、系统性的认知。希望本文的拆解与实现，能为每一位对大模型内部工作原理感到好奇、并希望从实践中构建体系化认知的开发者，提供一条清晰可循的路径和切实的帮助。</p>

<div class="article-footer fs14">
    <section id="references">
      <div class="header"><span>参考资料</span></div>
      <div class="body">
        <ul>
        <li class="post-title">
          <p><a
target="_blank" rel="noopener" href="https://github.com/rasbt/LLMs-from-scratch/">从零构建大语言模型</a></p>

        </li>
        </ul>
      </div>
    </section>
    
    <section id="license">
      <div class="header"><span>许可协议</span></div>
      <div class="body"><p>本文采用 <a
target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享
4.0 国际</a> 许可协议，转载请注明出处。</p>
</div>
    </section>
    
    <section id="share">
      <div class="header"><span>分享文章</span></div>
      <div class="body">
        <div class="link"><input class="copy-area" readonly="true" id="copy-link" value="https://hedon.top/2025/08/30/llm/note-llm-from-scratch/" /></div>
        <div class="social-wrap dis-select"><a class="social share-item wechat" onclick="util.toggle(&quot;qrcode-wechat&quot;)"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/b32ef3da1162a.svg" /></a><a class="social share-item weibo" target="_blank" rel="external nofollow noopener noreferrer" href="https://service.weibo.com/share/share.php?url=https://hedon.top/2025/08/30/llm/note-llm-from-scratch/&title=读书笔记丨《从零构建大语言模型》 - HedonWang&pics=/banner/note-llm-from-scratch.jpg&summary=通过 500 行代码实现，深入解析从零构建 GPT 风格大语言模型的完整流程：从数据处理、Transformer 架构核心、训练循环到文本生成策略，带你理解大模型背后的工程实现原理。"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/80c07e4dbb303.svg" /></a><a class="social share-item email" href="mailto:?subject=读书笔记丨《从零构建大语言模型》 - HedonWang&amp;body=https://hedon.top/2025/08/30/llm/note-llm-from-scratch/"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/a1b00e20f425d.svg" /></a><a class="social share-item link" onclick="util.copy(&quot;copy-link&quot;, &quot;复制成功&quot;)"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/8411ed322ced6.svg" /></a></div>
        
        <div class="qrcode" id="qrcode-wechat" style="opacity:0;height:0">
          <img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://api.qrserver.com/v1/create-qr-code/?size=256x256&data=https://hedon.top/2025/08/30/llm/note-llm-from-scratch/"/>
        </div>
        
      </div>
    </section>
    </div>
</article>
<div class="related-wrap" id="read-next"><section class="body"><div class="item" id="prev"><div class="note">较新文章</div><a href="/2025/09/05/qa/qa-traditional-backend-to-ai-engineer/">Q&A丨AI 视角下的后端技术重塑</a></div><div class="item" id="next"><div class="note">较早文章</div><a href="/2025/08/30/graceful-restart-from-tableflip-to-k8s/">优雅重启的范式转移：从 tableflip 到 Kubernetes 的 Go 服务升级终极指南</a></div></section></div>




  <div class="related-wrap md-text" id="comments">
    <section class='header cmt-title cap theme'>
      <p>快来参与讨论吧~</p>

    </section>
    <section class='body cmt-body giscus'>
      

<svg class="loading" style="vertical-align:middle;fill:currentColor;overflow:hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2709"><path d="M832 512c0-176-144-320-320-320V128c211.2 0 384 172.8 384 384h-64zM192 512c0 176 144 320 320 320v64C300.8 896 128 723.2 128 512h64z" p-id="2710"></path></svg>

<div id="giscus" src="https://giscus.app/client.js" data-repo="hedon954/hedonspace" data-repo-id="R_kgDOKt17sQ" data-category="Q&A" data-category-id="DIC_kwDOKt17sc4CbAt-" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="preferred_color_scheme" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous"></div>

    </section>
  </div>



<footer class="page-footer footnote"><hr><div class="text"><p>本站由 <a href="/">Hedon Wang</a> 使用 <a
target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.29.1">Stellar
1.29.1</a> 主题创建。 本博客所有文章除特别声明外，均采用 <a
target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA
4.0</a> 许可协议，转载请注明出处。</p>
</div></footer>
<div class="main-mask" onclick="sidebar.dismiss()"></div></div><aside class="l_right">
<div class="widgets">



<widget class="widget-wrapper toc" id="data-toc" collapse="false"><div class="widget-header dis-select"><span class="name">本文目录</span><a class="cap-action" onclick="sidebar.toggleTOC()" ><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg></a></div><div class="widget-body"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%E4%B8%8E%E9%87%87%E6%A0%B7"><span class="toc-text">1. 数据准备与采样</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%92%E5%88%86%E6%95%B0%E6%8D%AE%E9%9B%86-prepare_train_and_val_data"><span class="toc-text">1.1 划分数据集
prepare_train_and_val_data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E6%95%B0%E6%8D%AE%E9%9B%86-create_dataloader"><span class="toc-text">1.2 构建数据集
create_dataloader</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#dataset-dataloader"><span class="toc-text">1.2.1 Dataset &amp; DataLoader</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gptdataset"><span class="toc-text">1.2.2 GPTDataset</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E8%AF%8D%E5%85%83%E5%8C%96"><span class="toc-text">1.2.2.1 文本词元化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E9%87%87%E6%A0%B7"><span class="toc-text">1.2.2.2 滑动窗口进行数据采样</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E4%B8%8E%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">2. 初始化模型与优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE"><span class="toc-text">2.1 模型配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E6%80%BB%E8%A7%88"><span class="toc-text">2.2 模型结构总览</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E8%A1%A8%E7%A4%BA%E5%B1%82%E4%BB%8E%E7%A6%BB%E6%95%A3%E7%AC%A6%E5%8F%B7%E5%88%B0%E6%83%85%E5%A2%83%E5%8C%96%E5%90%91%E9%87%8F"><span class="toc-text">2.3
输入表示层：从离散符号到情境化向量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E5%85%83%E5%B5%8C%E5%85%A5"><span class="toc-text">2.3.1 词元嵌入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E5%B5%8C%E5%85%A5"><span class="toc-text">2.3.2 位置嵌入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dropout-%E6%8E%A9%E7%A0%81"><span class="toc-text">2.3.3 dropout 掩码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%A4%84%E7%90%86%E5%B1%82transformer-%E5%9D%97%E7%9A%84%E5%A0%86%E5%8F%A0"><span class="toc-text">2.4 核心处理层：Transformer
块的堆叠</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-text">2.4.1 注意力机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B2%A1%E6%9C%89%E5%8F%AF%E8%AE%AD%E7%BB%83%E6%9D%83%E9%87%8D%E7%9A%84%E7%AE%80%E5%8D%95%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-text">2.4.1.1
没有可训练权重的简单自注意力机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E5%B8%A6%E5%8F%AF%E8%AE%AD%E7%BB%83%E6%9D%83%E9%87%8D%E7%9A%84%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-text">2.4.1.2
实现带可训练权重的自注意力机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%A9%E7%94%A8%E5%9B%A0%E6%9E%9C%E6%B3%A8%E6%84%8F%E5%8A%9B%E9%9A%90%E8%97%8F%E6%9C%AA%E6%9D%A5%E8%AF%8D%E6%B1%87"><span class="toc-text">2.4.1.3
利用因果注意力隐藏未来词汇</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%86%E5%8D%95%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%89%A9%E5%B1%95%E5%88%B0%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">2.4.1.4
将单头注意力扩展到多头注意力</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%E8%BF%9B%E8%A1%8C%E5%BD%92%E4%B8%80%E5%8C%96%E6%BF%80%E6%B4%BB"><span class="toc-text">2.4.2
使用层归一化进行归一化激活</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E5%BC%BA%E5%88%B6%E6%A0%87%E5%87%86%E5%8C%96---%E8%A7%A3%E5%86%B3%E4%BF%A1%E5%8F%B7%E5%A4%B1%E6%8E%A7%E9%97%AE%E9%A2%98"><span class="toc-text">2.4.2.1
第一部分：强制标准化 - 解决信号失控问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%87%AA%E9%80%82%E5%BA%94%E8%B0%83%E6%95%B4---%E6%81%A2%E5%A4%8D%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A1%A8%E8%BE%BE%E8%83%BD%E5%8A%9B"><span class="toc-text">2.4.2.2
第二部分：可学习的自适应调整 - 恢复模型的表达能力</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E5%85%B7%E6%9C%89-gelu-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">2.4.3 实现具有 GELU
激活函数的前馈神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0%E5%BF%AB%E6%8D%B7%E8%BF%9E%E6%8E%A5"><span class="toc-text">2.4.4 添加快捷连接</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%B1%82%E4%BB%8E%E5%90%91%E9%87%8F%E5%88%B0%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83"><span class="toc-text">2.5 输出层：从向量到概率分布</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90"><span class="toc-text">2.6 基础文本生成</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-text">3. 训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="toc-text">3.1 模型训练流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%96%E5%B1%82%E5%BE%AA%E7%8E%AF"><span class="toc-text">3.1.1 外层循环</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%AD%A6%E4%B9%A0%E5%BE%AA%E7%8E%AF"><span class="toc-text">3.1.2 核心学习循环</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F"><span class="toc-text">3.1.3 监控系统</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E6%8D%9F%E5%A4%B1"><span class="toc-text">3.2 计算文本生成损失</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E6%8D%9F%E5%A4%B1%E8%AE%A1%E7%AE%97"><span class="toc-text">3.2.1 批量损失计算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B7%AE%E8%B7%9D%E7%9A%84%E6%9C%AC%E8%B4%A8%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-text">3.2.1.1 差距的本质是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%99%E4%B8%AA%E5%B7%AE%E8%B7%9D%E6%80%8E%E4%B9%88%E7%AE%97"><span class="toc-text">3.2.1.2 这个差距怎么算？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E8%BF%99%E4%B9%88%E7%AE%97"><span class="toc-text">3.2.1.3 为什么能这么算？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-text">3.2.2 模型评估</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-text">3.3 保存和加载模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90"><span class="toc-text">4. 文本生成</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B8%A9%E5%BA%A6%E7%BC%A9%E6%94%BE"><span class="toc-text">4.1 温度缩放</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#top-k-%E9%87%87%E6%A0%B7"><span class="toc-text">4.2 Top-k 采样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E5%90%88"><span class="toc-text">4.3 结合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%80%9D%E8%B7%AF"><span class="toc-text">4.4 其他思路</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li></ol></div><div class="widget-footer">

<a class="top" onclick="util.scrollTop()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 12c0-4.714 0-7.071 1.464-8.536C4.93 2 7.286 2 12 2c4.714 0 7.071 0 8.535 1.464C22 4.93 22 7.286 22 12c0 4.714 0 7.071-1.465 8.535C19.072 22 16.714 22 12 22s-7.071 0-8.536-1.465C2 19.072 2 16.714 2 12Z"/><path stroke-linecap="round" stroke-linejoin="round" d="m9 15.5l3-3l3 3m-6-4l3-3l3 3"/></g></svg><span>回到顶部</span></a><a class="buttom" onclick="util.scrollComment()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M10.46 1.25h3.08c1.603 0 2.86 0 3.864.095c1.023.098 1.861.3 2.6.752a5.75 5.75 0 0 1 1.899 1.899c.452.738.654 1.577.752 2.6c.095 1.004.095 2.261.095 3.865v1.067c0 1.141 0 2.036-.05 2.759c-.05.735-.153 1.347-.388 1.913a5.75 5.75 0 0 1-3.112 3.112c-.805.334-1.721.408-2.977.43a10.81 10.81 0 0 0-.929.036c-.198.022-.275.054-.32.08c-.047.028-.112.078-.224.232c-.121.166-.258.396-.476.764l-.542.916c-.773 1.307-2.69 1.307-3.464 0l-.542-.916a10.605 10.605 0 0 0-.476-.764c-.112-.154-.177-.204-.224-.232c-.045-.026-.122-.058-.32-.08c-.212-.023-.49-.03-.93-.037c-1.255-.021-2.171-.095-2.976-.429A5.75 5.75 0 0 1 1.688 16.2c-.235-.566-.338-1.178-.389-1.913c-.049-.723-.049-1.618-.049-2.76v-1.066c0-1.604 0-2.86.095-3.865c.098-1.023.3-1.862.752-2.6a5.75 5.75 0 0 1 1.899-1.899c.738-.452 1.577-.654 2.6-.752C7.6 1.25 8.857 1.25 10.461 1.25M6.739 2.839c-.914.087-1.495.253-1.959.537A4.25 4.25 0 0 0 3.376 4.78c-.284.464-.45 1.045-.537 1.96c-.088.924-.089 2.11-.089 3.761v1c0 1.175 0 2.019.046 2.685c.045.659.131 1.089.278 1.441a4.25 4.25 0 0 0 2.3 2.3c.515.214 1.173.294 2.429.316h.031c.398.007.747.013 1.037.045c.311.035.616.104.909.274c.29.17.5.395.682.645c.169.232.342.525.538.856l.559.944a.52.52 0 0 0 .882 0l.559-.944c.196-.331.37-.624.538-.856c.182-.25.392-.476.682-.645c.293-.17.598-.24.909-.274c.29-.032.639-.038 1.037-.045h.032c1.255-.022 1.913-.102 2.428-.316a4.25 4.25 0 0 0 2.3-2.3c.147-.352.233-.782.278-1.441c.046-.666.046-1.51.046-2.685v-1c0-1.651 0-2.837-.089-3.762c-.087-.914-.253-1.495-.537-1.959a4.25 4.25 0 0 0-1.403-1.403c-.464-.284-1.045-.45-1.96-.537c-.924-.088-2.11-.089-3.761-.089h-3c-1.651 0-2.837 0-3.762.089" clip-rule="evenodd"/><path fill="currentColor" d="M9 11a1 1 0 1 1-2 0a1 1 0 0 1 2 0m4 0a1 1 0 1 1-2 0a1 1 0 0 1 2 0m4 0a1 1 0 1 1-2 0a1 1 0 0 1 2 0"/></svg><span>参与讨论</span></a></div></widget>
</div></aside><div class='float-panel blur'>
  <button type='button' style='display:none' class='laptop-only rightbar-toggle mobile' onclick='sidebar.rightbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg>
  </button>
  <button type='button' style='display:none' class='mobile-only leftbar-toggle mobile' onclick='sidebar.leftbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 11c0-3.771 0-5.657 1.172-6.828C4.343 3 6.229 3 10 3h4c3.771 0 5.657 0 6.828 1.172C22 5.343 22 7.229 22 11v2c0 3.771 0 5.657-1.172 6.828C19.657 21 17.771 21 14 21h-4c-3.771 0-5.657 0-6.828-1.172C2 18.657 2 16.771 2 13z"/><path id="sep" stroke-linecap="round" d="M5.5 10h6m-5 4h4m4.5 7V3"/></g></svg>
  </button>
</div>
</div><div class="scripts">
<script type="text/javascript">
  const ctx = {
    date_suffix: {
      just: `刚刚`,
      min: `分钟前`,
      hour: `小时前`,
      day: `天前`,
    },
    root : `/`,
  };

  // required plugins (only load if needs)
  if (`local_search`) {
    ctx.search = {};
    ctx.search.service = `local_search`;
    if (ctx.search.service == 'local_search') {
      let service_obj = Object.assign({}, `{"field":"all","path":"/search.json","content":true,"skip_search":null,"sort":"-date"}`);
      ctx.search[ctx.search.service] = service_obj;
    }
  }
  const def = {
    avatar: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/3442075.svg`,
    cover: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/cover/76b86c0226ffd.svg`,
  };
  const deps = {
    jquery: `https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js`,
    marked: `https://cdn.jsdelivr.net/npm/marked@13.0.1/lib/marked.umd.min.js`
  }
  

</script>

<script type="text/javascript">
  const utils = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    css: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    js: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      if (src.startsWith('/')){
        src = ctx.root + src.substring(1);
      }
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    jq: (fn) => {
      if (typeof jQuery === 'undefined') {
        utils.js(deps.jquery).then(fn)
      } else {
        fn()
      }
    },
    
    onLoading: (el) => {
      if (el) {
        $(el).append('<div class="loading-wrap"><svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" stroke-opacity=".3" d="M12 3C16.9706 3 21 7.02944 21 12C21 16.9706 16.9706 21 12 21C7.02944 21 3 16.9706 3 12C3 7.02944 7.02944 3 12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="1.3s" values="60;0"/></path><path stroke-dasharray="15" stroke-dashoffset="15" d="M12 3C16.9706 3 21 7.02944 21 12"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.3s" values="15;0"/><animateTransform attributeName="transform" dur="1.5s" repeatCount="indefinite" type="rotate" values="0 12 12;360 12 12"/></path></g></svg></div>');
      }
    },
    onLoadSuccess: (el) => {
      if (el) {
        $(el).find('.loading-wrap').remove();
      }
    },
    onLoadFailure: (el) => {
      if (el) {
        $(el).find('.loading-wrap svg').remove();
        $(el).find('.loading-wrap').append('<svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" d="M12 3L21 20H3L12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.5s" values="60;0"/></path><path stroke-dasharray="6" stroke-dashoffset="6" d="M12 10V14"><animate fill="freeze" attributeName="stroke-dashoffset" begin="0.6s" dur="0.2s" values="6;0"/></path></g><circle cx="12" cy="17" r="1" fill="currentColor" fill-opacity="0"><animate fill="freeze" attributeName="fill-opacity" begin="0.8s" dur="0.4s" values="0;1"/></circle></svg>');
        $(el).find('.loading-wrap').addClass('error');
      }
    },
    request: (el, url, callback, onFailure) => {
      let retryTimes = 3;
      utils.onLoading(el);
      function req() {
        return new Promise((resolve, reject) => {
          let status = 0; // 0 等待 1 完成 2 超时
          let timer = setTimeout(() => {
            if (status === 0) {
              status = 2;
              timer = null;
              reject('请求超时');
              if (retryTimes == 0) {
                onFailure();
              }
            }
          }, 5000);
          fetch(url).then(function(response) {
            if (status !== 2) {
              clearTimeout(timer);
              resolve(response);
              timer = null;
              status = 1;
            }
            if (response.ok) {
              return response.json();
            }
            throw new Error('Network response was not ok.');
          }).then(function(data) {
            retryTimes = 0;
            utils.onLoadSuccess(el);
            callback(data);
          }).catch(function(error) {
            if (retryTimes > 0) {
              retryTimes -= 1;
              setTimeout(() => {
                req();
              }, 5000);
            } else {
              utils.onLoadFailure(el);
              onFailure();
            }
          });
        });
      }
      req();
    },
  };
</script>

<script>
  const sidebar = {
    leftbar: () => {
      if (l_body) {
        l_body.toggleAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    rightbar: () => {
      if (l_body) {
        l_body.toggleAttribute('rightbar');
        l_body.removeAttribute('leftbar');
      }
    },
    dismiss: () => {
      if (l_body) {
        l_body.removeAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    toggleTOC: () => {
      document.querySelector('#data-toc').classList.toggle('collapse');
    }
  }
</script>
<script type="text/javascript">
  (() => {
    const tagSwitchers = document.querySelectorAll('.tag-subtree.parent-tag > a > .tag-switcher-wrapper')
    for (const tagSwitcher of tagSwitchers) {
      tagSwitcher.addEventListener('click', (e) => {
        const parent = e.target.closest('.tag-subtree.parent-tag')
        parent.classList.toggle('expanded')
        e.preventDefault()
      })
    }

    // Get active tag from query string, then activate it.
    const urlParams = new URLSearchParams(window.location.search)
    const activeTag = urlParams.get('tag')
    if (activeTag) {
      let tag = document.querySelector(`.tag-subtree[data-tag="${activeTag}"]`)
      if (tag) {
        tag.querySelector('a').classList.add('active')
        
        while (tag) {
          tag.classList.add('expanded')
          tag = tag.parentElement.closest('.tag-subtree.parent-tag')
        }
      }
    }
  })()
</script>


<!-- required -->
<script src="/js/main.js?v=1.29.1" defer></script>

<script type="text/javascript">
  const applyTheme = (theme) => {
    if (theme === 'auto') {
      document.documentElement.removeAttribute('data-theme')
    } else {
      document.documentElement.setAttribute('data-theme', theme)
    }

    applyThemeToGiscus(theme)
  }

  const applyThemeToGiscus = (theme) => {
    theme = theme === 'auto' ? 'preferred_color_scheme' : theme

    const cmt = document.getElementById('giscus')
    if (cmt) {
      // This works before giscus load.
      cmt.setAttribute('data-theme', theme)
    }

    const iframe = document.querySelector('#comments > section.giscus > iframe')
    if (iframe) {
      // This works after giscus loaded.
      const src = iframe.src
      const newSrc = src.replace(/theme=[\w]+/, `theme=${theme}`)
      iframe.src = newSrc
    }
  }

  const switchTheme = () => {
    // light -> dark -> auto -> light -> ...
    const currentTheme = document.documentElement.getAttribute('data-theme')
    let newTheme;
    switch (currentTheme) {
      case 'light':
        newTheme = 'dark'
        break
      case 'dark':
        newTheme = 'auto'
        break
      default:
        newTheme = 'light'
    }
    applyTheme(newTheme)
    window.localStorage.setItem('Stellar.theme', newTheme)

    const messages = {
      light: `切换到浅色模式`,
      dark: `切换到深色模式`,
      auto: `切换到跟随系统配色`,
    }
    hud?.toast?.(messages[newTheme])
  }

  (() => {
    // Apply user's preferred theme, if any.
    const theme = window.localStorage.getItem('Stellar.theme')
    if (theme !== null) {
      applyTheme(theme)
    }
  })()
</script>


<!-- optional -->

  <script type="module">
  const el = document.querySelector('#comments #giscus');
  util.viewportLazyload(el, load_discus, false);

  function load_discus() {
    if (!el) return;
    try {
        el.innerHTML = '';
      } catch (error) {
        console.error(error);
      }
      const script = document.createElement('script');
      script.async = true;
      for (const key of Object.keys(el.attributes)) {
        const attr = el.attributes[key];
        if (['class', 'id'].includes(attr.name) === false) {
          script.setAttribute(attr.name, attr.value);
        }
      }
      el.appendChild(script);
  }
</script>




<script defer>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.services = Object.assign({}, JSON.parse(`{"mdrender":{"js":"/js/services/mdrender.js"},"siteinfo":{"js":"/js/services/siteinfo.js","api":"https://site-info-api-hedon.vercel.app/api/v1?url={href}"},"ghinfo":{"js":"/js/services/ghinfo.js"},"sites":{"js":"/js/services/sites.js"},"friends":{"js":"/js/services/friends.js"},"timeline":{"js":"/js/services/timeline.js"},"fcircle":{"js":"/js/services/fcircle.js"},"weibo":{"js":"/js/services/weibo.js"},"memos":{"js":"/js/services/memos.js"},"twikoo":{"js":"/js/services/twikoo_latest_comment.js"},"waline":{"js":"/js/services/waline_latest_comment.js"},"artalk":{"js":"/js/services/artalk_latest_comment.js"},"giscus":{"js":"/js/services/giscus_latest_comment.js"}}`));
    for (let id of Object.keys(ctx.services)) {
      const js = ctx.services[id].js;
      if (id == 'siteinfo') {
        ctx.cardlinks = document.querySelectorAll('a.link-card[cardlink]');
        if (ctx.cardlinks?.length > 0) {
          utils.js(js, { defer: true }).then(function () {
            setCardLink(ctx.cardlinks);
          });
        }
      } else {
        const els = document.getElementsByClassName(`ds-${id}`);
        if (els?.length > 0) {
          utils.jq(() => {
            if (id == 'timeline' || 'memos' || 'marked') {
              utils.js(deps.marked).then(function () {
                utils.js(js, { defer: true });
              });
            } else {
              utils.js(js, { defer: true });
            }
          });
        }
      }
    }
  });
</script>

<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.search = {
      path: `/search.json`,
    }
    utils.js('/js/search/local-search.js', { defer: true });
  });
</script><script>
  window.FPConfig = {
    delay: 0,
    ignoreKeywords: [],
    maxRPS: 5,
    hoverDelay: 25
  };
</script>
<script defer src="https://cdn.jsdelivr.net/npm/flying-pages@2/flying-pages.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.min.js"></script>
<script>
  // https://www.npmjs.com/package/vanilla-lazyload
  // Set the options globally
  // to make LazyLoad self-initialize
  window.lazyLoadOptions = {
    elements_selector: ".lazy",
  };
  // Listen to the initialization event
  // and get the instance of LazyLoad
  window.addEventListener(
    "LazyLoad::Initialized",
    function (event) {
      window.lazyLoadInstance = event.detail.instance;
    },
    false
  );
  document.addEventListener('DOMContentLoaded', function () {
    window.lazyLoadInstance?.update();
  });
</script><script>
  ctx.fancybox = {
    selector: `.timenode p>img, .md-text img:not([class]), .md-text .image img`,
    css: `https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0/dist/fancybox/fancybox.css`,
    js: `https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0/dist/fancybox/fancybox.umd.js`
  };
  var selector = '[data-fancybox]:not(.error)';
  if (ctx.fancybox.selector) {
    selector += `, ${ctx.fancybox.selector}`
  }
  var needFancybox = document.querySelectorAll(selector).length !== 0;
  if (!needFancybox) {
    const els = document.getElementsByClassName('ds-memos');
    if (els != undefined && els.length > 0) {
      needFancybox = true;
    }
  }
  if (needFancybox) {
    utils.css(ctx.fancybox.css);
    utils.js(ctx.fancybox.js, { defer: true }).then(function () {
      Fancybox.bind(selector, {
        hideScrollbar: false,
        Thumbs: {
          autoStart: false,
        },
        caption: (fancybox, slide) => {
          return slide.triggerEl.alt || slide.triggerEl.dataset.caption || null
        }
      });
    })
  }
</script>
<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    const swiper_api = document.getElementById('swiper-api');
    if (swiper_api != undefined) {
      utils.css(`https://unpkg.com/swiper@10.3.1/swiper-bundle.min.css`);
      utils.js(`https://unpkg.com/swiper@10.3.1/swiper-bundle.min.js`, { defer: true }).then(function () {
        const effect = swiper_api.getAttribute('effect') || '';
        var swiper = new Swiper('.swiper#swiper-api', {
          slidesPerView: 'auto',
          spaceBetween: 8,
          centeredSlides: true,
          effect: effect,
          rewind: true,
          pagination: {
            el: '.swiper-pagination',
            clickable: true,
          },
          navigation: {
            nextEl: '.swiper-button-next',
            prevEl: '.swiper-button-prev',
          },
        });
      })
    }
  });
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    processEscapes: true
  }
});
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script>
<script defer type="text/javascript" src="https://cdn.jsdelivr.net/npm/mermaid@v9/dist/mermaid.min.js"></script>
<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    var mermaid_config = {
      startOnLoad: true,
      theme:
        "" == "auto" &&
          window.matchMedia("(prefers-color-scheme: dark)").matches
          ? "dark"
          : "neutral",
      logLevel: 3,
      themeVariables: {
        darkMode: true
      },
      flowchart: {
        useMaxWidth: false,
        htmlLabels: true,
        curve: "linear"
      },
      gantt: {
        axisFormat: "%Y/%m/%d"
      },
      sequence: {
        actorMargin: 50
      }
    }
    mermaid.initialize(mermaid_config);
  });
</script><script>
  document.addEventListener('DOMContentLoaded', function () {
    window.codeElements = document.querySelectorAll('.code');
    if (window.codeElements.length > 0) {
      ctx.copycode = {
        default_text: `Copy`,
        success_text: `Copied`,
        toast: `复制成功`,
      };
      utils.js('/js/plugins/copycode.js');
    }
  });
</script>


<!-- inject -->

</div></body></html>
