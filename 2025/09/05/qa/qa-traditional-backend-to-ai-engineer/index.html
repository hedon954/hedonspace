
<!DOCTYPE html><html lang="zh-CN">

<head>
  <meta charset="utf-8">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.29.1" theme-name="Stellar" theme-version="1.29.1">
  
  <meta name="generator" content="Hexo 6.3.0">
  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel="preconnect" href="https://gcore.jsdelivr.net" crossorigin><link rel="preconnect" href="https://unpkg.com" crossorigin><link rel="preconnect" href="https://cdn.bootcdn.net" crossorigin>
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#000">
  <meta name="theme-color" content="#f9fafb">
  
  <title>Q&A丨AI 视角下的后端技术重塑 - HedonWang</title>

  
    <meta name="description" content="本文跟 Gemini 探讨在 AI 时代下，传统后端工程师如何结合自身优势，转型为 AI 应用开发工程师。">
<meta property="og:type" content="article">
<meta property="og:title" content="Q&amp;A丨AI 视角下的后端技术重塑">
<meta property="og:url" content="https://hedon.top/2025/09/05/qa/qa-traditional-backend-to-ai-engineer/index.html">
<meta property="og:site_name" content="HedonWang">
<meta property="og:description" content="本文跟 Gemini 探讨在 AI 时代下，传统后端工程师如何结合自身优势，转型为 AI 应用开发工程师。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-09-05T09:13:00.000Z">
<meta property="article:modified_time" content="2025-09-08T11:44:05.186Z">
<meta property="article:author" content="Hedon Wang">
<meta property="article:tag" content="思考">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <meta name="keywords" content="思考">

  <!-- feed -->
  
    <link rel="alternate" href="/atom.xml" title="HedonWang" type="application/atom+xml">
  

  <link rel="stylesheet" href="/css/main.css?v=1.29.1">


  
    <link rel="shortcut icon" href="/assets/favicon.png">
  

  

  <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon.png"><link rel="shortcut icon" href="/assets/favicon.png"><meta name="theme-color" content="#f8f8f8"><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC&display=swap" rel="stylesheet"><link rel="stylesheet" href="/css/markmap.css"><script src="https://cdn.jsdelivr.net/npm/markmap-autoloader@0.18"></script>
</head>
<body>

<div class="l_body s:aa content tech" id="start" layout="post" ><aside class="l_left"><div class="leftbar-container">


<header class="header"><div class="logo-wrap"><div class="icon"><img no-lazy class="icon" src="/assets/favicon.png" onerror="javascript:this.classList.add('error');this.src='https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/image/2659360.svg';"></div><a class="title" href="/"><div class="main" ff="title">HedonWang</div><div class="sub cap">君子求诸己，律己则安。</div></a></div></header>

<div class="nav-area">
<div class="search-wrapper" id="search-wrapper"><form class="search-form"><a class="search-button" onclick="document.getElementById(&quot;search-input&quot;).focus();"><svg t="1705074644177" viewBox="0 0 1025 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1560" width="200" height="200"><path d="M1008.839137 935.96571L792.364903 719.491476a56.783488 56.783488 0 0 0-80.152866 0 358.53545 358.53545 0 1 1 100.857314-335.166073 362.840335 362.840335 0 0 1-3.689902 170.145468 51.248635 51.248635 0 1 0 99.217358 26.444296 462.057693 462.057693 0 1 0-158.255785 242.303546l185.930047 185.725053a51.248635 51.248635 0 0 0 72.568068 0 51.248635 51.248635 0 0 0 0-72.978056z" p-id="1561"></path><path d="M616.479587 615.969233a50.428657 50.428657 0 0 0-61.498362-5.534852 174.655348 174.655348 0 0 1-177.525271 3.484907 49.403684 49.403684 0 0 0-58.833433 6.76482l-3.074918 2.869923a49.403684 49.403684 0 0 0 8.609771 78.10292 277.767601 277.767601 0 0 0 286.992355-5.739847 49.403684 49.403684 0 0 0 8.404776-76.667958z" p-id="1562"></path></svg></a><input type="text" class="search-input" id="search-input" placeholder="站内搜索"></form><div id="search-result"></div><div class="search-no-result">没有找到内容！</div></div>


<nav class="menu dis-select"><a class="nav-item active" title="博客" href="/" style="color:#1BCDFC"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M5.879 2.879C5 3.757 5 5.172 5 8v8c0 2.828 0 4.243.879 5.121C6.757 22 8.172 22 11 22h2c2.828 0 4.243 0 5.121-.879C19 20.243 19 18.828 19 16V8c0-2.828 0-4.243-.879-5.121C17.243 2 15.828 2 13 2h-2c-2.828 0-4.243 0-5.121.879M8.25 17a.75.75 0 0 1 .75-.75h3a.75.75 0 0 1 0 1.5H9a.75.75 0 0 1-.75-.75M9 12.25a.75.75 0 0 0 0 1.5h6a.75.75 0 0 0 0-1.5zM8.25 9A.75.75 0 0 1 9 8.25h6a.75.75 0 0 1 0 1.5H9A.75.75 0 0 1 8.25 9" clip-rule="evenodd"/><path fill="currentColor" d="M5.235 4.058C5 4.941 5 6.177 5 8v8c0 1.823 0 3.058.235 3.942L5 19.924c-.975-.096-1.631-.313-2.121-.803C2 18.243 2 16.828 2 14v-4c0-2.829 0-4.243.879-5.121c.49-.49 1.146-.707 2.121-.803zm13.53 15.884C19 19.058 19 17.822 19 16V8c0-1.823 0-3.059-.235-3.942l.235.018c.975.096 1.631.313 2.121.803C22 5.757 22 7.17 22 9.999v4c0 2.83 0 4.243-.879 5.122c-.49.49-1.146.707-2.121.803z" opacity=".5"/></svg></a><a class="nav-item" title="专栏" href="/topic/" style="color:#3DC550"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M14.25 4.48v3.057c0 .111 0 .27.02.406a.936.936 0 0 0 .445.683a.96.96 0 0 0 .783.072c.13-.04.272-.108.378-.159L17 8.005l1.124.534c.106.05.248.119.378.16a.958.958 0 0 0 .783-.073a.936.936 0 0 0 .444-.683c.021-.136.021-.295.021-.406V3.031c.113-.005.224-.01.332-.013C21.154 2.98 22 3.86 22 4.933v11.21c0 1.112-.906 2.01-2.015 2.08c-.97.06-2.108.179-2.985.41c-1.082.286-1.99 1.068-3.373 1.436c-.626.167-1.324.257-1.627.323V5.174c.32-.079 1.382-.203 1.674-.371c.184-.107.377-.216.576-.323m5.478 8.338a.75.75 0 0 1-.546.91l-4 1a.75.75 0 0 1-.364-1.456l4-1a.75.75 0 0 1 .91.546" clip-rule="evenodd"/><path fill="currentColor" d="M18.25 3.151c-.62.073-1.23.18-1.75.336a8.2 8.2 0 0 0-.75.27v3.182l.75-.356l.008-.005a1.13 1.13 0 0 1 .492-.13c.047 0 .094.004.138.01c.175.029.315.1.354.12l.009.005l.749.356V3.647z"/><path fill="currentColor" d="M12 5.214c-.334-.064-1.057-.161-1.718-.339C8.938 4.515 8.05 3.765 7 3.487c-.887-.234-2.041-.352-3.018-.412C2.886 3.007 2 3.9 2 4.998v11.146c0 1.11.906 2.01 2.015 2.079c.97.06 2.108.179 2.985.41c.486.129 1.216.431 1.873.726c1.005.451 2.052.797 3.127 1.034z" opacity=".5"/><path fill="currentColor" d="M4.273 12.818a.75.75 0 0 1 .91-.545l4 1a.75.75 0 1 1-.365 1.455l-4-1a.75.75 0 0 1-.545-.91m.909-4.545a.75.75 0 1 0-.364 1.455l4 1a.75.75 0 0 0 .364-1.455z"/></svg></a><a class="nav-item" title="关于我" href="/about/" style="color:#F44336"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="m13.629 20.472l-.542.916c-.483.816-1.69.816-2.174 0l-.542-.916c-.42-.71-.63-1.066-.968-1.262c-.338-.197-.763-.204-1.613-.219c-1.256-.021-2.043-.098-2.703-.372a5 5 0 0 1-2.706-2.706C2 14.995 2 13.83 2 11.5v-1c0-3.273 0-4.91.737-6.112a5 5 0 0 1 1.65-1.651C5.59 2 7.228 2 10.5 2h3c3.273 0 4.91 0 6.113.737a5 5 0 0 1 1.65 1.65C22 5.59 22 7.228 22 10.5v1c0 2.33 0 3.495-.38 4.413a5 5 0 0 1-2.707 2.706c-.66.274-1.447.35-2.703.372c-.85.015-1.275.022-1.613.219c-.338.196-.548.551-.968 1.262" opacity=".5"/><path fill="currentColor" d="M10.99 14.308c-1.327-.978-3.49-2.84-3.49-4.593c0-2.677 2.475-3.677 4.5-1.609c2.025-2.068 4.5-1.068 4.5 1.609c0 1.752-2.163 3.615-3.49 4.593c-.454.335-.681.502-1.01.502c-.329 0-.556-.167-1.01-.502"/></svg></a><a class="nav-item" title="思维导图" href="/html/mindmap.html" style="color:#F44336"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="m13.629 20.472l-.542.916c-.483.816-1.69.816-2.174 0l-.542-.916c-.42-.71-.63-1.066-.968-1.262c-.338-.197-.763-.204-1.613-.219c-1.256-.021-2.043-.098-2.703-.372a5 5 0 0 1-2.706-2.706C2 14.995 2 13.83 2 11.5v-1c0-3.273 0-4.91.737-6.112a5 5 0 0 1 1.65-1.651C5.59 2 7.228 2 10.5 2h3c3.273 0 4.91 0 6.113.737a5 5 0 0 1 1.65 1.65C22 5.59 22 7.228 22 10.5v1c0 2.33 0 3.495-.38 4.413a5 5 0 0 1-2.707 2.706c-.66.274-1.447.35-2.703.372c-.85.015-1.275.022-1.613.219c-.338.196-.548.551-.968 1.262" opacity=".5"/><path fill="currentColor" d="M10.99 14.308c-1.327-.978-3.49-2.84-3.49-4.593c0-2.677 2.475-3.677 4.5-1.609c2.025-2.068 4.5-1.068 4.5 1.609c0 1.752-2.163 3.615-3.49 4.593c-.454.335-.681.502-1.01.502c-.329 0-.556-.167-1.01-.502"/></svg></a></nav>
</div>
<div class="widgets">


<widget class="widget-wrapper post-list"><div class="widget-header dis-select"><span class="name">最近更新</span></div><div class="widget-body fs14"><a class="item title" href="/2025/09/08/redis/redis-datatype-hash-and-set/"><span class="title">Redis 数据类型丨Hash&Set</span></a><a class="item title" href="/2025/09/05/qa/qa-traditional-backend-to-ai-engineer/"><span class="title">Q&A丨AI 视角下的后端技术重塑</span></a><a class="item title" href="/2025/08/30/llm/note-llm-from-scratch/"><span class="title">读书笔记丨《从零构建大语言模型》</span></a><a class="item title" href="/2025/08/25/redis/redis-datatype-string/"><span class="title">Redis 数据类型丨String丨从第一性原理看 Redis 字符串的设计哲学 (基于 Redis 8.2.1 源码)</span></a><a class="item title" href="/2025/08/30/graceful-restart-from-tableflip-to-k8s/"><span class="title">优雅重启的范式转移：从 tableflip 到 Kubernetes 的 Go 服务升级终极指南</span></a><a class="item title" href="/2023/12/23/floating-point-number/"><span class="title">一文彻底掌握浮点数</span></a><a class="item title" href="/2025/08/21/llm/guide-to-lr-warmup-cosine-annealing-gradient-clipping/"><span class="title">模型训练核心技巧：学习率预热、余弦衰减与梯度裁剪</span></a><a class="item title" href="/2025/08/20/redis/redis-datatype-list/"><span class="title">Redis 数据类型丨List丨从双向链表到 Listpack 的演进之路 (基于 Redis 8.2.1 源码)</span></a><a class="item title" href="/2025/08/18/llm/pytorch/"><span class="title">告别死记硬背：一份真正理解 PyTorch 核心设计的指南</span></a><a class="item title" href="/2025/08/15/encryption-mode/"><span class="title">从 ECB 到 GCM：理解加密模式的演进</span></a></div></widget>
</div>

</div></aside><div class="l_main" id="main">





<div class="article banner top"><img class="bg lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/banner/qa-traditional-backend-to-ai-engineer.jpg">
  <div class="content">
    <div class="top bread-nav footnote"><div class="left"><div class="flex-row" id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a>
<span class="sep"></span><a class="cap breadcrumb" href="/">文章</a><span class="sep"></span><a class="cap breadcrumb-link" href="/categories/ai%E9%97%AE%E7%AD%94/">ai问答</a></div>
<div class="flex-row" id="post-meta"><span class="text created">发布于：<time datetime="2025-09-05T09:13:00.000Z">2025-09-05</time></span><span class="sep updated"></span><span class="text updated">更新于：<time datetime="2025-09-08T11:44:05.186Z">2025-09-08</time></span></div></div></div>
    
    <div class="bottom only-title">
      
      <div class="text-area">
        <h1 class="text title"><span>Q&A丨AI 视角下的后端技术重塑</span></h1>
        
      </div>
    </div>
    
  </div>
  </div><article class="md-text content"><p>现在（2025.9），AI
技术突飞猛进，既是机会，也是压力。作为一名传统后端工程师（Go/Rust），笔者一直在想：<u>如何顺势完成从后端开发到
AI 应用开发的转型？</u></p>
<p>在摸索这条路的过程中，焦虑常常多于笃定。AI 框架、AI Agent
应用、新技术、新工具层出不穷，标题一个比一个猛——<code>"它终于来了 xxx"</code>、<code>"👋🏻 再见 xxx"</code>、<code>"史上最强 xxx"</code>……</p>
<p>为了把焦虑变成路径，笔者决定和 Gemini
来一场更系统的探讨：<u>不空谈宏论，而是请它在后端工程师日常用到的技术上补上一层"AI
视角"，用一条从已知走向未知的学习路线把问题说清楚。</u></p>
<p>具体来说，笔者希望 Gemini 围绕
Go/Rust、MySQL/Redis/Kafka，以及《DDIA（设计数据密集型应用）》和《亿级流量系统架构设计与实战》这两本书展开，为这些熟悉的主题都加上一个"AI
视角"。</p>
<p>废话不多说，接下来我们来看看 Gemini 给出的答卷吧~</p>
<hr />
<h1 id="术">术</h1>
<h2 id="一ai-时代的基础架构转变">一、AI 时代的基础架构转变</h2>
<p>人工智能（AI）的发展并非要取代经典的后端工程原则，而是要求我们对其进行演进。我们将深入剖析从确定性的、低延迟的
Web
服务到概率性的、高延迟的、有状态系统的根本性转变。这一转变是理解和构建下一代
AI 原生应用的基础。</p>
<h3 id="从同步请求响应到异步任务编排">1.1
从同步请求/响应到异步任务编排</h3>
<p>传统的后端架构建立在同步请求/响应模型之上，客户端发起一个 HTTP
请求，并阻塞等待服务器在几百毫秒内返回结果。然而，大型语言模型（LLM）的引入彻底颠覆了这一范式。一次
LLM
调用可能需要数秒甚至数分钟才能完成，这在传统同步模型中是不可接受的。因此，架构的核心必须转向异步任务编排。</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250905183444729.png" /></p>
<h4 id="任务队列架构">任务队列架构</h4>
<p>任务队列架构（Task Queue
Architecture）是应对高延迟挑战的首要模式。它将耗时的操作与即时响应解耦，从而保证了用户体验的流畅性。其标准流程如下：</p>
<ol type="1">
<li><strong>任务提交</strong>：客户端通过一个初始 API 调用（例如
<code>POST /api/v1/generate_report</code>）提交一个长耗时任务。请求体中包含所有必要参数，如报告主题、数据源等。</li>
<li><strong>任务入队与即时响应</strong>：API
服务器接收到请求后，并不直接执行任务。相反，它将任务封装成一个消息，推送到一个消息队列（如
Kafka 或 RabbitMQ）中。随后，服务器立即向客户端返回一个唯一的
<code>task_id</code>，这个过程通常在几十毫秒内完成，从而释放客户端连接，避免了超时。</li>
<li><strong>异步处理</strong>：一个独立的、可水平扩展的工作者（Worker）集群消费消息队列中的任务。这些工作者负责执行实际的、耗时的
LLM 调用、数据处理和结果生成。</li>
<li><strong>结果持久化</strong>：任务完成后，工作者将结果（或指向结果的引用）存储在一个持久化存储系统中，如关系型数据库或
Redis，并与 <code>task_id</code> 关联。</li>
</ol>
<h4 id="结果交付机制">结果交付机制</h4>
<p>一旦任务被异步处理，客户端需要一种机制来获取最终结果。主要有两种模式：</p>
<ul>
<li><strong>轮询（Polling）</strong>：客户端使用获取到的
<code>task_id</code>，周期性地调用一个状态查询 API（例如
<code>GET /api/v1/tasks/&#123;task_id&#125;/status</code>）。服务器返回任务的当前状态（如
<code>PENDING</code>, <code>IN_PROGRESS</code>, <code>SUCCESS</code>,
<code>FAILED</code>）。当状态为 <code>SUCCESS</code>
时，响应中会包含最终结果或结果的访问链接。这种方法实现简单，但会产生大量无效请求，效率较低。</li>
<li><strong>WebSocket 或服务器推送事件（Server-Sent Events,
SSE）</strong>：这是一种更高效、用户体验更佳的模式。客户端在提交任务后，与服务器建立一个持久连接。当任务完成时，服务器通过此连接主动将结果推送给客户端。对于
LLM 的流式生成（token-by-token streaming），SSE
尤其适用，它能让用户实时看到文本的生成过程，极大地改善了感知延迟。</li>
</ul>
<h4 id="异步系统中的韧性设计">异步系统中的韧性设计</h4>
<p>异步架构的引入也带来了新的韧性挑战。借鉴《设计数据密集型应用》（DDIA）和现代系统设计的原则，必须构建一个能够"为失败而设计"的系统
。</p>
<ul>
<li><strong>死信队列（Dead-Letter Queues, DLQ）</strong>：当一个任务因为
LLM API
错误、数据格式问题或其他原因处理失败，并且重试次数达到上限后，该任务消息不应被丢弃，而应被发送到一个专门的
DLQ。这使得开发人员可以后续分析失败原因，进行手动干预或修复，而不会丢失用户请求
。</li>
<li><strong>指数退避重试（Exponential Backoff
Retries）</strong>：对外部服务（如 LLM
API）的调用可能会遇到瞬时故障或速率限制。工作者在处理失败时，应采用带抖动的指数退避策略进行重试，避免在短时间内用大量重试请求冲击下游服务
。</li>
<li><strong>幂等性（Idempotency）</strong>：在分布式系统中，消息可能会被重复投递。工作者必须设计成幂等的，即多次处理同一个任务消息应产生与一次处理相同的结果。这通常通过在任务消息中包含一个唯一的幂等性密钥来实现，工作者在处理前检查该密钥是否已被处理过
。</li>
</ul>
<h3 id="在分布式对话式世界中管理状态">1.2
在分布式、对话式世界中管理状态</h3>
<p>传统的高并发后端服务通常被设计为无状态的，以便于水平扩展和负载均衡。然而，AI
对话天生就是有状态的。用户发出的第二句"那第二个呢？"完全依赖于第一句的上下文。如果这两次请求被负载均衡到不同的服务实例上，系统将无法理解对话的延续性。</p>
<h4 id="外部化状态存储">外部化状态存储</h4>
<p>解决这个矛盾的规范方案是将状态从服务内存中剥离，存储到外部共享的存储系统中。这种"外部化状态存储"模式确保了任何服务实例都可以通过访问共享存储来获取完整的对话上下文。</p>
<ul>
<li><strong>MySQL/PostgreSQL</strong>：作为对话历史的长期、持久化存储系统。一个设计良好的
schema 应至少包含 <code>users</code>、<code>sessions</code> 和
<code>messages</code> 三张表。<code>messages</code>
表记录每一条消息的内容、发送者（用户或
AI）、时间戳，并通过外键关联到特定的 <code>sessions</code>
表，<code>sessions</code> 表再关联到 <code>users</code>
表。这种结构化的存储不仅保证了数据的持久性，还便于进行后续的分析和审计
。</li>
<li><strong>Redis</strong>：作为 AI
的高性能短期工作记忆。对于正在进行的活跃对话，将其最近的几轮交互历史缓存在
Redis 中，可以极大地降低对主数据库的读取压力。使用 Redis 的
<code>LIST</code> 或 <code>HASH</code>
数据结构，并为每个会话设置一个合理的过期时间（TTL），例如 30
分钟，是一种常见的实践。这确保了在对话期间可以快速加载上下文，同时自动清理不活跃的会话数据
。</li>
</ul>
<h4 id="权衡分析粘性会话">权衡分析：粘性会话</h4>
<p>粘性会话是一种在负载均衡层实现的简单方案，它将来自同一用户的所有请求都路由到同一个服务器实例。虽然这可以解决短期内的状态管理问题，但对于严肃、可扩展的
AI 应用而言，它是一种反模式。其主要缺陷包括：</p>
<ul>
<li><strong>单点故障</strong>：如果该服务器实例宕机，用户的所有会话状态将丢失，对话无法继续。</li>
<li><strong>负载不均</strong>：无法实现真正的负载均衡，可能导致某些服务器实例成为热点，资源利用率低下。</li>
<li><strong>扩展性差</strong>：在服务扩缩容时，会话状态的管理变得复杂，可能导致会话中断。</li>
</ul>
<p>这些缺陷违背了现代分布式系统设计的核心原则——韧性和可扩展性
。因此，外部化状态存储是更加推荐的生产级方案。</p>
<h4 id="对话历史摘要">对话历史摘要</h4>
<p>当对话变得非常长时，将完整的历史记录附加到每个 LLM 请求中会消耗大量的
Token，从而增加成本和延迟。一种高级的优化策略是引入对话摘要机制。系统可以设计一个后台任务，当检测到某个会话的历史记录超过特定长度（例如
5000 个 Token）时，自动调用一个 LLM
来将之前的对话内容浓缩成一段摘要。在后续的请求中，系统只需传递这段摘要和最近几轮的对话，即可在保留关键上下文的同时，显著节省
Token 消耗 。</p>
<h3 id="为概率性系统设计韧性和可观测性">1.3
为概率性系统设计韧性和可观测性</h3>
<p>传统系统的韧性设计主要关注硬件故障、网络分区和软件缺陷等确定性问题
。AI
系统引入了一种全新的、更隐蔽的失败模式：概率性方差。系统可能在基础设施层面完全“健康”，但其输出的内容却是错误的、带有偏见的或有害的。</p>
<h4 id="面向-llm-的可观测性技术栈">面向 LLM 的可观测性技术栈</h4>
<p>传统的监控（Metrics, Logging, Tracing）需要扩展以适应 LLM
的特性。一个完整的 LLM 可观测性技术栈应包括：</p>
<ul>
<li><strong>性能指标</strong>：
<ul>
<li>延迟：首 Token 生成时间（Time-to-First-Token,
TTFT）、总生成时间。</li>
<li>吞吐量：每秒请求数（RPS）、每分钟处理的 Token 数（TPM）。</li>
</ul></li>
<li><strong>成本指标</strong>：
<ul>
<li>Token 消耗：精确追踪每次请求、每个用户、每个功能模块的输入与输出
Token 数量。</li>
<li>API 成本：将 Token 消耗与模型定价关联，实现实时的成本监控。</li>
</ul></li>
<li><strong>质量指标</strong>：
<ul>
<li>幻觉率：追踪模型生成事实性错误的频率。</li>
<li>回答相关性：评估回答是否切中用户问题。</li>
<li>安全性：检测提示注入（Prompt Injection）攻击、有害内容生成等。</li>
<li>模型漂移：监控模型输出的统计分布随时间的变化，以发现性能衰退。</li>
<li>这些质量指标通常需要人工反馈（例如，用户点击“赞”或“踩”）或自动化的评估流水线来衡量。</li>
</ul></li>
<li><strong>追踪（Tracing）</strong>：对于由多个 LLM
调用和工具使用组成的复杂 Agent
链，端到端的分布式追踪至关重要。它能可视化整个执行流程，帮助定位延迟瓶颈或逻辑错误。诸如
Langfuse 这样的专用工具在此领域提供了强大的支持 。</li>
</ul>
<h4 id="优雅降级模式">优雅降级模式</h4>
<p>当 AI 系统面临压力或故障时，应能优雅地降级，而不是完全崩溃。</p>
<ul>
<li><strong>模型回退（Model
Fallbacks）</strong>：设计一个模型优先级策略。当主力的、昂贵的高性能模型（如
GPT-4）调用失败、超时或返回错误时，系统可以自动捕获异常，并使用一个更便宜、更快的次级模型（如
Claude 3.5 Sonnet 或本地部署的 Llama
模型）来处理请求。这保证了服务的可用性，尽管输出质量可能略有下降 。</li>
<li><strong>断路器（Circuit Breakers）</strong>：在调用外部 LLM API
的客户端中实现断路器模式。当 API
的错误率超过预设阈值时，断路器会跳闸，在一段时间内直接拒绝新的请求，而不是让它们超时。这可以防止单个下游服务的故障引发整个系统的级联崩溃。</li>
<li><strong>确定性回退（Deterministic
Fallbacks）</strong>：对于那些必须返回结构化数据（如
JSON）的关键任务，如果 LLM
在多次重试后仍然无法生成格式正确的输出，系统应放弃 LLM
调用，转而执行一个简单的、基于规则的确定性逻辑，以确保核心功能的健壮性
。</li>
</ul>
<h4 id="失败定义的演进">"失败"定义的演进</h4>
<p>在 AI
驱动的系统中，"失败"不再是一个简单的二元状态（正常/宕机），而是一个质量降级的连续谱。传统服务的失败是明确的，例如返回一个
HTTP 500 错误或请求超时。而一个 LLM
服务的"失败"则可能是返回一个语法完美、结构正确的 JSON
对象，但其中却包含着微妙的幻觉、逻辑矛盾或偏见言论 。</p>
<p>这种转变意味着传统的健康检查（health
check）机制，即简单地检查服务是否返回 HTTP
200，已经变得毫无意义。一个真正具有韧性的 AI
系统，其健康的概念必须从"可用性"扩展到"质量"。这要求建立一个"<strong>语义健康检查</strong>"层。该层会持续地运行一组预定义的黄金测试用例（golden
test cases），或根据业务规则对模型的实时输出进行评估，从而量化模型的质量
。</p>
<p>因此，AI 系统的韧性工程与
MLOps（机器学习运维）变得密不可分。后端团队现在必须将自动化评估流水线（Evals
pipeline）作为生产准备和监控的核心组成部分。模型的质量严重下降应被视为与服务宕机同等级别的
P1 级事故，并触发相应的告警和应急响应流程</p>
<h2 id="二数据层的重构为-ai-设计存储于检索">二、数据层的重构：为 AI
设计存储于检索</h2>
<p>在 AI
时代，数据层的功能被极大地扩展了。它不再仅仅是存储业务数据的仓库，而是扮演着
AI
系统的长期记忆、短期记忆和语义记忆的角色。本部分将重新审视数据库技术，并探讨如何为
AI 应用构建一个高效、可扩展的数据基石。</p>
<h3 id="关系型数据库mysqlpostgresai-的记忆系统">2.1
关系型数据库（MySQL/Postgres）：AI 的记忆系统</h3>
<p>尽管向量数据库在 AI
领域备受关注，但关系型数据库（RDBMS）仍然是任何生产级检索增强生成（Retrieval-Augmented
Generation, RAG）系统的核心支柱。它为 LLM
处理的非结构化数据提供了结构化的元数据和"地面实况"（ground
truth），是系统可信度和可追溯性的保障。</p>
<p><strong>高级 RAG 数据库 Schema 设计</strong></p>
<p>一个生产就绪的 RAG 系统需要一个精心设计的数据库
schema，以管理从原始文档到最终对话的全生命周期。以下是一个经过验证的
schema 范例 ：</p>
<ul>
<li><strong><code>documents</code> 表</strong>：存储源文档的元数据。
<ul>
<li><code>id</code> (PK), <code>file_name</code>,
<code>source_url</code>, <code>document_type</code> (e.g., PDF,
Markdown), <code>uploader_id</code> (FK), <code>processing_status</code>
(e.g., PENDING, PROCESSED, FAILED), <code>created_at</code>,
<code>updated_at</code>。</li>
</ul></li>
<li><strong><code>document_chunks</code> 表</strong>：这是 RAG
的核心表，存储切分后的数据块。
<ul>
<li><code>id</code> (PK), <code>document_id</code> (FK to
<code>documents</code>), <code>chunk_text</code> (TEXT),
<code>chunk_metadata</code> (JSONB, e.g., page number, section headers),
<code>vector_id</code> (VARCHAR, indexed)。</li>
<li><code>vector_id</code>
字段至关重要，它建立了关系型数据与向量数据库中嵌入向量之间的一一对应关系。这使得当从向量数据库检索到一个相似的向量时，系统可以快速回溯到其原始文档、上下文和元数据，实现了完整的可追溯性。</li>
</ul></li>
<li><strong><code>prompts</code> 表</strong>：用于版本化管理 Prompt
模板。
<ul>
<li><code>id</code> (PK), <code>prompt_name</code> (VARCHAR, unique),
<code>version</code> (INT), <code>template_text</code> (TEXT),
<code>variables</code> (JSONB), <code>is_active</code> (BOOLEAN)。</li>
<li>将 Prompt
与应用代码解耦，允许运营或算法人员在不重新部署服务的情况下，通过修改数据库中的模板来优化、测试和回滚
Prompt，这是关键的 MLOps 实践。</li>
</ul></li>
<li><strong><code>conversation_history</code> 表</strong>：如 1.2
节所述，用于持久化存储对话历史。
<ul>
<li><code>id</code> (PK), <code>session_id</code> (FK),
<code>user_id</code> (FK), <code>message_content</code> (TEXT),
<code>sender_role</code> (e.g., 'user', 'ai'),
<code>timestamp</code>。</li>
</ul></li>
</ul>
<h3 id="内存数据库redisai-的工作记忆">2.2 内存数据库（Redis）：AI
的工作记忆</h3>
<p>Redis 的亚毫秒级延迟使其成为 AI 系统理想的 L1
缓存或工作记忆，能够显著降低重复性或状态依赖操作的延迟和成本。</p>
<p><strong>语义缓存（Semantic Caching）</strong></p>
<p>这是 Redis 在 AI
领域最具变革性的应用。传统的缓存基于键的精确匹配，而语义缓存则基于含义的相似性。其工作流程如下：</p>
<ol type="1">
<li>当系统收到一个新的用户查询时，首先调用嵌入模型将其转换为一个向量。</li>
<li>然后，在一个专门用于存储"历史查询及其答案"的 Redis
向量索引中，执行一次向量相似性搜索。</li>
<li>如果找到了一个或多个在语义上高度相似（例如，余弦相似度 &gt;
0.95）且已被回答过的查询，系统可以直接返回其缓存的答案，从而完全跳过对昂贵的
LLM API 的调用。</li>
<li>对于问答机器人、客服等场景，大量用户的提问在语义上是重复的。语义缓存这一模式能够拦截大部分此类请求，极大地降低
API 调用成本和响应延迟。</li>
</ol>
<p><strong>对话历史缓存</strong></p>
<p>如 1.2 节所述，将活跃对话的最近几轮交互缓存在 Redis 中，并设置
TTL。这避免了在对话的每一次轮转中都去查询主数据库，显著提升了交互的流畅性。</p>
<p><strong>Agent 中间步骤缓存</strong></p>
<p>对于复杂的、多步骤的 Agent
工作流（例如：规划一次为期五天的东京旅行），Agent
可能需要依次调用多个工具（查询航班、搜索酒店、规划行程等）。可以将每一步工具执行成功后的结果缓存在
Redis 中，键为 <code>task_id</code> 和 <code>step_number</code>。如果
Agent 在第四步失败需要重试，它可以直接从 Redis
中加载前三步的缓存结果，而无需从头开始执行，这节省了大量的时间和 API
调用成本。</p>
<p><img
src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250905215321383.png" /></p>
<h3 id="rag-摄入流水线数据密集型设计的案例研究">2.3 RAG
摄入流水线：数据密集型设计的案例研究</h3>
<p>生产级的 RAG 首先是一个数据工程问题，其次才是一个 LLM
问题。最终输出的质量遵循"垃圾进，垃圾出"的原则，而许多"垃圾"正是在数据摄入和切块（Chunking）阶段产生的
。</p>
<p><strong>流水线阶段</strong></p>
<p>一个健壮的 RAG 摄入流水线应包含以下阶段：</p>
<ol type="1">
<li><strong>加载 (Load)</strong>：使用文档加载器从各种数据源（如
S3、网页、Notion、Confluence）摄入原始文档 。</li>
<li><strong>提取与清洗 (Extract &amp; Clean)</strong>：从 PDF、HTML
等格式中解析出纯文本。然后进行数据清洗，包括移除模板化的页眉页脚、标准化文本格式（如统一编码）、纠正常见拼写错误等
。</li>
<li><strong>切块
(Chunk)</strong>：这是整个流水线中最关键、也最需要技巧的一步。切块的质量直接决定了检索结果的质量。</li>
</ol>
<p><strong>切块策略对比分析</strong></p>
<p>切块是一个看似简单但实则复杂的问题，错误的选择会导致大海捞针或上下文中丢失等问题，即相关信息被切分到不同块中或被淹没在大量不相关信息中，从而影响
LLM 的最终表现
。将切块从一门艺术转变为一项工程决策，需要对不同策略进行系统性评估。</p>
<p>即相关信息被切分到不同块中或被淹没在大量不相关信息中，从而影响 LLM
的最终表现
。将切块从一门“艺术”转变为一项工程决策，需要对不同策略进行系统性评估。</p>
<table>
<colgroup>
<col style="width: 13%" />
<col style="width: 28%" />
<col style="width: 19%" />
<col style="width: 18%" />
<col style="width: 19%" />
</colgroup>
<thead>
<tr class="header">
<th>策略名称</th>
<th>描述</th>
<th>优点</th>
<th>缺点</th>
<th>最佳适用场景</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>固定大小切块 (Fixed-Size)</strong></td>
<td>按固定数量的字符或 Token 进行切分，可设置重叠部分</td>
<td>实现简单，块大小可预测，便于批处理。</td>
<td>常常会粗暴地切断句子和完整的语义单元，破坏上下文。</td>
<td>格式统一、无明显结构特征的简单文本，如日志文件。</td>
</tr>
<tr class="even">
<td><strong>递归字符切块 (Recursive Character)</strong></td>
<td>使用一个优先级列表（如 <code>\n\n</code>, <code>\n</code>,
<code></code>）进行递归切分，直到块大小达标</td>
<td>努力尊重原文的段落、句子等语义边界，是很好的通用选择。</td>
<td>对于格式混乱的文本，仍可能产生不理想的切分。</td>
<td>大多数通用文本文档，如新闻文章、博客、网页内容。</td>
</tr>
<tr class="odd">
<td><strong>文档感知切块 (Document-Aware)</strong></td>
<td>根据文档自身的结构进行切分，如按 Markdown 的标题、HTML
的标签、代码的函数或类</td>
<td>生成的块具有极高的语义内聚性和上下文完整性。</td>
<td>需要为每种文档类型编写或使用特定的解析器。</td>
<td>结构化或半结构化文档，如 Markdown、HTML、源代码文件。</td>
</tr>
<tr class="even">
<td><strong>语义切块 (Semantic)</strong></td>
<td>使用嵌入向量将语义上相似的句子聚合在一起，形成一个块</td>
<td>语义内聚性最高；块的划分基于“意义”而非语法或格式。</td>
<td>计算成本高，需要在切块阶段额外进行一次嵌入和聚类。</td>
<td>内容密集、叙事性强的文本，其中概念和主题比结构更重要。</td>
</tr>
<tr class="odd">
<td><strong>Agentic 切块 (Agentic)</strong></td>
<td>利用 LLM 自身来判断文档中最合理的切分边界</td>
<td>潜力巨大，能模拟人类编辑的逻辑来切分复杂文档。</td>
<td>速度极慢、成本高昂且结果不确定，目前仍处于实验阶段。</td>
<td>其他方法均告失败的、高度复杂和异构的文档。</td>
</tr>
</tbody>
</table>
<h2 id="三神经系统使用-kafka-进行异步处理">三、神经系统：使用 Kafka
进行异步处理</h2>
<p>在本部分中，Kafka
的角色被重新定义：它不再仅仅是一个用于服务解耦的消息队列，而是整个 AI
系统的中央数据总线和"神经系统"，负责编排从数据摄入到智能体协作的各种复杂工作流。</p>
<h3 id="kafka-作为-rag-和微调流水线的支柱">3.1 Kafka 作为 RAG
和微调流水线的支柱</h3>
<p><strong>RAG 摄入流水线</strong></p>
<p>如前所述，Kafka 是构建可扩展、可观测的 RAG
摄入流水线的理想选择。一个标准的 Kafka 拓扑结构如下 ：</p>
<ol type="1">
<li><strong><code>documents-to-process</code> 主题
(Topic)</strong>：当新文档被上传或发现时，一个生产者服务将文档的 URI
或引用发布到此主题。</li>
<li><strong><code>chunks-to-embed</code>
主题</strong>：一个切块器（Chunker）服务消费
<code>documents-to-process</code>
主题。它下载文档、根据预定策略进行切块，然后将每个数据块（包含文本和元数据）作为独立消息发布到此主题。</li>
<li><strong><code>chunks-to-index</code>
主题</strong>：一个嵌入器（Embedder）服务消费
<code>chunks-to-embed</code>
主题。它调用嵌入模型为每个数据块生成向量，然后将包含数据块、元数据和向量的消息发布到此主题。</li>
<li><strong>索引</strong>：最后，一个索引器（Indexer）服务消费
<code>chunks-to-index</code>
主题，并将数据块的元数据写入关系型数据库，同时将向量写入向量数据库。</li>
</ol>
<p>这种基于 Kafka
的流水线架构具有高度的解耦性。每个服务（切块器、嵌入器、索引器）都可以独立开发、部署、扩展和监控，从而构建一个极具弹性和性能的系统。</p>
<p><strong>微调反馈闭环</strong></p>
<p>除了数据摄入，Kafka
还能构建一个实时的模型微调（Fine-Tuning）反馈闭环，实现模型的持续学习和优化
。</p>
<ol type="1">
<li><strong>收集反馈</strong>：应用前端或后端服务将用户的隐式或显式反馈（例如，对回答点"赞"或"踩"、用户手动修正
AI 的回答、对话的最终满意度评分等）作为事件发布到 Kafka 的
<code>feedback</code> 主题。</li>
<li><strong>实时处理</strong>：一个流处理应用（如使用 Apache Flink 或
Kafka Streams）消费 <code>feedback</code>
主题。它对反馈数据进行实时聚合、过滤和清洗，筛选出高质量的训练样本（例如，被用户明确标记为"好"的问答对）。</li>
<li><strong>数据沉淀</strong>：处理后的高质量数据被写入一个专门用于模型训练的数据湖或数据仓库。</li>
<li><strong>触发微调</strong>：当积累的新训练样本达到一定数量（例如 1000
条）时，流处理应用会发布一个事件到 <code>trigger-finetuning-job</code>
主题。</li>
<li><strong>自动化训练</strong>：一个 MLOps
服务监听此主题，并自动启动一个新的模型微调作业，使用最新的数据对基础模型进行优化。</li>
</ol>
<p>这个闭环将用户交互与模型迭代无缝连接起来，使 LLM
能够动态地适应特定领域的需求和用户偏好。</p>
<h3 id="为-llm-工作负载设计-kafka">3.2 为 LLM 工作负载设计 Kafka</h3>
<p>将 Kafka 应用于 LLM 工作负载时，需要考虑其独特的数据特性。</p>
<p><strong>主题与分区策略</strong></p>
<p>为了保证对话的上下文顺序，使用一个与对话或用户相关的标识符（如
<code>user_id</code> 或
<code>session_id</code>）作为消息的分区键（Partition
Key）至关重要。这确保了来自同一个对话的所有事件都会被发送到同一个分区，并由同一个消费者实例按顺序处理，从而避免了上下文错乱的问题
。</p>
<p><strong>消息负载管理</strong></p>
<p>LLM
的请求和响应，尤其是包含长对话历史的，可能会非常大。处理大消息负载有以下策略：</p>
<ul>
<li><strong>序列化与压缩</strong>：使用如 Avro
这样的二进制序列化框架，它提供了 schema 演进的支持，比 JSON
更紧凑。同时，启用高效的压缩算法（如 lz4 或
zstd）可以显著减少消息的体积，降低网络传输和存储开销 。</li>
<li><strong>声明检查模式</strong>：对于超过 Kafka 消息大小限制（通常为
1MB）的超大负载，可以采用声明检查模式。即将实际的大负载内容（如整个文档）存储在外部对象存储（如
S3）中，而在 Kafka 消息中只传递该对象的引用（URI 或
key）。消费者接收到消息后，再根据引用去 S3 下载完整内容。</li>
</ul>
<p><strong>消费者组扩展</strong></p>
<ul>
<li><strong>无状态任务</strong>：对于像"嵌入器"这样无状态的任务，可以通过增加消费者组中的消费者实例数量来轻松实现水平扩展，从而提高处理吞吐量。</li>
<li><strong>有状态任务</strong>：对于需要维护顺序的有状态任务（如处理特定用户的对话流），扩展性取决于分区的数量。增加分区数可以提高并行度，但需要预先规划。</li>
</ul>
<h3 id="从服务解耦到智能体编排">3.3 从服务解耦到智能体编排</h3>
<p>Kafka 在 AI
系统中的应用，代表了一次深刻的架构范式演进。在传统的微服务架构中，Kafka
主要用于服务解耦，以提升系统的韧性和可扩展性。而在多智能体（Multi-Agent）AI
系统中，Kafka
的角色升华为智能体之间的发现、协作与通信总线，从而催生出预先未明确设计的复杂、涌现式工作流。</p>
<p><strong>基于 Kafka 的智能体架构</strong></p>
<p>一个复杂的任务，如“分析某公司的最新季度财报”，可以被分解并由多个专职智能体通过
Kafka 协作完成 ：</p>
<ol type="1">
<li>一个<strong>编排者智能体 (Orchestrator Agent)</strong>
接收到高层目标后，将其分解，并向 <code>tasks</code>
主题发布一个初始任务，例如
<code>&#123;"task_type": "fetch_financial_report", "company": "XYZ"&#125;</code>。</li>
<li>一个专门的<strong>搜索智能体 (Search Agent)</strong> 订阅了
<code>fetch_financial_report</code>
类型的任务。它监听到该任务后，执行网络搜索或 API
调用，找到财报，然后将财报的原始内容发布到 <code>data-to-analyze</code>
主题。</li>
<li>一个<strong>分析智能体 (Analysis Agent)</strong> 订阅了
<code>data-to-analyze</code>
主题。它读取财报内容，进行关键指标提取和分析，然后将结构化的分析结果发布到
<code>analysis-results</code> 主题。</li>
<li>一个<strong>总结智能体 (Summarization Agent)</strong> 订阅了
<code>analysis-results</code>
主题，读取分析结果并生成一份人类可读的摘要报告，发布到
<code>final-reports</code> 主题。</li>
<li>最后，编排者智能体从 <code>final-reports</code>
主题获取最终报告，并将其呈现给用户。</li>
</ol>
<p>这种架构是去中心化且高度可扩展的。未来如果需要增加新的能力，例如"情感分析"，只需开发一个新的<strong>情感分析智能体</strong>，让它订阅
<code>data-to-analyze</code> 主题，并将结果发布到一个新的
<code>sentiment-results</code> 主题即可，而无需修改任何现有智能体的代码
。</p>
<p><strong>Kafka 作为涌现式智能的基础</strong></p>
<p>传统事件驱动架构（EDA）中的事件通常是事实的宣告，例如
<code>OrderCreated</code>
。工作流是相对固定的，服务的响应是被动和预定义的。</p>
<p>相比之下，多智能体系统中的通信更具动态性和目的性
。一个智能体发布的消息可能不是一个事实，而是一个子目标、一个证据片段或一个求助请求。Kafka
在此扮演了经典 AI 中的黑板系统（Blackboard
System）角色：一个共享的协作空间。一个智能体的输出可以成为另一个智能体自发行动的输入，而无需一个中心化的编排器对它们进行显式的硬编码连接。</p>
<p>这意味着系统的整体智能超越了其各个组成部分智能的总和。后端架构师的角色也随之演变：<u>不再仅仅是设计数据管道的管道工，而是设计一个能让智能体高效互动的数字生态系统的设计师</u>。这种架构范式的转变，将是未来构建更高级、更自主
AI 系统的关键。</p>
<h2 id="四高性能实现go-与-rust">四、高性能实现：Go 与 Rust</h2>
<p>将架构理念转化为现实，需要选择合适的编程语言。本部分将探讨为什么 Go
和 Rust 这两种现代语言在 AI
后端堆栈的不同层次上表现出色，并如何协同工作以构建高性能系统。</p>
<h3 id="go并发-ai-编排语言">4.1 Go：并发 AI 编排语言</h3>
<p>Go 语言的并发模型，特别是其轻量级线程 Goroutine 和用于通信的
Channel，与 AI 后端的核心工作负载——编排大量并发的、I/O 密集的对 LLM API
和其他外部服务的调用——几乎完美契合 。</p>
<p>Go 中的生产级并发模式：</p>
<ul>
<li><p><strong>扇出/扇入模式 (Fan-out/Fan-in) 用于并行 RAG
检索</strong>：在 RAG
的检索阶段，为了获取最全面的上下文，通常需要同时从多个数据源（如向量数据库、关系型数据库、全文搜索引擎、Web
API）进行查询。使用 Go 的 <code>sync.WaitGroup</code> 和
Channel，可以轻松实现并行检索，从而将总延迟降低到最慢的那个数据源的延迟水平。</p>
<p>一个具体的实现模式是：为每个数据源启动一个 Goroutine 进行查询。所有
Goroutine 将其结果发送到同一个 Channel。主 Goroutine
等待所有查询完成后（通过 <code>WaitGroup</code>），再从 Channel
中收集并合并所有结果。</p></li>
<li><p><strong>工作者池 (Worker Pools) 用于 API 速率限制</strong>：LLM
API 通常有每分钟请求数（RPM）和每分钟 Token
数（TPM）的限制。为了避免因超出限制而被拒绝服务（HTTP 429
错误），可以使用 Go 实现一个工作者池。该池维护固定数量的
Goroutine，从一个任务 Channel 中获取请求并执行。这可以有效地控制对外部
API 的并发调用数量，平滑请求峰值，并实现优雅的背压（backpressure）
。</p></li>
<li><p><strong>使用 Channel
实现流式响应</strong>：为了提供更好的用户体验，LLM
的响应通常以流式（token-by-token）方式返回。Go 的 Channel
是实现这一功能的理想工具。后端服务在接收到 LLM API 返回的 token
流时，可以立即将其写入一个 Channel。另一个 Goroutine 则从该 Channel 读取
token，并通过服务器推送事件（SSE）或 WebSocket
将其转发给前端客户端。</p></li>
</ul>
<h3 id="rust高性能推理语言">4.2 Rust：高性能推理语言</h3>
<p>如果说 Go 是 AI 编排层的王者，那么 Rust 则在 AI
技术栈性能最关键的核心——推理引擎——中大放异彩。Rust
对内存安全的极致追求、零成本抽象以及对底层硬件的精细控制，使其成为从 GPU
硬件中压榨出每一分性能的理想选择 。</p>
<p>Rust 在 AI 技术栈中的角色：</p>
<ul>
<li><strong>推理引擎</strong>：尽管许多流行的推理框架（如 vLLM）使用
Python
构建，但在追求极致效率和低资源消耗的生产环境中，越来越多的公司开始转向
Rust。例如，Cloudflare 使用 Rust 开发其下一代 LLM 推理引擎
Infire，以期在低级别实现细节上获得完全控制，从而最大化内存、网络 I/O 和
GPU 的利用率，超越 Python 方案的性能极限 。</li>
<li><strong>性能关键的数据管道组件</strong>：在数据处理流水线中，某些环节（如自定义的分词器、高效的数据序列化/反序列化层）对性能要求极高。Rust
是构建这些组件的绝佳选择，其性能可以媲美
C/C++，同时提供了现代语言的内存安全保障。</li>
</ul>
<h3 id="gorust-协同工作的多语言架构">4.3 Go/Rust
协同工作的多语言架构</h3>
<p>对于复杂的 AI
系统，推荐采用多语言（Polyglot）架构，以发挥不同语言的优势：</p>
<ul>
<li><strong>Go 作为编排层</strong>：负责处理 API
网关、业务逻辑、服务间通信、工作流编排等上层任务。其强大的并发能力和简洁的语法非常适合快速开发和维护复杂的分布式服务。</li>
<li><strong>Rust
作为性能核心</strong>：负责实现性能最敏感的"热路径"组件，如推理服务、嵌入模型服务或高性能数据预处理库。这些
Rust 组件可以作为独立的服务部署，或通过外部函数接口（FFI）被 Go
服务调用。</li>
</ul>
<p>这种架构组合了 Go 的开发效率和 Rust 的运行效率，是构建生产级、高性能
AI 系统的理想范式</p>
<h2 id="五高级架构范式与综合">五、高级架构范式与综合</h2>
<p>本部分将综合前述内容，探讨如何应对 AI
系统中最棘手的挑战，并展望未来的架构发展趋势。</p>
<h3 id="驯服不确定性生产就绪工作流的工具箱">5.1
驯服不确定性：生产就绪工作流的工具箱</h3>
<p>LLM
的内在不确定性（Non-determinism）是其在金融、医疗等任务关键型企业系统中应用的最大障碍
。即使设置
<code>temperature=0</code>，相同的输入在不同时间也可能产生不完全相同的输出，这给系统的可靠性和可测试性带来了巨大挑战
。以下是一个旨在为概率性模型构建确定性“护栏”的综合工具箱。</p>
<p>提升可靠性的策略：</p>
<ol type="1">
<li><strong>强制结构化输出 (Structured Outputs)</strong>：利用 LLM
提供商的特定功能（如 OpenAI 的 JSON 模式、Anthropic
的工具调用功能），强制模型返回符合预定义 JSON schema
的输出。这从根本上消除了因格式错误导致的解析失败，是保证系统健壮性的第一步
。</li>
<li><strong>严格的验证层 (Validation Layer)</strong>：在接收到 LLM
的结构化输出后，绝不能直接信任其内容。必须将其传递给一个严格的验证层（例如，在
Python 中使用 Pydantic，在 Go
中使用结构体验证库）进行数据类型、范围和业务逻辑的校验，然后再传递给下游系统。</li>
<li><strong>带反馈的智能重试 (Intelligent Retries with
Feedback)</strong>：如果验证失败，简单的重试（即重复发送相同的
Prompt）效果不佳。更智能的方法是构建一个新的 Prompt，其中包含原始
Prompt、模型返回的错误输出以及具体的验证错误信息，然后请求 LLM
"请根据以下错误修正你的输出"。这种"自我修正"的循环能显著提高最终成功的概率
。</li>
<li><strong>确定性解码 (Deterministic
Decoding)</strong>：在要求高度一致性的场景下，可以通过解码策略来降低随机性。
<ul>
<li><strong>贪心解码 (Greedy Decoding)</strong>：将模型的
<code>temperature</code> 参数设置为 0，使模型在每一步都选择概率最高的
Token。</li>
<li><strong>固定随机种子 (Fixing the
Seed)</strong>：在支持设置随机种子的模型或框架中，固定该值。</li>
<li>虽然这些方法不能 100%
保证确定性（因为底层硬件和软件优化也可能引入随机性），但它们能极大地减少输出的可变性
。</li>
</ul></li>
<li><strong>集成方法 (Ensemble
Methods)</strong>：向多个不同的模型（或同一个模型多次）提交相同的请求，然后对返回的多个结果进行投票或合并处理。例如，对于分类任务，采用少数服从多数的原则；对于内容生成任务，可以选择最一致或最全面的答案。这种方法以增加成本和延迟为代价，换取了更高的稳定性和可靠性
。</li>
</ol>
<h3 id="智能体架构的兴起">5.2 智能体架构的兴起</h3>
<p>许多 AI 应用的未来正从简单的提示-响应或 RAG
模式，转向更自主的智能体（Agent）模式。智能体能够进行推理、规划，并使用工具来完成复杂的多步骤目标
。</p>
<p>关键的智能体设计模式：</p>
<ul>
<li><strong>工具使用 (Tool
Use)</strong>：这是智能体架构的核心。系统需要设计成允许 LLM
根据用户意图，自主决定调用哪些外部工具（如
API、数据库查询、代码执行器）来获取信息或执行操作。架构师的核心任务是为这些工具的调用设计一个安全、可靠且可观测的执行环境
。</li>
<li><strong>反思/自我批判 (Reflection /
Self-Critique)</strong>：这是一种强大的模式，智能体能够评估并迭代改进自己的输出。例如，一个智能体首先生成报告的初稿；然后，系统启动一个批评家智能体（或使用不同
Prompt
的同一个智能体）来审查初稿，指出其中的逻辑谬误、事实错误或风格问题；最后，初代智能体根据批评意见生成一份经过修订的终稿。这个过程模拟了人类的写作和审查流程，能够显著提升输出质量
。</li>
<li><strong>规划
(Planning)</strong>：对于复杂任务（例如，为我的团队组织一次异地团建），智能体需要具备将其分解为一系列可执行子任务的能力（例如：1.
收集团队成员的偏好；2. 搜索符合条件的场地；3. 检查场地可用性；4.
预订场地和交通等）。像 LangGraph
这样的新兴框架，正致力于为这种有状态的、循环的智能体工作流提供结构化的编程模型
。</li>
</ul>
<h2 id="总结现代-ai-工程师的融合技能栈">总结：现代 AI
工程师的融合技能栈</h2>
<p>"术"篇系统性地剖析了在 AI
时代，后端架构师所面临的核心挑战与范式转变。从根本上说，构建 AI
原生应用不是对传统分布式系统知识的颠覆，而是一次深刻的演进和融合。</p>
<p>现代高级后端工程师必须成为一个多面手，其技能栈需要融合三大领域的深度专业知识：</p>
<ol type="1">
<li><strong>分布式系统设计</strong>：DDIA
中关于可靠性、可扩展性和可维护性的原则依然是基石。但现在必须用新的视角去应用它们，以应对高延迟、有状态和概率性失败等新挑战。</li>
<li><strong>数据工程</strong>：精通 Kafka
这样的数据流平台，以及掌握从数据摄入、清洗、切块到索引的全套 RAG
流水线构建能力，已成为核心竞争力。数据质量直接决定了 AI
系统的智能上限。</li>
<li><strong>机器学习运维
(MLOps)</strong>：后端工程师不再能将模型视为黑盒。必须构建和维护面向 LLM
的可观测性系统，建立自动化的模型评估和反馈闭环，并将模型的质量问题视为与系统宕机同等重要的生产事故。</li>
</ol>
<p>最终，衡量卓越工程能力的新标准，在于是否能架构出不仅可扩展、可靠，而且在面对概率性不确定性时依然具有适应性和韧性的系统。掌握这种在确定性工程与概率性智能之间游刃有余的能力，将是定义下一代顶尖技术专家的关键。</p>
<h1 id="道">道</h1>
<p>前面我们深入探讨了在 AI
时代如何应用各项后端技术的"术"。然而，任何精妙的"术"都源于其背后的"道"——那些不随具体技术更迭而改变的根本性原则。若想在
AI 浪潮中立于不败之地，我们必须掌握这些第一性原理。</p>
<p>所以在了解了上篇 Gemini 给出的种种 AI
应用开发解决方案之后，我们尝试让 Gemini 为我们梳理在 AI
后端架构下的四大不变法则，它们是构建未来智能系统的思想基石，帮助我们理解万变中的根本。</p>
<h2 id="法则一世界观的革命-从确定性到概率性">法则一：世界观的革命 ——
从确定性到概率性</h2>
<p>软件工程，在其诞生以来的大部分时间里，都是一个建立在确定性（Determinism）基石上的理想国
。在这里 <code>2+2</code> 永远等于
<code>4</code>，一个函数对相同的输入，永远返回相同的输出。我们的核心挑战是管理复杂的、但可预测的逻辑交互
。这使得软件工程长期以来成为工程领域的异类。一位土木工程师设计的桥梁必须考虑材料强度的公差、风载荷的随机性，其设计目标是"大概率不会坍塌"；而软件工程师则追求"绝对正确"。</p>
<p>大型语言模型（LLM）的出现，彻底颠覆了这一确定性的世界观。当你向 LLM
发出一个提示（Prompt）时，你并非在调用一个传统的函数，而是在<strong>从一个庞大的概率分布中进行采样</strong>
。这意味着，即使所有参数（如
<code>temperature=0</code>）都设为最确定的模式，两次完全相同的输入也可能产生不完全相同的输出，这种现象被称为非确定性（Non-determinism）。</p>
<p>这一根本性的转变，要求我们从"道"的层面重塑对系统设计的认知：</p>
<ol type="1">
<li><strong>"正确"的重新定义</strong>：在确定性世界里，正确是二元的（对或错）。在概率性世界里，正确变成了一个<strong>统计学概念</strong>。一个
AI
系统的输出不再是绝对正确，而是"在多大置信度上是可接受的"。系统的目标从"杜绝错误"转变为"管理和量化错误率"。</li>
<li><strong>"测试"的范式迁移</strong>：传统的单元测试依赖于断言
<code>assert function(input) == expected_output</code>。当
<code>function</code>
的输出是概率性的，这种测试方法便失效了。新的测试范式必须转向<strong>统计验证</strong>：运行大量测试用例，评估输出的整体分布是否符合预期，衡量幻觉率、相关性等宏观质量指标
。</li>
<li><strong>"失败"的认知升级</strong>：传统系统的失败是显性的，如服务宕机、API
返回 500 错误。AI
系统的失败则更加隐蔽和复杂，它可能在基础设施层面完全健康，但其输出的内容却是错误的、有害的或带有偏见的
。因此，架构师必须将<strong>"质量降级"视为与"服务宕机"同等级别的生产事故</strong>，并为此设计相应的监控、告警和优雅降级机制。</li>
</ol>
<blockquote>
<p>[!IMPORTANT]</p>
<p><strong>核心心法</strong>：放弃对绝对控制的执念，拥抱并管理不确定性。将系统设计从追求"不出错的逻辑"转变为构建一个<strong>能够包容、评估并从概率性组件的错误中恢复的、具有韧性的确定性框架</strong>
。</p>
</blockquote>
<h2 id="法则二思维的跃迁-从无状态服务到有状态推理">法则二：思维的跃迁 ——
从无状态服务到有状态推理</h2>
<p>在过去的十年里，为了应对海量并发，微服务架构的核心信条之一就是<strong>无状态（Stateless）</strong>
。任何一个服务实例都可以处理任何一个请求，因为状态被外部化到了共享的数据库或缓存中。这极大地简化了水平扩展和故障恢复。</p>
<p>然而，AI
的核心能力——无论是多轮对话还是复杂的智能体（Agent）规划——都<strong>天生强依赖于上下文，即本质上是有状态的（Stateful）</strong>。一个没有记忆的
AI
无法进行有意义的推理。这种对状态的内在需求，迫使我们进行一次深刻的思维跃迁。</p>
<ol type="1">
<li><strong>"状态"的内涵扩展</strong>：在传统后端语境中，状态通常指用户的会话数据（Session）或购物车信息。在
AI 语境中，状态的内涵被极大地丰富了，它演变成了 AI
的<strong>记忆系统</strong>，一个多层次、多维度的认知核心 。
<ul>
<li><strong>短期记忆（工作记忆）</strong>：当前对话的上下文，用于即时交互，通常存储在
Redis 等高速缓存中 。</li>
<li><strong>长期记忆</strong>：跨越多次会话的知识和用户偏好，是实现个性化的基础
。</li>
<li><strong>情景记忆（Episodic
Memory）</strong>：对特定事件和过去交互的记忆，用于从具体经验中学习
。</li>
<li><strong>语义记忆（Semantic
Memory）</strong>：关于世界的事实、规则和知识，通常通过 RAG
从知识库中检索 。</li>
</ul></li>
<li><strong>架构角色的转变</strong>：过去，状态管理是被剥离和外部化的对象，以保证核心服务的纯粹无状态。现在，<strong>记忆管理（Memory
Management）本身成为了 AI
系统的核心架构组件</strong>。架构师的工作不再仅仅是选择一个数据库来存储状态，而是要设计一个高效、分层的记忆系统，确保
AI 能够在正确的时间、以正确的成本，访问到正确的上下文信息 。</li>
</ol>
<blockquote>
<p>[!IMPORTANT]</p>
<p><strong>核心心法</strong>：将 AI
的"状态"从需要管理的负担，升维为需要精心设计的核心资产。架构的重心从"如何消除状态"转变为<strong>"如何构建一个高效、持久且可扩展的认知记忆核心"</strong>。</p>
</blockquote>
<h2 id="法则三优化的新方程-从效率到价值">法则三：优化的新方程 ——
从效率到价值</h2>
<p>传统后端系统的优化目标非常纯粹：在有限的资源下，追求<strong>更低延迟（Latency）</strong>和<strong>更高吞吐量（Throughput）</strong>。这是一个二维的优化问题，工程师们通过算法优化、缓存、异步处理等手段，在这个二维空间里寻找最优解。</p>
<p>LLM
的引入，为这个经典的优化问题增加了两个全新的、至关重要的维度：<strong>单次调用的货币成本（Cost）</strong>和<strong>概率性的输出质量（Quality）</strong>。每一次对
LLM API 的调用都在直接消耗预算，并且其返回结果的质量是波动的。</p>
<p>这使得后端架构的优化目标从一个纯粹的技术效率问题，演变成一个复杂的<strong>四维价值方程：<code>价值 = f(延迟, 吞吐量, 成本, 质量)</code></strong>。</p>
<ol type="1">
<li><strong>决策的业务化</strong>：选择哪个模型、是否使用缓存、是否启用更复杂的
Agent
链，这些不再仅仅是技术决策，而是深刻的<strong>业务和产品决策</strong>。例如，一个低延迟、低成本但质量稍逊的模型可能适用于草稿生成，而一个高成本、高延迟但质量顶尖的模型则适用于最终报告的定稿。架构师必须与产品经理紧密合作，理解不同场景下用户对这四个维度的不同容忍度。</li>
<li><strong>架构的动态化</strong>：最优解不再是静态的。一个优秀的 AI
后端架构应该是<strong>价值驱动和动态自适应的</strong>。它应该能够根据请求的类型、用户的重要性、当前的系统负载，甚至预算的消耗情况，动态地在不同的模型、缓存策略和执行路径之间进行路由和切换
。例如，系统可以设计成：
<ul>
<li><strong>语义缓存优先</strong>：在调用 LLM
之前，先检查是否有语义相似的问题可以直接返回缓存答案，从而将成本降为零。</li>
<li><strong>模型分级与回退</strong>：优先尝试廉价快速的模型，如果其输出质量不达标（通过评估函数判断），再升级到更昂贵的模型。或者在主力模型不可用时，自动降级到备用模型
。</li>
<li><strong>成本预算控制</strong>：实时追踪 Token
消耗，当接近预算阈值时，可以切换到成本更低的模式或对非核心功能进行熔断
。</li>
</ul></li>
</ol>
<blockquote>
<p>[!IMPORTANT]</p>
<p><strong>核心心法</strong>：将系统优化的目标从追求单一维度的"快"，转变为在多维空间中寻找"价值最优"。架构师的角色从性能工程师扩展为<strong>系统经济学家</strong>，其设计的系统应具备在延迟、吞吐、成本和质量之间进行智能权衡与动态调整的能力。</p>
</blockquote>
<h2 id="法则四系统的进化-从指令式编排到涌现式生态">法则四：系统的进化 ——
从指令式编排到涌现式生态</h2>
<p>微服务架构通过将庞大的单体应用分解为独立的、功能单一的服务，极大地提升了开发效率和系统的可扩展性
。这些服务之间的协作通常是<strong>指令式和预定义</strong>的，通过 API
调用或消息队列，遵循着由工程师精心设计的、相对固定的工作流（Workflow）。</p>
<p>AI
智能体（Agent）的出现，预示着一种全新的系统范式：从工程师主导的"编排"（Orchestration）到<strong>智能体自主协作的"涌现"（Emergence）</strong>。</p>
<ol type="1">
<li><strong>从"管道工"到"生态设计师"</strong>：在传统微服务架构中，架构师的角色在某种程度上像一个管道工，负责设计和连接服务之间的数据管道。在多智能体系统中，架构师的角色更像一个<strong>生态设计师</strong>。其核心任务不再是硬编码每一个交互步骤，而是创造一个环境和一套规则，让多个专职的、自主的智能体能够在这个环境中<strong>发现彼此、进行通信、协同合作</strong>，以完成一个更高层次的、甚至在设计之初未被完全预料到的复杂目标
。</li>
<li><strong>拥抱"涌现行为"与"可观测性"</strong>：当多个自主智能体开始交互时，系统的整体行为可能超越其任何单个组成部分的能力总和，产生所谓的"涌现行为"。这种行为既是智能体系统威力的来源，也是其复杂性和风险的根源。它可能带来创新的解决方案，也可能导致意想不到的、有害的连锁反应。因此，<strong>可观测性（Observability）</strong>在智能体架构中的重要性被提升到了前所未有的高度。我们需要全新的工具和理念来追踪和理解智能体的决策路径、工具调用、以及智能体之间的交互模式，从而在它们涌现出问题时能够及时发现、理解并干预
。</li>
</ol>
<blockquote>
<p>[!IMPORTANT]</p>
<p><strong>核心心法</strong>：将系统视为一个活的、演化的生态，而非一台静态的、精密的机器。架构师的目标是设计一个<strong>促进有益协作、同时又能约束和观测潜在风险的智能体环境</strong>，从构建可预测的系统，转向引导和管理一个不断演化的、具有涌现智能的系统。</p>
</blockquote>
<h1 id="结论">结论</h1>
<p>技术的"术"日新月异，今天我们讨论的 Kafka、Redis、Go 或
Rust，明天可能会有新的替代者。然而，上述四大"道"——<strong>拥抱概率性、构建记忆核心、优化价值方程、设计涌现生态</strong>——构成了
AI 时代后端架构的根本。</p>
<p>掌握了这些核心思想，无论未来的技术细节如何演变，我们都能抓住其本质，设计出真正健壮、高效且智能的系统，从而在这场深刻的技术变革中，始终保持前瞻性和领导力。</p>

<div class="article-footer fs14">
    <section id="license">
      <div class="header"><span>许可协议</span></div>
      <div class="body"><p>本文采用 <a
target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享
4.0 国际</a> 许可协议，转载请注明出处。</p>
</div>
    </section>
    
    <section id="share">
      <div class="header"><span>分享文章</span></div>
      <div class="body">
        <div class="link"><input class="copy-area" readonly="true" id="copy-link" value="https://hedon.top/2025/09/05/qa/qa-traditional-backend-to-ai-engineer/" /></div>
        <div class="social-wrap dis-select"><a class="social share-item wechat" onclick="util.toggle(&quot;qrcode-wechat&quot;)"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/b32ef3da1162a.svg" /></a><a class="social share-item weibo" target="_blank" rel="external nofollow noopener noreferrer" href="https://service.weibo.com/share/share.php?url=https://hedon.top/2025/09/05/qa/qa-traditional-backend-to-ai-engineer/&title=Q&A丨AI 视角下的后端技术重塑 - HedonWang&pics=/banner/qa-traditional-backend-to-ai-engineer.jpg&summary=本文跟 Gemini 探讨在 AI 时代下，传统后端工程师如何结合自身优势，转型为 AI 应用开发工程师。"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/80c07e4dbb303.svg" /></a><a class="social share-item email" href="mailto:?subject=Q&A丨AI 视角下的后端技术重塑 - HedonWang&amp;body=https://hedon.top/2025/09/05/qa/qa-traditional-backend-to-ai-engineer/"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/a1b00e20f425d.svg" /></a><a class="social share-item link" onclick="util.copy(&quot;copy-link&quot;, &quot;复制成功&quot;)"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/8411ed322ced6.svg" /></a></div>
        
        <div class="qrcode" id="qrcode-wechat" style="opacity:0;height:0">
          <img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://api.qrserver.com/v1/create-qr-code/?size=256x256&data=https://hedon.top/2025/09/05/qa/qa-traditional-backend-to-ai-engineer/"/>
        </div>
        
      </div>
    </section>
    </div>
</article>
<div class="related-wrap" id="read-next"><section class="body"><div class="item" id="prev"><div class="note">较新文章</div><a href="/2025/09/08/redis/redis-datatype-hash-and-set/">Redis 数据类型丨Hash&Set</a></div><div class="item" id="next"><div class="note">较早文章</div><a href="/2025/08/30/llm/note-llm-from-scratch/">读书笔记丨《从零构建大语言模型》</a></div></section></div>




  <div class="related-wrap md-text" id="comments">
    <section class='header cmt-title cap theme'>
      <p>快来参与讨论吧~</p>

    </section>
    <section class='body cmt-body giscus'>
      

<svg class="loading" style="vertical-align:middle;fill:currentColor;overflow:hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2709"><path d="M832 512c0-176-144-320-320-320V128c211.2 0 384 172.8 384 384h-64zM192 512c0 176 144 320 320 320v64C300.8 896 128 723.2 128 512h64z" p-id="2710"></path></svg>

<div id="giscus" src="https://giscus.app/client.js" data-repo="hedon954/hedonspace" data-repo-id="R_kgDOKt17sQ" data-category="Q&A" data-category-id="DIC_kwDOKt17sc4CbAt-" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="preferred_color_scheme" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous"></div>

    </section>
  </div>



<footer class="page-footer footnote"><hr><div class="text"><p>本站由 <a href="/">Hedon Wang</a> 使用 <a
target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.29.1">Stellar
1.29.1</a> 主题创建。 本博客所有文章除特别声明外，均采用 <a
target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA
4.0</a> 许可协议，转载请注明出处。</p>
</div></footer>
<div class="main-mask" onclick="sidebar.dismiss()"></div></div><aside class="l_right">
<div class="widgets">



<widget class="widget-wrapper toc" id="data-toc" collapse="false"><div class="widget-header dis-select"><span class="name">本文目录</span><a class="cap-action" onclick="sidebar.toggleTOC()" ><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg></a></div><div class="widget-body"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%AF"><span class="toc-text">术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80ai-%E6%97%B6%E4%BB%A3%E7%9A%84%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E8%BD%AC%E5%8F%98"><span class="toc-text">一、AI 时代的基础架构转变</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E5%90%8C%E6%AD%A5%E8%AF%B7%E6%B1%82%E5%93%8D%E5%BA%94%E5%88%B0%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E7%BC%96%E6%8E%92"><span class="toc-text">1.1
从同步请求&#x2F;响应到异步任务编排</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E9%98%9F%E5%88%97%E6%9E%B6%E6%9E%84"><span class="toc-text">任务队列架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C%E4%BA%A4%E4%BB%98%E6%9C%BA%E5%88%B6"><span class="toc-text">结果交付机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%82%E6%AD%A5%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E9%9F%A7%E6%80%A7%E8%AE%BE%E8%AE%A1"><span class="toc-text">异步系统中的韧性设计</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8%E5%88%86%E5%B8%83%E5%BC%8F%E5%AF%B9%E8%AF%9D%E5%BC%8F%E4%B8%96%E7%95%8C%E4%B8%AD%E7%AE%A1%E7%90%86%E7%8A%B6%E6%80%81"><span class="toc-text">1.2
在分布式、对话式世界中管理状态</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%96%E9%83%A8%E5%8C%96%E7%8A%B6%E6%80%81%E5%AD%98%E5%82%A8"><span class="toc-text">外部化状态存储</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9D%83%E8%A1%A1%E5%88%86%E6%9E%90%E7%B2%98%E6%80%A7%E4%BC%9A%E8%AF%9D"><span class="toc-text">权衡分析：粘性会话</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E8%AF%9D%E5%8E%86%E5%8F%B2%E6%91%98%E8%A6%81"><span class="toc-text">对话历史摘要</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E6%A6%82%E7%8E%87%E6%80%A7%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9F%A7%E6%80%A7%E5%92%8C%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7"><span class="toc-text">1.3
为概率性系统设计韧性和可观测性</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9D%A2%E5%90%91-llm-%E7%9A%84%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7%E6%8A%80%E6%9C%AF%E6%A0%88"><span class="toc-text">面向 LLM 的可观测性技术栈</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E9%9B%85%E9%99%8D%E7%BA%A7%E6%A8%A1%E5%BC%8F"><span class="toc-text">优雅降级模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%B1%E8%B4%A5%E5%AE%9A%E4%B9%89%E7%9A%84%E6%BC%94%E8%BF%9B"><span class="toc-text">&quot;失败&quot;定义的演进</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E6%95%B0%E6%8D%AE%E5%B1%82%E7%9A%84%E9%87%8D%E6%9E%84%E4%B8%BA-ai-%E8%AE%BE%E8%AE%A1%E5%AD%98%E5%82%A8%E4%BA%8E%E6%A3%80%E7%B4%A2"><span class="toc-text">二、数据层的重构：为 AI
设计存储于检索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93mysqlpostgresai-%E7%9A%84%E8%AE%B0%E5%BF%86%E7%B3%BB%E7%BB%9F"><span class="toc-text">2.1
关系型数据库（MySQL&#x2F;Postgres）：AI 的记忆系统</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E6%95%B0%E6%8D%AE%E5%BA%93redisai-%E7%9A%84%E5%B7%A5%E4%BD%9C%E8%AE%B0%E5%BF%86"><span class="toc-text">2.2 内存数据库（Redis）：AI
的工作记忆</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rag-%E6%91%84%E5%85%A5%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E5%9E%8B%E8%AE%BE%E8%AE%A1%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6"><span class="toc-text">2.3 RAG
摄入流水线：数据密集型设计的案例研究</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E7%A5%9E%E7%BB%8F%E7%B3%BB%E7%BB%9F%E4%BD%BF%E7%94%A8-kafka-%E8%BF%9B%E8%A1%8C%E5%BC%82%E6%AD%A5%E5%A4%84%E7%90%86"><span class="toc-text">三、神经系统：使用 Kafka
进行异步处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka-%E4%BD%9C%E4%B8%BA-rag-%E5%92%8C%E5%BE%AE%E8%B0%83%E6%B5%81%E6%B0%B4%E7%BA%BF%E7%9A%84%E6%94%AF%E6%9F%B1"><span class="toc-text">3.1 Kafka 作为 RAG
和微调流水线的支柱</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA-llm-%E5%B7%A5%E4%BD%9C%E8%B4%9F%E8%BD%BD%E8%AE%BE%E8%AE%A1-kafka"><span class="toc-text">3.2 为 LLM 工作负载设计 Kafka</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E6%9C%8D%E5%8A%A1%E8%A7%A3%E8%80%A6%E5%88%B0%E6%99%BA%E8%83%BD%E4%BD%93%E7%BC%96%E6%8E%92"><span class="toc-text">3.3 从服务解耦到智能体编排</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E9%AB%98%E6%80%A7%E8%83%BD%E5%AE%9E%E7%8E%B0go-%E4%B8%8E-rust"><span class="toc-text">四、高性能实现：Go 与 Rust</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#go%E5%B9%B6%E5%8F%91-ai-%E7%BC%96%E6%8E%92%E8%AF%AD%E8%A8%80"><span class="toc-text">4.1 Go：并发 AI 编排语言</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rust%E9%AB%98%E6%80%A7%E8%83%BD%E6%8E%A8%E7%90%86%E8%AF%AD%E8%A8%80"><span class="toc-text">4.2 Rust：高性能推理语言</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gorust-%E5%8D%8F%E5%90%8C%E5%B7%A5%E4%BD%9C%E7%9A%84%E5%A4%9A%E8%AF%AD%E8%A8%80%E6%9E%B6%E6%9E%84"><span class="toc-text">4.3 Go&#x2F;Rust
协同工作的多语言架构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E9%AB%98%E7%BA%A7%E6%9E%B6%E6%9E%84%E8%8C%83%E5%BC%8F%E4%B8%8E%E7%BB%BC%E5%90%88"><span class="toc-text">五、高级架构范式与综合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A9%AF%E6%9C%8D%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%94%9F%E4%BA%A7%E5%B0%B1%E7%BB%AA%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%9A%84%E5%B7%A5%E5%85%B7%E7%AE%B1"><span class="toc-text">5.1
驯服不确定性：生产就绪工作流的工具箱</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%99%BA%E8%83%BD%E4%BD%93%E6%9E%B6%E6%9E%84%E7%9A%84%E5%85%B4%E8%B5%B7"><span class="toc-text">5.2 智能体架构的兴起</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%E7%8E%B0%E4%BB%A3-ai-%E5%B7%A5%E7%A8%8B%E5%B8%88%E7%9A%84%E8%9E%8D%E5%90%88%E6%8A%80%E8%83%BD%E6%A0%88"><span class="toc-text">总结：现代 AI
工程师的融合技能栈</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%81%93"><span class="toc-text">道</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%95%E5%88%99%E4%B8%80%E4%B8%96%E7%95%8C%E8%A7%82%E7%9A%84%E9%9D%A9%E5%91%BD-%E4%BB%8E%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%88%B0%E6%A6%82%E7%8E%87%E6%80%A7"><span class="toc-text">法则一：世界观的革命 ——
从确定性到概率性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%95%E5%88%99%E4%BA%8C%E6%80%9D%E7%BB%B4%E7%9A%84%E8%B7%83%E8%BF%81-%E4%BB%8E%E6%97%A0%E7%8A%B6%E6%80%81%E6%9C%8D%E5%8A%A1%E5%88%B0%E6%9C%89%E7%8A%B6%E6%80%81%E6%8E%A8%E7%90%86"><span class="toc-text">法则二：思维的跃迁 ——
从无状态服务到有状态推理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%95%E5%88%99%E4%B8%89%E4%BC%98%E5%8C%96%E7%9A%84%E6%96%B0%E6%96%B9%E7%A8%8B-%E4%BB%8E%E6%95%88%E7%8E%87%E5%88%B0%E4%BB%B7%E5%80%BC"><span class="toc-text">法则三：优化的新方程 ——
从效率到价值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%95%E5%88%99%E5%9B%9B%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%BF%9B%E5%8C%96-%E4%BB%8E%E6%8C%87%E4%BB%A4%E5%BC%8F%E7%BC%96%E6%8E%92%E5%88%B0%E6%B6%8C%E7%8E%B0%E5%BC%8F%E7%94%9F%E6%80%81"><span class="toc-text">法则四：系统的进化 ——
从指令式编排到涌现式生态</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-text">结论</span></a></li></ol></div><div class="widget-footer">

<a class="top" onclick="util.scrollTop()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 12c0-4.714 0-7.071 1.464-8.536C4.93 2 7.286 2 12 2c4.714 0 7.071 0 8.535 1.464C22 4.93 22 7.286 22 12c0 4.714 0 7.071-1.465 8.535C19.072 22 16.714 22 12 22s-7.071 0-8.536-1.465C2 19.072 2 16.714 2 12Z"/><path stroke-linecap="round" stroke-linejoin="round" d="m9 15.5l3-3l3 3m-6-4l3-3l3 3"/></g></svg><span>回到顶部</span></a><a class="buttom" onclick="util.scrollComment()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M10.46 1.25h3.08c1.603 0 2.86 0 3.864.095c1.023.098 1.861.3 2.6.752a5.75 5.75 0 0 1 1.899 1.899c.452.738.654 1.577.752 2.6c.095 1.004.095 2.261.095 3.865v1.067c0 1.141 0 2.036-.05 2.759c-.05.735-.153 1.347-.388 1.913a5.75 5.75 0 0 1-3.112 3.112c-.805.334-1.721.408-2.977.43a10.81 10.81 0 0 0-.929.036c-.198.022-.275.054-.32.08c-.047.028-.112.078-.224.232c-.121.166-.258.396-.476.764l-.542.916c-.773 1.307-2.69 1.307-3.464 0l-.542-.916a10.605 10.605 0 0 0-.476-.764c-.112-.154-.177-.204-.224-.232c-.045-.026-.122-.058-.32-.08c-.212-.023-.49-.03-.93-.037c-1.255-.021-2.171-.095-2.976-.429A5.75 5.75 0 0 1 1.688 16.2c-.235-.566-.338-1.178-.389-1.913c-.049-.723-.049-1.618-.049-2.76v-1.066c0-1.604 0-2.86.095-3.865c.098-1.023.3-1.862.752-2.6a5.75 5.75 0 0 1 1.899-1.899c.738-.452 1.577-.654 2.6-.752C7.6 1.25 8.857 1.25 10.461 1.25M6.739 2.839c-.914.087-1.495.253-1.959.537A4.25 4.25 0 0 0 3.376 4.78c-.284.464-.45 1.045-.537 1.96c-.088.924-.089 2.11-.089 3.761v1c0 1.175 0 2.019.046 2.685c.045.659.131 1.089.278 1.441a4.25 4.25 0 0 0 2.3 2.3c.515.214 1.173.294 2.429.316h.031c.398.007.747.013 1.037.045c.311.035.616.104.909.274c.29.17.5.395.682.645c.169.232.342.525.538.856l.559.944a.52.52 0 0 0 .882 0l.559-.944c.196-.331.37-.624.538-.856c.182-.25.392-.476.682-.645c.293-.17.598-.24.909-.274c.29-.032.639-.038 1.037-.045h.032c1.255-.022 1.913-.102 2.428-.316a4.25 4.25 0 0 0 2.3-2.3c.147-.352.233-.782.278-1.441c.046-.666.046-1.51.046-2.685v-1c0-1.651 0-2.837-.089-3.762c-.087-.914-.253-1.495-.537-1.959a4.25 4.25 0 0 0-1.403-1.403c-.464-.284-1.045-.45-1.96-.537c-.924-.088-2.11-.089-3.761-.089h-3c-1.651 0-2.837 0-3.762.089" clip-rule="evenodd"/><path fill="currentColor" d="M9 11a1 1 0 1 1-2 0a1 1 0 0 1 2 0m4 0a1 1 0 1 1-2 0a1 1 0 0 1 2 0m4 0a1 1 0 1 1-2 0a1 1 0 0 1 2 0"/></svg><span>参与讨论</span></a></div></widget>
</div></aside><div class='float-panel blur'>
  <button type='button' style='display:none' class='laptop-only rightbar-toggle mobile' onclick='sidebar.rightbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg>
  </button>
  <button type='button' style='display:none' class='mobile-only leftbar-toggle mobile' onclick='sidebar.leftbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 11c0-3.771 0-5.657 1.172-6.828C4.343 3 6.229 3 10 3h4c3.771 0 5.657 0 6.828 1.172C22 5.343 22 7.229 22 11v2c0 3.771 0 5.657-1.172 6.828C19.657 21 17.771 21 14 21h-4c-3.771 0-5.657 0-6.828-1.172C2 18.657 2 16.771 2 13z"/><path id="sep" stroke-linecap="round" d="M5.5 10h6m-5 4h4m4.5 7V3"/></g></svg>
  </button>
</div>
</div><div class="scripts">
<script type="text/javascript">
  const ctx = {
    date_suffix: {
      just: `刚刚`,
      min: `分钟前`,
      hour: `小时前`,
      day: `天前`,
    },
    root : `/`,
  };

  // required plugins (only load if needs)
  if (`local_search`) {
    ctx.search = {};
    ctx.search.service = `local_search`;
    if (ctx.search.service == 'local_search') {
      let service_obj = Object.assign({}, `{"field":"all","path":"/search.json","content":true,"skip_search":null,"sort":"-date"}`);
      ctx.search[ctx.search.service] = service_obj;
    }
  }
  const def = {
    avatar: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/3442075.svg`,
    cover: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/cover/76b86c0226ffd.svg`,
  };
  const deps = {
    jquery: `https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js`,
    marked: `https://cdn.jsdelivr.net/npm/marked@13.0.1/lib/marked.umd.min.js`
  }
  

</script>

<script type="text/javascript">
  const utils = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    css: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    js: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      if (src.startsWith('/')){
        src = ctx.root + src.substring(1);
      }
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    jq: (fn) => {
      if (typeof jQuery === 'undefined') {
        utils.js(deps.jquery).then(fn)
      } else {
        fn()
      }
    },
    
    onLoading: (el) => {
      if (el) {
        $(el).append('<div class="loading-wrap"><svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" stroke-opacity=".3" d="M12 3C16.9706 3 21 7.02944 21 12C21 16.9706 16.9706 21 12 21C7.02944 21 3 16.9706 3 12C3 7.02944 7.02944 3 12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="1.3s" values="60;0"/></path><path stroke-dasharray="15" stroke-dashoffset="15" d="M12 3C16.9706 3 21 7.02944 21 12"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.3s" values="15;0"/><animateTransform attributeName="transform" dur="1.5s" repeatCount="indefinite" type="rotate" values="0 12 12;360 12 12"/></path></g></svg></div>');
      }
    },
    onLoadSuccess: (el) => {
      if (el) {
        $(el).find('.loading-wrap').remove();
      }
    },
    onLoadFailure: (el) => {
      if (el) {
        $(el).find('.loading-wrap svg').remove();
        $(el).find('.loading-wrap').append('<svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" d="M12 3L21 20H3L12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.5s" values="60;0"/></path><path stroke-dasharray="6" stroke-dashoffset="6" d="M12 10V14"><animate fill="freeze" attributeName="stroke-dashoffset" begin="0.6s" dur="0.2s" values="6;0"/></path></g><circle cx="12" cy="17" r="1" fill="currentColor" fill-opacity="0"><animate fill="freeze" attributeName="fill-opacity" begin="0.8s" dur="0.4s" values="0;1"/></circle></svg>');
        $(el).find('.loading-wrap').addClass('error');
      }
    },
    request: (el, url, callback, onFailure) => {
      let retryTimes = 3;
      utils.onLoading(el);
      function req() {
        return new Promise((resolve, reject) => {
          let status = 0; // 0 等待 1 完成 2 超时
          let timer = setTimeout(() => {
            if (status === 0) {
              status = 2;
              timer = null;
              reject('请求超时');
              if (retryTimes == 0) {
                onFailure();
              }
            }
          }, 5000);
          fetch(url).then(function(response) {
            if (status !== 2) {
              clearTimeout(timer);
              resolve(response);
              timer = null;
              status = 1;
            }
            if (response.ok) {
              return response.json();
            }
            throw new Error('Network response was not ok.');
          }).then(function(data) {
            retryTimes = 0;
            utils.onLoadSuccess(el);
            callback(data);
          }).catch(function(error) {
            if (retryTimes > 0) {
              retryTimes -= 1;
              setTimeout(() => {
                req();
              }, 5000);
            } else {
              utils.onLoadFailure(el);
              onFailure();
            }
          });
        });
      }
      req();
    },
  };
</script>

<script>
  const sidebar = {
    leftbar: () => {
      if (l_body) {
        l_body.toggleAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    rightbar: () => {
      if (l_body) {
        l_body.toggleAttribute('rightbar');
        l_body.removeAttribute('leftbar');
      }
    },
    dismiss: () => {
      if (l_body) {
        l_body.removeAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    toggleTOC: () => {
      document.querySelector('#data-toc').classList.toggle('collapse');
    }
  }
</script>
<script type="text/javascript">
  (() => {
    const tagSwitchers = document.querySelectorAll('.tag-subtree.parent-tag > a > .tag-switcher-wrapper')
    for (const tagSwitcher of tagSwitchers) {
      tagSwitcher.addEventListener('click', (e) => {
        const parent = e.target.closest('.tag-subtree.parent-tag')
        parent.classList.toggle('expanded')
        e.preventDefault()
      })
    }

    // Get active tag from query string, then activate it.
    const urlParams = new URLSearchParams(window.location.search)
    const activeTag = urlParams.get('tag')
    if (activeTag) {
      let tag = document.querySelector(`.tag-subtree[data-tag="${activeTag}"]`)
      if (tag) {
        tag.querySelector('a').classList.add('active')
        
        while (tag) {
          tag.classList.add('expanded')
          tag = tag.parentElement.closest('.tag-subtree.parent-tag')
        }
      }
    }
  })()
</script>


<!-- required -->
<script src="/js/main.js?v=1.29.1" defer></script>

<script type="text/javascript">
  const applyTheme = (theme) => {
    if (theme === 'auto') {
      document.documentElement.removeAttribute('data-theme')
    } else {
      document.documentElement.setAttribute('data-theme', theme)
    }

    applyThemeToGiscus(theme)
  }

  const applyThemeToGiscus = (theme) => {
    theme = theme === 'auto' ? 'preferred_color_scheme' : theme

    const cmt = document.getElementById('giscus')
    if (cmt) {
      // This works before giscus load.
      cmt.setAttribute('data-theme', theme)
    }

    const iframe = document.querySelector('#comments > section.giscus > iframe')
    if (iframe) {
      // This works after giscus loaded.
      const src = iframe.src
      const newSrc = src.replace(/theme=[\w]+/, `theme=${theme}`)
      iframe.src = newSrc
    }
  }

  const switchTheme = () => {
    // light -> dark -> auto -> light -> ...
    const currentTheme = document.documentElement.getAttribute('data-theme')
    let newTheme;
    switch (currentTheme) {
      case 'light':
        newTheme = 'dark'
        break
      case 'dark':
        newTheme = 'auto'
        break
      default:
        newTheme = 'light'
    }
    applyTheme(newTheme)
    window.localStorage.setItem('Stellar.theme', newTheme)

    const messages = {
      light: `切换到浅色模式`,
      dark: `切换到深色模式`,
      auto: `切换到跟随系统配色`,
    }
    hud?.toast?.(messages[newTheme])
  }

  (() => {
    // Apply user's preferred theme, if any.
    const theme = window.localStorage.getItem('Stellar.theme')
    if (theme !== null) {
      applyTheme(theme)
    }
  })()
</script>


<!-- optional -->

  <script type="module">
  const el = document.querySelector('#comments #giscus');
  util.viewportLazyload(el, load_discus, false);

  function load_discus() {
    if (!el) return;
    try {
        el.innerHTML = '';
      } catch (error) {
        console.error(error);
      }
      const script = document.createElement('script');
      script.async = true;
      for (const key of Object.keys(el.attributes)) {
        const attr = el.attributes[key];
        if (['class', 'id'].includes(attr.name) === false) {
          script.setAttribute(attr.name, attr.value);
        }
      }
      el.appendChild(script);
  }
</script>




<script defer>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.services = Object.assign({}, JSON.parse(`{"mdrender":{"js":"/js/services/mdrender.js"},"siteinfo":{"js":"/js/services/siteinfo.js","api":"https://site-info-api-hedon.vercel.app/api/v1?url={href}"},"ghinfo":{"js":"/js/services/ghinfo.js"},"sites":{"js":"/js/services/sites.js"},"friends":{"js":"/js/services/friends.js"},"timeline":{"js":"/js/services/timeline.js"},"fcircle":{"js":"/js/services/fcircle.js"},"weibo":{"js":"/js/services/weibo.js"},"memos":{"js":"/js/services/memos.js"},"twikoo":{"js":"/js/services/twikoo_latest_comment.js"},"waline":{"js":"/js/services/waline_latest_comment.js"},"artalk":{"js":"/js/services/artalk_latest_comment.js"},"giscus":{"js":"/js/services/giscus_latest_comment.js"}}`));
    for (let id of Object.keys(ctx.services)) {
      const js = ctx.services[id].js;
      if (id == 'siteinfo') {
        ctx.cardlinks = document.querySelectorAll('a.link-card[cardlink]');
        if (ctx.cardlinks?.length > 0) {
          utils.js(js, { defer: true }).then(function () {
            setCardLink(ctx.cardlinks);
          });
        }
      } else {
        const els = document.getElementsByClassName(`ds-${id}`);
        if (els?.length > 0) {
          utils.jq(() => {
            if (id == 'timeline' || 'memos' || 'marked') {
              utils.js(deps.marked).then(function () {
                utils.js(js, { defer: true });
              });
            } else {
              utils.js(js, { defer: true });
            }
          });
        }
      }
    }
  });
</script>

<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.search = {
      path: `/search.json`,
    }
    utils.js('/js/search/local-search.js', { defer: true });
  });
</script><script>
  window.FPConfig = {
    delay: 0,
    ignoreKeywords: [],
    maxRPS: 5,
    hoverDelay: 25
  };
</script>
<script defer src="https://cdn.jsdelivr.net/npm/flying-pages@2/flying-pages.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.min.js"></script>
<script>
  // https://www.npmjs.com/package/vanilla-lazyload
  // Set the options globally
  // to make LazyLoad self-initialize
  window.lazyLoadOptions = {
    elements_selector: ".lazy",
  };
  // Listen to the initialization event
  // and get the instance of LazyLoad
  window.addEventListener(
    "LazyLoad::Initialized",
    function (event) {
      window.lazyLoadInstance = event.detail.instance;
    },
    false
  );
  document.addEventListener('DOMContentLoaded', function () {
    window.lazyLoadInstance?.update();
  });
</script><script>
  ctx.fancybox = {
    selector: `.timenode p>img, .md-text img:not([class]), .md-text .image img`,
    css: `https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0/dist/fancybox/fancybox.css`,
    js: `https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0/dist/fancybox/fancybox.umd.js`
  };
  var selector = '[data-fancybox]:not(.error)';
  if (ctx.fancybox.selector) {
    selector += `, ${ctx.fancybox.selector}`
  }
  var needFancybox = document.querySelectorAll(selector).length !== 0;
  if (!needFancybox) {
    const els = document.getElementsByClassName('ds-memos');
    if (els != undefined && els.length > 0) {
      needFancybox = true;
    }
  }
  if (needFancybox) {
    utils.css(ctx.fancybox.css);
    utils.js(ctx.fancybox.js, { defer: true }).then(function () {
      Fancybox.bind(selector, {
        hideScrollbar: false,
        Thumbs: {
          autoStart: false,
        },
        caption: (fancybox, slide) => {
          return slide.triggerEl.alt || slide.triggerEl.dataset.caption || null
        }
      });
    })
  }
</script>
<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    const swiper_api = document.getElementById('swiper-api');
    if (swiper_api != undefined) {
      utils.css(`https://unpkg.com/swiper@10.3.1/swiper-bundle.min.css`);
      utils.js(`https://unpkg.com/swiper@10.3.1/swiper-bundle.min.js`, { defer: true }).then(function () {
        const effect = swiper_api.getAttribute('effect') || '';
        var swiper = new Swiper('.swiper#swiper-api', {
          slidesPerView: 'auto',
          spaceBetween: 8,
          centeredSlides: true,
          effect: effect,
          rewind: true,
          pagination: {
            el: '.swiper-pagination',
            clickable: true,
          },
          navigation: {
            nextEl: '.swiper-button-next',
            prevEl: '.swiper-button-prev',
          },
        });
      })
    }
  });
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    processEscapes: true
  }
});
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script>
<script defer type="text/javascript" src="https://cdn.jsdelivr.net/npm/mermaid@v9/dist/mermaid.min.js"></script>
<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    var mermaid_config = {
      startOnLoad: true,
      theme:
        "" == "auto" &&
          window.matchMedia("(prefers-color-scheme: dark)").matches
          ? "dark"
          : "neutral",
      logLevel: 3,
      themeVariables: {
        darkMode: true
      },
      flowchart: {
        useMaxWidth: false,
        htmlLabels: true,
        curve: "linear"
      },
      gantt: {
        axisFormat: "%Y/%m/%d"
      },
      sequence: {
        actorMargin: 50
      }
    }
    mermaid.initialize(mermaid_config);
  });
</script><script>
  document.addEventListener('DOMContentLoaded', function () {
    window.codeElements = document.querySelectorAll('.code');
    if (window.codeElements.length > 0) {
      ctx.copycode = {
        default_text: `Copy`,
        success_text: `Copied`,
        toast: `复制成功`,
      };
      utils.js('/js/plugins/copycode.js');
    }
  });
</script>


<!-- inject -->

</div></body></html>
