<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>HedonWang</title>
  
  <subtitle>君子求诸己，律己则安。</subtitle>
  <link href="https://hedon.top/atom.xml" rel="self"/>
  
  <link href="https://hedon.top/"/>
  <updated>2025-09-01T11:31:10.498Z</updated>
  <id>https://hedon.top/</id>
  
  <author>
    <name>Hedon Wang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>读书笔记丨《从零构建大语言模型》</title>
    <link href="https://hedon.top/2025/08/30/llm/note-llm-from-scratch/"/>
    <id>https://hedon.top/2025/08/30/llm/note-llm-from-scratch/</id>
    <published>2025-08-30T03:22:00.000Z</published>
    <updated>2025-09-01T11:31:10.498Z</updated>
    
    <content type="html"><![CDATA[<p>最近在看《从零构建大语言模型》这本书，跟着思路自己也动手码了一个基础款的LLM，代码不多，拢共 500 来行，完整代码 <ahref="https://github.com/hedon-ai-road/llm-from-scratch/blob/main/all_in_one.py">llm-from-scrtach</a>。</p><p>所以，这篇读书笔记不打算空谈理论，就想换个方式：试着把这 500行代码的来龙去脉讲清楚。通过解说代码，把从零构建一个大模型需要经历哪些环节、每个环节的目标和大概原理给串起来。</p><p>笔者并非该方向的专业人士，很多东西不会讲得太深（我自己也不懂），所以很多时候都是点到即止。这篇文章更适合那些和我一样的开发者，希望能从一个接地气的工程角度，对大模型的构建原理有个宏观的了解。</p><p>我们将采用一种贴近软件开发者思维的<strong>代码执行流</strong>视角，从程序的入口<code>main</code>函数开始，顺着调用栈逐层深入，探究数据处理、模型构建、训练循环的每一个细节。当一个模块被调用时，我们便深入其中，直至最底层的实现。</p><p>这趟旅程将遵循以下路线图：</p><ul><li><strong>从 <code>main</code>函数出发</strong>：探寻程序的入口与总调度中心。</li><li><strong>深入数据流水线</strong>：看原始文本如何被加工成模型能够消化的食粮。</li><li><strong>解构核心引擎</strong>：层层剖析<code>GPTModel</code>、<code>TransformerBlock</code> 直至最深处的<code>MultiHeadAttention</code> 机制。</li><li><strong>启动训练循环</strong>：见证模型如何通过损失计算和权重更新，从随机变得智能。</li><li><strong>见证文本生成</strong>：观察训练好的模型如何像我们一样，逐字逐句地"思考"和"创作"。</li></ul><p>让我们即刻出发，揭开大语言模型背后的代码之谜。</p><h1 id="数据准备与采样">1. 数据准备与采样</h1><p><img src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831134803291.png" style="zoom:33%;" /></p><p><code>prepare_train_and_val_data</code> 的具体实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_train_and_val_data</span>(<span class="params">file_path, tokenizer</span>) -&gt; <span class="built_in">tuple</span>[DataLoader, DataLoader]:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;准备训练和验证数据加载器&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 步骤 1: 读取原始文本</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        text_data = file.read()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 步骤 2: 按比例分割成训练和验证两部分：90%训练，10%验证</span></span><br><span class="line">    train_ratio = <span class="number">0.9</span></span><br><span class="line">    split_idx = <span class="built_in">int</span>(train_ratio * <span class="built_in">len</span>(text_data))</span><br><span class="line">    train_data = text_data[:split_idx]</span><br><span class="line">    val_data = text_data[split_idx:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 步骤 3: 分别为两部分文本创建 DataLoader</span></span><br><span class="line">    train_loader = create_dataloader(</span><br><span class="line">        tokenizer,</span><br><span class="line">        train_data,</span><br><span class="line">        batch_size=<span class="number">2</span>,</span><br><span class="line">        max_length=GPT_CONFIG_124M[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">        stride=GPT_CONFIG_124M[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">        drop_last=<span class="literal">True</span>,</span><br><span class="line">        shuffle=<span class="literal">True</span>,</span><br><span class="line">        num_workers=<span class="number">0</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val_loader = create_dataloader(</span><br><span class="line">        tokenizer,</span><br><span class="line">        val_data,</span><br><span class="line">        batch_size=<span class="number">2</span>,</span><br><span class="line">        max_length=GPT_CONFIG_124M[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">        stride=GPT_CONFIG_124M[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">        drop_last=<span class="literal">False</span>,</span><br><span class="line">        shuffle=<span class="literal">False</span>,</span><br><span class="line">        num_workers=<span class="number">0</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> train_loader, val_loader</span><br></pre></td></tr></table></figure><h2 id="划分数据集-prepare_train_and_val_data">1.1 划分数据集prepare_train_and_val_data</h2><p><code>prepare_train_and_val_data</code>的代码并不复杂，就是将数据集划分为训练集和验证集，那么第一个问题就来了：<strong><u>模型为何需要训练集和验证集？</u></strong></p><p>和人类一样，模型通过<strong>看例子</strong>来学习。我们给它一本"教科书"和配套的"练习册"（<strong>训练集</strong>），让它反复练习，寻找规律。但我们如何知道它是真的学会了，还是仅仅背下了答案（过拟合）呢？</p><p>答案是，我们需要一场它从未见过的<strong>模拟考试</strong>（<strong>验证集</strong>）。</p><ul><li><strong>训练集 (TrainingSet)</strong>：这是模型学习的<strong>唯一资料</strong>。模型会尽全力去拟合训练集中的数据，目标是在这个数据集上获得尽可能低的出错率（损失）。这对应了代码中90% 的文本数据 。</li><li><strong>验证集 (ValidationSet)</strong>：这是一份<strong>被隔离的数据</strong>，模型在训练过程中<strong>绝对不能</strong>用它来更新自己的权重。我们只在训练的特定阶段用它来“考”一下模型，看看模型在“新题型”上的表现如何。这对应了代码中10% 的文本数据 。</li></ul><p>如果模型在训练集上表现优异（比如损失很低），但在验证集上表现糟糕，这就亮起了<strong>过拟合</strong>的红灯。这说明模型只是死记硬背了训练题，而没有学到普适的规律。<code>prepare_train_and_val_data</code>函数的核心使命，就是为模型准备好这两份至关重要的数据集。</p><p>它调用了 <code>create_dataloader</code> 函数。我们必须注意<code>train_loader</code> 和 <code>val_loader</code>在配置上的一个<strong>关键区别</strong>：</p><ul><li><p><strong><code>train_loader</code> 的 <code>shuffle</code> 设置为<code>True</code></strong> 。</p><p>这是为了<strong>保证训练的有效性</strong>。在每一轮 (epoch)训练开始时，<code>DataLoader</code>都会将训练样本的顺序完全打乱。这就像我们学习时会打乱单词卡片的顺序一样，可以防止模型学到样本的出场顺序这种无关信息，从而迫使它学习更具泛化性的语言规律。</p></li><li><p><strong><code>val_loader</code> 的 <code>shuffle</code> 设置为<code>False</code></strong> 。</p><p>这是为了<strong>保证评估的客观性和一致性</strong>。验证集是我们的模拟考试，我们希望每次考试的卷子（题目顺序）都是一样的，这样才能客观地比较模型在不同训练阶段的得分，判断它是否真的在进步。</p></li></ul><h2 id="构建数据集-create_dataloader">1.2 构建数据集create_dataloader</h2><p><code>prepare_train_and_val_data</code>只是一个调度者，真正的数据加工发生在它调用的<code>create_dataloader</code> 和 <code>GPTDataset</code> 中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataloader</span>(<span class="params">tokenizer, txt, batch_size=<span class="number">4</span>, max_length=<span class="number">256</span>, stride=<span class="number">128</span>, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>, num_workers=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建数据加载器&quot;&quot;&quot;</span></span><br><span class="line">    dataset = GPTDataset(txt, tokenizer, max_length, stride)</span><br><span class="line">    dataloader = DataLoader(</span><br><span class="line">        dataset,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=shuffle,</span><br><span class="line">        drop_last=drop_last,</span><br><span class="line">        num_workers=num_workers,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br></pre></td></tr></table></figure><p><code>create_dataloader</code>是一个简单的封装，它的核心是做了两件事：</p><ol type="1"><li><code>dataset = GPTDataset(txt, tokenizer, max_length, stride)</code>：实例化一个<code>GPTDataset</code> 对象。</li><li><code>dataloader = DataLoader(dataset, ...)</code>：将这个<code>dataset</code> 对象包装成一个 PyTorch 的<code>DataLoader</code>。</li></ol><h3 id="dataset-dataloader">1.2.1 Dataset &amp; DataLoader</h3><p>在深入 <code>GPTDataset</code> 结构之前，这里我们先对 PyTorch 中的 2个关键数据类型 <code>Dataset</code> 和 <code>DataLoader</code>进行简要介绍，对于 Pytorch 更详细的介绍可参考笔者这篇 <ahref="https://hedon.top/2025/08/18/llm/pytorch/">告别死记硬背：一份真正理解PyTorch 核心设计的指南</a>。</p><p>简而言之，<code>Dataset</code> 和 <code>DataLoader</code>是为了解决"数据集是什么"和"如何使用数据集"这 2个核心问题，更具体的来说，在数据准备阶段，我们可能会面临以下几个问题：</p><ol type="1"><li>原始数据格式各异，如何统一读取？</li><li>数据集可能非常大，无法一次性载入内存，怎么办？</li><li>训练时需要对数据进行批量 (batching)、打乱 (shuffling) 和预处理(preprocessing)，如何高效实现？</li><li>如何利用多核 CPU 来加速数据加载，避免 GPU 等待？</li></ol><p>PyTorch 的解决方案就是 <code>Dataset</code> 和<code>DataLoader</code> ：</p><ul><li><code>Dataset</code>：<strong>它定义了"数据集"是什么</strong>。这是一个抽象类，你只需要继承它并实现两个方法：<code>__len__</code>(返回数据集大小)和 <code>__getitem__</code> (根据索引 <code>idx</code>返回一条数据)。它解决了如何获取单条数据的问题，将数据访问的逻辑封装起来。</li><li><code>DataLoader</code>：<strong>它定义了"如何使用数据集"</strong>。它接收一个<code>Dataset</code>对象，并在此基础上，优雅地解决了所有工程问题：<ul><li><code>batch_size</code>：自动将单条数据打包成一个 batch。</li><li><code>shuffle=True</code>：在每个 epoch开始时自动打乱数据顺序。</li><li><code>num_workers</code>：启动多个子进程并行加载数据，极大地提高了数据供给效率。</li><li><code>collate_fn</code>：自定义如何将多条样本合并成一个batch，对于处理非标准数据（如不同长度的句子）非常有用。</li></ul></li></ul><p>它们之间的关系如图所示：</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830165323656.png"alt="Dataset 与 DataLoader" /><figcaption aria-hidden="true">Dataset 与 DataLoader</figcaption></figure><h3 id="gptdataset">1.2.2 GPTDataset</h3><p>现在我们可以来看 <code>GPTDataset</code> 的具体逻辑了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;GPT训练数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, txt, tokenizer, max_length, stride</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="variable language_">self</span>.input_ids = []</span><br><span class="line">        <span class="variable language_">self</span>.target_ids = []</span><br><span class="line"></span><br><span class="line">        token_ids = tokenizer.encode(txt)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用滑动窗口创建训练样本</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(token_ids) - max_length, stride):</span><br><span class="line">            input_chunk = token_ids[i:i+max_length]</span><br><span class="line">            target_chunk = token_ids[i+<span class="number">1</span>:i+max_length+<span class="number">1</span>]</span><br><span class="line">            <span class="variable language_">self</span>.input_ids.append(torch.tensor(input_chunk))</span><br><span class="line">            <span class="variable language_">self</span>.target_ids.append(torch.tensor(target_chunk))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.input_ids)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.input_ids[idx], <span class="variable language_">self</span>.target_ids[idx]</span><br></pre></td></tr></table></figure><p><code>GPTDataset</code> 继承了 <code>PyTorch</code> 的<code>Dataset</code> 类型，我们重点来看它的构造函数<code>__init__()</code>，它分为 2 个步骤：</p><ol type="1"><li>将文本数据转为词元 ID 列表；</li><li>使用滑动窗口逐个构建<strong>输入-目标对</strong>，构建整个数据集，分别置于<code>input_ids</code> 和 <code>target_ids</code> 这 2 个字段中。</li></ol><h4 id="文本词元化">1.2.2.1 文本词元化</h4><p>现在到了本篇的第一个真正意义上的理论环节，我们需要先搞清楚词元化（即下面这一行代码）到底是在做什么？为什么要这样？有哪些具体的方式方法？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">token_ids = tokenizer.encode(txt)</span><br></pre></td></tr></table></figure><p>包括大语言模型在内的深度神经网络模型是无法直接处理原始文本的。由于文本数据是离散的，因此我们无法直接用它来执行神经网络训练所需的数学运算。我们需要一种将单词表示为连续值的向量格式的方法（通常是张量Tensor）。</p><p>将数据转换为向量格式的过程通常称为嵌入（embedding），如下图所示：</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830170327803.png" /></p><p>要理解<code>Tensor</code>，我们需要先建立一个最重要的心智模型：<strong>Tensor的每一个维度 (dimension) 都有其特定的语义含义</strong>。</p><p>一个典型的 4D Tensor <code>(B, C, H, W)</code> 在计算机视觉中，其形状<code>(16, 3, 224, 224)</code> 并不是一串孤立的数字，它的意思是：</p><ul><li><strong>B (Batch size) = 16</strong>: 这个 Tensor 里有 16张独立的图像。</li><li><strong>C (Channels) = 3</strong>: 每张图像有 3 个通道（R, G,B）。</li><li><strong>H (Height) = 224</strong>: 每张图像的高度是 224 像素。</li><li><strong>W (Width) = 224</strong>: 每张图像的宽度是 224 像素。</li></ul><p>在多个维度综合起来语义含义越接近的词，它们的词嵌入向量在空间表示中就越相近，也就越"相似"，如下图所示：</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830170432509.png" /></p><p>当把文本转为词嵌入向量之后，我们的训练模型就可以识别这些数据并利用它们进行学习了。</p><p>一个完整的文本处理步骤，大概如下图所示： <imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830170953531.png" /></p><ol type="1"><li><p>输入文件<code>This is an example.</code>：这是所有处理的起点，是我们希望模型去理解和回应的原始、非结构化的人类语言。</p></li><li><p>词元化：原始文本被切分成独立的单元：<code>This</code>,<code>is</code>, <code>an</code>, <code>example</code>,<code>.</code>。</p><p>计算机模型无法一次性理解一整个句子。它需要将句子分解成更小的、标准化的单元，这些单元被称为<strong>词元(Token)</strong>。如图中的文字描述，词元既可以是单词，也可以是标点符号之类的特殊字符。</p></li><li><p>转换为词元 ID：每个词元被映射到一个唯一的整数：<code>This</code>-&gt; <code>40134</code>, <code>is</code> -&gt; <code>2052</code>,<code>an</code> -&gt; <code>133</code>, <code>example</code> -&gt;<code>389</code>, <code>.</code> -&gt; <code>12</code>。</p><p>计算机不认识字符串 <code>This</code>，但它能高效地处理数字<code>40134</code>。这一步是<strong>将语言世界映射到数字世界</strong>的关键。每一个ID 都对应着模型词汇表（一个巨大的“字典”）中的一个条目。</p></li><li><p>生成词元嵌入：一串数字 ID变成了多个向量（关于词嵌入的具体细节，我们会在后续进行展开）。</p><p>即我们前面提到的，单个数字 ID（如<code>40134</code>）本身是孤立的，不包含任何语义信息，所以我们需要将其转为词嵌入向量，在训练过程中，模型会不断调整这些向量，使得<strong>意思相近的词元，其向量在空间中的位置也相互靠近</strong>。</p></li><li><p>模型处理与输出：这些嵌入向量组成的序列，最终被送入<strong>类 GPT的纯解码器 Transformer</strong> 。这是模型的核心大脑。Transformer模型会分析这些向量之间的关系，理解整个句子的上下文，然后进行计算。经过<strong>后续处理步骤</strong>（如选择概率最高的词元）后，模型会生成一个<strong>输出文本</strong>。</p></li></ol><blockquote><p>[!IMPORTANT]</p><p>小结一下，从<strong>人类语言 (字符串) -&gt; 语言单元 (词元) -&gt;机器语言 (数字 ID) -&gt; 数学对象 (嵌入向量) -&gt;模型输入</strong>，每一步都是为了让原始的、非结构化的文本，变得结构化、数值化，并富含语义信息，最终成为能够被神经网络高效处理的原料。</p></blockquote><p>一个简单的分词器实现如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleTokenizer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="variable language_">self</span>.str_to_int = vocab</span><br><span class="line">        <span class="variable language_">self</span>.int_to_str = &#123;i:s <span class="keyword">for</span> s, i <span class="keyword">in</span> vocab.items()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text</span>):</span><br><span class="line">        preprocessed = re.split(<span class="string">r&#x27;([,.:;?_!&quot;()\&#x27;]|--|\s)&#x27;</span>, text)</span><br><span class="line">        preprocessed = [item.strip() <span class="keyword">for</span> item <span class="keyword">in</span> preprocessed <span class="keyword">if</span> item.strip()]</span><br><span class="line">        preprocessed = [item <span class="keyword">if</span> item <span class="keyword">in</span> <span class="variable language_">self</span>.str_to_int <span class="keyword">else</span> <span class="string">&quot;&lt;|unk|&gt;&quot;</span> <span class="keyword">for</span> item <span class="keyword">in</span> preprocessed]</span><br><span class="line">        ids = [<span class="variable language_">self</span>.str_to_int[s] <span class="keyword">for</span> s <span class="keyword">in</span> preprocessed]</span><br><span class="line">        <span class="keyword">return</span> ids</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, ids</span>):</span><br><span class="line">        text = <span class="string">&quot; &quot;</span>.join([<span class="variable language_">self</span>.int_to_str[i] <span class="keyword">for</span> i <span class="keyword">in</span> ids])</span><br><span class="line">        <span class="comment"># Remove the spaces before specific punctuation marks.</span></span><br><span class="line">        text = re.sub(<span class="string">r&#x27;\s+([,.?!&quot;()\&#x27;])&#x27;</span>, <span class="string">r&#x27;\1&#x27;</span>, text)</span><br><span class="line">        <span class="keyword">return</span> text</span><br></pre></td></tr></table></figure><ol type="1"><li><code>__init__</code> 初始化词典，里面每一个词元都唯一对应一个ID；</li><li><code>encode</code> 原始文本转为一系列词元ID，对于不识别的词元，会使用 <code>&lt;|unk|&gt;</code>特殊标识进行占位，一般来说，还会使用诸如<code>&lt;|endoftext|&gt;</code>等特殊标识符来表示文本结束等特殊语义。</li><li><code>decode</code> 将词元 ID 列表转回原始文本。</li></ol><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830172025895.png" /></p><p>回到本篇的代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">token_ids = tokenizer.encode(txt)</span><br></pre></td></tr></table></figure><p>这里我们使用的是现有的 Python 开源库 <code>tiktoken</code>，它基于Rust 的源代码非常高效地实现了 BPE（Byte Pair Encoder） 算法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line"></span><br><span class="line">tokenizer = tiktoken.get_encoding(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">text1 = <span class="string">&quot;Hello, do you like tea?&quot;</span></span><br><span class="line">text2 = <span class="string">&quot;In the sunlit terraces of the palace.&quot;</span></span><br><span class="line">text = <span class="string">&quot; &lt;|endoftext|&gt; &quot;</span>.join((text1, text2))</span><br><span class="line">ids = tokenizer.encode(text, allowed_special=&#123;<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(ids)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(ids))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 20562, 13]</span><br><span class="line">Hello, do you like tea? &lt;|endoftext|&gt; In the sunlit terraces of the palace.</span><br></pre></td></tr></table></figure><p>通过输出，我们可以看到 <code>&lt;|endoftext|&gt;</code>词元被分配了一个较大的词元 ID，即 <code>50256</code>。事实上，用于训练GPT-2、GPT-3 和 ChatGPT 中使用的原始模型的 BPE 分词器的词汇总量为<code>50257</code>，这意味着 <code>&lt;|endoftext|&gt;</code>被分配了最大的词元 ID。</p><p>另外，BPE 分词器可以正确地编码和解码未知单词，比如<code>someunknownPlace</code>。BPE分词器是如何做到在不使用&lt;|unk|&gt;词元的前提下处理任何未知词汇的呢？</p><p>BPE算法的原理是将不在预定义词汇表中的单词分解为更小的子词单元甚至单个字符，从而能够处理词汇表之外的单词。因此，得益于 BPE算法，如果分词器在分词过程中遇到不熟悉的单词，它可以将其表示为子词词元或字符序列，如下图所示。</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830172322896.png" /></p><p>将未知单词分解为单个字符的能力确保了分词器以及用其训练的大语言模型能够处理任何文本，即使文本中包含训练数据中不存在的单词。</p><h4 id="滑动窗口进行数据采样">1.2.2.2 滑动窗口进行数据采样</h4><p>分析完了词元化的背后底层逻辑后，我们来看这一部分的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, txt, tokenizer, max_length, stride</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用滑动窗口创建训练样本</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(token_ids) - max_length, stride):</span><br><span class="line">            input_chunk = token_ids[i:i+max_length]</span><br><span class="line">            target_chunk = token_ids[i+<span class="number">1</span>:i+max_length+<span class="number">1</span>]</span><br><span class="line">            <span class="variable language_">self</span>.input_ids.append(torch.tensor(input_chunk))</span><br><span class="line">            <span class="variable language_">self</span>.target_ids.append(torch.tensor(target_chunk))</span><br></pre></td></tr></table></figure><p>要理解这段代码，我们需要回归到大语言模型（文本模型）是唯一任务：<strong><u>根据你给出的上文，猜出下一个词应该是什么</u></strong>。</p><p>例如，对于句子<code>Time is an illusion</code>，我们可以为模型制作如下一系列的练习题：</p><ul><li><strong>问题</strong>：<code>Time</code> -&gt;<strong>答案</strong>：<code>is</code></li><li><strong>问题</strong>：<code>Time is</code> -&gt;<strong>答案</strong>：<code>an</code></li><li><strong>问题</strong>：<code>Time is an</code> -&gt;<strong>答案</strong>：<code>illusion</code></li></ul><p>模型需要通过海量的这类"问答对"进行练习，才能逐渐掌握语言的规律。如果手动去制作上亿个这样的问答对，显然是不现实的。代码中的"滑动窗口"机制，就是为了解决这个问题。我们用一个具体的例子来解释这个<code>for</code> 循环：</p><ul><li>假设 <code>max_length = 5</code></li><li>假设一段文本分词后的 <code>token_ids</code> 是<code>[10, 20, 30, 40, 50, 60]</code></li></ul><p>当 <code>for</code> 循环第一次执行时 (<code>i=0</code>)：</p><ul><li><strong><code>input_chunk = token_ids[0:5]</code></strong> 会切出<code>[10, 20, 30, 40, 50]</code><ul><li>这就是提供给模型的<strong>上下文</strong>，也就是<strong>问题</strong>。</li></ul></li><li><strong><code>target_chunk = token_ids[1:6]</code></strong> 会切出<code>[20, 30, 40, 50, 60]</code><ul><li>这就是模型需要预测的<strong>正确答案</strong>。</li></ul></li></ul><p><img src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830172936403.png" alt="滑动窗口示意图" style="zoom:33%;" /></p><p>所以我们通过这样一个 for 循环，就可以根据传入的文本 <code>txt</code>快速生成大量的<strong>输入-目标对</strong>供给模型进行训练和检验。</p><hr /><blockquote><p>[!IMPORTANT]</p><p>回到 <code>prepare_train_and_val_data</code>函数，现在我们可以用一句话概括它的全部工作：<strong>它是一个数据准备总管，负责将一本原始小说，严格划分为用于学习的训练集和用于考试的验证集，并最终将它们都加工成模型可以直接使用的、一批一批的、包含(输入-目标)对的标准化数据传送带。</strong></p></blockquote><h1 id="初始化模型与优化器">2. 初始化模型与优化器</h1><p><img src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831134830004.png" style="zoom:33%;" /></p><ol type="1"><li><code>GPTModel()</code> 初始化一个 GPT 模型实例；</li><li><code>model.to(device)</code> 是 PyTorch中用于将模型移动到指定设备（CPU 或 GPU）的方法，深度学习模型在 GPU上训练速度比 CPU快很多，通过这种方式可以确保模型和输入数据在同一个设备上。</li><li><code>torch.optim.AdamW()</code>创建一个优化器（optimizer），用于训练神经网络模型。<code>AdamW</code>是一种优化算法，在训练过程中，优化器会接收损失函数计算出的梯度、使用AdamW算法更新模型参数和帮助模型逐步收敛到最优解。在本篇中，我们不对这个进行过多的解释，因为这并不在我们的核心学习目标上。</li></ol><h2 id="模型配置">2.1 模型配置</h2><p>在深入代码细节之前，我们先看 <code>GPT_CONFIG_124M</code>这个<strong>配置字典</strong>。它就像是建造 GPT模型大厦的<strong>设计蓝图</strong>，定义了模型的规模和所有关键参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">GPT_CONFIG_124M = &#123;</span><br><span class="line">    <span class="string">&quot;vocab_size&quot;</span>: <span class="number">50257</span>,    <span class="comment"># 词汇表大小</span></span><br><span class="line">    <span class="string">&quot;context_length&quot;</span>: <span class="number">256</span>,  <span class="comment"># 上下文长度</span></span><br><span class="line">    <span class="string">&quot;emb_dim&quot;</span>: <span class="number">768</span>,         <span class="comment"># 嵌入维度</span></span><br><span class="line">    <span class="string">&quot;n_heads&quot;</span>: <span class="number">12</span>,          <span class="comment"># 注意力头数量</span></span><br><span class="line">    <span class="string">&quot;n_layers&quot;</span>: <span class="number">12</span>,         <span class="comment"># Transformer层数</span></span><br><span class="line">    <span class="string">&quot;drop_rate&quot;</span>: <span class="number">0.1</span>,       <span class="comment"># dropout率</span></span><br><span class="line">    <span class="string">&quot;qkv_bias&quot;</span>: <span class="literal">False</span>       <span class="comment"># QKV线性层是否使用偏置</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p><code>vocab_size</code>: 词汇表里有多少个不同的词元(Token)。<code>50257</code> 是 GPT-2使用的标准词汇表大小，即我们前面讨论的 BPE 分词器的词汇表大小。</p></li><li><p><code>context_length</code>:模型一次能处理的<strong>最长文本长度</strong>（以词元计）。这里是256，意味着模型一次最多能看 256 个词元。</p></li><li><p><code>emb_dim</code>:<strong>嵌入维度</strong>。这是模型内部表示每个词元的向量长度。768维意味着每个词都会被转换成一个包含 768个数字的向量，这是模型理解语言的基础。</p></li><li><p><code>n_heads</code> 和 <code>n_layers</code>:这两个参数共同决定了模型的<strong>深度和宽度</strong>。<code>n_layers=12</code>表示我们的模型会堆叠 12 个 <code>TransformerBlock</code>，而<code>n_heads=12</code> 表示在每个 Block 内部的注意力机制都有 12个"头"，让模型能从多个角度分析文本。</p></li><li><p><code>drop_rate</code>: Dropout比率。这是防止模型过拟合的重要技术。0.1 表示在训练时，每个神经元有 10%的概率被临时"关闭"，迫使模型学习更鲁棒的特征表示。</p></li><li><p><code>qkv_bias</code>:查询-键-值偏置。这个参数控制是否在注意力计算中添加偏置项。False表示不使用偏置，这是 GPT-2 的设计选择，可能有助于模型的稳定性。</p></li></ul><blockquote><p>有些概念你可能还不认识，没关系，我们继续往下看，待会就懂了！· ·</p></blockquote><h2 id="模型结构总览">2.2 模型结构总览</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;完整的GPT模型实现&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.tok_emb = nn.Embedding(cfg[<span class="string">&quot;vocab_size&quot;</span>], cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.pos_emb = nn.Embedding(cfg[<span class="string">&quot;context_length&quot;</span>], cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.drop_emb = nn.Dropout(cfg[<span class="string">&quot;drop_rate&quot;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 堆叠多个Transformer块</span></span><br><span class="line">        <span class="variable language_">self</span>.trf_blocks = nn.Sequential(</span><br><span class="line">            *[TransformerBlock(cfg) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(cfg[<span class="string">&quot;n_layers&quot;</span>])],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.final_norm = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.out_head = nn.Linear(cfg[<span class="string">&quot;emb_dim&quot;</span>], cfg[<span class="string">&quot;vocab_size&quot;</span>], bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, in_idx</span>):</span><br><span class="line">        batch_size, seq_len = in_idx.shape</span><br><span class="line">        <span class="comment"># 词嵌入 + 位置嵌入</span></span><br><span class="line">        tok_embeds = <span class="variable language_">self</span>.tok_emb(in_idx)</span><br><span class="line">        pos_embeds = <span class="variable language_">self</span>.pos_emb(torch.arange(seq_len, device=in_idx.device))</span><br><span class="line">        x = tok_embeds + pos_embeds</span><br><span class="line">        x = <span class="variable language_">self</span>.drop_emb(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.trf_blocks(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.final_norm(x)</span><br><span class="line">        logits = <span class="variable language_">self</span>.out_head(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><p>整个 GPT 模型的架构如下图所示：</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830175651213.png" /></p><p><code>GPTModel</code>类是整个语言模型的顶层封装，其设计目标是构建一个<strong>端到端的、具备自回归（auto-regressive）生成能力</strong>的序列处理架构。从根本上说，任何一个此类模型都必须解决三个核心问题：</p><ol type="1"><li><strong>输入表示 (InputRepresentation)</strong>：如何将离散的、一维的词元 ID序列，转化为模型能够处理的、富含信息的连续多维向量？</li><li><strong>上下文编码 (ContextualEncoding)</strong>：如何对输入序列中的每个元素进行深度处理，使其向量表示能够充分融合整个序列（尤其是其上文）的上下文信息？</li><li><strong>输出投影 (OutputProjection)</strong>：如何将模型内部经过深度处理的上下文向量，重新映射回词汇表空间，以生成对下一个词元的概率预测？</li></ol><p><code>GPTModel</code>的结构正是围绕这三个核心问题，划分成了三个逻辑清晰的功能区块。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>):</span><br><span class="line">        <span class="comment"># 区块一：输入表示层</span></span><br><span class="line">        <span class="variable language_">self</span>.tok_emb = nn.Embedding(...)</span><br><span class="line">        <span class="variable language_">self</span>.pos_emb = nn.Embedding(...)</span><br><span class="line">        <span class="variable language_">self</span>.drop_emb = nn.Dropout(...)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 区块二：上下文编码器堆栈</span></span><br><span class="line">        <span class="variable language_">self</span>.trf_blocks = nn.Sequential(...)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 区块三：输出投影层</span></span><br><span class="line">        <span class="variable language_">self</span>.final_norm = LayerNorm(...)</span><br><span class="line">        <span class="variable language_">self</span>.out_head = nn.Linear(...)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, in_idx</span>):</span><br><span class="line">        <span class="comment"># 执行区块一的功能</span></span><br><span class="line">        tok_embeds = <span class="variable language_">self</span>.tok_emb(in_idx)</span><br><span class="line">        pos_embeds = <span class="variable language_">self</span>.pos_emb(...)</span><br><span class="line">        x = tok_embeds + pos_embeds</span><br><span class="line">        x = <span class="variable language_">self</span>.drop_emb(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 执行区块二的功能</span></span><br><span class="line">        x = <span class="variable language_">self</span>.trf_blocks(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 执行区块三的功能</span></span><br><span class="line">        x = <span class="variable language_">self</span>.final_norm(x)</span><br><span class="line">        logits = <span class="variable language_">self</span>.out_head(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><h2 id="输入表示层从离散符号到情境化向量">2.3输入表示层：从离散符号到情境化向量</h2><p>数据流的第一步是将输入的词元索引 <code>in_idx</code>转换为包含位置信息的向量表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GPTModel forward 方法的起始部分</span></span><br><span class="line">tok_embeds = <span class="variable language_">self</span>.tok_emb(in_idx)</span><br><span class="line">pos_embeds = <span class="variable language_">self</span>.pos_emb(torch.arange(seq_len, device=in_idx.device))</span><br><span class="line">x = tok_embeds + pos_embeds</span><br><span class="line">x = <span class="variable language_">self</span>.drop_emb(x)</span><br></pre></td></tr></table></figure><h3 id="词元嵌入">2.3.1 词元嵌入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.tok_emb = nn.Embedding(cfg[<span class="string">&quot;vocab_size&quot;</span>], cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br></pre></td></tr></table></figure><p>正如前文所说的，计算机无法直接处理"单词"这样的符号。为了进行数学运算，必须将每个离散的词元映射到一个高维的连续向量空间中。这个过程被称为嵌入(Embedding)。<code>nn.Embedding</code>是一个简单的查找表。它本质上是一个权重矩阵，维度为<code>(vocab_size, emb_dim)</code>。输入一个词元的索引，它会返回该索引对应的行向量。这个向量是可训练的，模型在训练过程中会不断调整这些向量，使得在向量空间中语义相近的词元彼此靠近。</p><h3 id="位置嵌入">2.3.2 位置嵌入</h3><p>理论上，词元嵌入非常适合作为大语言模型的输入。然而，大语言模型存在一个小缺陷——它们的自注意力机制（见后文）无法感知词元在序列中的位置或顺序。嵌入层的工作机制是，无论词元ID 在输入序列中的位置如何，相同的词元 ID始终被映射到相同的向量表示，如下图所示。</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830193802429.png" /></p><p>举个最简单的例子："人咬狗"和"狗咬人"在上述机制看来，包含的词元集合是相同的。然而，顺序在自然语言中至关重要。</p><p>为了实现这一点，可以将位置信息进行嵌入，一般有以下 3种位置嵌入方式：</p><ul><li><strong>绝对位置嵌入 (absolute positionalembedding)</strong>：直接与序列中的特定位置相关联。对于输入序列的每个位置，该方法都会向对应词元的嵌入向量中添加一个独特的位置嵌入，以明确指示其在序列中的确切位置。例如，序列中的第一个词元会有一个特定的位置嵌入，第二个词元则会有另一个不同的位置嵌入，以此类推。这种方式可以是可学习的，也可以是通过固定的数学函数（如正弦/余弦函数）生成的。</li><li><strong>相对位置嵌入 (relative positionalembedding)</strong>：关注的是词元之间的相对位置或距离，而非它们的绝对位置。该方法通常在计算注意力分数时，引入一个与词元间距离相关的偏置项，从而让模型学习的是词元之间的"间隔"关系，而不是它们在序列中的"具体坐标"。这种方法使得模型能够更好地适应不同长度（包括在训练过程中从未见过的长度）的序列。</li><li><strong>旋转位置嵌入 (Rotary Positional Embedding,RoPE)</strong>：通过一种创新的方式将位置信息融入自注意力机制中。它并非将位置向量直接添加到词元嵌入上，而是根据词元的绝对位置，对其在注意力计算中使用的查询（Query）和键（Key）向量进行旋转。这种精妙的旋转操作使得任意两个词元之间的注意力分数，能够自然地表示出它们的相对位置关系，从而让模型在处理位置信息时既高效又具备强大的长度泛化能力。</li></ul><p>GPT-2采用的是可学习的绝对位置嵌入，这种方式简单直接，模型可以在训练中自行学会每个位置的'坐标'信息，对于其设计的固定上下文长度（如1024）来说已经足够有效。而像 RoPE这样的相对位置嵌入，则在处理超长文本和提升长度泛化能力方面表现更优，因此被Llama 等更新的模型所采用。</p><p>本书使用的是<strong>绝对位置嵌入</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;完整的GPT模型实现&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_emb = nn.Embedding(cfg[<span class="string">&quot;context_length&quot;</span>], cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, in_idx</span>):</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        pos_embeds = <span class="variable language_">self</span>.pos_emb(torch.arange(seq_len, device=in_idx.device))</span><br><span class="line">        x = tok_embeds + pos_embeds</span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><ol type="1"><li><strong>参数初始化</strong>：<code>self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])</code>这行代码会初始化一个权重矩阵，也称为查找表。该矩阵的维度是<code>(context_length, emb_dim)</code>。矩阵的每一行都是一个向量，且每一行都唯一对应一个从<code>0</code> 到 <code>context_length - 1</code>的绝对位置索引。这些行向量是模型的可训练参数，其初始值通常是随机设定的。</li><li><strong>向量查找</strong>：当模型处理一个具体输入时，<code>torch.arange(seq_len)</code>会首先生成一个包含该输入序列所有位置索引的张量 (tensor)，例如<code>[0, 1, 2, ..., seq_len-1]</code>。随后，这个位置索引张量被传递给<code>self.pos_emb</code>层。该层会根据索引值，从第一步初始化的权重矩阵中，精确地查找并提取出每一行对应的位置向量，最终构成一个维度为<code>(seq_len, emb_dim)</code> 的位置嵌入张量<code>pos_embeds</code>。</li><li><strong>信息融合</strong>：<code>x = tok_embeds + pos_embeds</code>执行向量的逐元素加法操作。此操作将代表词元语义信息的<code>tok_embeds</code> 张量与上一步生成的位置信息<code>pos_embeds</code> 张量合并。</li></ol><p>如下图所示：</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830195222938.png" /></p><h3 id="dropout-掩码">2.3.3 dropout 掩码</h3><p>至此，我们已经通过词元嵌入和位置嵌入的结合，得到了一个信息完备的输入向量。这个向量既包含了词元的语义信息，也明确了其在序列中的顺序，可以说是为模型准备了一份完美的"学习材料"。</p><p>然而，在将这份完美的材料送入 Transformer的核心进行深度加工之前，我们还需要进行一个看似矛盾，却至关重要的操作——故意引入一些不确定性。为什么要这么做呢？<strong><u>这是为了防止模型在训练中变得过于依赖输入的每一个细节，从而陷入"死记硬背"的陷阱，也就是我们常说的"过拟合"。</u></strong>为了让模型学会从不完美的信息中也能提取核心规律，我们需要引入一种正则化技术。这正是我们接下来要讨论的<strong>Dropout</strong>。</p><p><img src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/1*iWQzxhVlvadk6VAJjsgXgg.png" alt="Dropout 技术示意图" style="zoom:50%;" /></p><p>它的主要目的是<strong>防止模型过拟合</strong>，提升模型的<strong>泛化能力</strong>。在<strong>训练期间</strong>，它会随机地将一部分输入数据置为零，迫使模型不能过度依赖于任何少数的特征，从而学习到更加鲁棒的模式。在<strong>评估和预测时</strong>，Dropout会自动失效，不会对数据做任何改动。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;完整的GPT模型实现&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="variable language_">self</span>.drop_emb = nn.Dropout(cfg[<span class="string">&quot;drop_rate&quot;</span>])</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, in_idx</span>):</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        x = <span class="variable language_">self</span>.drop_emb(x)</span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><p>我们本次的实现总共会有 2 个地方应用到 dropout 技术：</p><ol type="1"><li>在词元嵌入和位置嵌入相加之后，进入第一个 Transformer Block之前，即上面的代码所做的事情。这是模型遇到的<strong>第一层正则化</strong>。它直接作用于融合了语义和位置信息的初始输入向量<code>x</code>。通过随机将输入向量中的某些特征置为零，它迫使<strong>后续所有</strong>的Transformer Block都不能过度依赖输入向量中的任何单一维度。这相当于从源头上增加了训练难度，要求整个模型学习到对输入特征扰动不敏感的、更本质的规律。</li><li>在多头注意力模块内部，计算出注意力权重 <code>attn_weights</code>并经过 softmax 归一化之后，在用它去加权 <code>Value</code>向量之前。这种 dropout<strong>不作用于输入向量本身，而是作用于注意力权重</strong>。注意力权重决定了在生成一个词的表示时，应该关注上下文中其他词的程度。在这里应用dropout，会随机地将某些词与词之间的注意力连接切断（权重置为0）。这可以防止模型在学习时走捷径，比如过度依赖于某个特定的前文词汇。它鼓励模型去考虑更广泛的上下文信息，而不是仅仅依赖几个最强的信号。（关于注意力模块，我们后面会详细讨论）</li></ol><hr /><blockquote><p>[!IMPORTANT] 到目前为止，我们已经完整地剖析了 <code>GPTModel</code>的<strong>输入表示层</strong>。我们从第一性原理出发，理解了为什么需要将离散的词元ID，通过<strong>词元嵌入（TokenEmbedding）</strong>和<strong>位置嵌入（PositionalEmbedding）</strong>，转化为一个融合了<strong>语义</strong>与<strong>顺序</strong>信息的、信息完备的高维向量<code>x</code>。</p><p>这个过程的本质，是将人类的符号语言，翻译成了神经网络能够进行数学运算的、结构化的内部语言。</p><p>我们还探讨了 <code>Dropout</code>技术。它像一个严格的教练，通过在训练中随机遮盖部分信息，强迫模型不能死记硬背，必须学会从不完整的信息中提炼出更本质、更鲁棒的规律，从而提升其泛化能力。</p><p>现在，我们有了一批准备就绪、信息丰富且经过初步正则化处理的训练材料。然而，此时此刻，序列中的每一个向量虽然知道了自己是谁以及在哪，但它仍然是一个<strong>独立的、上下文无关的个体</strong>。它并不知道自己与其他词元之间存在着怎样复杂的句法和语义关联。</p><p>那么，模型是如何让这些孤立的向量开始"交流"，理解彼此之间的关系，并最终形成对整个序列的深度理解呢？</p><p>答案，就藏在 Transformer 架构的革命性核心——<strong>自注意力机制(Self-Attention Mechanism)</strong> 之中。下面我们就来深入 LLM中最关键的部分，探究 Transformer 架构的层层细节！</p></blockquote><h2 id="核心处理层transformer-块的堆叠">2.4 核心处理层：Transformer块的堆叠</h2><p>在 <code>GPTModel</code> 中，我们共使用了 <code>n_layers</code> 个<code>TransformerBlock</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">       <span class="comment"># ...</span></span><br><span class="line">        <span class="comment"># 堆叠多个Transformer块</span></span><br><span class="line">        <span class="variable language_">self</span>.trf_blocks = nn.Sequential(</span><br><span class="line">            *[TransformerBlock(cfg) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(cfg[<span class="string">&quot;n_layers&quot;</span>])],</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><p>我们先来看 <code>TransformerBlock</code> 的结构：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer块：多头注意力 + 前馈网络 + 残差连接&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.att = MultiHeadAttention(</span><br><span class="line">            d_in=cfg[<span class="string">&quot;emb_dim&quot;</span>],</span><br><span class="line">            d_out=cfg[<span class="string">&quot;emb_dim&quot;</span>],</span><br><span class="line">            context_length=cfg[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">            num_heads=cfg[<span class="string">&quot;n_heads&quot;</span>],</span><br><span class="line">            dropout=cfg[<span class="string">&quot;drop_rate&quot;</span>],</span><br><span class="line">            qkv_bias=cfg[<span class="string">&quot;qkv_bias&quot;</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.ff = FeedForward(cfg)</span><br><span class="line">        <span class="variable language_">self</span>.norm1 = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.norm2 = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.drop_shortcut = nn.Dropout(cfg[<span class="string">&quot;drop_rate&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 第一个子层：多头注意力 + 残差连接</span></span><br><span class="line">        shortcut = x</span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.att(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.drop_shortcut(x)</span><br><span class="line">        x = x + shortcut</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二个子层：前馈网络 + 残差连接</span></span><br><span class="line">        shortcut = x</span><br><span class="line">        x = <span class="variable language_">self</span>.norm2(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.ff(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.drop_shortcut(x)</span><br><span class="line">        x = x + shortcut</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>它的结构示意图如下所示：</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830202334178.png" /></p><h3 id="注意力机制">2.4.1 注意力机制</h3><p>深入探讨大语言模型核心的自注意力机制之前，让我们考虑一下在大语言模型出现之前的没有注意力机制的架构中所存在的问题。假设我们想要开发一个将文本从一种语言翻译成另一种语言的语言翻译模型。如下图所示，由于源语言和目标语言的语法结构不同，我们无法简单地逐个单词进行翻译。</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830202911825.png" /></p><p>这正是传统的序列处理模型（如RNN）一个根本缺陷的体现：<strong>信息瓶颈</strong>。它们通过一个循环结构顺序处理文本，导致序列末端的信息很难直接关联到序列开头的遥远信息。</p><blockquote><p>[!IMPORTANT]</p><p><strong>自注意力机制 (Self-Attention)</strong>的提出，正是为了打破这种信息瓶颈。其根本思想是：<strong>为序列中的每个元素，建立与其他所有元素的直接连接，并动态计算这些连接的强度（即注意力权重）</strong>。这样，模型在处理任何一个词元时，都能拥有一个全局视野，直接审视并借鉴整个上下文。</p></blockquote><p>在 <code>TransformerBlock</code>的内部，<code>MultiHeadAttention</code>模块是其第一个、也是最为关键的子层。它是整个 GPT模型"智能"的根本来源。要理解它，我们不能一蹴而就。很幸运的是，《从零构建大语言模型》的作者SebastianRaschka，为我们提供了一条从简单到复杂的演进路径，如下图所示，这部分的内容非常精华且重要，所以笔者将尽可能将这部分的内容进行完整记录，以帮助读者们更好的理解自注意力机制。</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830203719560.png" /></p><h4 id="没有可训练权重的简单自注意力机制">2.4.1.1没有可训练权重的简单自注意力机制</h4><p>在深入研究包含可训练权重的复杂版本之前，书中首先实现了一个不含任何可训练权重的简化自注意力机制，以便阐明其核心概念。</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830204044917.png" /></p><p>如上图所示，这个机制的目标是为输入序列中的每一个词元（Token），计算出一个<strong>上下文向量（ContextVector）</strong>。这个上下文向量是一种增强版的嵌入，它不仅包含了当前词元自身的信息，还融合了序列中所有其他词元的信息。这对于理解句子中单词间的关系至关重要 。</p><p>计算这个上下文向量的过程分为三步：</p><ol type="1"><li><strong>计算注意力分数</strong>：衡量每个词对其他词的"相关性"或"相似度"。计算每对词之间的点积，得到相似度分数。点积在这里可以被看作是一种衡量相似度的方式：两个向量的点积越大，代表它们之间的对齐程度或相似度越高，注意力分数也越高。</li><li><strong>归一化获取注意力权重</strong>：得到的注意力分数是一些原始的数值，它们的尺度不一。为了使其规范化并易于解释，我们使用<strong>Softmax 函数</strong>对这些分数进行处理 。Softmax函数能将一组任意实数转换为一个概率分布，确保所有输出值的和为1，并且每个值都是正数。这样得到的数值就是"注意力权重"，代表了在当前查询下，序列中每个词元的重要性。</li><li><strong>计算上下文向量</strong>：最后一步，将序列中的每一个词元嵌入向量与其对应的注意力权重相乘，然后将所有结果向量相加。最终得到的向量就是我们想要的上下文向量，它是整个输入序列的加权和，权重由刚刚计算出的注意力权重决定。</li></ol><p>这 3 个步骤实现了自注意力机制的核心思想：</p><ul><li>看：计算每个词对其他词的关注度</li><li>权衡：将关注度转换为权重</li><li>融合：根据权重融合所有词的信息</li></ul><p>最终效果：每个词的向量表示都包含了整个序列的上下文信息，而不仅仅是自己的信息。这样模型就能理解词与词之间的关系，比如<code>"journey starts"</code> 中的 <code>"starts"</code> 会更多地关注<code>"journey"</code> 的信息。</p><p>现在我们用代码来演示一下，假设我们有以下输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.tensor(</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">0.43</span>, <span class="number">0.15</span>, <span class="number">0.89</span>],   <span class="comment"># Your</span></span><br><span class="line">        [<span class="number">0.55</span>, <span class="number">0.87</span>, <span class="number">0.66</span>],   <span class="comment"># journey</span></span><br><span class="line">        [<span class="number">0.57</span>, <span class="number">0.85</span>, <span class="number">0.64</span>],   <span class="comment"># starts</span></span><br><span class="line">        [<span class="number">0.22</span>, <span class="number">0.58</span>, <span class="number">0.33</span>],   <span class="comment"># with</span></span><br><span class="line">        [<span class="number">0.77</span>, <span class="number">0.25</span>, <span class="number">0.10</span>],   <span class="comment"># one</span></span><br><span class="line">        [<span class="number">0.05</span>, <span class="number">0.80</span>, <span class="number">0.55</span>],   <span class="comment"># step</span></span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>笔者将尝试从第一性原理出发，对代码进行一步步拆解，说明白这个过程为什么能够实现我们期望的目标：<strong>为每个词元（Token）生成一个包含了上下文信息的向量</strong>。</p><p>这个问题的核心在于，我们要证明<strong>最终的上下文向量 (ContextVector)的确融合了其他词元的信息，并且是根据"相关性"来融合的</strong>。</p><p>我们将以你例子中的词 <code>starts</code> (第三个词元)为例，来全程追踪它的变化。</p><p><code>starts</code> 的原始输入向量是:<code>[0.57, 0.85, 0.64]</code>。这个向量只代表 <code>starts</code>本身，它对句子中的其他词一无所知，是孤立的。<u>我们的目标是生成一个新的向量，让这个新的向量知道它前面有<code>Your journey</code>，后面有 <code>with one step</code>。</u></p><p>第一步我们计算注意力分数，是为了发现"谁与我最相关"：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">attn_scores = torch.empty(<span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line"><span class="keyword">for</span> i, x_i <span class="keyword">in</span> <span class="built_in">enumerate</span>(inputs):</span><br><span class="line">    <span class="keyword">for</span> j, x_j <span class="keyword">in</span> <span class="built_in">enumerate</span>(inputs):</span><br><span class="line">        attn_scores[i, j] = torch.dot(x_i, x_j)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line"><span class="comment"># tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],</span></span><br><span class="line"><span class="comment">#         [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],</span></span><br><span class="line"><span class="comment">#         [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],</span></span><br><span class="line"><span class="comment">#         [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],</span></span><br><span class="line"><span class="comment">#         [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],</span></span><br><span class="line"><span class="comment">#         [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])</span></span><br></pre></td></tr></table></figure><p><code>attn_scores</code> 的第 3行是：<code>[0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605]</code>。这行数字告诉我们，<code>starts</code>这个词与句子中每个词的原始相关性得分分别是：</p><ul><li>与 <code>Your</code> 的相关性: <code>0.9422</code></li><li>与 <code>journey</code> 的相关性: <code>1.4754</code> &lt;--<strong>非常高</strong></li><li>与 <code>starts</code> (自身) 的相关性: <code>1.4570</code> &lt;--<strong>非常高</strong></li><li>与 <code>with</code> 的相关性: <code>0.8296</code></li><li>与 <code>one</code> 的相关性: <code>0.7154</code></li><li>与 <code>step</code> 的相关性: <code>1.0605</code></li></ul><p>仅从这一步看，这个机制已经成功地从数学上<strong>发现</strong>了<code>starts</code> 与 <code>journey</code>之间的紧密关系，因为它们的点积分数是最高的之一。这完全符合我们对语言的直觉（"旅程"和"开始"在语义上强相关）。</p><p>第一步得到的分数是原始的、未经缩放的数值，不易于作为权重使用。比如<code>1.4754</code>究竟代表多大的重要性？我们无法直接判断。所以第二步就是进行归一化获取注意力权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. 归一化获取注意力权重</span></span><br><span class="line">attn_weights = torch.softmax(attn_scores, dim=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(attn_weights)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -1 表示在最后一个维度进行归一化，因为 attn_scores 是一个 [行, 列]，所以这里是在列上进行归一化，使得每行的值（在列维度的总和）为 1</span></span><br><span class="line"><span class="comment"># &quot;沿着列的方向&quot; = 在每一行内部，从左到右（列 0 到列 5）进行归一化</span></span><br><span class="line"><span class="comment"># 每一行都独立进行这个过程，结果每一行的 6 个数字加起来都等于 1</span></span><br><span class="line">row_2_sum = <span class="built_in">sum</span>([<span class="number">0.1385</span>, <span class="number">0.2379</span>, <span class="number">0.2333</span>, <span class="number">0.1240</span>, <span class="number">0.1082</span>, <span class="number">0.1581</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Row 2 sum:&quot;</span>, row_2_sum)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;All row sums:&quot;</span>, attn_weights.<span class="built_in">sum</span>(dim=-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line"><span class="comment"># tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],</span></span><br><span class="line"><span class="comment">#         [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],</span></span><br><span class="line"><span class="comment">#         [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],</span></span><br><span class="line"><span class="comment">#         [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],</span></span><br><span class="line"><span class="comment">#         [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],</span></span><br><span class="line"><span class="comment">#         [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])</span></span><br><span class="line"><span class="comment"># Row 2 sum: 1.0</span></span><br><span class="line"><span class="comment"># All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])</span></span><br></pre></td></tr></table></figure><p>现在，这些数字的意义变得非常清晰了： 当模型在处理 <code>starts</code>这个词时，它应该将它的注意力这样分配：</p><ul><li><code>13.90%</code> 的注意力给 <code>Your</code></li><li><code>23.69%</code> 的注意力给 <code>journey</code> &lt;--<strong>权重最高</strong></li><li><code>23.26%</code> 的注意力给 <code>starts</code> (自身)</li><li><code>12.42%</code> 的注意力给 <code>with</code></li><li><code>11.08%</code> 的注意力给 <code>one</code></li><li><code>15.65%</code> 的注意力给 <code>step</code></li></ul><p>这一步将第一步发现的相关性，转化为了具体的、可操作的重要性权重。它明确地告诉我们，为了理解<code>starts</code>，我们需要重点参考 <code>journey</code> 和<code>starts</code> 自身的信息。</p><p>接下来，这是最关键的一步，我们终于要创造那个"增强版"的向量（上下文向量）了。我们的目标是为词<code>i</code> (例如 <code>starts</code>)创建一个<strong>新的表示</strong> <spanclass="math inline">\(C_i\)</span>。这个新的表示 <spanclass="math inline">\(C_i\)</span> 必须满足两个条件：</p><ol type="1"><li>它必须包含<strong>所有</strong>其他词 <code>j</code> (从<code>Your</code> 到 <code>step</code>) 的信息。</li><li>每个词 <code>j</code>贡献的信息量，应该由我们刚刚算出的<strong>注意力权重</strong> <spanclass="math inline">\({w_{ij}}\)</span>来决定。权重越高的词，影响越大。</li></ol><p>现在，让我们思考一下，在数学上，特别是向量空间中，有什么运算可以同时满足这两个条件？<strong>答案就是加权平均(Weighted Average) 或 加权和 (Weighted Sum)。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. 计算上下文向量</span></span><br><span class="line">all_contexts_vec = attn_weights @ inputs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line"><span class="comment"># tensor([[0.4421, 0.5931, 0.5790],</span></span><br><span class="line"><span class="comment">#         [0.4419, 0.6515, 0.5683],</span></span><br><span class="line"><span class="comment">#         [0.4431, 0.6496, 0.5671],</span></span><br><span class="line"><span class="comment">#         [0.4304, 0.6298, 0.5510],</span></span><br><span class="line"><span class="comment">#         [0.4671, 0.5910, 0.5266],</span></span><br><span class="line"><span class="comment">#         [0.4177, 0.6503, 0.5645]])</span></span><br></pre></td></tr></table></figure><p>矩阵乘法 <code>attn_weights @ inputs</code>是一种非常高效的、一次性为所有词计算加权求和的方式。本质上是通过以下方式计算的：</p><p><span class="math display">\[C_i = \sum_{j=1}^N w_{ij} \cdot V_j\]</span></p><p>其中，<span class="math inline">\(w_{ij}\)</span> 是词 i 对词 j的注意力权重，<span class="math inline">\(V_j\)</span> 是词 j的原始向量。</p><p>现在我们来看这个公式的两个部分：</p><p><strong>第一部分：<span class="math inline">\(w_{ij}\cdotV{j}\)</span>（缩放）</strong>：通过这一步，我们为句子中的<strong>每一个词</strong>，都生成了一个<strong>待贡献</strong>的向量。这个向量的意义和原始词一样，但它的影响力已经被其对应的注意力权重精确地调整好了。</p><p><strong>第二部分：∑j=1N（求和）</strong>：这一步是把所有这些待贡献的向量<strong>全部加起来</strong>：<spanclass="math inline">\(C_{starts}=(w_{s,y}⋅V_y)+(w_{s,j}⋅V_j)+(w_{s,s}⋅V_s)+…\)</span>。这一步的几何意义是：在那个高维的意义空间里，我们从原点出发，先沿着加强版<code>journey</code> 向量走一段，再接着走削弱版<code>one</code>向量的方向，再走加强版 <code>starts</code>自身的方向......把所有词的贡献都走完，最终到达的那个<strong>新的位置</strong>，就是我们的上下文向量<span class="math inline">\(C_{starts}\)</span>。</p><p>这个三步过程之所以能起到我们想要的作用，是因为它完美地模拟了人类理解语言的一个核心逻辑：</p><ol type="1"><li><strong>关注焦点</strong>：当我们读到一个词时，我们会本能地寻找与它最相关的词。这个机制通过计算点积，<strong>找到了</strong>这些相关词。</li><li><strong>分配精力</strong>：我们不会对所有相关的词都投入相同的精力。这个机制通过Softmax，将"相关性"转化为"重要性"权重，<strong>量化了</strong>应该投入多少精力。</li><li><strong>综合理解</strong>：我们基于这些焦点和精力分配，在大脑中形成对当前词的综合理解。这个机制通过加权求和，将所有词的信息根据重要性权重<strong>融合</strong>在一起，生成了最终的上下文向量。</li></ol><p>通过这个过程，输出的每一个向量都从"<u>我是谁</u>"的孤立状态，变成了"<u>在这样一个句子里，我是谁</u>"的上下文感知状态，从而实现了我们的最终目标。</p><h4 id="实现带可训练权重的自注意力机制">2.4.1.2实现带可训练权重的自注意力机制</h4><p>在上个阶段，<strong>注意力机制本身没有自己独立的、可以在训练中被优化的参数</strong>。模型在训练时，虽然可以学习和调整输入的<code>x</code>向量（即词嵌入本身），但它<strong>无法学习如何更好地计算注意力</strong>。无论输入的向量如何变化，计算注意力的公式始终不变。而且这种方式过于僵化。一个词元的向量表示<code>x</code>需要同时承载多种信息，它既要代表自身的语义，又要能很好地跟其他词元的向量进行点积来判断相关性。这就像要求一个人同时扮演运动员和裁判员，角色发生了混淆，难以做到最优。</p><p>所以第二个版本的根本问题就是：<u>如何让模型<strong>学会</strong>去关注什么？如何让相似度的计算方式本身变得<strong>灵活和强大</strong>？</u></p><p>解决方案是引入角色分工，让专业的角色做专业的事。第二个版本中，我们不再直接使用原始的输入向量<code>x</code>，而是引入三个独立的可训练线性变换层(<code>nn.Linear</code>)：<strong>Query (Q)</strong>、<strong>Key(K)</strong> 和 <strong>Value (V)</strong>。</p><ul><li><strong>查询向量(Query)</strong>：代表当前这个词，主动去查询句子中其他词与自己的关系。可以理解为：我(starts) 是谁？</li><li><strong>键向量(Key)</strong>：代表句子中的每个词，用来被其他词查询的。可以理解为：我是(journey)，你可以通过这个‘键’来了解我。</li><li><strong>值向量(Value)</strong>：代表句子中每个词所携带的真正信息。一旦查询完毕，确定了关系密切度，我们就从这个值中提取信息。</li></ul><p>至关重要的是，这三个权重矩阵 <spanclass="math inline">\(W_q\)</span>、<spanclass="math inline">\(W_k\)</span>、<spanclass="math inline">\(W_v\)</span>是<strong>可训练的</strong>。这意味着在训练过程中，模型会不断优化它们，学会如何将原始输入<code>x</code> 转换成最有效的 Q、K 和V，从而学会<strong>如何更好地去关注</strong>，让注意力的计算本身变得灵活而强大。（所以才称这个版本是带可训练权重的自注意力机制）</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/1*moKYjUdtx-uEyYMbhPWbIw.png"alt="Linear transformation of the word embedding to obtain Query, Key, and Value vectors — From(https://epichka.com/blog/2023/qkv-transformer/)" /><figcaption aria-hidden="true">Linear transformation of the wordembedding to obtain Query, Key, and Value vectors —From(https://epichka.com/blog/2023/qkv-transformer/)</figcaption></figure><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttention_v2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out, qkv_bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        d_in (int): 输入向量的维度。</span></span><br><span class="line"><span class="string">        d_out (int): 查询(Query)、键(Key)和值(Value)向量的输出维度。</span></span><br><span class="line"><span class="string">        qkv_bias (bool): 是否为Q, K, V线性层添加偏置项。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 使用 nn.Linear 层来定义 Q, K, V 的线性变换。</span></span><br><span class="line">        <span class="comment"># 相比手动创建 nn.Parameter，nn.Linear 提供了更优的权重初始化，并可选择性地包含偏置项，</span></span><br><span class="line">        <span class="comment"># 是 PyTorch 中实现线性变换的标准做法。</span></span><br><span class="line">        <span class="variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x (torch.Tensor): 输入张量，形状为 [批量大小, 序列长度, d_in]。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. 线性投影：将输入 x 转换为 Q, K, V 表示</span></span><br><span class="line">        <span class="comment"># 每个输入词元的向量都会通过独立的线性层，生成其在查询、键、值三个空间中的新表示。</span></span><br><span class="line">        keys = <span class="variable language_">self</span>.W_key(x)      <span class="comment"># 形状: [批量大小, 序列长度, d_out]</span></span><br><span class="line">        queries = <span class="variable language_">self</span>.W_query(x)  <span class="comment"># 形状: [批量大小, 序列长度, d_out]</span></span><br><span class="line">        values = <span class="variable language_">self</span>.W_value(x)    <span class="comment"># 形状: [批量大小, 序列长度, d_out]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 计算注意力分数：通过点积衡量 Query 和 Key 的相似度</span></span><br><span class="line">        <span class="comment"># queries 与 keys 的转置 (.T) 进行矩阵相乘，得到一个注意力分数矩阵。</span></span><br><span class="line">        <span class="comment"># 矩阵中的每个元素 attn_scores[i, j] 代表第 i 个查询与第 j 个键之间的原始相关性。</span></span><br><span class="line">        attn_scores = queries @ keys.T</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 缩放与归一化：将分数转换为最终的注意力权重</span></span><br><span class="line">        <span class="comment"># a. 缩放(Scaling): 将分数除以 key 向量维度的平方根。这一步对于稳定训练至关重要，</span></span><br><span class="line">        <span class="comment">#    可以防止在维度过高时，点积结果过大导致 softmax 梯度消失。</span></span><br><span class="line">        <span class="comment"># b. 归一化(Normalization): 使用 softmax 函数将缩放后的分数转换为概率分布，</span></span><br><span class="line">        <span class="comment">#    确保每一行（代表每个查询）的注意力权重总和为1。</span></span><br><span class="line">        attn_weights = torch.softmax(</span><br><span class="line">            attn_scores / keys.shape[-<span class="number">1</span>] ** <span class="number">0.5</span>, dim=-<span class="number">1</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. 计算上下文向量：对 Value 向量进行加权求和</span></span><br><span class="line">        <span class="comment"># 将上一步得到的注意力权重矩阵与 values 矩阵相乘。</span></span><br><span class="line">        <span class="comment"># 这一步是根据注意力权重，对所有词元的 Value 信息进行加权聚合，</span></span><br><span class="line">        <span class="comment"># 最终为每个词元生成一个融合了全局上下文信息的新向量。</span></span><br><span class="line">        context_vec = attn_weights @ values</span><br><span class="line">        <span class="keyword">return</span> context_vec</span><br></pre></td></tr></table></figure><p>大体流程可参考下图理解：</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/1*6BEwO4jKy9UC7AzU6nIPPg.png"alt="Dot-product attention procedure —From(https://epichka.com/blog/2023/qkv-transformer/)" /><figcaption aria-hidden="true">Dot-product attention procedure—From(https://epichka.com/blog/2023/qkv-transformer/)</figcaption></figure><blockquote><p>[!NOTE]</p><p><strong>缩放点积注意力的原理</strong>：对嵌入维度进行归一化是为了避免梯度过小，从而提升训练性能。例如，在类GPT 大语言模型中，嵌入维度通常大于1000，这可能导致点积非常大，从而在反向传播时由于 softmax函数的作用导致梯度非常小。当点积增大时，softmax函数会表现得更像阶跃函数，导致梯度接近零。这些小梯度可能会显著减慢学习速度或使训练停滞。</p><p>因此，通过嵌入维度的平方根进行缩放解释了为什么这种自注意力机制也被称为缩放点积注意力机制。</p></blockquote><h4 id="利用因果注意力隐藏未来词汇">2.4.1.3利用因果注意力隐藏未来词汇</h4><p>对于像 GPT这样用于文本生成的模型，有一个核心要求：<u>在预测序列中的下一个词元时，模型只能看到当前位置及之前的信息，绝不能偷看未来的词元。</u>标准的自注意力机制会一次性访问整个输入序列，这显然不符合要求。为了解决这个问题，我们引入了<strong>因果注意力（CausalAttention）</strong>，也称为掩码注意力（Masked Attention）。</p><p>实现因果注意力的关键在于<strong>掩码（Masking）</strong>操作。具体做法是在计算出注意力分数之后、应用Softmax 函数之前，对注意力分数矩阵进行修改。我们会创建一个"上三角"掩码矩阵，其中主对角线及以下的元素为0，而主对角线以上的元素为负无穷大 (<code>-inf</code>) 。</p><p>当这个掩码矩阵被加到注意力分数矩阵上时，所有代表"未来"位置的分数都会变成负无穷大。经过 Softmax 函数处理后，这些负无穷大的值对应的概率会变为 0。这样一来，任何词元在计算其上下文向量时，其注意力权重都只会分布在它自身及之前的位置上，从而有效地隐藏了未来的词汇。</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830233836610.png" /></p><p>代码实现逻辑如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># diagonal=1 表示不包含主对角线，只包含上三角部分</span></span><br><span class="line">mask = torch.triu(torch.ones(context_length, context_length), diagonal=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(mask)</span><br><span class="line"><span class="comment"># masked_fill() 用指定值填充掩码为True的位置</span></span><br><span class="line">masked = attn_scores.masked_fill(mask.<span class="built_in">bool</span>(), -torch.inf)</span><br><span class="line"><span class="built_in">print</span>(masked)</span><br><span class="line"><span class="comment"># masked / keys.shape[-1]**0.5 进行缩放，torch.softmax 进行归一化</span></span><br><span class="line">attn_weights = torch.softmax(masked / keys.shape[-<span class="number">1</span>]**<span class="number">0.5</span>, dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(attn_weights)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line">tensor([[-<span class="number">0.0763</span>,    -inf,    -inf,    -inf,    -inf,    -inf],</span><br><span class="line">        [-<span class="number">0.0408</span>,  <span class="number">0.0038</span>,    -inf,    -inf,    -inf,    -inf],</span><br><span class="line">        [-<span class="number">0.0423</span>,  <span class="number">0.0025</span>,  <span class="number">0.0074</span>,    -inf,    -inf,    -inf],</span><br><span class="line">        [-<span class="number">0.0090</span>,  <span class="number">0.0404</span>,  <span class="number">0.0416</span>,  <span class="number">0.0233</span>,    -inf,    -inf],</span><br><span class="line">        [-<span class="number">0.0586</span>, -<span class="number">0.0207</span>, -<span class="number">0.0141</span>, -<span class="number">0.0227</span>,  <span class="number">0.1097</span>,    -inf],</span><br><span class="line">        [ <span class="number">0.0040</span>,  <span class="number">0.0504</span>,  <span class="number">0.0502</span>,  <span class="number">0.0317</span>,  <span class="number">0.0320</span>,  <span class="number">0.0354</span>]],</span><br><span class="line">       grad_fn=&lt;MaskedFillBackward0&gt;)</span><br><span class="line">tensor([[<span class="number">1.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.4921</span>, <span class="number">0.5079</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.3259</span>, <span class="number">0.3364</span>, <span class="number">0.3376</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.2442</span>, <span class="number">0.2529</span>, <span class="number">0.2531</span>, <span class="number">0.2498</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.1919</span>, <span class="number">0.1971</span>, <span class="number">0.1980</span>, <span class="number">0.1968</span>, <span class="number">0.2161</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.1632</span>, <span class="number">0.1686</span>, <span class="number">0.1686</span>, <span class="number">0.1664</span>, <span class="number">0.1664</span>, <span class="number">0.1668</span>]],</span><br><span class="line">       grad_fn=&lt;SoftmaxBackward0&gt;)</span><br></pre></td></tr></table></figure><p>此外，为了防止模型在训练中过拟合，还可以在注意力权重矩阵上应用我们前面提到的<strong>Dropout</strong>技术。即在训练过程中，随机地将一部分注意力权重置为零，这有助于增强模型的泛化能力。</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830234226162.png" /></p><p>代码实现逻辑如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dropout = torch.nn.Dropout(<span class="number">0.5</span>) <span class="comment"># 使用 50% 的 dropout 率</span></span><br><span class="line">example = torch.ones(<span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(example)</span><br><span class="line"><span class="built_in">print</span>(dropout(example))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对权重矩阵进行 dropout</span></span><br><span class="line"><span class="built_in">print</span>(dropout(attn_weights))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line">tensor([[<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">0.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">2.</span>, <span class="number">0.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">0.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">2.</span>, <span class="number">0.</span>, <span class="number">2.</span>, <span class="number">0.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">0.</span>]])</span><br><span class="line">tensor([[<span class="number">2.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.0000</span>, <span class="number">0.6790</span>, <span class="number">0.6818</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.5059</span>, <span class="number">0.5040</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.4336</span>, <span class="number">0.0000</span>],</span><br><span class="line">        [<span class="number">0.3242</span>, <span class="number">0.3345</span>, <span class="number">0.0000</span>, <span class="number">0.3339</span>, <span class="number">0.3433</span>, <span class="number">0.3292</span>]],</span><br><span class="line">       grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure><p>可以看到大约一半的值被置为0，且原来的值被放大了，用于位置权重的整体平衡。</p><p>一个完整的简单因果注意力类的参考实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CausalAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    一个实现了因果自注意力（Causal Self-Attention）的模块。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    因果特性确保在处理序列中的任何一个词元（token）时，</span></span><br><span class="line"><span class="string">    注意力机制只能关注到当前位置及之前位置的词元，而不能看到未来的词元。</span></span><br><span class="line"><span class="string">    这对于自回归（auto-regressive）的文本生成任务至关重要。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out, context_length, dropout, qkv_bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化方法。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">        d_in (int): 输入嵌入向量的维度。</span></span><br><span class="line"><span class="string">        d_out (int): 查询(Query)、键(Key)和值(Value)向量的输出维度。</span></span><br><span class="line"><span class="string">        context_length (int): 模型的最大序列长度，用于创建因果掩码。</span></span><br><span class="line"><span class="string">        dropout (float): 应用于注意力权重矩阵的 dropout 比率。</span></span><br><span class="line"><span class="string">        qkv_bias (bool): 是否为 Q, K, V 的线性层添加偏置项。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d_out = d_out</span><br><span class="line">        <span class="comment"># 定义用于生成 Query, Key, Value 的线性变换层</span></span><br><span class="line">        <span class="variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="comment"># 定义 Dropout 层，用于正则化，防止过拟合</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># 创建并注册因果掩码（causal mask）</span></span><br><span class="line">        <span class="comment"># register_buffer 将一个张量注册为模块的缓冲区，它不会被视为模型参数（即不会在训练中被更新），</span></span><br><span class="line">        <span class="comment"># 但会随着模型移动（例如，.to(device)）。</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(</span><br><span class="line">            <span class="string">&#x27;mask&#x27;</span>,</span><br><span class="line">            <span class="comment"># torch.triu 创建一个上三角矩阵。diagonal=1 表示主对角线（及以下）的元素都为0，</span></span><br><span class="line">            <span class="comment"># 只有主对角线上方的元素为1。这个矩阵用于屏蔽未来的位置。</span></span><br><span class="line">            torch.triu(torch.ones(context_length, context_length), diagonal=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        执行前向传播。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">        x (torch.Tensor): 输入张量，形状为 [批量大小, 序列长度, 输入嵌入维度 d_in]。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 获取输入的维度信息</span></span><br><span class="line">        b, num_tokens, d_in = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. 将输入 x 投影到 Query, Key, Value 空间</span></span><br><span class="line">        keys = <span class="variable language_">self</span>.W_key(x)      <span class="comment"># 输出形状: [b, num_tokens, d_out]</span></span><br><span class="line">        queries = <span class="variable language_">self</span>.W_query(x) <span class="comment"># 输出形状: [b, num_tokens, d_out]</span></span><br><span class="line">        values = <span class="variable language_">self</span>.W_value(x)  <span class="comment"># 输出形状: [b, num_tokens, d_out]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 计算注意力分数</span></span><br><span class="line">        <span class="comment"># 将 keys 的最后两个维度转置，以便进行矩阵乘法</span></span><br><span class="line">        <span class="comment"># 形状从 [b, num_tokens, d_out] 变为 [b, d_out, num_tokens]</span></span><br><span class="line">        <span class="comment"># queries @ keys.transpose(...) 计算每个查询与所有键的点积</span></span><br><span class="line">        attn_scores = queries @ keys.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 输出形状: [b, num_tokens, num_tokens]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 应用因果掩码</span></span><br><span class="line">        <span class="comment"># masked_fill_ 是一个原地操作（in-place），它会直接修改 attn_scores 张量</span></span><br><span class="line">        <span class="comment"># self.mask.bool()[:num_tokens, :num_tokens] 会选取与当前输入序列长度匹配的掩码部分</span></span><br><span class="line">        <span class="comment"># 并将所有需要屏蔽的“未来”位置的分数填充为负无穷大</span></span><br><span class="line">        attn_scores.masked_fill_(</span><br><span class="line">            <span class="variable language_">self</span>.mask.<span class="built_in">bool</span>()[:num_tokens, :num_tokens], -torch.inf</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. 缩放分数并应用 softmax 得到注意力权重</span></span><br><span class="line">        <span class="comment"># a. 缩放 (Scaling): 除以 key 维度的平方根，稳定梯度</span></span><br><span class="line">        <span class="comment"># b. Softmax: 将分数转换为概率分布。由于未来位置的分数是负无穷，</span></span><br><span class="line">        <span class="comment">#    经过 softmax 后它们的权重将变为0。</span></span><br><span class="line">        attn_weights = torch.softmax(</span><br><span class="line">            attn_scores / keys.shape[-<span class="number">1</span>]**<span class="number">0.5</span>, dim=-<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. 应用 Dropout</span></span><br><span class="line">        <span class="comment"># 在训练阶段，随机将一些注意力权重置为0，以防止过拟合</span></span><br><span class="line">        attn_weights = <span class="variable language_">self</span>.dropout(attn_weights)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 6. 计算上下文向量</span></span><br><span class="line">        <span class="comment"># 将注意力权重与 value 向量相乘，得到加权和</span></span><br><span class="line">        context_vec = attn_weights @ values <span class="comment"># 输出形状: [b, num_tokens, d_out]</span></span><br><span class="line">        <span class="keyword">return</span> context_vec</span><br></pre></td></tr></table></figure><h4 id="将单头注意力扩展到多头注意力">2.4.1.4将单头注意力扩展到多头注意力</h4><p>虽然带有可训练权重的因果自注意力机制已经非常强大，但它仍然有局限性：<u>模型在某个位置只能学习到一种注意力模式</u>。为了让模型能够从不同角度、不同表示子空间共同关注信息，原始Transformer 论文引入了<strong>多头注意力（Multi-HeadAttention）</strong>机制 。</p><p>"多头"的核心思想是并行地运行多次注意力计算，而不是只进行一次。具体实现如下：</p><ol type="1"><li><strong>分割成多个头</strong>：我们不再只有一组 <spanclass="math inline">\(W_q\)</span>、<spanclass="math inline">\(W_k\)</span>、<spanclass="math inline">\(W_v\)</span>权重矩阵，而是为每个头都创建一组独立的权重矩阵 。例如，如果我们有 12个头，那我们就有 12 组这样的矩阵。</li><li><strong>并行计算注意力</strong>：每个头都独立地对输入执行缩放点积注意力计算（包含因果掩码）。由于每个头拥有不同的权重矩阵，它们会将输入投影到不同的表示子空间，从而学习到输入序列的不同方面特征。例如，一个头可能关注语法结构，另一个头可能关注语义关联。</li><li><strong>拼接与投影</strong>：在所有头都完成计算后，我们会得到多个输出上下文向量。我们将这些向量<strong>拼接（concatenate）</strong>在一起，形成一个更长的向量。</li><li><strong>最终线性投影</strong>：最后，这个拼接后的长向量会通过一个额外的线性层（<code>out_proj</code>）进行投影，将其维度恢复到模型期望的维度，并融合所有头学习到的信息。</li></ol><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830235007200.png" /></p><p>在代码中，可以通过实现一个简单的<code>MultiHeadAttentionWrapper</code>类来达到这一目标，<code>MultiHeadAttentionWrapper</code>类堆叠了多个之前实现的 <code>CausalAttention</code> 模块实例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttentionWrapper</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;一个实现多头注意力的封装类&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in, d_out, context_length,</span></span><br><span class="line"><span class="params">                dropout, num_heads, qkv_bias=<span class="literal">False</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.heads = nn.ModuleList(  <span class="comment"># 堆叠多个 CausalAttention</span></span><br><span class="line">            [CausalAttention(</span><br><span class="line">                d_in, d_out, context_length, dropout, qkv_bias,</span><br><span class="line">            ) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> (num_heads)]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.cat([head(x) <span class="keyword">for</span> head <span class="keyword">in</span> <span class="variable language_">self</span>.heads], dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>书中还提到了一种更高效的实现方式：与其创建多组独立的权重矩阵，不如创建一个更大的权重矩阵，一次性完成对所有头的查询、键、值向量的计算，然后通过重塑（reshape）和转置（transpose）操作将结果分割成多个头。这种方法在数学上是等价的，但利用了现代硬件进行大规模矩阵运算的优势，计算效率更高。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">输入: [2, 6, 3]</span></span><br><span class="line"><span class="string">    ↓</span></span><br><span class="line"><span class="string">Q,K,V: [2, 6, 2]</span></span><br><span class="line"><span class="string">    ↓</span></span><br><span class="line"><span class="string">重塑: [2, 6, 2, 1] (2个头，每个头1维)</span></span><br><span class="line"><span class="string">    ↓</span></span><br><span class="line"><span class="string">转置: [2, 2, 6, 1] (2个头并行计算)</span></span><br><span class="line"><span class="string">    ↓</span></span><br><span class="line"><span class="string">注意力: [2, 2, 6, 6] (每个头有自己的注意力矩阵)</span></span><br><span class="line"><span class="string">    ↓</span></span><br><span class="line"><span class="string">上下文: [2, 2, 6, 1] (每个头的结果)</span></span><br><span class="line"><span class="string">    ↓</span></span><br><span class="line"><span class="string">合并: [2, 6, 2] (所有头的结果合并)</span></span><br><span class="line"><span class="string">    ↓</span></span><br><span class="line"><span class="string">输出投影: [2, 6, 2]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;一个高效的多头注意力类&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_in: <span class="built_in">int</span>, d_out: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                context_length: <span class="built_in">int</span>, dropout: <span class="built_in">float</span>, num_heads: <span class="built_in">int</span>, qkv_bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> (d_out % num_heads == <span class="number">0</span>), <span class="string">&quot;d_out must be divisible by num_heads&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.d_out = d_out  <span class="comment"># 输出维度</span></span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads <span class="comment"># 头数量</span></span><br><span class="line">        <span class="variable language_">self</span>.head_dim = d_out // num_heads <span class="comment"># 每个头的维度</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化可训练的权重矩阵，分别代表查询向量、键向量、值向量</span></span><br><span class="line">        <span class="variable language_">self</span>.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line">        <span class="variable language_">self</span>.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用一个线性层来组合头的输出</span></span><br><span class="line">        <span class="variable language_">self</span>.out_proj = nn.Linear(d_out, d_out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 掩码 + dropout</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(</span><br><span class="line">            <span class="string">&quot;mask&quot;</span>,</span><br><span class="line">            torch.triu(torch.ones(context_length, context_length), diagonal=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># [batch_size, sequence_length, embedding_dim]</span></span><br><span class="line">        b, num_tokens, d_in = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算权重矩阵 Q/K/V</span></span><br><span class="line">        keys: Tensor = <span class="variable language_">self</span>.W_key(x)</span><br><span class="line">        queries: Tensor = <span class="variable language_">self</span>.W_query(x)</span><br><span class="line">        values: Tensor = <span class="variable language_">self</span>.W_value(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 重塑为多头格式</span></span><br><span class="line">        <span class="comment"># 将 [batch, seq_len, d_out] 重塑为 [batch, seq_len, num_heads, head_dim]</span></span><br><span class="line">        <span class="comment"># [2, 6, 4] -&gt; [2, 6, 2, 2]</span></span><br><span class="line">        keys = keys.view(b, num_tokens, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        values = values.view(b, num_tokens, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">        queries = queries.view(b, num_tokens, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 调整维度顺序，让每个头独立计算注意力，便于批量处理所有头</span></span><br><span class="line">        <span class="comment"># 从形状 (b, num_tokens, num_heads, head_dim)</span></span><br><span class="line">        <span class="comment"># 转换到 (b, num_heads, num_tokens, head_dim)</span></span><br><span class="line">        keys = keys.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        values = values.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        queries = queries.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算注意力分数，这样每个批次(2)的每个头(2)都有了一个 6×6 的注意力分数矩阵</span></span><br><span class="line">        attn_scores = queries @ keys.transpose(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># [2, 2, 6, 2] @ [2, 2, 2, 6] = [2, 2, 6, 6]</span></span><br><span class="line">        mask_bool: Tensor = <span class="variable language_">self</span>.mask.<span class="built_in">bool</span>()[:num_tokens, :num_tokens]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用因果掩码</span></span><br><span class="line">        attn_scores.masked_fill_(mask_bool, -torch.inf)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 归一化权重</span></span><br><span class="line">        attn_weights = torch.softmax(attn_scores / keys.shape[-<span class="number">1</span>]**<span class="number">0.5</span>, dim=-<span class="number">1</span>) <span class="comment"># [2, 2, 6, 6]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用 dropout 掩码减少过拟合</span></span><br><span class="line">        attn_weights = <span class="variable language_">self</span>.dropout(attn_weights) <span class="comment"># [2, 2, 6, 6]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每个头计算自己的上下文向量</span></span><br><span class="line">        context_vec: Tensor = (attn_weights @ values).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># [2, 2, 6, 6] @ [2, 2, 6, 2] = [2, 2, 6, 2] -&gt; [2, 6, 2, 2]</span></span><br><span class="line">        <span class="comment"># 重塑回原始格式 [2, 6, 2, 2] -&gt; [2, 6, 4]</span></span><br><span class="line">        context_vec = context_vec.contiguous().view(b, num_tokens, <span class="variable language_">self</span>.d_out)</span><br><span class="line">        <span class="comment"># 通过输出投影层 [2, 6, 4]</span></span><br><span class="line">        context_vec = <span class="variable language_">self</span>.out_proj(context_vec)</span><br><span class="line">        <span class="keyword">return</span> context_vec</span><br></pre></td></tr></table></figure><p>第二个版本 <code>MultiHeadAttention</code>之所以更好，根本原因在于它<strong>将多次小规模的独立计算，整合为一次大规模的并行计算</strong>，从而最大化地利用了现代硬件（尤其是GPU）的并行处理能力。</p><p>第一个版本 <code>MultiHeadAttentionWrapper</code>的根本问题是<strong>计算被拆散了</strong>。对于一个有 12个头的模型，这意味着要执行 <strong>12 组</strong>独立的 Q, K, V矩阵乘法。在 GPU 上，每次独立的矩阵乘法都需要一次内核启动（kernellaunch），这个启动本身是有开销的。执行 12次小规模的计算，其总开销远大于执行 1次等效的大规模计算。这就像让一个工人去搬 12次箱子，每次只搬一个，远不如让他用推车一次性搬完 12 个箱子来得快。</p><p>这里面的向量变化可能有一些复杂，感兴趣的读者可以参考下图进行辅助理解。</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250830235453791.png"alt="多头注意力完整流程可视化" /><figcaption aria-hidden="true">多头注意力完整流程可视化</figcaption></figure><blockquote><p>[!IMPORTANT]</p><p>到此为止，我们已经走完了一条从最简陋到最完备的注意力机制演进之路。我们从一个不带任何可训练参数的简单点积模型出发，理解了"看-权衡-融合"的核心思想；接着，通过引入可学习的QKV矩阵，赋予了模型<strong>学会如何去关注</strong>的能力；随后，我们用因果掩码为模型戴上了眼罩，强制它遵守时间顺序，只能回顾过去；最后，通过多头机制和高效的并行化实现，我们构建出了GPT 模型真正的<strong>认知核心</strong>——<code>MultiHeadAttention</code>模块。</p><p>这个模块是 Transformer架构的灵魂。它为模型提供了一个动态的、可学习的机制，使其能够在处理每一个词元时，都能审视全局（或全局的过去），并精确地计算出上下文中每一个其他词元对当前词元的重要性，最终生成一个富含深度上下文信息的新表示。</p></blockquote><p>然而，一个强大的引擎（<code>MultiHeadAttention</code>）本身还不足以构成一辆性能优越的赛车（<code>TransformerBlock</code>）。我们还需要稳定系统、传动装置和进一步的加工环节。这就引出了我们接下来的问题：</p><ul><li>模型在通过注意力机制<strong>融合</strong>了上下文信息之后，如何对这些新信息进行进一步的<strong>加工和思考</strong>？</li><li>当我们把 12个这样强大的计算层堆叠在一起时，如何保证训练过程的稳定，防止梯度消失或爆炸？</li><li>在经过如此复杂的变换后，如何确保原始的、未经处理的信息不会在层层传递中丢失？</li></ul><p>让我们先来回顾一下 <code>TransformerBlock</code> 的结构：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer块：多头注意力 + 前馈网络 + 残差连接&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.att = MultiHeadAttention(  <span class="comment"># &lt;----- 我们已经搞定了！</span></span><br><span class="line">            d_in=cfg[<span class="string">&quot;emb_dim&quot;</span>],</span><br><span class="line">            d_out=cfg[<span class="string">&quot;emb_dim&quot;</span>],</span><br><span class="line">            context_length=cfg[<span class="string">&quot;context_length&quot;</span>],</span><br><span class="line">            num_heads=cfg[<span class="string">&quot;n_heads&quot;</span>],</span><br><span class="line">            dropout=cfg[<span class="string">&quot;drop_rate&quot;</span>],</span><br><span class="line">            qkv_bias=cfg[<span class="string">&quot;qkv_bias&quot;</span>],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># &lt;----- 接下来我们继续来解决后面的内容</span></span><br><span class="line">        <span class="variable language_">self</span>.ff = FeedForward(cfg)</span><br><span class="line">        <span class="variable language_">self</span>.norm1 = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.norm2 = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.drop_shortcut = nn.Dropout(cfg[<span class="string">&quot;drop_rate&quot;</span>])</span><br></pre></td></tr></table></figure><p>答案就藏在构成 <code>TransformerBlock</code>的另外几个关键组件中。接下来，我们将把目光从注意力机制本身移开，去探索环绕在它周围的左膀右臂——<strong>前馈网络(FeedForward Network)</strong>、<strong>层归一化 (LayerNormalization)</strong> 和 <strong>残差连接 (Shortcut/ResidualConnections)</strong>，看看它们是如何协同工作，共同构成 Transformer架构坚实可靠的核心处理单元的。</p><ul><li><strong>层归一化 (LayerNorm)</strong>:它的根本作用是<strong>稳定训练过程</strong>。在数据经过复杂的注意力计算或前馈网络变换后，其数值分布可能会变得非常不稳定。层归一化就像一个调节器，在每个子层处理之前，都将数据拉回到一个标准的、易于处理的分布上，确保信息流的稳定。</li><li><strong>前馈神经网络 (FeedForward Network)</strong>:如果说注意力机制负责<strong>融合</strong>来自上下文的信息，那么前馈网络则负责对这些融合后的信息进行<strong>加工和思考</strong>。它是一个小型的、独立处理每个词元位置的神经网络，用于提取更高级、更抽象的特征，增加模型的非线性表达能力。</li><li><strong>快捷连接 (Shortcut/Residual Connection)</strong>:这是训练深度网络的关键技巧。它允许信息绕过某个处理层（如注意力或前馈网络），直接传递到下一层。这确保了即使在经过多达12层甚至更多的深度变换后，最原始的输入信息也不会完全丢失，同时极大地缓解了深度学习中的梯度消失问题，让深度堆叠成为可能。</li></ul><h3 id="使用层归一化进行归一化激活">2.4.2使用层归一化进行归一化激活</h3><p>一个深度神经网络就像一个多级信息加工流水线。数据（信号）在每一层都会被权重矩阵进行复杂的数学变换。当层数很深时，每一层微小的变化都可能被逐层放大。这会导致两个极端问题：</p><ol type="1"><li><strong>信号爆炸</strong>：某些层的输出值变得非常大，导致后续计算溢出，训练过程崩溃。</li><li><strong>信号消失</strong>：某些层的输出值变得非常小，接近于零，导致信息无法有效传递到更深层，模型学不到东西。这两种情况统称为<strong>内部协变量偏移 (Internal CovariateShift)</strong>，它使得训练过程极其不稳定，就像在一条崎岖不平的山路上开车，油门（学习率）稍有不慎就会冲出赛道。</li></ol><p><strong>第一性原理解决方案：强制信号标准化</strong></p><p>最直接的解决方案，就是在信息进入每个核心处理单元（如注意力和前馈网络）之前，强制进行一次校准或标准化。<strong>层归一化</strong>正是扮演了这个角色。它的核心思想是，不管上一层传来的数据分布如何，它都强行将这批数据的均值调整为0，方差调整为 1。这相当于在流水线的每个关键工序前都安装了一个<strong>稳压器</strong>，确保无论输入信号如何波动，进入工序的信号始终是稳定、标准化的。</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831000639468.png" /></p><p>在 <code>TransformerBlock</code>中，层归一化被放置在<strong>多头注意力和前馈网络之前</strong>(<code>self.norm1</code> 和 <code>self.norm2</code>)。这确保了这两个进行核心计算的模块接收到的输入始终处于一个稳定且易于处理的范围内，从而极大地稳定了整个深度模型的训练过程。</p><p><code>LayerNorm</code> 的代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;层归一化实现&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, emb_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.eps = <span class="number">1e-5</span></span><br><span class="line">        <span class="variable language_">self</span>.scale = nn.Parameter(torch.ones(emb_dim))</span><br><span class="line">        <span class="variable language_">self</span>.shift = nn.Parameter(torch.zeros(emb_dim))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mean = x.mean(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        var = x.var(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>, unbiased=<span class="literal">False</span>)</span><br><span class="line">        norm_x = (x-mean) / torch.sqrt(var + <span class="variable language_">self</span>.eps)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.scale * norm_x + <span class="variable language_">self</span>.shift</span><br></pre></td></tr></table></figure><p>让我们从第一性原理出发，来根本性地解释 <code>LayerNorm</code>的这份代码实现。它的每一行都服务于一个核心目的：<strong>在保持模型表达能力的同时，稳定深度网络的训练过程</strong>。我们可以将这个实现拆解为两个核心部分来理解：<strong>强制标准化</strong>和 <strong>可学习的自适应调整</strong>。</p><h4 id="第一部分强制标准化---解决信号失控问题">2.4.2.1第一部分：强制标准化 - 解决信号失控问题</h4><p>这是代码的核心计算部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># forward 方法中的核心计算</span></span><br><span class="line">mean = x.mean(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">var = x.var(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>, unbiased=<span class="literal">False</span>)</span><br><span class="line">norm_x = (x-mean) / torch.sqrt(var + <span class="variable language_">self</span>.eps)</span><br></pre></td></tr></table></figure><ul><li><code>mean = x.mean(dim=-1, keepdim=True)</code> 和<code>var = x.var(dim=-1, ...)</code>：这两行代码计算了<strong>每一个</strong>输入样本<strong>在其特征维度（<code>emb_dim</code>）上</strong>的均值和方差。<code>dim=-1</code>是关键，它指定了归一化是沿着特征维度进行的，而不是像批归一化（BatchNorm）那样跨批次进行。这使得 <code>LayerNorm</code>的效果与批次大小无关，在处理可变长度序列时尤其稳定。</li><li><code>norm_x = (x-mean) / torch.sqrt(var + self.eps)</code>：这是标准的<strong>标准化公式</strong>（减去均值，再除以标准差）。它将原始输入<code>x</code> 转换为了一个均值为 0、方差为 1 的新向量<code>norm_x</code>。<ul><li><code>self.eps = 1e-5</code>：<code>eps</code> (epsilon)是一个极小的常数，它的唯一作用是<strong>防止分母为零</strong>。如果某个样本的方差恰好为0，没有 <code>eps</code> 就会导致除零错误，<code>eps</code>保证了计算的数值稳定性 1。</li><li><code>unbiased=False</code>：这是一个实现细节，表示在计算方差时分母是<code>N</code> 而不是 <code>N-1</code>。选择 <code>False</code>是为了<strong>与原始 GPT-2 模型的实现保持兼容</strong>，因为其最初是使用TensorFlow 实现的，而这是 TensorFlow 的默认行为 2。</li></ul></li></ul><p>至此，我们已经强制将输入信号稳定在一个 <code>N(0, 1)</code>的标准正态分布上。但这又带来了新的问题。</p><h4 id="第二部分可学习的自适应调整---恢复模型的表达能力">2.4.2.2第二部分：可学习的自适应调整 - 恢复模型的表达能力</h4><p>这是在 <code>__init__</code> 中定义并在 <code>forward</code>最后使用的部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># __init__ 中</span></span><br><span class="line"><span class="variable language_">self</span>.scale = nn.Parameter(torch.ones(emb_dim))</span><br><span class="line"><span class="variable language_">self</span>.shift = nn.Parameter(torch.zeros(emb_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment"># forward 的最后一步</span></span><br><span class="line"><span class="keyword">return</span> <span class="variable language_">self</span>.scale * norm_x + <span class="variable language_">self</span>.shift</span><br></pre></td></tr></table></figure><p><strong>根本问题</strong>：将每一层的输入都强制变为均值为 0、方差为 1的分布，这种做法可能<strong>过于暴力和死板</strong>。它虽然稳定了训练，但也可能限制了模型的表达能力。也许对于某个特定层来说，一个均值为10、方差为 5的输入分布才是最优的。我们不希望因为追求稳定而扼杀了模型学习这种分布的可能性。</p><p><strong>解决方案</strong>：在强制标准化之后，再赋予模型<strong>撤销或重新调整</strong>这种标准化的能力。这是通过两个可学习的参数<code>scale</code> 和 <code>shift</code> 来实现的。</p><ul><li><code>self.scale</code>(增益)：这是一个与特征维度相同大小的可学习向量。它与标准化后的<code>norm_x</code> 进行逐元素相乘。它被初始化为全1，所以在训练刚开始时，它不起任何作用（乘以 1 等于不变）。</li><li><code>self.shift</code>(偏置)：这也是一个可学习的向量。它被加到缩放后的结果上。它被初始化为全0，所以在训练开始时，它也不起作用（加上 0 等于不变）。</li></ul><p><strong>这步的精髓在于</strong>：模型在训练过程中，可以通过反向传播自由地学习<code>scale</code> 和 <code>shift</code> 的最佳值。</p><ul><li>如果模型发现强制标准化 <code>N(0, 1)</code>对当前层来说是最好的，它就会让 <code>scale</code> 保持接近1，<code>shift</code> 保持接近 0。</li><li>如果模型发现一个不同的分布更好，它就可以学会相应的<code>scale</code> 和 <code>shift</code> 值，将 <code>norm_x</code>线性变换到任何它认为最优的均值和方差。</li></ul><p>总的来说，<code>LayerNorm</code> 的实现是一个精妙的两步过程：</p><ol type="1"><li><strong>先稳定</strong>：通过强制的标准化，将可能失控的输入信号拉回到一个稳定的<code>N(0, 1)</code> 分布，解决了深度网络训练不稳定的根本问题。</li><li><strong>后放开</strong>：通过引入可学习的 <code>scale</code> 和<code>shift</code>参数，赋予模型恢复甚至创造全新分布的自由度，解决了强制标准化可能带来的表达能力受限的问题。</li></ol><p>最终，这个实现既保证了训练的<strong>稳定性</strong>，又保留了模型的<strong>灵活性和表达能力</strong>。</p><h3 id="实现具有-gelu-激活函数的前馈神经网络">2.4.3 实现具有 GELU激活函数的前馈神经网络</h3><p>自注意力机制的核心是<strong>加权求和</strong>(<code>attn_weights @ values</code>)。虽然计算权重时有<code>softmax</code>引入了非线性，但信息融合的最后一步本质上是一个线性组合。如果整个<code>TransformerBlock</code>只依赖于注意力机制来处理信息，那么模型的表达能力将受到限制。它擅长<strong>融合</strong>信息，但在对融合后的信息进行<strong>深度加工</strong>方面能力不足。</p><p><strong>第一性原理解决方案：为每个词元提供独立的非线性处理空间</strong></p><p>为了弥补这一不足，我们需要一个专门的组件来对注意力机制输出的上下文向量进行进一步的、更复杂的非线性变换。<strong>前馈网络(FFN)</strong> 就是这个组件。 它通常由两个线性层和一个非线性激活函数（如GELU）组成。它会对序列中的<strong>每一个词元向量独立地</strong>进行一次"升维-非线性激活-降维"的操作。</p><ol type="1"><li><strong>升维</strong>：第一个线性层将向量维度扩大（例如从 768维扩展到 3072维），这为模型提供了更广阔的特征空间来表示和加工信息。</li><li><strong>非线性激活(GELU)</strong>：这是关键一步，打破了线性变换的局限，允许模型学习输入和输出之间更复杂、更抽象的关系。</li><li><strong>降维</strong>：第二个线性层将维度恢复到原始大小，以便于下一层<code>TransformerBlock</code> 处理。</li></ol><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831001916054.png" /></p><p>前馈神经网络 <code>FeedForward</code> 的实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;前馈神经网络&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layers = nn.Sequential(</span><br><span class="line">            nn.Linear(cfg[<span class="string">&quot;emb_dim&quot;</span>], <span class="number">4</span> * cfg[<span class="string">&quot;emb_dim&quot;</span>]),</span><br><span class="line">            GELU(),</span><br><span class="line">            nn.Linear(<span class="number">4</span> * cfg[<span class="string">&quot;emb_dim&quot;</span>], cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.layers(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GELU</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;GELU激活函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.5</span> * x * (<span class="number">1</span> + torch.tanh(</span><br><span class="line">            torch.sqrt(torch.tensor(<span class="number">2.0</span> / torch.pi)) *</span><br><span class="line">            (x + <span class="number">0.044715</span> * torch.<span class="built_in">pow</span>(x, <span class="number">3</span>))</span><br><span class="line">        ))</span><br></pre></td></tr></table></figure><ol type="1"><li><strong>引入非线性</strong>：通过 <code>GELU</code>激活函数，让模型有能力学习复杂的数据模式。</li><li><strong>深度加工信息</strong>：通过"升维-降维"的结构，为模型提供一个更广阔的计算空间来提取和转换特征，同时保持整个<code>TransformerBlock</code>输入输出维度的一致性，使其能够被方便地深度堆叠。</li></ol><h3 id="添加快捷连接">2.4.4 添加快捷连接</h3><p>当网络非常深时（例如堆叠 12 层 <code>Transformer</code>块），会遇到两个致命问题：</p><ol type="1"><li><strong>梯度消失 (VanishingGradients)</strong>：在训练时，用于更新权重的梯度信号需要从最后一层反向传播到第一层。每经过一层，梯度都会被乘以该层的权重。在深层网络中，这些连乘操作很可能导致梯度信号迅速衰减，等传到浅层网络时已经微乎其微，导致浅层参数几乎不更新，模型无法有效训练。</li><li><strong>信息退化 (Information Degradation)</strong>：输入向量<code>x</code> 每经过一个 <code>TransformerBlock</code>，都会被复杂的注意力机制和前馈网络完全重构。在经过多层变换后，最原始、最直接的语义和位置信息可能会被冲淡甚至丢失。</li></ol><p><strong>第一性原理解决方案：建立信息/梯度的"高速公路"</strong></p><p>解决方案出奇地简单而有效：在每个复杂处理单元（如注意力和前馈网络）旁边，建立一条<strong>直连通道</strong>，让输入可以直接跳过这个单元，与该单元的输出相加。这就是<strong>快捷连接</strong>或<strong>残差连接</strong>。</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831002424412.png" /></p><p>在 <code>TransformerBlock</code> 中，残差连接的实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        shortcut = x      <span class="comment"># &lt; ------ 保存原始输入</span></span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.att(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.drop_shortcut(x)</span><br><span class="line">        x = x + shortcut  <span class="comment"># &lt; ------ 残差连接</span></span><br><span class="line"></span><br><span class="line">        shortcut = x      <span class="comment"># &lt; ------ 保存原始输入</span></span><br><span class="line">        x = <span class="variable language_">self</span>.norm2(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.ff(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.drop_shortcut(x)</span><br><span class="line">        x = x + shortcut  <span class="comment"># &lt; ------ 残差连接</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>残差连接在这段代码中体现在以下两个关键操作上：</p><ol type="1"><li><code>shortcut = x</code>:这是<strong>分叉路口</strong>，将原始信息备份到 <code>shortcut</code>变量中，开辟了直连通道。</li><li><code>x = x + shortcut</code>:这是<strong>十字路口汇合</strong>，将主干道上经过复杂处理的信息与旁路上的原始信息重新组合。</li></ol><p>具体如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一个子层：多头注意力 + 残差连接</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------- 残差连接的起点 -------------------</span></span><br><span class="line"><span class="comment"># 1. 保存原始输入：在进行任何变换之前，我们先把原始的输入 x 保存到一个名为 shortcut 的变量中。</span></span><br><span class="line"><span class="comment">#    这就相当于开辟了一条“快捷通道”或“旁路”，让原始信息可以绕过复杂的处理。</span></span><br><span class="line">shortcut = x</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------- 主处理路径 (F(x)) -------------------</span></span><br><span class="line"><span class="comment"># 2. 对输入进行复杂变换：</span></span><br><span class="line"><span class="comment">#    - 先进行层归一化</span></span><br><span class="line"><span class="comment">#    - 再通过多头注意力机制</span></span><br><span class="line"><span class="comment">#    - 最后应用 Dropout</span></span><br><span class="line"><span class="comment">#    这一系列操作的结果，更新了变量 x。现在的 x 已经不再是原始输入，</span></span><br><span class="line"><span class="comment">#    而是经过注意力模块深度加工后的“增量信息”或“残差”。</span></span><br><span class="line">x = <span class="variable language_">self</span>.norm1(x)</span><br><span class="line">x = <span class="variable language_">self</span>.att(x)</span><br><span class="line">x = <span class="variable language_">self</span>.drop_shortcut(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------- 残差连接的终点 -------------------</span></span><br><span class="line"><span class="comment"># 3. 将原始输入与变换后的结果相加：</span></span><br><span class="line"><span class="comment">#    这行代码是残差连接最关键的体现。我们将“快捷通道”中的原始信息 (shortcut)，</span></span><br><span class="line"><span class="comment">#    与“主处理路径”上经过复杂变换后的增量信息 (x) 进行逐元素相加。</span></span><br><span class="line"><span class="comment">#    这样，最终的输出既包含了新学到的上下文关系，又没有丢失最原始的输入信息。</span></span><br><span class="line">x = x + shortcut</span><br></pre></td></tr></table></figure><h2 id="输出层从向量到概率分布">2.5 输出层：从向量到概率分布</h2><p>我们从顶层的 <code>GPTModel</code> 容器开始，构建了其核心的可堆叠单元<code>TransformerBlock</code>。在 <code>TransformerBlock</code>内部，我们不仅实现了其进行上下文信息融合的核心模块——<code>MultiHeadAttention</code>，还集成了确保其稳定和高效运行的关键辅助组件：<code>LayerNorm</code>、<code>FeedForward</code>网络和残差连接。</p><p>现在，我们回到 <code>GPTModel</code>的结构上，看看还剩余哪些部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">       <span class="comment"># 前面都已经介绍了...</span></span><br><span class="line">        <span class="variable language_">self</span>.final_norm = LayerNorm(cfg[<span class="string">&quot;emb_dim&quot;</span>])</span><br><span class="line">        <span class="variable language_">self</span>.out_head = nn.Linear(cfg[<span class="string">&quot;emb_dim&quot;</span>], cfg[<span class="string">&quot;vocab_size&quot;</span>], bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, in_idx</span>):</span><br><span class="line">        <span class="comment"># 前面都已经介绍了...</span></span><br><span class="line">        x = <span class="variable language_">self</span>.final_norm(x)</span><br><span class="line">        logits = <span class="variable language_">self</span>.out_head(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><p>经过 <code>n_layers</code> 层 <code>TransformerBlock</code>的深度处理后，我们得到了一个张量 <code>x</code>，其维度为<code>(batch_size, context_length, emb_dim)</code>。这个张量中的每一个向量都蕴含了丰富的上下文信息。然而，这仍然是模型的<strong>内部表示</strong>。模型的最终任务是<strong>预测下一个词元</strong>。</p><p><strong>根本问题</strong>：如何将这个 <code>emb_dim</code>维度的、连续的内部状态向量，转换为一个覆盖整个词汇表（<code>vocab_size</code>）的、离散的预测结果？</p><p><strong>第一性原理解决方案：投影回词汇空间</strong></p><p>这个转换过程由输出层完成，它包含两个步骤：</p><ol type="1"><li><strong>最终归一化 (<code>self.final_norm</code>)</strong>:在进行最后的投影之前，对 <code>Transformer</code>栈的输出再进行一次层归一化。这可以看作是进入最终决策阶段前的一次信号整理，确保输入到输出头的数值分布是稳定的，这有助于后续损失计算和梯度传播的稳定性。</li><li><strong>输出头投影 (<code>self.out_head</code>)</strong>:这是至关重要的一步。<code>self.out_head</code>是一个标准的线性层，其权重矩阵的维度是<code>(emb_dim, vocab_size)</code>。<ul><li><strong>它的作用</strong>：将每一个经过深度处理的、代表特定位置上下文信息的<code>emb_dim</code> 维向量，<strong>线性投影</strong>到一个<code>vocab_size</code> 维的空间中。</li><li><strong>输出的含义</strong>：这个 <code>vocab_size</code>维的新向量被称为<strong>logits</strong>。它的每一个维度都唯一对应词汇表中的一个词元。该维度上的数值，就代表模型预测该词元是下一个词的<strong>原始置信度分数</strong>（未经归一化的对数概率）。分数越高，模型认为该词元出现的可能性越大。</li></ul></li></ol><p>最终，<code>forward</code> 函数返回的 <code>logits</code>张量，其维度为<code>(batch_size, context_length, vocab_size)</code>，精确地包含了模型在每一个输入位置上，对词汇表中所有词元的预测分数。这是模型进行思考和计算后，给出的最终答卷。</p><h2 id="基础文本生成">2.6 基础文本生成</h2><p>至此，我们已经完成了 <code>GPTModel</code> 的全部架构代码实现。</p><p>当前，我们拥有一个结构上完整、参数可扩展的 GPT模型蓝图。然而，必须明确的是，这个模型的所有可训练参数（<code>nn.Embedding</code>、<code>nn.Linear</code>、<code>LayerNorm</code>中的权重和偏置）均由<strong>随机值</strong>初始化。因此，尽管模型结构已经完备，但它不具备任何语言知识，无法执行任何有意义的任务，其输出将是无意义的随机内容。</p><p>不过，在让我们的模型具备输出有意义的内容之前，我们还是先来实现模型文本生成的能力。GPT模型将输出张量转化为生成文本的过程涉及多个步骤，如下图所示。这些步骤包括解码输出张量、根据概率分布选择词元，以及将这些词元转换为人类可读的文本。</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831005826451.png" /></p><p>下图更加详细地展示的下一词元生成过程说明了 GPT模型如何在给定输入的情况下生成下一个词元。在每一步中，模型输出一个矩阵，其中的向量表示有可能的下一个词元。将与下一个词元对应的向量提取出来，并通过softmax函数转换为概率分布。在包含这些概率分数的向量中，找到最高值的索引，这个索引对应于词元ID。然后将这个词元 ID解码为文本，生成序列中的下一个词元。最后，将这个词元附加到之前的输入中，形成新的输入序列，供下一次迭代使用。这个逐步的过程使得模型能够按顺序生成文本，从最初的输入上下文中构建连贯的短语和句子。</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831005901397.png" /></p><p>让我们来实现一个文本生成工具，如下代码所示，<code>generate_and_print_sample</code>是一个用于快速验证和展示的便捷工具，它封装了从编码、生成到解码的全过程，并妥善处理了模型的训练/评估模式切换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_and_print_sample</span>(<span class="params">model, tokenizer, device, start_context</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成文本样本&quot;&quot;&quot;</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    context_size = model.pos_emb.weight.shape[<span class="number">0</span>]</span><br><span class="line">    encoded = text_to_token_ids(start_context, tokenizer).to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        token_ids = generate_text_simple(</span><br><span class="line">            model=model, idx=encoded,</span><br><span class="line">            max_new_tokens=<span class="number">50</span>, context_length=context_size,</span><br><span class="line">        )</span><br><span class="line">    decoded_text = token_ids_to_text(token_ids, tokenizer)</span><br><span class="line">    <span class="built_in">print</span>(decoded_text.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot; &quot;</span>))</span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_text_simple</span>(<span class="params">model, idx, max_new_tokens, context_length</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用模型生成文本&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line">        idx_cond = idx[:, -context_length:]</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            logits = model(idx_cond)</span><br><span class="line"></span><br><span class="line">        logits = logits[:, -<span class="number">1</span>, :]</span><br><span class="line">        probas = torch.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">        idx_next = torch.argmax(probas, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> idx</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">text_to_token_ids</span>(<span class="params">text, tokenizer</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将文本转换为token ID&quot;&quot;&quot;</span></span><br><span class="line">    encoded = tokenizer.encode(text, allowed_special=&#123;<span class="string">&#x27;&lt;|endoftext|&gt;&#x27;</span>&#125;)</span><br><span class="line">    encoded_tensor = torch.tensor(encoded).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> encoded_tensor</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">token_ids_to_text</span>(<span class="params">token_ids, tokenizer</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将token ID转换回文本&quot;&quot;&quot;</span></span><br><span class="line">    flat = token_ids.squeeze(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> tokenizer.decode(flat.tolist())</span><br></pre></td></tr></table></figure><p>我们尝试调用一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">generate_and_print_sample(model, tokenizer, device, <span class="string">&quot;Every effort moves you&quot;</span>)</span><br></pre></td></tr></table></figure><p>可以看到输出是毫无意义的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Every effort moves you Mexican rarity implementing NouPsychCle...&quot; Contributamong enable lacked complications tendon conclud Nearly oddly insign Champions senseless poopuclear shuts dove aspirinentionrous Miniasions fearsomeRanked adore disadvantages disregkeepvocensed eased museums William glovesople Palace shooters increases felony chops Batteryracuse Advertising cease</span><br></pre></td></tr></table></figure><p>那么，如何将这个参数随机化的架构，转变为一个能够理解和生成语言的功能性模型呢？答案是通过<strong>模型训练(Model Training)</strong>。</p><h1 id="训练模型">3. 训练模型</h1><p><img src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831134852813.png" style="zoom:33%;" /></p><p>本篇我们将进入将架构赋予生命的核心环节：<strong>实现训练循环(TrainingLoop)</strong>。我们将详细介绍模型如何通过处理大量文本数据，在一个反复迭代的过程中，系统性地调整其内部数以亿计的参数。</p><p>我们将具体探讨<strong>损失函数 (Loss Function)</strong>的计算、<strong>优化器 (Optimizer)</strong> 的作用以及<strong>反向传播(Backpropagation)</strong>的机制，这些是驱动模型从随机状态向智能状态收敛的根本动力。</p><h2 id="模型训练流程">3.1 模型训练流程</h2><p><strong>训练流程</strong>的根本目的，就是通过一个系统性的、迭代的优化过程，让初始化的<code>GPTModel</code>这个"空壳大脑"通过学习海量的数据样本，逐步调整其内部参数，最终掌握预测下一个词元的规律。</p><p>模型学习的数学基础是<strong>梯度下降 (GradientDescent)</strong>。其核心思想可以归结为：</p><ol type="1"><li><strong>定义目标</strong>：我们需要一个<strong>损失函数 (LossFunction)</strong>来量化模型当前预测与真实答案之间的差距。差距越大，损失值越高。</li><li><strong>寻找方向</strong>：通过微积分计算损失函数对模型中每一个参数的<strong>梯度(Gradient)</strong>。梯度指明了在该参数上，能让损失值<strong>上升最快</strong>的方向。</li><li><strong>进行修正</strong>：我们让参数朝着梯度的<strong>相反方向</strong>迈出一小步。这一小步的步长由<strong>学习率(Learning Rate)</strong> 控制。</li><li><strong>反复迭代</strong>：不断重复"预测-&gt;计算损失-&gt;计算梯度-&gt;更新参数"的过程，模型的参数就会被逐步优化，使得损失值越来越小，预测越来越准。</li></ol><p><code>train_model_simple</code> 函数就是这一原理的精确代码实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model_simple</span>(<span class="params">model, train_loader, val_loader,</span></span><br><span class="line"><span class="params">                    optimizer, device, num_epochs,</span></span><br><span class="line"><span class="params">                    eval_freq, eval_iter, start_context, tokenizer</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型的主循环&quot;&quot;&quot;</span></span><br><span class="line">    train_losses, val_losses, track_tokens_seen = [], [], []</span><br><span class="line">    tokens_seen, global_step = <span class="number">0</span>, -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> input_batch, target_batch <span class="keyword">in</span> train_loader:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss = calc_loss_batch(input_batch, target_batch, model, device)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            tokens_seen += input_batch.numel()</span><br><span class="line">            global_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 定期评估</span></span><br><span class="line">            <span class="keyword">if</span> global_step % eval_freq == <span class="number">0</span>:</span><br><span class="line">                train_loss, val_loss = evaluate_model(</span><br><span class="line">                    model, train_loader, val_loader, device, eval_iter)</span><br><span class="line">                train_losses.append(train_loss)</span><br><span class="line">                val_losses.append(val_loss)</span><br><span class="line">                track_tokens_seen.append(tokens_seen)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Ep <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> (Step <span class="subst">&#123;global_step:06d&#125;</span>):&quot;</span></span><br><span class="line">                    <span class="string">f&quot;Train loss <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span>, &quot;</span></span><br><span class="line">                    <span class="string">f&quot;Val loss <span class="subst">&#123;val_loss:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每3个epoch生成样本</span></span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">3</span> == <span class="number">0</span>:</span><br><span class="line">            generate_and_print_sample(model, tokenizer, device, start_context)</span><br><span class="line">    <span class="keyword">return</span> train_losses, val_losses, track_tokens_seen</span><br></pre></td></tr></table></figure><p>我们可以将这个函数的结构分解为三个层次：<strong>外层循环</strong>、<strong>核心学习循环</strong>和<strong>监控系统</strong>。</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831011653589.png" /></p><h3 id="外层循环">3.1.1 外层循环</h3><p><code>for epoch in range(num_epochs):</code>定义了模型需要完整地看几遍整个训练数据集。一个 <strong>Epoch</strong>代表对所有训练数据的一次完整遍历。让模型反复看同样的数据，是为了使其有机会从不同的批次组合和随机顺序中，更深入地学习数据中的模式。</p><h3 id="核心学习循环">3.1.2 核心学习循环</h3><p><code>for input_batch, target_batch in train_loader:</code>是学习发生的真正场所。对于从 <code>train_loader</code>中取出的每一个数据批次，模型都会严格执行梯度下降的"四步曲"：</p><ol type="1"><li><p>清空旧梯度<code>optimizer.zero_grad()</code></p><p>PyTorch的梯度计算默认是<strong>累加</strong>的。如果不手动清零，当前批次计算出的梯度会和之前所有批次的梯度叠加在一起，导致错误的更新方向。因此，在每次计算新梯度前，必须先“清空缓存”。</p></li><li><p>前向传播与计算损失 <code>loss = calc_loss_batch(...)</code></p><p>我们需要知道模型在当前参数下的表现有多差。<code>calc_loss_batch</code>函数内部会调用<code>model(input_batch)</code>，完成一次<strong>前向传播</strong>，得到预测的logits。然后，使用<strong>交叉熵损失函数</strong><code>cross_entropy</code> 来计算预测 logits 和真实<code>target_batch</code> 之间的差距，得到一个量化误差的标量<code>loss</code>。</p></li><li><p>反向传播计算梯度 <code>loss.backward()</code></p><p>知道了总误差（<code>loss</code>）后，我们需要将这个误差"分摊"到每一个导致误差的参数上，即计算损失对每一个模型参数的偏导数。这是PyTorch <code>autograd</code>引擎的核心功能。这一行代码会自动地、高效地完成整个<strong>反向传播</strong>过程，计算出模型中所有可训练参数的梯度，并存储在它们的<code>.grad</code> 属性中。</p><blockquote><p>不熟悉反向传播概念的读者，可参考：<ahref="https://hedon.top/2025/07/27/llm/back-propagation/">大白话解释反向传播算法</a></p></blockquote></li><li><p>更新模型参数 <code>optimizer.step()</code></p><p>有了修正方向（梯度）后，需要一个执行者来实际地调整参数。优化器（如<code>AdamW</code>）会根据 <code>loss.backward()</code>计算出的梯度，以及自身的更新规则（如学习率），去更新模型中的每一个参数，完成一次学习和进化。</p></li></ol><h3 id="监控系统">3.1.3 监控系统</h3><p>仅仅闷头学习是不够的，我们还需要知道学得怎么样。这个函数内置了两套监控系统：</p><ul><li><strong>定量评估(<code>if global_step % eval_freq == 0</code>)</strong>：每隔<code>eval_freq</code> 步，就调用 <code>evaluate_model</code>函数。该函数会暂停训练 (<code>model.eval()</code>)，在不计算梯度(<code>torch.no_grad()</code>)的模式下，快速计算模型在<strong>训练集</strong>和<strong>验证集</strong>上的损失。通过观察这两个损失的变化，我们可以清晰地了解模型的学习状态。</li><li><strong>定性观察 (<code>if epoch % 3 == 0</code>)</strong>：每隔几个epoch，就调用 <code>generate_and_print_sample</code>函数。它会给模型一个固定的开头(<code>start_context</code>)，让模型在当前的学习状态下续写一段文本。通过观察从最初的“胡言乱语”到逐渐生成通顺句子的过程，我们可以获得最直观的反馈。</li></ul><h2 id="计算文本生成损失">3.2 计算文本生成损失</h2><p>在 <code>train_model_simple</code> 中，有两个关键的辅助函数：</p><ul><li><code>calc_loss_batch</code>：对于给定的一个批次数据，计算出模型预测与真实答案之间的差距有多大。</li><li><code>evaluate_model</code>：在不更新模型参数的前提下，客观地评估模型在当前阶段的学习效果。</li></ul><h3 id="批量损失计算">3.2.1 批量损失计算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calc_loss_batch</span>(<span class="params">input_batch, target_batch, model, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算单个批次的损失&quot;&quot;&quot;</span></span><br><span class="line">    input_batch = input_batch.to(device)</span><br><span class="line">    target_batch = target_batch.to(device)</span><br><span class="line">    logits = model(input_batch)</span><br><span class="line">    loss = torch.nn.functional.cross_entropy(</span><br><span class="line">        logits.flatten(<span class="number">0</span>, <span class="number">1</span>), target_batch.flatten(),</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>这里我们首先需要弄清楚一个核心问题：<strong><u>什么叫做模型预测与真实答案之间的差距？这个差距怎么算？为什么能这么算？</u></strong></p><h4 id="差距的本质是什么">3.2.1.1 差距的本质是什么？</h4><ul><li><strong>模型的预测</strong>：<code>logits</code>。对于输入序列中的每一个位置，模型都会输出一个长度为<code>vocab_size</code> (例如 50257)的向量。这个向量里的每一个数值，代表模型认为对应的词元是正确答案的<strong>原始置信度分数</strong>。分数越高，代表模型越确信。</li><li><strong>真实答案</strong>：<code>target_batch</code>。这是一个具体的、唯一的词元ID。例如，对于输入 <code>"Time is"</code>，正确的下一个词是<code>"an"</code>，那么真实答案就是 <code>"an"</code> 对应的那个唯一的ID。</li><li><strong>差距</strong>：差距就是<strong>模型赋予"真实答案"的那个置信度分数，与它本应达到的理想状态（绝对确信）之间的距离</strong>。如果模型给真实答案的置信度分数很高，而给其他所有错误答案的分数都很低，那么这个差距就很小。反之，如果模型给真实答案的分数很低，那么差距就很大。</li></ul><h4 id="这个差距怎么算">3.2.1.2 这个差距怎么算？</h4><p><code>torch.nn.functional.cross_entropy</code>这个函数虽然只有一行，但它在内部完成了两个关键的数学步骤，来将我们上面描述的抽象差距转化为一个可量化的数值（损失）。</p><p><strong>第一步：使用 <code>Softmax</code>将置信度分数转为概率</strong></p><p>模型的 <code>logits</code>只是原始分数，有正有负，大小不一，不方便直接比较。我们需要将它转换成一个标准的<strong>概率分布</strong>，即所有可能答案的概率加起来等于1。<code>Softmax</code> 函数就是做这个的。</p><p>例如，假设词汇表只有 5 个词，对于某个位置，模型输出的<code>logits</code> 是 <code>[1.0, 4.0, 2.0, -1.0, 0.0]</code>。经过<code>Softmax</code> 转换后，它会变成类似<code>[0.02, 0.68, 0.06, 0.00, 0.24]</code> 的概率分布。</p><p>现在，模型的预测变得清晰了：它有 68%的把握认为第二个词是正确答案。</p><p><strong>第二步：使用负对数似然 (Negative Log-Likelihood)计算损失</strong></p><p>现在我们有了模型的概率预测，也知道了唯一的正确答案（比如就是第二个词）。我们如何量化这个预测的好坏呢？</p><p>我们只需要看模型<strong>赋予那个正确答案的概率值</strong>。在这个例子里，是<code>0.68</code>。</p><ul><li><strong>理想情况</strong>：如果模型完美，它应该给正确答案 100%的概率，即 <code>1.0</code>。</li><li><strong>我们希望</strong>：让模型赋予正确答案的概率尽可能接近<code>1.0</code>。</li></ul><p>负对数似然就是实现这个目标的完美工具。它的公式是 <spanclass="math inline">\(Loss=−log(p_{correct})\)</span>，其中 <spanclass="math inline">\(p_{correct}\)</span>是模型赋予正确答案的概率。</p><p>让我们看看它的特性：</p><ul><li>当 <span class="math inline">\(p_{correct}→1.0\)</span>（模型预测很准）时，<span class="math inline">\(Loss=−log(1.0) →0\)</span> 损失非常小。</li><li>当 <span class="math inline">\(p_{correct} →0\)</span>（模型预测离谱）时，<span class="math inline">\(Loss=−log(0) →infty\)</span> 损失会变得非常大。</li></ul><p>这个特性棒极了！它<strong>极大地惩罚了那些离谱的错误预测</strong>，从而在反向传播时产生巨大的梯度，迫使模型去修正这个严重的错误。</p><p><code>cross_entropy</code> 函数将 <code>Softmax</code>和<strong>负对数似然</strong>这两步合并在了一起，不仅方便使用，而且在数值计算上更加稳定。</p><h4 id="为什么能这么算">3.2.1.3 为什么能这么算？</h4><p>从根本上说，这是因为我们将<strong>语言建模问题，转化为了一个序列性的多分类问题(Multi-Class Classification Problem)</strong>。</p><p>在序列的每一个时间步，模型都在做一个分类任务：从<code>vocab_size</code>个可能的类别（词元）中，选出最有可能的那一个。</p><p><strong>交叉熵 (Cross-Entropy)</strong>源自信息论，是衡量两个概率分布之间差异的标准方法。在这里，这两个分布是：</p><ol type="1"><li><strong>模型的预测分布</strong>：经过 <code>Softmax</code>后的那个概率向量。</li><li><strong>真实的理想分布</strong>：一个 <strong>one-hot</strong>向量。即在正确答案的索引位置为 1，其他所有位置为 0 的向量（例如<code>[0, 1, 0, 0, 0]</code>）。</li></ol><p>最小化交叉熵损失，就是在<strong>迫使模型的预测分布去无限逼近那个理想的、尖锐的真实分布</strong>。通过在海量文本上不断地做这件事，模型就不得不去学习语言的内在规律和模式，以便在任何给定的上下文后，都能生成一个最接近真实世界的下一个词的概率分布。</p><p>最后，代码中的 <code>.flatten(0, 1)</code> 操作，是将<code>(batch_size, context_length)</code>这两个维度压平。这相当于告诉损失函数："别把它们看作是一批句子，请把这批数据里<strong>所有位置的预测任务，都当作是独立的分类问题</strong>来同等对待"，从而高效地一次性计算出整个批次的总损失。</p><blockquote><p>[!NOTE]</p><p>对交叉熵损失概念依旧不是很熟悉的读者，可以参考：<ahref="https://hedon.top/2025/08/13/llm/cross-entropy-loss/">大白话解释交叉熵损失</a></p></blockquote><h3 id="模型评估">3.2.2 模型评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_model</span>(<span class="params">model, train_loader, val_loader, device, eval_iter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估模型性能&quot;&quot;&quot;</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)</span><br><span class="line">        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">return</span> train_loss, val_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_loss_loader</span>(<span class="params">data_loader, model, device, num_batches=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算数据加载器的平均损失&quot;&quot;&quot;</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data_loader) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">float</span>(<span class="string">&quot;nan&quot;</span>)</span><br><span class="line">    <span class="keyword">elif</span> num_batches <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        num_batches = <span class="built_in">len</span>(data_loader)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        num_batches = <span class="built_in">min</span>(num_batches, <span class="built_in">len</span>(data_loader))</span><br><span class="line">    <span class="keyword">for</span> i, (input_batch, target_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">        <span class="keyword">if</span> i &lt; num_batches:</span><br><span class="line">            loss = calc_loss_batch(input_batch, target_batch, model, device)</span><br><span class="line">            total_loss += loss.item()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> total_loss / num_batches</span><br></pre></td></tr></table></figure><p>这段代码的结构清晰地体现了评估的核心原则：</p><ol type="1"><li><strong>状态切换 (<code>model.eval()</code> 和<code>model.train()</code>)</strong>：这是评估流程的开关。<code>model.eval()</code>会关闭 <code>Dropout</code>等只在训练时使用的层，确保评估结果的稳定和可复现。评估结束后，<code>model.train()</code>会重新打开它们，让训练继续。</li><li><strong>隔离环境(<code>with torch.no_grad()</code>)</strong>：这是为了确保评估的纯粹性。在这个代码块中，PyTorch不会跟踪计算图和梯度。这不仅能防止任何意外的参数更新，还能大幅减少内存占用和计算时间，让评估更高效。</li><li><strong>委托计算(<code>calc_loss_loader</code>)</strong>：<code>evaluate_model</code>将具体的计算任务委托给<code>calc_loss_loader</code>。这个函数是一个通用的损失计算器，它迭代指定数量的批次，调用<code>calc_loss_batch</code>累加损失，最后返回平均损失。这种分层设计让代码更整洁。</li><li><strong>双重检验</strong>：同时计算<strong>训练损失</strong>和<strong>验证损失</strong>是至关重要的。<ul><li>训练损失持续下降，说明模型在努力学习。</li><li>验证损失也随之下降，说明模型学到了普适的规律（泛化能力好）。</li><li>如果训练损失下降，但验证损失开始上升，这就是<strong>过拟合</strong>的信号，说明模型开始"死记硬背"训练题，而不是真正理解。</li></ul></li></ol><h2 id="保存和加载模型">3.3 保存和加载模型</h2><p>到目前为止，我们已经讨论了如何从数值上评估训练进展，并从头开始预训练了一个大语言模型。尽管样例中使用的大语言模型和数据集都相对较小，但这足以表明预训练大语言模型代价高昂。因此，保存大语言模型的参数非常重要，这样就不必每次使用它时都重新运行训练。</p><p>这部分很简单，可以直接参考 <ahref="https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html">PyTorch- Saving and Loading Models</a></p><ul><li><p>保存模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dump_model_file = <span class="string">&quot;model_and_optimizer.pth&quot;</span></span><br><span class="line">torch.save(&#123;</span><br><span class="line">    <span class="string">&quot;model_state_dict&quot;</span>: model.state_dict(),</span><br><span class="line">    <span class="string">&quot;optimizer_state_dict&quot;</span>: optimizer.state_dict(),</span><br><span class="line">&#125;, dump_model_file)</span><br></pre></td></tr></table></figure></li><li><p>加载模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">checkpoint = torch.load(dump_model_file, map_location=device)</span><br><span class="line">model = GPTModel(GPT_CONFIG_124M)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&quot;model_state_dict&quot;</span>])</span><br><span class="line">optimizer = torch.optim.AdamW(</span><br><span class="line">    model.parameters(),</span><br><span class="line">    lr=<span class="number">5e-4</span>, weight_decay=<span class="number">0.1</span>,</span><br><span class="line">)</span><br><span class="line">optimizer.load_state_dict(checkpoint[<span class="string">&quot;optimizer_state_dict&quot;</span>])</span><br><span class="line">model.train()</span><br></pre></td></tr></table></figure></li></ul><p>既然我们能加载自己之前训练的模型，那是不是可以加载别人训练的更好的模型呢？当然！幸运的是，OpenAI公开分享了它们的 GPT-2模型的权重，从而省去了我们自己在大型语料库上重新训练模型所需投入的数万到数十万美元。因此，我们可以将这些权重加载到GPTModel 类中，并使用该模型进行文本生成。这里， 权重指的是存储在 PyTorch的 Linear 层和 Embedding 层的 <code>.weight</code>属性中的权重参数。前面在训练模型时，我们通过<code>model.parameters()</code> 访问过。</p><p>这部分不在本篇的核心讨论目标之中，感兴趣的读者可以参考：<ahref="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/01_main-chapter-code">LLMs-from-scratch-ch05</a>。</p><h1 id="文本生成">4. 文本生成</h1><p><img src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831135822616.png" style="zoom:33%;" /></p><p>终于来到我们最后一个环节了：文本生成！</p><p>前面我们在 2.6 章节已经介绍了使用 <code>generate_text_simple</code>进行最基本的文本生成了。本篇我们将介绍一个更具实际意义的文本生成函数<code>generate</code>，它是 <code>generate_text_simple</code>经过两种技术（温度缩放和 Top-k 采样）改进而来的。</p><p>还是回到第一性原理上，<strong><u>为什么我们需要采取额外的技术来优化文本生成？</u></strong></p><p>要回答这个问题，我们必须搞清楚 <code>generate_text_simple</code>中使用的最基本生成方法——<strong>贪婪解码 (GreedyDecoding)</strong>——的根本缺陷是什么。</p><p>在 <code>generate_text_simple</code> 函数中，核心决策步骤是<code>idx_next = torch.argmax(probas, dim=-1, keepdim=True)</code>。这行代码的意思是：在模型预测的所有词元的概率中，永远选择那个<strong>概率最高</strong>的词元作为下一个词。</p><p>这种方法虽然简单直接，但存在三个致命的问题：</p><ol type="1"><li><strong>重复和乏味</strong>：模型很容易陷入重复的循环中。例如，如果"the" 是最常见的下一个词，模型可能会不断生成 "the thethe..."。因为它只看眼前概率最高的一步，缺乏全局视野，导致生成的文本非常单调和机械。</li><li><strong>确定性和可预测性</strong>：对于同一个输入，贪婪解码的输出永远是完全相同的。这对于需要创造力和多样性的任务（如写故事、回答开放性问题）来说是不可接受的。</li><li><strong>错失更优解</strong>：有时，概率第二或第三高的词，可能在长远来看会引导出一个更通顺、更有意义的句子。贪婪解码这种"短视"的策略，会因为眼前的"最优"选择而错失全局的"更优"路径。</li></ol><p><strong>根本问题在于，语言本身不是一个永远选择最常见单词的确定性过程，它充满了多样性和一定的随机性。</strong>我们需要一种方法，既能让模型主要选择那些靠谱的、概率高的词，又能引入适度的随机性，让它偶尔能灵光一闪，选择一些不那么常见但同样合理的词，从而生成更自然、更有趣的文本。</p><p><strong>温度缩放 (Temperature Scaling)</strong> 和 <strong>Top-k 采样(Top-k Sampling)</strong> 就是解决这个问题的两种强大技术。</p><h2 id="温度缩放">4.1 温度缩放</h2><p>温度缩放是一种在从 <code>logits</code>计算最终概率时，调节模型"自信度"的技术。</p><p>它的核心公式是：</p><p><span class="math display">\[probabilities = Softmax(\frac{logits}{temperature})\]</span></p><blockquote><p>因为关键的 Softmax 函数是非线性的，它会将 logits被温度缩放后<strong>减小的差值</strong>转换成更<strong>平缓</strong>的概率分布，或将<strong>放大的差值</strong>转换成更<strong>尖锐</strong>的概率分布，所以最终结果会改变。</p></blockquote><ul><li><strong>当 <code>temperature</code> &gt; 1 (例如1.5)</strong>：<code>logits</code>会被缩小，使得不同词元之间的分数差距变小。经过 <code>Softmax</code>后，概率分布会变得更<strong>平缓</strong>。这意味着，模型会降低对高概率词的执念，同时提升对低概率词的关注度，从而有更大的机会选择不那么常见的词。这会增加生成文本的<strong>多样性和创造性</strong>，但过高则可能导致内容不连贯。</li><li><strong>当 <code>temperature</code> &lt; 1 (例如0.7)</strong>：<code>logits</code>会被放大，使得分数差距拉大。<code>Softmax</code>后的概率分布会变得更<strong>陡峭</strong>。模型会更加确信那些它认为概率最高的词，降低选择其他词的可能性。这使得生成文本更<strong>稳定和保守</strong>，更贴近训练数据的模式。</li><li><strong>当 <code>temperature</code> 趋近于0</strong>：这会极端地放大最高分数的 <code>logit</code>，使得<code>Softmax</code> 的结果无限接近于<code>argmax</code>，最终效果等同于贪婪解码。</li></ul><p>所以你现在应该知道 ChatGPT 接口中这个参数的来源了吧！</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831141218682.png" /></p><h2 id="top-k-采样">4.2 Top-k 采样</h2><p>即便我们用温度调节了想象力，仍然存在一个问题：词汇表非常大，有很多词元的概率虽然不为零，但实际上是完全不合理的。如果在采样时不幸选中了它们，就会产生无意义的文本。</p><p>Top-k采样的思想非常直观：<strong>我们只在最靠谱的一小撮候选词中进行采样。</strong></p><p>它的步骤如下：</p><ol type="1"><li><strong>筛选</strong>：在模型生成了所有词元的概率分布后，我们只保留其中概率最高的<code>k</code> 个词元。</li><li><strong>重新分配概率</strong>：将这 <code>k</code>个词元的概率进行归一化，使它们的概率之和为 1。</li><li><strong>采样</strong>：在这个小得多的、由靠谱候选词组成的集合中，根据新的概率分布进行随机采样。</li></ol><p>例如，如果设置<code>k=5</code>，那么模型在决定下一个词时，只会从它认为最有可能的 5个词中进行选择。这极大地<strong>降低了生成离谱或不相关词汇的风险</strong>，同时又通过在少数几个好的选项中进行随机抽样，保留了文本的多样性。它在模型的"创造力"和"连贯性"之间取得了绝佳的平衡。</p><p>值得一提的是，在 ChatGPT 的 API 中，并没有提供 <code>top_k</code>这个参数，相反的，它提供的是 <code>top_p</code>。</p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250831142404381.png" /></p><p><code>top_k</code> 和 <code>top_p</code>都是为了解决同样的问题：如何在一个合理的范围内进行随机采样，以避免模型生成无意义的词。但它们的实现方式决定了各自的优劣。</p><p><code>top_k</code> 强制模型只从概率最高的 <code>k</code>个词中选择。问题在于，这个 <code>k</code>是一个固定值，无法适应模型在不同情况下的自信度。</p><ul><li><strong>当模型非常确定时</strong>：比如在 "The capital of France is"之后，模型对 "Paris" 的预测概率可能高达 99%。此时如果你的 <code>k</code>设置为 10，你仍然会把 9 个几乎不可能的选项（比如 "London","Berlin"）纳入采样范围，这可能会引入不必要的噪声。</li><li><strong>当模型非常不确定时</strong>：比如在一个开放式创作的开头"Once upon a time, there wasa"，可能有非常多合理的词，它们的概率分布可能非常平缓（例如，前 20个词的概率都差不多）。此时如果你的 <code>k</code> 设置为5，你就会武断地切掉很多同样合理的选项，限制了模型的创造力。</li></ul><p><code>top_p</code>不限制候选词的数量，而是限制候选词的<strong>累积概率</strong>。例如，设置<code>top_p: 0.9</code>，模型会从高到低选择词元，直到它们的概率总和达到90%，然后只在这个动态生成的候选集里进行采样。</p><ul><li><strong>在模型非常确定的情况下</strong>： "Paris" 的概率是99%，已经超过了 90% 的阈值。因此，候选集里<strong>只有 "Paris"一个词</strong>。这完美地保留了模型的确定性。</li><li><strong>在模型非常不确定的情况下</strong>：为了凑够 90%的概率，可能需要把<strong>前 20个词</strong>都包含进来。这同样完美地适应了模型的不确定性，允许它在一个更广阔、更具创造力的空间里进行选择。</li></ul><h2 id="结合">4.3 结合</h2><p>将上述两种技术结合起来，就得到了我们最终实现的文本生成函数<code>generate</code> 了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">model, idx, max_new_tokens, context_length,</span></span><br><span class="line"><span class="params">        temperature=<span class="number">0.0</span>, top_k=<span class="literal">None</span>, eos_id=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;带温度缩放、top_k 筛选的文本生成策略&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line">        idx_cond = idx[:, -context_length:]</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            logits = model(idx_cond)</span><br><span class="line">        logits = logits[:, -<span class="number">1</span>, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用 top_k 采样筛选 logits</span></span><br><span class="line">        <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            top_logits, _ = torch.topk(logits, top_k)</span><br><span class="line">            min_val = top_logits[:, -<span class="number">1</span>]</span><br><span class="line">            logits = torch.where(</span><br><span class="line">                logits &lt; min_val,</span><br><span class="line">                torch.tensor(<span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)).to(logits.device),</span><br><span class="line">                logits,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> temperature &gt; <span class="number">0.0</span>:</span><br><span class="line">            <span class="comment"># 使用温度缩放</span></span><br><span class="line">            logits = logits / temperature</span><br><span class="line">            probs = torch.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">            idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 当不使用温度缩放时，执行贪心解码，选取下一个词元</span></span><br><span class="line">            idx_next = torch.argmax(logits, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果遇到序列结束词元，则提前停止生成</span></span><br><span class="line">        <span class="keyword">if</span> idx_next == eos_id:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure><h2 id="其他思路">4.4 其他思路</h2><p>除了常见的温度采样和 top-k/top-p采样，还有多种技术可以用来控制模型的文本生成策略，每种技术都有其独特的优缺点和适用场景。</p><p><strong>1. Beam Search (集束搜索)</strong></p><p>这是一种基于搜索的启发式算法，旨在找到一个整体概率最高的序列，而不是仅仅关注每一步的最优选择。</p><ul><li><strong>工作原理</strong>：在生成的每一步，它会保留 <code>k</code>个（<code>k</code> 在这里被称为集束宽度或 beamwidth）最可能的候选序列。在下一步，它会从这 <code>k</code>个序列出发，生成所有可能的下一个词，并计算新序列的总概率，然后再次只保留总概率最高的<code>k</code> 个序列。这个过程会一直持续到生成结束。</li><li><strong>优势</strong>：通过探索多种可能性，它通常能生成比贪婪搜索（GreedySearch，即每步都选概率最高的词）更流畅、更全局最优的序列。</li><li><strong>劣势</strong>：它倾向于生成高频、安全的文本，可能会缺乏多样性和创造性。同时，计算开销比简单的采样方法要大。</li></ul><p><strong>2. Contrastive Search (对比搜索)</strong></p><p>这是一种较新的解码方法，旨在通过结合模型的概率和词元间的相似性来提升生成文本的连贯性和多样性，有效减少重复。</p><ul><li><strong>工作原理</strong>：在每一步选择下一个词元时，它会同时考虑两个因素：<ol type="1"><li><strong>模型置信度</strong>：下一个词元的概率要高。</li><li><strong>多样性/惩罚</strong>：下一个词元不应该和前文已经生成的词元过于相似。它通过计算候选词元与上文的相似性得分，并从模型概率中减去这个相似性得分作为惩罚项。</li></ol></li><li><strong>优势</strong>：在许多评测中，对比搜索被证明可以在不需要对模型进行任何额外训练的情况下，显著优于传统的解码方法，尤其在减少文本重复和提升连贯性方面表现突出。</li></ul><p><strong>3. Mirostat 采样</strong></p><p>这是一种自适应的采样算法，它的目标是让生成文本的"惊奇度"（Perplexity，一种衡量不确定性的指标）维持在一个预设的目标值附近。</p><ul><li><strong>工作原理</strong>：Mirostat会在生成过程中持续监控输出文本的困惑度（Perplexity）。如果当前文本的困惑度低于目标值（意味着文本过于平淡、可预测），算法就会动态调整采样策略（如调整<code>top-k</code> 的 <code>k</code>值）来增加随机性。反之，如果困惑度太高（文本可能不连贯），它就会降低随机性。</li><li><strong>优势</strong>：它通过一个反馈循环来直接控制生成文本的统计特性，可以有效避免陷入无聊陷阱（过度重复）和困惑陷阱（内容不连贯）。</li></ul><p><strong>总结</strong></p><table style="width:100%;"><colgroup><col style="width: 15%" /><col style="width: 31%" /><col style="width: 30%" /><col style="width: 22%" /></colgroup><thead><tr class="header"><th>解码策略</th><th>核心思想</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr class="odd"><td><strong>Beam Search</strong></td><td>保留 <code>k</code> 个最可能的序列，追求全局最优。</td><td>连贯性好，适合翻译、摘要等任务。</td><td>多样性差，计算成本较高。</td></tr><tr class="even"><td><strong>Contrastive Search</strong></td><td>结合模型概率和与上文的相异度来选择。</td><td>显著减少重复，提升连贯性。</td><td>算法相对复杂。</td></tr><tr class="odd"><td><strong>Mirostat</strong></td><td>动态调整采样，使文本的困惑度维持在目标水平。</td><td>直接控制文本的统计特性，避免重复和不连贯。</td><td>需要设定一个合适的目标困惑度值。</td></tr></tbody></table><h1 id="总结">总结</h1><p>至此，我们 500行代码的旅程也接近了尾声。本文完整记录了从零开始构建一个 GPT风格语言模型的全过程，旨在将一个复杂的系统拆解为一系列清晰、可执行的步骤。</p><p>首先，从数据处理入手，阐述了如何将原始文本语料通过词元化、滑动窗口采样等方法，构建成模型训练所需的、包含输入-目标对的批量化张量（batchedtensors）。</p><p>接着，深入剖析了 <strong>Transformer模型的核心架构</strong>。从输入层的词元与位置嵌入，到作为核心处理单元的TransformerBlock堆叠。在此过程中，详细解释了多头因果自注意力机制、前馈网络、层归一化和残差连接等关键组件的原理与作用，展示了它们如何协同工作以融合上下文信息并稳定深度网络的训练。</p><p>在模型结构之后，文章介绍了完整的<strong>训练循环</strong>。这包括前向传播、交叉熵损失计算、反向传播和优化器更新参数的完整流程，并展示了如何通过验证集监控训练状态，以评估模型的学习效果和泛化能力。</p><p>最后，文章探讨了<strong>文本生成阶段的解码策略</strong>，分析了从基础的贪婪解码到更高级的温度采样、Top-k和 Top-p等方法的原理，以及它们如何被用于控制生成文本的多样性与连贯性。</p><p>本文的核心主线是展示一个复杂的大语言模型系统，实际上可以被拆解为一系列目标明确、逻辑清晰的子问题和对应的工程实现。通过逐一解决从数据表示、上下文融合、深度网络训练稳定性到高质量文本生成等一系列挑战，我们最终将这些独立的模块化解决方案组合成一个功能完备的系统。</p><p>通过这种从零开始的构建过程，我们不仅能理解各个技术点的作用，更能把握它们之间如何相互关联、协同工作，从而对整个大语言模型的工作原理形成一个结构化、系统性的认知。希望本文的拆解与实现，能为每一位对大模型内部工作原理感到好奇、并希望从实践中构建体系化认知的开发者，提供一条清晰可循的路径和切实的帮助。</p>]]></content>
    
    
    <summary type="html">通过 500 行代码实现，深入解析从零构建 GPT 风格大语言模型的完整流程：从数据处理、Transformer 架构核心、训练循环到文本生成策略，带你理解大模型背后的工程实现原理。</summary>
    
    
    
    <category term="读书笔记" scheme="https://hedon.top/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="读书笔记" scheme="https://hedon.top/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="大模型" scheme="https://hedon.top/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>优雅重启的范式转移：从 tableflip 到 Kubernetes 的 Go 服务升级终极指南</title>
    <link href="https://hedon.top/2025/08/30/graceful-restart-from-tableflip-to-k8s/"/>
    <id>https://hedon.top/2025/08/30/graceful-restart-from-tableflip-to-k8s/</id>
    <published>2025-08-30T02:31:45.000Z</published>
    <updated>2025-09-01T11:07:55.351Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言同一个目标两个世界">前言：同一个目标，两个世界</h3><p>在软件开发的世界里，实现服务的"零停机更新"是一个永恒的追求。它意味着我们的服务可以在发布新版本、修复Bug甚至变更配置时，依然对用户保持连续可用，这是衡量一个系统成熟与否的关键指标。</p><p>在 Go 的生态中，<code>tableflip</code>库以其精巧绝伦的设计，为我们展示了一种在单机时代实现优雅重启的"魔法"。它通过<code>fork/exec</code>和文件描述符传递，实现了进程级的无缝交接，令人拍案叫绝。</p><p>然而，当我们踏入 Kubernetes所引领的云原生时代，会惊奇地发现，这个曾经的屠龙之技似乎变得水土不服，甚至被视为一种反模式(anti-pattern)。为什么一个如此优雅的方案，会在新的环境中失效？</p><p>本文将带您踏上这段优雅重启的范式转移之旅。我们将从<code>tableflip</code>的第一性原理出发，深入剖析其工作机制；然后，我们将切换视角，审视Kubernetes 是如何以一种截然不同的哲学来定义和实现优雅；最后，我们将深入Kubernetes实践的每一个细节，从探针、竞态条件到有状态服务和多服务进程，为您在云原生世界中构建高可用Go 应用，提供一份清晰、详尽的终极指南。</p><h3 id="旧世界的艺术品-tableflip-的魔法">1. 旧世界的艺术品 ——<code>tableflip</code> 的魔法</h3><p><code>tableflip</code>的核心思想，是在一个稳定的、长生命周期的环境（如一台虚拟机或物理机）中，用一个新的进程实例，<strong>原地、无缝地替换</strong>掉一个旧的进程实例，而对外服务的端口始终保持监听。</p><p>它的魔法源于一个经典的 Unix/Linux系统特性：父进程可以将其打开的文件描述符（File Descriptors,FD）传递给子进程。对于一个网络服务而言，最重要的文件描述符，就是那个监听网络端口的<code>socket FD</code>。</p><p><code>tableflip</code> 的工作流程，可以通过下图清晰地展示：</p><pre class="mermaid">graph TD    %% Define Node Shapes    classDef state fill:#d4f0f0,stroke:#333,stroke-width:2px;    classDef action fill:#fff2cc,stroke:#333,stroke-width:2px;    classDef process fill:#f8cecc,stroke:#b85450,stroke-width:2px;    classDef traffic fill:#dae8fc,stroke:#6c8ebf,stroke-width:2px;    %% Initial State    A["服务运行中 (v1)<br/>父进程 accept() 所有连接"]:::state;    B{"收到 SIGUSR2 更新信号"}:::action;    %% Core Actions    C{"fork/exec 创建子进程 (v2)"}:::action;    D{"通过 UDS 传递 Socket FD"}:::action;    %% State Split - The core of the graceful restart    E["<b>子进程 (v2) 行为</b><br/>继承 Socket FD<br/>开始 accept() <b>新</b>的连接"]:::process;    F["<b>父进程 (v1) 行为</b><br/>停止 accept() 新连接<br/>继续处理<b>已建立</b>的连接"]:::process;    %% Final Action    G["所有旧连接处理完毕<br/>父进程干净地退出"]:::action;    %% Final State    H["服务运行中 (v2)<br/>子进程 accept() 所有连接"]:::state;    %% Traffic Flow    NewReq("新的客户端请求"):::traffic;    OldReq("已建立的连接"):::traffic;    %% Chart Flow    A --> B;    B --> C;    C --> D;    D --> E;    D --> F;    F --> G;    E --> H;    G --> H;    NewReq --> E;    OldReq --> F;</pre><p>从外部客户端看来，服务的端口从未关闭，请求始终被处理，一次完美的零停机更新就这样在进程层面完成了。</p><h3 id="新世界的哲学-kubernetes-的宏大编排">2. 新世界的哲学 ——Kubernetes 的宏大编排</h3><p>现在，让我们把视角切换到 Kubernetes。Kubernetes 的世界观与<code>tableflip</code>的假设完全不同。它的核心哲学是<strong>不可变基础设施 (ImmutableInfrastructure)</strong>。</p><p>在这个哲学下，运行中的容器 (Pod)被视为<strong>短暂的、可任意替代的</strong>（ephemeral anddisposable），就像牧群中的牛羊 (cattle)，而不是需要精心照料的宠物(pets)。我们从不"修复"或"升级"一个正在运行的容器，我们只用一个新的、配置好的容器去<strong>替换</strong>它。</p><p>Kubernetes 实现零停机更新的机制，是<strong>滚动替换 (RollingUpdate)</strong>，这是一场由更高维度（<code>Deployment</code>控制器）编排的、跨越整个集群的宏大工程。</p><h3 id="范式冲突-为什么-tableflip-水土不服">3. 范式冲突 —— 为什么<code>tableflip</code> 水土不服</h3><p><code>tableflip</code>的优雅，建立在一个稳定的、可直接操控进程的底层环境之上。而 Kubernetes恰恰抽象掉了这个底层，带来了更高维度的管理模型。二者的冲突，源于根本性的“世界观”不合。</p><ol type="1"><li><strong>抽象层级不匹配</strong>: <code>tableflip</code> 在<strong>Pod 内部</strong> 玩"进程接力"，而 Kubernetes 在 <strong>Pod外部</strong> 玩"Pod 替换"。你在旧 Pod 内部做的任何进程替换，对于Kubernetes 的宏大更新流程来说，是毫无意义的。</li><li><strong>资源竞争与 OOMKilled</strong>: <code>tableflip</code> 在执行<code>Upgrade()</code>的短暂瞬间，父子两个进程会同时存在，这意味着应用的内存和 CPU消耗可能会瞬间翻倍。在资源受严格限制的 Kubernetes Pod 中，这极易触发OOMKilled（Out of Memory Killer），优雅重启变成了"暴力猝死"。</li><li><strong>功能冗余与复杂化</strong>: Kubernetes 的<code>Deployment</code> + <code>Service</code> +<code>Readiness Probe</code>已经提供了一套经过大规模生产验证的、跨节点的零停机更新方案。<code>tableflip</code>想要解决的问题，在 Kubernetes的世界里已经由更高维度的架构设计解决了。</li></ol><h3 id="k8s-的优雅之道-go-开发者深度实践指南">4. K8s 的优雅之道 —— Go开发者深度实践指南</h3><p>既然旧世界的魔法已经失效，我们就必须学习并掌握新世界的规则。在Kubernetes中，真正的优雅，是应用程序与编排平台之间的一场精妙的“双人舞”。</p><h4 id="序曲一切从-server.shutdown-开始">4.1 序曲：一切从<code>server.Shutdown()</code> 开始</h4><p>无论平台如何演变，应用自身具备优雅关闭的能力是所有高级实践的起点。一个基础的、具备优雅关闭能力的Go 服务应该如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    server := &amp;http.Server&#123;Addr: <span class="string">&quot;:8080&quot;</span>&#125;</span><br><span class="line">    <span class="comment">// ... 你的业务 handler ...</span></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        <span class="keyword">if</span> err := server.ListenAndServe(); err != <span class="literal">nil</span> &amp;&amp; err != http.ErrServerClosed &#123;</span><br><span class="line">            log.Fatalf(<span class="string">&quot;ListenAndServe(): %v&quot;</span>, err)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    quit := <span class="built_in">make</span>(<span class="keyword">chan</span> os.Signal, <span class="number">1</span>)</span><br><span class="line">    signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)</span><br><span class="line">    &lt;-quit <span class="comment">// 阻塞直到收到信号</span></span><br><span class="line"></span><br><span class="line">    log.Println(<span class="string">&quot;Shutting down server...&quot;</span>)</span><br><span class="line">    ctx, cancel := context.WithTimeout(context.Background(), <span class="number">30</span>*time.Second)</span><br><span class="line">    <span class="keyword">defer</span> cancel()</span><br><span class="line">    <span class="keyword">if</span> err := server.Shutdown(ctx); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        log.Fatal(<span class="string">&quot;Server shutdown failed:&quot;</span>, err)</span><br><span class="line">    &#125;</span><br><span class="line">    log.Println(<span class="string">&quot;Server exited properly&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码正确地响应了 Kubernetes 的"请关闭"信号(<code>SIGTERM</code>)，是优雅之路的第一步。</p><h4 id="k8s-的眼睛深入理解探针-probes">4.2 K8s 的眼睛：深入理解探针(Probes)</h4><p>Kubernetes 如何知道你的新 Pod “准备就绪”了？它如何判断一个运行中的Pod 是否“卡死”了？答案是<strong>探针 (Probes)</strong>。</p><pre class="mermaid">stateDiagram-v2    state "Pending" as P    state "ContainerCreating" as CC    state "Running" as R    [*] --> P    P --> CC    CC --> R    state R {        direction LR        state "Startup Probe" as SP        state "Liveness/Readiness Probes" as LRP        state "Ready" as RDY        state "NotReady" as NRDY        state "Restarting" as RST        [*] --> SP : 容器启动        SP --> LRP : 启动探针成功        SP --> RST : 启动探针失败        LRP --> RDY : 就绪探针成功        LRP --> NRDY : 就绪探针失败        RDY --> LRP : 周期性检查        NRDY --> LRP : 周期性检查        state "Liveness Check" as LC        state "Readiness Check" as RC        LRP: LC & RC        LC --> [*] : 存活探针失败 --> RST    }</pre><ul><li><strong>存活探针 (Liveness Probe)</strong>:像一个心跳检测仪，失败会导致容器<strong>重启</strong>。</li><li><strong>就绪探针 (Readiness Probe)</strong>:像一块营业中/休息中的牌子，失败会导致流量被<strong>停止</strong>。</li><li><strong>启动探针 (Startup Probe)</strong>:为启动缓慢的应用提供额外的宽限期。</li></ul><p>对于一个需要预热缓存的 Go 应用，我们应该分别实现<code>/healthz</code> (Liveness) 和 <code>/readyz</code> (Readiness)端点，并在 Kubernetes YAML 中精确配置。</p><h4 id="魔鬼在细节中破解优雅终止的竞态条件">4.3魔鬼在细节中：破解优雅终止的竞态条件</h4><p>一个致命的魔鬼隐藏在细节中：当一个 Pod 被终止时，<code>Service</code>端点列表的更新在整个集群中的传播<strong>不是瞬时的</strong>。这会导致竞态条件。</p><p><strong>错误的关闭流程 - 竞态条件</strong></p><pre class="mermaid">sequenceDiagram    participant Kubelet as Kubelet    participant App as Go 应用 (Pod)    participant Endpoints as Endpoints Controller    participant KubeProxy as Kube-Proxy (在其他节点)    participant Client as 客户端    Kubelet->>App: 发送 SIGTERM 信号    App->>App: 立即调用 server.Shutdown()    Note right of App: 应用停止接受新连接    Endpoints->>Endpoints: 将 Pod 从 Service 端点移除 (有延迟)    Client->>KubeProxy: 发起新请求    Note over KubeProxy: 此时，Kube-Proxy 的本地规则还未更新    KubeProxy->>App: 转发请求到即将关闭的 Pod    App-->>KubeProxy: Connection Refused!    KubeProxy-->>Client: 返回连接错误</pre><p><strong>解决方案：<code>preStop</code> 生命周期钩子</strong>，这是Kubernetes 提供的标准答案。</p><p><strong>正确的关闭流程 - <code>preStop</code> Hook</strong></p><pre class="mermaid">sequenceDiagram    participant Kubelet as Kubelet    participant App as Go 应用 (Pod)    participant Endpoints as Endpoints Controller    participant KubeProxy as Kube-Proxy    Kubelet->>Endpoints: Pod 状态变为 "Terminating", Endpoints Controller 立即移除 Pod    Note over Endpoints, KubeProxy: Endpoints 更新开始传播到所有 Kube-Proxy    Kubelet->>App: 执行 preStop Hook (e.g., "sleep 10")    Note over App: 应用仍在运行，但新流量已开始停止    par 等待期间        KubeProxy->>KubeProxy: 更新本地网络规则，不再转发到此 Pod    and        App->>App: "sleep 10" 正在执行    end    Kubelet->>App: preStop 结束后，发送 SIGTERM 信号    App->>App: 调用 server.Shutdown()    Note right of App: 此时已无新流量进入，从容处理存量请求</pre><p>配置如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ... in your container spec</span></span><br><span class="line"><span class="attr">lifecycle:</span></span><br><span class="line">  <span class="attr">preStop:</span></span><br><span class="line">    <span class="attr">exec:</span></span><br><span class="line">      <span class="comment"># 在发送 SIGTERM 信号之前，先执行这个 sleep 命令</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&quot;/bin/sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;sleep 10&quot;</span>]</span><br></pre></td></tr></table></figure><p>这个小小的 <code>preStop</code>hook，将应用代码与基础设施的传播延迟解耦，是实现真正优雅关闭的点睛之笔。</p><h4 id="当服务拥有记忆有状态应用-statefulset">4.4当服务拥有记忆：有状态应用 (<code>StatefulSet</code>)</h4><p>对于数据库、消息队列这类有状态服务，<code>Deployment</code>的随机替换策略是灾难性的。为此，Kubernetes 提供了<code>StatefulSet</code>，它提供了三大保证：</p><ol type="1"><li><strong>稳定的网络身份</strong>: Pod 名称固定 (<code>-0</code>,<code>-1</code>, ...)，并拥有独立的 DNS 记录。</li><li><strong>稳定的持久化存储</strong>: 每个 Pod 绑定一个专属的存储卷(PV)。</li><li><strong>有序的部署和更新</strong>: 严格按照序号<code>0 -&gt; N</code> 部署，按照 <code>N -&gt; 0</code>更新和删除。</li></ol><p>对于有状态服务，平滑更新的内涵变成了<strong>状态的无损交接</strong>，这需要应用本身具备集群和主从切换能力。</p><h4 id="终极优雅将复杂性交给服务网格-service-mesh">4.5终极优雅：将复杂性交给服务网格 (Service Mesh)</h4><p>有没有一种方式，让应用代码回归纯粹，完全不关心这些运维细节呢？答案是<strong>服务网格 (Service Mesh)</strong>。它通过 <strong>Sidecar代理模式</strong>，将所有通用的网络通信逻辑从应用中剥离出来。</p><p>在服务网格的世界里，关闭流程变得对应用完全透明，由 Sidecar代理自动完成所有优雅的流量排空，让你的 Go 应用可以极度简化。</p><h4 id="融会贯通应对真实世界的多服务进程">4.6融会贯通：应对真实世界的多服务进程</h4><p>一个进程可能同时提供多种服务（例如，一个 HTTP 服务 + 一个 TCP服务）。此时，生命周期的管理也需要"整体思维"。</p><ul><li><strong>启动时</strong>: 需要一个<strong>聚合健康端点</strong>。在Go 应用中创建一个唯一的 <code>/readyz</code>接口，它的逻辑是当且仅当<strong>内部所有服务都就绪</strong>时，才返回<code>HTTP 200</code>。</li><li><strong>关闭时</strong>:需要一个<strong>编排式的关闭流程</strong>。收到 <code>SIGTERM</code>后，立刻翻转内部的聚合就绪状态，让 <code>/readyz</code> 失败，然后依赖<code>preStop</code> hook 等待，最后按顺序优雅地关闭所有内部服务。</li></ul><h3id="结语拥抱范式转移在云原生世界中优雅前行">结语：拥抱范式转移，在云原生世界中优雅前行</h3><p>从 <code>tableflip</code> 到Kubernetes，我们看到的不是一个技术的"优劣"之争，而是一场深刻的<strong>范式转移</strong>。</p><p><code>tableflip</code>是单机时代，工程师们凭借对底层系统深刻的理解，创造出的精巧艺术品。它代表了一种<strong>面向进程、命令式</strong>的优雅。</p><p>而 Kubernetes 的滚动更新，则是在分布式时代，通过<strong>面向API、声明式</strong>的宏大编排，实现的系统级的优雅。它将复杂性上移到平台，从而将应用开发者解放出来，让他们能更专注于业务逻辑本身。</p>]]></content>
    
    
    <summary type="html">本文将带您踏上优雅重启的范式转移之旅，从 tableflip 的第一性原理出发，深入剖析其工作机制；然后，我们将切换视角，审视 Kubernetes 是如何以一种截然不同的哲学来定义和实现优雅；最后，我们将深入 Kubernetes 实践的每一个细节，从探针、竞态条件到有状态服务和多服务进程，为您在云原生世界中构建高可用 Go 应用，提供一份清晰、详尽的终极指南。</summary>
    
    
    
    <category term="解决方案" scheme="https://hedon.top/categories/%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    
    
    <category term="解决方案" scheme="https://hedon.top/tags/%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    
  </entry>
  
  <entry>
    <title>Redis 数据类型丨String丨从第一性原理看 Redis 字符串的设计哲学 (基于 Redis 8.2.1 源码)</title>
    <link href="https://hedon.top/2025/08/25/redis/redis-datatype-string/"/>
    <id>https://hedon.top/2025/08/25/redis/redis-datatype-string/</id>
    <published>2025-08-25T11:41:00.000Z</published>
    <updated>2025-09-01T11:07:55.352Z</updated>
    
    <content type="html"><![CDATA[<p>当人们初次接触 Redis 时，<code>String</code>类型往往是他们认识的第一个数据结构。<code>SET key value</code>，<code>GET key</code>，简单直观，易于上手。很多人因此认为，RedisString就是一个朴素的字符串键值对。然而，这个看似简单的表面之下，隐藏着一个由精妙设计、极致优化和深刻权衡构建起来的微观世界。</p><p>这篇文章将基于 <ahref="https://github.com/redis/redis/blob/8.2.1/src/sds.h">Redis8.2.1</a>带领你进行一次深度探索。我们不满足于"是什么"，而是要从计算机科学的<strong>第一性原理</strong>出发，去探寻"为什么这么设计"。读完本文，你将理解Redis String 并不仅仅是一种数据类型，它更是整个 Redis设计哲学的完美缩影。</p><p>首先，我们下一个结论：<font color="red"><strong>Redis 的 String是一个可以存储字符串、整数、浮点数乃至二进制数据 (如图片或序列化的对象)的数据类型，其最大容量为 512 MB。它是 Redis所有数据结构中最基础的一种，像 Hash、List等结构的底层实现也大量用到了它</strong>。</font></p><h2 id="地基之下redis-为何要重新发明字符串">1. 地基之下：Redis为何要重新发明字符串？</h2><p>在 C 语言中，字符串是以空字符 <code>\0</code>结尾的字符数组。它简单，但也带来了诸多限制和风险。Redis的缔造者并没有选择直接使用它，而是从零开始构建了一个名为 <strong>SDS(Simple Dynamic String)</strong> 的结构。</p><p>SDS 的设计解决了 C 字符串的以下痛点：</p><ul><li><p><strong>获取长度的时间复杂度</strong></p><ul><li><strong>C 字符串</strong>: 必须遍历整个字符串直到遇到<code>\0</code>，时间复杂度为O(N)。当字符串很长时，这是一个昂贵的操作。</li><li><strong>Redis SDS</strong>: 结构中直接包含一个 <code>len</code>字段来记录当前长度，因此获取长度的时间复杂度是O(1)。这对于频繁获取长度的场景是巨大的性能提升。</li></ul></li><li><p><strong>杜绝缓冲区溢出 (Buffer Overflow)</strong></p><ul><li><strong>C 字符串</strong>: <code>strcat</code>等函数不会检查目标数组的剩余空间，极易造成缓冲区溢出，这是一个严重的安全漏洞。</li><li><strong>Redis SDS</strong>: 当对 SDS 进行修改时 (如<code>APPEND</code>)，API 会先检查其内部记录的剩余空间(<code>free</code> 字段)是否足够。如果不够，它会先扩展内存空间，然后再执行修改。这从根本上杜绝了溢出的可能性。</li></ul></li><li><p><strong>二进制安全 (Binary Safe)</strong></p><ul><li><strong>C 字符串</strong>: 由于以 <code>\0</code>作为结尾标识，它不能存储任何包含 <code>\0</code>的数据，比如图片、音频或 Protobuf 序列化后的数据。</li><li><strong>Redis SDS</strong>: 它通过 <code>len</code>字段来判断字符串的实际结尾，而非特殊字符。因此，你可以将任何字节流存入SDS，真正做到了二进制安全。</li></ul></li><li><p><strong>空间预分配与惰性释放</strong></p><p>为了避免每次追加操作都重新分配内存 (这是一个耗时的系统调用)，SDS采用了一种智能的内存分配策略：</p><ul><li><strong>空间预分配</strong>: 当对 SDS进行扩展时，它会分配比实际需要更多的空间。如果修改后 SDS 的长度<code>len</code> 小于 1MB，则会额外分配与 <code>len</code> 相同的空间(即 <code>free = len</code>)。如果 <code>len</code> 超过1MB，则会额外分配固定的 1MB空间。这大大减少了连续增长字符串时的内存重分配次数。</li><li><strong>惰性空间释放</strong>: 当缩短 SDS字符串时，程序并不会立即将多余的内存交还给操作系统，而是通过更新<code>free</code> 字段来记录这些空闲空间，以备未来的增长操作使用。</li></ul></li></ul><p>为了将内存优化到极致，SDS的设计者并未采用"一刀切"的头部结构，而是实现了一套"量体裁衣"的方案。它根据字符串的长度，动态选择不同大小的头部结构，以求用最少的元数据开销来管理字符串。下面是Redis 源码中 <ahref="https://github.com/redis/redis/blob/8.2.1/src/sds.h">sds.h</a>的核心定义：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Note: sdshdr5 is never used, we just access the flags byte directly.</span></span><br><span class="line"><span class="comment"> * However is here to document the layout of type 5 SDS strings. */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> __<span class="title">attribute__</span> ((__<span class="title">packed__</span>)) <span class="title">hisdshdr5</span> &#123;</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> flags; <span class="comment">/* 3 lsb of type, and 5 msb of string length */</span></span><br><span class="line">    <span class="type">char</span> buf[];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> __<span class="title">attribute__</span> ((__<span class="title">packed__</span>)) <span class="title">hisdshdr8</span> &#123;</span></span><br><span class="line">    <span class="type">uint8_t</span> len; <span class="comment">/* used */</span></span><br><span class="line">    <span class="type">uint8_t</span> alloc; <span class="comment">/* excluding the header and null terminator */</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> flags; <span class="comment">/* 3 lsb of type, 5 unused bits */</span></span><br><span class="line">    <span class="type">char</span> buf[];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> __<span class="title">attribute__</span> ((__<span class="title">packed__</span>)) <span class="title">hisdshdr16</span> &#123;</span></span><br><span class="line">    <span class="type">uint16_t</span> len; <span class="comment">/* used */</span></span><br><span class="line">    <span class="type">uint16_t</span> alloc; <span class="comment">/* excluding the header and null terminator */</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> flags; <span class="comment">/* 3 lsb of type, 5 unused bits */</span></span><br><span class="line">    <span class="type">char</span> buf[];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> __<span class="title">attribute__</span> ((__<span class="title">packed__</span>)) <span class="title">hisdshdr32</span> &#123;</span></span><br><span class="line">    <span class="type">uint32_t</span> len; <span class="comment">/* used */</span></span><br><span class="line">    <span class="type">uint32_t</span> alloc; <span class="comment">/* excluding the header and null terminator */</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> flags; <span class="comment">/* 3 lsb of type, 5 unused bits */</span></span><br><span class="line">    <span class="type">char</span> buf[];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> __<span class="title">attribute__</span> ((__<span class="title">packed__</span>)) <span class="title">hisdshdr64</span> &#123;</span></span><br><span class="line">    <span class="type">uint64_t</span> len; <span class="comment">/* used */</span></span><br><span class="line">    <span class="type">uint64_t</span> alloc; <span class="comment">/* excluding the header and null terminator */</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> flags; <span class="comment">/* 3 lsb of type, 5 unused bits */</span></span><br><span class="line">    <span class="type">char</span> buf[];</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>相信当看到 <code>sdshdr5</code> 到 <code>sdshdr64</code>这一系列结构的时候，不少读者要问一个问题：<strong>为什么需要这么多不同的头结构(header)？</strong></p><p>答案根植于一个核心的权衡：<strong>用最少的元数据 (metadata)开销来管理任意长度的字符串</strong>。如果只有一个能容纳 64位长度的巨大头部，那么当我们存储大量只有几个字节的短字符串时，头部本身（17字节）的开销将远大于数据本身，这会造成巨大的内存浪费。</p><p>因此，Redis的设计者采取了<strong>分类处理</strong>的策略：根据字符串的长度，为其选择一个大小恰到好处的头部结构。</p><p>在深入看差异之前，我们先看所有结构（除 <code>sdshdr5</code>外）都包含的四个关键成员：</p><ul><li><code>len</code>: 一个无符号整数，记录了 <code>buf</code>数组中当前已使用的字节数，即字符串的实际长度。这是实现 O(1)复杂度获取字符串长度的关键。</li><li><code>alloc</code>: 一个无符号整数，记录了为 <code>buf</code>数组分配的总字节数，<strong>不包括</strong>头部自身和末尾的空字符<code>\0</code>。<code>alloc - len</code> 就是预留的空闲空间，用于高效的<code>APPEND</code> 操作。</li><li><code>flags</code>: 一个 8 位的无符号字符。其中，低 3 位 (LSB)用来存储 SDS 的类型编码 (Type)。例如，<code>SDS_TYPE_8</code> 对应<code>sdshdr8</code>，<code>SDS_TYPE_16</code> 对应<code>sdshdr16</code> 等。SDS 的函数库通过读取这个 <code>flags</code>字段，就能知道当前处理的是哪种类型的 SDS header，从而正确地解析出<code>len</code> 和 <code>alloc</code>。</li><li><code>buf[]</code>: 这是一个 C99 的特性，称为<strong>柔性数组成员(Flexible ArrayMember)</strong>。它必须是结构的最后一个成员，并且在定义时大小为空。它的作用是，当我们为这个结构分配内存时，可以一次性分配头部和数据所需的<strong>连续内存空间</strong>。这对于提高CPU 缓存命中率至关重要。</li></ul><p>接下来我们来探索一下 <code>__attribute__ ((__packed__))</code>的底层奥秘，这个属性是 GCC/Clang编译器的扩展，它告诉编译器：<strong>请不要为了内存对齐 (MemoryAlignment) 而在结构成员之间添加任何填充字节 (Padding)</strong>。</p><p>现代 CPU 访问内存不是逐字节进行的，而是以字 (Word) 为单位（比如 4字节或 8字节）。如果一个数据结构的大小刚好是字长的整数倍，并且其成员的地址也都是字长的倍数，CPU的访问效率最高。为此，编译器默认会在结构体成员之间插入一些空白的填充字节，以保证对齐。</p><p>Redis 的 SDS 设计依赖一个巧妙的技巧：<strong>SDS API返回给用户的指针是 <code>buf</code>的起始地址，而不是结构体的起始地址</strong>。当需要获取长度时，API会通过这个 <code>buf</code> 的指针向前偏移固定的字节数来找到<code>len</code> 字段。例如，对于 <code>sdshdr8</code>，<code>len</code>字段就在 <code>buf</code> 指针的前 3个字节处。如果编译器进行了填充，这个固定的偏移量就会失效。<code>__packed__</code>确保了内存布局的紧凑和可预测性，让这种指针运算成为可能。</p><p>现在我们来看每个结构的具体用途：</p><ul><li><code>struct sdshdr5</code><ul><li><strong>超级优化</strong>:这是一个极端的优化，用于存储极短的字符串。它没有独立的 <code>len</code>和 <code>alloc</code> 字段。整个头部只有一个 <code>flags</code>字节。</li><li><strong>位域技巧</strong>: 这个字节被拆分使用：低 3 位存类型，高 5位存长度。因此，<code>sdshdr5</code> 最多能表示的长度是25−1=31。由于没有 <code>alloc</code>字段，这种类型的字符串是只读的，任何修改都会导致其被转换成其他 SDS类型。</li></ul></li><li><code>struct sdshdr8</code><ul><li><strong>头部大小</strong>: <code>len</code>(1 byte) +<code>alloc</code>(1 byte) + <code>flags</code>(1 byte) = <strong>3字节</strong>。</li><li><strong>容量</strong>: <code>len</code> 是<code>uint8_t</code>，最大可以表示的长度是 28−1=255 字节。</li><li><strong>场景</strong>: 适用于存储长度在 32 到 255字节之间的短字符串。</li></ul></li><li><code>struct sdshdr16</code><ul><li><strong>头部大小</strong>: <code>len</code>(2 bytes) +<code>alloc</code>(2 bytes) + <code>flags</code>(1 byte) = <strong>5字节</strong>。</li><li><strong>容量</strong>: <code>len</code> 是<code>uint16_t</code>，最大可以表示的长度是 216−1=65,535 字节 (64KB)。</li><li><strong>场景</strong>: 适用于中等长度的字符串。</li></ul></li><li><code>struct sdshdr32</code><ul><li><strong>头部大小</strong>: <code>len</code>(4 bytes) +<code>alloc</code>(4 bytes) + <code>flags</code>(1 byte) = <strong>9字节</strong>。</li><li><strong>容量</strong>: <code>len</code> 是<code>uint32_t</code>，最大可以表示的长度是 232−1≈4 GB。</li><li><strong>场景</strong>: 适用于非常长的字符串。</li></ul></li><li><code>struct sdshdr64</code><ul><li><strong>头部大小</strong>: <code>len</code>(8 bytes) +<code>alloc</code>(8 bytes) + <code>flags</code>(1 byte) = <strong>17字节</strong>。</li><li><strong>容量</strong>: <code>len</code> 是<code>uint64_t</code>，理论上可以表示巨大无比的字符串，但受限于 RedisString 最大 512 MB 的设计约束。</li><li><strong>场景</strong>: 用于需要超过 4GB 长度的场景（尽管在 Redis的实际使用中很少见）。</li></ul></li></ul><p>这段代码看似简单，却蕴含了 Redis 设计者对 C 语言、内存布局和 CPU工作的深刻理解。它告诉我们：</p><ol type="1"><li><strong>没有银弹</strong>:针对不同规模的问题，采用不同的解决方案。SDS通过类型的划分，实现了在不同长度字符串下的最优内存开销。</li><li><strong>深入硬件</strong>: 了解内存对齐、CPU缓存等底层机制，可以写出性能更高的代码。<code>__packed__</code>和柔性数组成员的使用就是明证。</li><li><strong>动态适应</strong>: Redis 的 SDS库是智能的。当你创建一个短字符串时，它会使用<code>sdshdr8</code>。如果你不断 <code>APPEND</code> 内容，一旦长度超过255，SDS 库会自动进行内存重分配，并将头部升级为<code>sdshdr16</code>，这个过程对用户完全透明。</li></ol><h2 id="动态之舞三种编码的智能平衡术">2.动态之舞：三种编码的智能平衡术</h2><p>如果说 SDS 是坚实的地基，那么智能编码体系就是其上灵动的舞者。Redis对外暴露了统一的 String接口，但对内，它会根据数据的实际特征，悄悄地为其选择最优的编码格式。</p><p>这种设计的核心，是为了解决<strong>通用性与专用性</strong>的矛盾。一个通用的字符串结构无法对纯数字这类特殊场景进行优化。为此，Redis准备了三套“服装”：<code>int</code>, <code>embstr</code>,<code>raw</code>。</p><p>让我们从第一性原理出发，探寻这背后的设计动机。</p><h3 id="核心矛盾通用性-vs.-专用性">核心矛盾：通用性 vs. 专用性</h3><p>首先，Redis 作为一个键值数据库，它的 Value必须具备<strong>通用性</strong>。这意味着它应该能存储任何东西，从数字<code>123</code> 到字符串<code>"hello world"</code>，再到一段复杂的二进制数据。从这个角度看，将所有东西都视为字节序列（字符串）是最简单、最通用的做法。</p><p>然而，如果真的将所有东西都存为普通字符串，就会遇到<strong>效率瓶颈</strong>：</p><ol type="1"><li><strong>内存浪费</strong>: 存储数字 <code>100</code>，如果用字符串<code>"100"</code> 形式，需要 3 个字节。如果用一个 64 位整型(<code>long</code>) 存储，虽然会占用 8 个字节，但 Redis有更巧妙的方法来优化它。更重要的是，频繁创建和销毁大量小字符串对象，其元数据开销和内存碎片不容忽视。</li><li><strong>计算低效</strong>: 如果你想对存储的数字 <code>"100"</code>执行 <code>INCR</code> (加 1) 操作。对于字符串，CPU 需要先将<code>"100"</code> 转换为整数 <code>100</code>，然后执行加法得到<code>101</code>，最后再将 <code>101</code> 转换为字符串<code>"101"</code>存回去。这个过程涉及多次类型转换，远不如直接在整数上执行一次加法指令来得快。</li></ol><p>Redis的智能编码体系，正是为了解决<strong>对外接口统一</strong>与<strong>对内实现高效</strong>这一核心矛盾而设计的。它让Redis在享受通用性带来的便利的同时，又能获得专用数据类型带来的性能和内存优势。</p><p>下面我们来逐一分析 <code>int</code>, <code>embstr</code>,<code>raw</code> 这三种编码，看看它们分别解决了什么问题。</p><h3id="obj_encoding_int为数字而生的极致优化">OBJ_ENCODING_INT：为数字而生的极致优化</h3><blockquote><p>解决纯数字的存储和计算效率问题。</p></blockquote><p>当你 SET 一个可以被 64 位有符号整数 (long) 表示的值时，Redis不会为其分配一个 sds 字符串结构。它会使用 <code>int</code> 编码。</p><p>这里的精髓在于一个极其巧妙的指针复用技巧。在 64位系统中，一个指针变量本身会占用 8 个字节。Redis 的核心数据结构<code>redisObject</code> 包含一个 <code>void *ptr</code>指针，通常指向真正的数据（比如一个 <code>sds</code> 结构）。</p><p>Redis 的设计者发现，一个 long 类型也是 8 个字节。因此，当存储一个long 型整数时，Redis不再分配额外的内存去存储数据，而是<u>直接将这个整数值存放在了<code>redisObject</code> 的 <code>ptr</code> 指针所占用的 8字节空间里</u>！</p><p>这样有 2 个好处：</p><ol type="1"><li><strong>零内存开销</strong>: 除了 <code>redisObject</code>结构本身的开销外，数据存储的额外开销为 0。</li><li><strong>极致计算性能</strong>: 执行<code>INCR</code>/<code>DECR</code> 等命令时，CPU可以直接在内存中进行原生整数运算，无需任何类型转换，速度快如闪电。</li></ol><h3id="obj_encoding_embstr为短字符串设计的快车道">OBJ_ENCODING_EMBSTR：为短字符串设计的"快车道"</h3><blockquote><p>解决大量短字符串带来的内存分配开销和内存碎片问题。</p></blockquote><p>当我们存储一个较长的字符串时，通常需要两次内存分配：一次为<code>redisObject</code> 结构分配，另一次为 <code>sds</code>结构（包含头部和数据本身）分配。这两块内存通常是不连续的。</p><p>对于短字符串（在较新版本中是长度 &lt;= 44 字节），Redis认为两次分配过于浪费。于是 <code>embstr</code>编码应运而生。<u>它只进行一次内存分配，申请一块连续的内存空间，同时容纳<code>redisObject</code> 的元信息和 <code>sds</code>的实际数据</u>。</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250825233436780.png"alt="Redis String embstr 和 raw 编码内存布局对比" /><figcaption aria-hidden="true">Redis String embstr 和 raw编码内存布局对比</figcaption></figure><p>这样有 2 个好处：</p><ol type="1"><li><strong>减少分配次数</strong>: 创建和销毁 <code>embstr</code>只需要一次 <code>malloc</code>/<code>free</code>，降低了管理开销。</li><li><strong>提升缓存效率 (Cache Locality)</strong>:这是最重要的优势。CPU从内存读取数据时，不是一个字节一个字节地读，而是按缓存行 (CacheLine)（通常是 64 字节）读取。由于 <code>redisObject</code>和字符串数据是连续的，当访问 <code>redisObject</code>时，字符串数据很可能已经被一同加载到了高速的 CPU缓存中。下次再访问字符串数据时，就能直接从缓存命中，避免了访问慢速主存的延迟。</li></ol><p>注意：<code>embstr</code>编码的字符串是<strong>只读</strong>的。一旦你尝试修改它（例如<code>APPEND</code>），Redis 会立即将其转换为 <code>raw</code>编码，因为无法在原有的连续内存块上进行原地扩容。</p><h3id="obj_encoding_raw通用且灵活的标准模式">OBJ_ENCODING_RAW：通用且灵活的"标准模式"</h3><blockquote><p>作为最通用的编码，处理所有长字符串和被修改过的短字符串。</p></blockquote><p>这是标准的 SDS 实现，<code>redisObject</code> 和 <code>sds</code>结构通过指针关联，分别位于不同的内存区域。</p><p>由于数据区 (<code>sds</code>) 和元信息区 (<code>redisObject</code>)是分离的，当字符串需要增长时（如 <code>APPEND</code>），可以独立地对<code>sds</code> 进行内存重分配（realloc），而无需触动<code>redisObject</code>。这使得对长字符串的修改变得高效。</p><h3 id="编码转换">编码转换</h3><pre class="mermaid">flowchart TD    A[值创建] --> B{值的类型和内容}    B -->|"64位整数范围"| C[int编码]    B -->|"字符串且长度 ≤ 44字节"| D[embstr编码]    B -->|"字符串且长度 > 44字节"| E[raw编码]    C --> F{操作类型}    D --> G{操作类型}    E --> H{操作类型}    F -->|"数值运算 INCR/DECR"| I[保持int编码]    F -->|"字符串操作 APPEND/SETRANGE"| J["int → raw转换"]    G -->|"任何修改操作"| K["embstr → raw转换"]    G -->|"只读操作 GET"| L[保持embstr编码]    H -->|"任何操作"| M[保持raw编码]    J --> N[分配raw内存]    N --> O[将int转换为字符串]    O --> P[存储到raw结构]    P --> Q[更新redisObject.ptr]    K --> R[分配raw内存]    R --> S[复制字符串数据]    S --> T[释放embstr内存]    T --> U[更新redisObject.ptr]    style C fill:#e3f2fd,stroke:#2196f3,stroke-width:2px    style D fill:#e8f5e8,stroke:#28a745,stroke-width:2px    style E fill:#ffebee,stroke:#d73a49,stroke-width:2px    style J fill:#fff3cd,stroke:#ffc107,stroke-width:2px    style K fill:#fff3cd,stroke:#ffc107,stroke-width:2px</pre><h2 id="揭秘-44-一个数字背后的硬核原理">3. 揭秘 44：一个数字背后的硬核原理</h2><p><code>embstr</code> 的 44字节限制，并非随意设定，而是精确计算的结果。</p><p><strong>核心目标</strong>：让整个 <code>embstr</code>对象正好放入内存分配器（如 jemalloc）的 <strong>64字节</strong>内存块中，以最大化内存效率和 CPU 缓存性能。</p><p><strong>推导过程</strong>： 一个 64 字节的内存块，需要容纳：</p><ol type="1"><li><code>redisObject</code> 结构体：<strong>16 字节</strong></li><li><code>sdshdr8</code> 头部（短字符串使用的最小 SDS 头）：<strong>3字节</strong></li><li>SDS 结尾的空字符 <code>\0</code>：<strong>1 字节</strong></li><li>字符串实际内容（Payload）：<strong>X 字节</strong></li></ol><p>于是，我们得到方程：</p><p><span class="math display">\[16+3+X+1=64\]</span></p><p>解得：</p><p><span class="math display">\[X=44\]</span></p><p>这里的 <code>X</code>，也就是44，指的是字符串内容的<strong>字节数</strong>。对于ASCII，它等于字符数；但对于 UTF-8等多字节编码，则必须计算其实际占用的字节。例如，15 个中文字符（占据<span class="math inline">\(15×3=45\)</span> 字节）的长度虽然远小于44，但其字节数超过了限制，因此必须使用 <code>raw</code> 编码。</p><h2 id="回归实践string-的真实世界">4. 回归实践：String 的真实世界</h2><p>理论的深刻，最终要回归实践的价值。正是基于上述精妙设计，Redis String才能在真实世界中扮演如此多样的角色：</p><ul><li><strong>缓存层</strong>：缓存数据库查询结果、API响应，是其最经典的用法。</li><li><strong>原子计数器</strong>：利用 <code>INCR</code>的原子性，轻松实现高并发的网站 PV、文章点赞等功能。</li><li><strong>分布式锁</strong>：<code>SET key value EX seconds NX</code>一行命令，是实现分布式锁的最核心逻辑。</li><li><strong>位图 (Bitmap)</strong>：通过 <code>SETBIT</code> 和<code>BITCOUNT</code>，以极小的空间成本实现用户签到、日活统计等功能。</li><li><strong>共享会话</strong>：在分布式应用中存储用户Session，简单高效。</li></ul>]]></content>
    
    
    <summary type="html">本篇基于 Redis 8.2.1 源码，从第一性原理看 Redis 字符串的设计哲学，带你深入理解 Redis 的 String 数据类型。</summary>
    
    
    
    <category term="Redis" scheme="https://hedon.top/categories/Redis/"/>
    
    
    <category term="Redis" scheme="https://hedon.top/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>模型训练核心技巧：学习率预热、余弦衰减与梯度裁剪</title>
    <link href="https://hedon.top/2025/08/21/llm/guide-to-lr-warmup-cosine-annealing-gradient-clipping/"/>
    <id>https://hedon.top/2025/08/21/llm/guide-to-lr-warmup-cosine-annealing-gradient-clipping/</id>
    <published>2025-08-21T07:30:20.000Z</published>
    <updated>2025-08-21T08:34:02.142Z</updated>
    
    <content type="html"><![CDATA[<p>本篇我们来深入探讨一下学习率预热（Learning RateWarmup）、余弦衰减（Cosine Annealing）和梯度裁剪（GradientClipping）这三种在深度学习训练中非常实用的优化技巧。</p><p>首先，这三个技巧的核心目标是一致的：<strong>让模型在复杂的高维损失函数空间中，更稳定、更高效地找到一个好的解（局部最优解或全局最优解）</strong>。</p><p>它们分别从不同角度解决了训练过程中可能遇到的问题：</p><ul><li><strong>学习率预热 (Warmup)</strong>：解决训练初期的不稳定性。</li><li><strong>余弦衰减 (CosineAnnealing)</strong>：解决训练中后期的精细调整和收敛问题。</li><li><strong>梯度裁剪 (GradientClipping)</strong>：解决训练过程中可能出现的梯度爆炸问题，充当“安全带”。</li></ul><p>接下来我们逐一解析。</p><h2 id="学习率预热-learning-rate-warmup">学习率预热 (Learning RateWarmup)</h2><h3 id="结论先行">结论先行</h3><p>在训练开始的几个周期（epoch）或迭代（step）内，将学习率（LearningRate）从一个非常小的值（例如0）线性或非线性地增加到预设的初始学习率。预热阶段结束后，再采用预设的学习率衰减策略（如余弦衰减）。</p><p><img src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250821155358300.png" style="zoom: 33%;" /></p><h3 id="本质是什么">本质是什么</h3><p>在训练之初，模型的权重是随机初始化的，可以说它对数据一无所知。如果此时直接用一个较大的学习率（LearningRate），就好比让一个新手司机上来就踩满油门，结果很可能是车辆失控（模型参数被带到很差的空间），导致训练初期的剧烈震荡，甚至无法收敛。</p><p>学习率预热就是为了解决这个问题。它在训练开始的几个周期（epoch）或迭代（step）内，将学习率从一个非常小的值（甚至是0）逐步提升到你预设的初始学习率。</p><p>它的本质是<strong>在模型尚未稳定时，通过控制更新步长来增加训练的稳定性</strong>。</p><p>这是一种 "先慢后快"的策略。它承认了模型在训练初期处于一个非常不稳定的状态，因此需要一个缓冲期。通过这个缓冲期，模型可以安全地度过最不稳定的阶段，为后续高效的训练打下坚实的基础。</p><h3 id="好处有哪些">好处有哪些</h3><ol type="1"><li><strong>防止模型在训练初期"震荡"或"发散"</strong>：在训练刚开始时，模型的权重是随机初始化的，它们距离最优解非常遥远。此时如果直接使用一个较大的学习率，梯度更新的步子会迈得很大。这就像在一张崎岖不平的地图上蒙眼寻宝，一开始就猛冲一步，很可能会直接冲进一个很差的区域（损失函数的“悬崖”），导致损失剧增，模型难以收敛。</li><li><strong>给模型时间适应数据</strong>：在训练初期，模型对数据还没有任何认知。一个较小的学习率可以让模型"温柔"地开始学习，逐渐适应数据的分布，稳定地学习到一些浅层的、鲁棒的特征。等模型对数据有了一定的"感觉"后，再增大学习率进行快速优化，效果会更好。</li></ol><h3 id="如何评估预热步数">如何评估预热步数</h3><p>设定预热步数的核心原则是：<strong>确保在学习率达到其最大值时，模型的训练已经进入了一个相对稳定的状态</strong>。</p><ul><li><strong>太短的预热</strong>：学习率很快就上升到最大值，此时模型可能还没来得及"适应"数据，依然处于非常不稳定的状态。这可能会导致训练初期的损失出现剧烈震荡甚至不收敛，预热的效果大打折扣。</li><li><strong>太长的预热</strong>：模型在很长一段时间内都使用非常小的学习率进行训练，收敛速度过慢，浪费了大量的计算资源和时间。</li></ul><p>我们的目标就是在这两者之间找到一个平衡点。</p><h4 id="前人经验">前人经验</h4><p>在实践中，预热步数通常有两种设定方式：</p><p><strong>1.按训练总步数的比例设定</strong>：这是最常用、也最推荐的一种方法。它将预热阶段的长度与整个训练过程的长度动态地关联起来。</p><ul><li><strong>经验法则</strong>：通常将 <strong>总训练步数的 6% -10%</strong> 作为预热步数。</li><li><strong>为什么有效</strong>：这个比例确保了无论你的总训练时间是长是短，预热都只占其中一小部分，既能起到稳定作用，又不会拖慢整体进度。例如，如果你计划总共训练<code>100,000</code> 步，那么设置 <code>6,000</code> 到<code>10,000</code>步的预热是一个非常合理的起点。</li><li><strong>适用场景</strong>：非常适合训练大型模型（如 BERT,GPT）或在大型数据集上从头开始训练。</li></ul><p><strong>2.按固定的周期数（Epochs）设定</strong>：对于某些数据集和训练流程，按Epoch 设定更为直观。</p><ul><li><strong>经验法则</strong>：通常设置为 <strong>1 到 2 个Epoch</strong>。</li><li><strong>为什么有效</strong>：一个 Epoch意味着模型已经完整地看过一遍所有训练数据。经过一轮完整的“阅览”，模型通常已经初步适应了数据分布，此时再提升到最大学习率是比较安全的。</li><li><strong>适用场景</strong>：当数据集不是特别巨大，或者在进行微调（Fine-tuning）任务时，这种方法简单有效。</li></ul><h4 id="实践是检验真理的唯一标准">实践是检验真理的唯一标准</h4><p>当然，上述 2 个方案都是经验值，最好的方法还是通过实验来验证 ——评估预热步数是否合适的最佳指标就是 <strong>训练初期的损失曲线 (LossCurve)</strong>。</p><ol type="1"><li><strong>选择一个基准值</strong>：根据上面的经验法则，选择一个起始值。例如，如果你在微调一个BERT 模型，可以先尝试 <code>1 epoch</code> 的预热。</li><li><strong>观察损失曲线</strong>：开始训练，并密切关注训练日志中前几个Epoch 的损失变化。<ul><li><strong>理想的曲线</strong>：在预热阶段，损失平稳下降。预热结束后，学习率达到最大值，损失开始加速下降，整个过程平滑过渡，没有出现剧烈的尖峰或抖动。</li><li><strong>预热可能过短的迹象</strong>：预热结束后，损失突然出现一个明显的<strong>尖峰(Spike)</strong>，或者开始剧烈震荡，然后才慢慢恢复下降。这说明学习率增长过快，模型没能平稳过渡。</li><li><strong>预热可能过长的迹象</strong>：损失曲线在开始的相当长一段时间内下降得极为缓慢，几乎是一条平线。这说明模型在用一个过小的学习率“浪费时间”。</li></ul></li><li><strong>调整并对比</strong>：<ul><li>如果发现损失有尖峰，<strong>增加</strong> 预热步数（例如从 1 epoch增加到 2 epochs）。</li><li>如果发现初始收敛太慢，可以尝试 <strong>减少</strong> 预热步数。</li></ul></li></ol><p>通过几次短时间的实验（不需要跑完整个训练，观察前几个 epoch即可），你就能很快地为你的特定任务找到一个合适的预热步数范围。</p><h4 id="推荐方案">推荐方案</h4><table><colgroup><col style="width: 40%" /><col style="width: 35%" /><col style="width: 24%" /></colgroup><thead><tr class="header"><th>场景</th><th>推荐的起始策略</th><th>评估方法</th></tr></thead><tbody><tr class="odd"><td><strong>大型模型从头训练</strong> (e.g., GPT, BERT on largecorpus)</td><td>将总训练步数的 <strong>10%</strong> 作为预热步数。</td><td>观察损失曲线是否平滑，没有尖峰。</td></tr><tr class="even"><td><strong>中小型模型的微调</strong> (e.g., Fine-tuning ResNet on acustom dataset)</td><td><strong>1 到 2 个 Epoch</strong> 对应的步数。</td><td>观察损失曲线，确保预热结束后能快速收敛。</td></tr><tr class="odd"><td><strong>不确定如何选择时</strong></td><td><strong>从 1 个 Epoch开始</strong>，这通常是一个安全且不会太慢的选择。</td><td>通过短时实验，观察损失曲线并进行微调。</td></tr></tbody></table><h2 id="余弦衰减-cosine-annealing">余弦衰减 (Cosine Annealing)</h2><h3 id="结论先行-1">结论先行</h3><p>一种学习率的衰减策略。它不像传统的步进式衰减（Step Decay，例如每 30个 epoch 学习率乘以0.1）那样是跳崖式下降，而是让学习率随着训练的进行，像余弦函数<code>cos(x)</code> 在 <code>[0, π/2]</code>区间一样，平滑地从初始值下降到接近 0。</p><p><img src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250821155626400.png" style="zoom:33%;" /></p><h3 id="本质是什么-1">本质是什么</h3><p>当模型训练进入中后期，我们通常需要降低学习率，帮助模型在最优点附近进行更精细的搜索。传统的步进式衰减虽然有效，但其"跳崖式"的下降方式有时过于粗暴。</p><p>余弦衰减提供了一种更优雅的方案。它让学习率随着训练的进行，像余弦函数一样平滑地从初始值下降到接近0。</p><p>它的本质是：<strong>一种 "先探索，后精调"的动态调整策略</strong>。</p><ul><li><strong>前期/中期</strong>：学习率下降缓慢，保持相对较高的值，让模型有能力跳出局部陷阱，探索更广阔的空间。</li><li><strong>后期</strong>：学习率下降加速，让模型能以更小的步长在最优解附近精细微调。</li></ul><blockquote><p>这就像飞机降落。飞行员不会在到达目的地后直接关闭引擎（步进衰减），而是会沿着平滑的下滑曲线（余弦曲线）逐渐降低速度和高度，最终实现平稳着陆。</p></blockquote><h3 id="好处有哪些-1">好处有哪些</h3><ol type="1"><li><strong>避免在接近最优点时来回震荡</strong>：在训练后期，模型已经非常接近最优解。此时如果学习率依然较大，可能会导致模型在最优解附近来回跳动，始终无法精确收敛。余弦衰减通过缓慢、平滑地降低学习率，使得模型能够以更小的步长，更精细地在最优点附近进行搜索，从而更容易找到那个谷底。</li><li><strong>在较长时间内维持相对较大的学习率</strong>：与步进式衰减相比，余弦衰减在前期和中期下降得更慢。这意味着模型有更长的时间在损失空间中进行探索，这有助于它跳出一些不好的局部最优解（saddlepoints or poor local minima），去寻找一个更好的解。</li></ol><h2 id="梯度裁剪-gradient-clipping">梯度裁剪 (Gradient Clipping)</h2><h3 id="结论先行-2">结论先行</h3><p>在进行梯度下降更新权重之前，设定一个梯度的阈值。如果当前计算出的梯度向量的L2范数（可以理解为梯度的"长度"或"大小"）超过了这个阈值，就按比例缩小这个梯度向量，使其范数恰好等于该阈值。</p><p>$$ ||g|| &gt; : \ g g</p><p>$$</p><p>其中 <span class="math inline">\(g\)</span> 是梯度向量，<spanclass="math inline">\(||g||\)</span> 是它的 L2 范数，也称为欧几里得范数(Euclidean Norm)，公式如下：</p><p><span class="math display">\[||\vec{x}||_2 = \sqrt{x_1^2 + x_2^2 + \dots + x_n^2}\]</span></p><p><img src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/gradient-clipping-example.jpg" style="zoom:33%;" /></p><h3 id="本质是什么-2">本质是什么</h3><p>在深度网络（尤其是 RNN,Transformer）中，梯度在反向传播过程中可能会因为连乘效应而变得异常巨大，这就是<strong>梯度爆炸 (ExplodingGradients)</strong>。一次梯度爆炸带来的权重更新可能是毁灭性的，它会瞬间摧毁模型学到的所有知识，导致损失变为<code>NaN</code>。</p><p>梯度裁剪 (Gradient Clipping)就是防止这种灾难的"安全带"。它为梯度的大小设定一个上限，如果某次计算出的梯度超过了这个上限，就将其按比例缩小，但<strong>保持其方向不变</strong>。</p><p>它的本质是：<strong>为训练过程增加一个安全约束，牺牲极端情况下的理论最优更新，换取整个训练过程的稳定性和鲁棒性。</strong></p><h3 id="有什么好处">有什么好处</h3><p>这个问题的核心在于 <strong>长距离依赖 (Long-termDependencies)</strong> 和 <strong>深度（层数）</strong>。</p><p>在像 RNN 或 Transformer这样的模型中，信息需要在很长的时间步或很深的层级之间传递。在反向传播计算梯度时，根据链式法则，梯度会涉及到一系列雅可比矩阵（JacobianMatrix）的连乘。</p><p><span class="math display">\[\frac{\partial L}{\partial h_t} = \frac{\partial L}{\partial h_{t+k}}\cdot \frac{\partial h_{t+k}}{\partial h_{t+k-1}} \cdots \frac{\partialh_{t+1}}{\partial h_t}\]</span></p><ul><li>如果这些矩阵的范数持续大于1，那么连乘的结果就会呈指数级增长，导致梯度爆炸。</li><li>如果持续小于 1，则会导致梯度消失。</li></ul><p>梯度裁剪正是为了处理前一种情况。RNN因为在时间维度上共享权重，这种连乘效应尤其显著。Transformer虽然没有时间上的循环，但其非常深的网络结构（例如，一个接一个的self-attention 和 FFNblock）同样会形成很长的计算路径，使得梯度在反向传播时也容易出现爆炸或消失的问题。</p><p>梯度裁剪通过设定一个上限，确保单次更新的步长不会过大，从而防止了这种灾难性事件的发生。</p><h3 id="裁剪方式">裁剪方式</h3><p>前面我们的描述中默认的裁剪方式是：<strong>范数裁剪 (Clipping byNorm)</strong>，这也是最常用、最推荐的方式。但其实还有另一种方式，叫做<strong>值裁剪（Clippingby Value）</strong>。理解它们的区别非常重要。</p><p><strong>范数裁剪 (Clipping by Norm)</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><p>计算所有参数梯度的 L2范数（可以理解为整个梯度向量的“长度”），如果这个范数超过了设定的阈值<code>max_norm</code>，就将整个梯度向量按比例缩小，使其范数恰好等于<code>max_norm</code>。</p><p>这种裁剪<strong>保持梯度的方向不变</strong>，只缩放其大小。这非常重要，因为梯度的方向指明了损失函数下降最快的方向，我们希望保留这个正确的信息，只是不想让步子迈得太大。</p><p><strong>值裁剪 (Clipping by Value)</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><p>为梯度的每一个元素设定一个区间的<code>[min_value, max_value]</code>。然后遍历梯度向量中的每一个元素，如果某个元素的值小于<code>min_value</code>，就把它设为 <code>min_value</code>；如果大于<code>max_value</code>，就把它设为 <code>max_value</code>。</p><p>这种方法会 <strong>改变梯度的方向</strong>。想象一个二维梯度向量<code>g = [10, 0.1]</code>，如果设置裁剪区间为<code>[-1, 1]</code>，裁剪后它会变成<code>g' = [1, 0.1]</code>。原来的方向几乎是沿着 x轴，但裁剪后的方向明显向 y轴偏移了。这种方向上的改变可能会误导模型的更新。</p><hr /><p>由于范数裁剪保留了梯度的正确方向，在绝大多数情况下，<strong>范数裁剪是比值裁剪更好的选择</strong>。我们通常所说的梯度裁剪也默认是指范数裁剪。</p><h3 id="如何选择裁剪阈值">如何选择裁剪阈值</h3><p>在上述范数裁剪（后续梯度裁剪均默认为范数裁剪）的示例代码中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><p><code>max_norm</code>是一个超参数，即裁剪阈值，它的设定没有一个放之四海而皆准的黄金数值，但有一个非常有效的经验法则来确定它：</p><ol type="1"><li><strong>初始阶段不裁剪</strong>：在你的模型和数据集上，先跑几个训练迭代（iterations），但暂时不使用梯度裁剪。</li><li><strong>监控梯度范数</strong>：在每个训练步（<code>loss.backward()</code>之后，<code>optimizer.step()</code>之前），计算并记录下模型参数的梯度总范数。</li><li><strong>分析范数分布</strong>：收集上百个迭代的梯度范数值，观察它们的分布。你会发现，大部分时候梯度范数会处在一个比较稳定的范围内，但偶尔会出现一些非常大的"尖峰"，这些就是梯度爆炸的时刻。</li><li><strong>设定阈值</strong>：选择一个比大多数"稳定"梯度范数略大，但又能明显限制住那些"尖峰"的值。通常可以选择梯度范数分布的某个高百分位点，比如90% 或 95% 分位点，作为一个不错的起始值。</li></ol><p><strong>例如</strong>：你观察到 95% 的梯度范数都在 0.5 到 5.0之间，但偶尔会飙升到 50 或 100。那么，将 <code>max_norm</code> 设置为5.0 或者 10.0就是一个合理的选择。这样既不会影响正常的训练，又能有效防止极端情况下的训练崩溃。常见的<code>max_norm</code> 值通常在 1.0 到 10.0 之间。</p><p>在 PyTorch 中，梯度裁剪的位置非常关键。它必须在<code>loss.backward()</code> 之后（此时梯度已经被计算出来）和<code>optimizer.step()</code> 之前（在用梯度更新权重之前）调用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个标准的训练循环</span></span><br><span class="line">optimizer.zero_grad()        <span class="comment"># 1. 清空旧梯度</span></span><br><span class="line"></span><br><span class="line">loss = model(inputs, labels) <span class="comment"># 2. 前向传播计算损失</span></span><br><span class="line">loss.backward()              <span class="comment"># 3. 反向传播计算梯度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 梯度裁剪发生在这里 ---</span></span><br><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">1.0</span>) <span class="comment"># 4. 裁剪梯度</span></span><br><span class="line"></span><br><span class="line">optimizer.step()             <span class="comment"># 5. 使用裁剪后的梯度更新权重</span></span><br></pre></td></tr></table></figure><h2 id="代码案例">代码案例</h2><p>接下来我们以一个完整的大语言模型（Large LanguageModel）训练过程，来将这 3 个优化思路串起来，本篇案例参考了 <ahref="https://github.com/rasbt/LLMs-from-scratch">LLMs-from-scratch</a>，感兴趣的读者可参阅此书。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">model, train_loader, val_loader, optimizer, device,</span></span><br><span class="line"><span class="params">                num_epochs, eval_freq, eval_iter, start_context, tokenizer,</span></span><br><span class="line"><span class="params">                warmup_steps, initial_lr=<span class="number">3e-05</span>, min_lr=<span class="number">1e-6</span></span>):</span><br><span class="line">    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []</span><br><span class="line">    tokens_seen, global_step = <span class="number">0</span>, -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    peak_lr = optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>]</span><br><span class="line">    total_training_steps = <span class="built_in">len</span>(train_loader) * num_epochs <span class="comment"># 计算训练过程中的所有迭代步数</span></span><br><span class="line">    lr_increment = (peak_lr - initial_lr) / warmup_steps  <span class="comment"># 计算在预热阶段学习率的增量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> input_batch, target_batch <span class="keyword">in</span> train_loader:</span><br><span class="line">            global_step +=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> global_step &lt; warmup_steps:</span><br><span class="line">                lr = initial_lr + global_step * lr_increment    <span class="comment"># &lt;---- 学习率预热阶段</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                progress = ((global_step - warmup_steps) /</span><br><span class="line">                                    (total_training_steps - warmup_steps))</span><br><span class="line">                lr = min_lr + (peak_lr - min_lr) * <span class="number">0.5</span> * (      <span class="comment"># &lt;---- 余弦衰减阶段</span></span><br><span class="line">                    <span class="number">1</span> + math.cos(math.pi * progress))</span><br><span class="line">            <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:  <span class="comment"># 在优化器上应用计算后的学习率</span></span><br><span class="line">                param_group[<span class="string">&quot;lr&quot;</span>] = lr</span><br><span class="line">            track_lrs.append(lr)</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()  <span class="comment"># 清空旧梯度</span></span><br><span class="line">            loss = calc_loss_batch(input_batch, target_batch, model, device) <span class="comment"># 前向传播计算交叉熵损失</span></span><br><span class="line">            loss.backward() <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">            <span class="keyword">if</span> global_step &gt;= warmup_steps: <span class="comment"># &lt;--- 在预热阶段后使用梯度裁剪来避免梯度爆炸</span></span><br><span class="line">                torch.nn.utils.clip_grad_norm_(</span><br><span class="line">                    model.parameters(), max_norm=<span class="number">1.0</span></span><br><span class="line">                )</span><br><span class="line">            optimizer.step() <span class="comment"># 使用裁剪后的梯度更新权重</span></span><br><span class="line"></span><br><span class="line">            tokens_seen += input_batch.numel()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 输出调试信息，用于观测训练进展</span></span><br><span class="line">            <span class="keyword">if</span> global_step % eval_freq == <span class="number">0</span>:</span><br><span class="line">                train_loss, val_loss = evaluate_model(</span><br><span class="line">                    model, train_loader, val_loader, device, eval_iter</span><br><span class="line">                )</span><br><span class="line">                train_losses.append(train_loss)</span><br><span class="line">                val_losses.append(val_loss)</span><br><span class="line">                track_tokens_seen.append(tokens_seen)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Ep <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> (Step <span class="subst">&#123;global_step:06d&#125;</span>):&quot;</span></span><br><span class="line">                    <span class="string">f&quot;Train loss <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span>, &quot;</span></span><br><span class="line">                    <span class="string">f&quot;Val loss <span class="subst">&#123;val_loss:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 运用当前模型进行文本生成，观察模型能力</span></span><br><span class="line">        generate_and_print_sample(model, tokenizer, device, start_context)</span><br><span class="line">    <span class="keyword">return</span> train_losses, val_losses, track_tokens_seen, track_lrs</span><br></pre></td></tr></table></figure><p>完整代码可参考：<ahref="https://github.com/hedon-ai-road/llm-from-scratch/blob/main/5-%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B.ipynb">llm-from-scratch</a></p><h2 id="总结">总结</h2><p>深度学习模型的训练过程如同一场充满挑战的远航，不稳定的开局、难以收敛的困境和突如其来的训练崩溃是常见的"风浪"。本文深入探讨了三种为这场远航保驾护航的核心技巧：</p><ul><li><strong>学习率预热 (Learning RateWarmup)</strong>：它确保了我们能有一个"温柔的启动"，通过在训练初期使用极小的学习率并逐步提升，有效避免了因模型尚未适应数据而导致的剧烈震荡。</li><li><strong>余弦衰减 (CosineAnnealing)</strong>：它为我们规划了"平滑的航程"，以一种先慢后快的方式优雅地降低学习率，兼顾了前中期的广泛探索和后期的精细收敛，帮助模型更精准地抵达最优解。</li><li><strong>梯度裁剪 (GradientClipping)</strong>：它是全程必备的"安全带"，通过为梯度设置上限，有效防止了因梯度爆炸引发的"核爆"事故，保证了训练过程的稳定和鲁棒。</li></ul><p>文章最后的代码示例生动地展示了，这三个技巧并非孤立存在，而是三位一体的协同策略。在一个典型的训练流程中，我们以<strong>预热</strong>开启，用<strong>余弦衰减</strong>贯穿全程，并由<strong>梯度裁剪</strong>时刻守护。</p><p>掌握并善用三个优化技巧，将不再是玄学调参，而是有章可循的工程科学，能让你的模型训练过程更加稳定、高效，最终得到更优的性能。</p>]]></content>
    
    
    <summary type="html">本篇深入探讨了深度学习训练中的三大核心优化技巧，学习率预热解决训练初期不稳定性，余弦衰减实现精细调整和平滑收敛，梯度裁剪防止梯度爆炸。从原理到实践，全面解析如何让模型在高维损失空间中更稳定、更高效地找到最优解。</summary>
    
    
    
    <category term="大模型" scheme="https://hedon.top/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="机器学习" scheme="https://hedon.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="深度学习" scheme="https://hedon.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="大模型" scheme="https://hedon.top/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="训练优化" scheme="https://hedon.top/tags/%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96/"/>
    
    <category term="学习率预热" scheme="https://hedon.top/tags/%E5%AD%A6%E4%B9%A0%E7%8E%87%E9%A2%84%E7%83%AD/"/>
    
    <category term="余弦衰退" scheme="https://hedon.top/tags/%E4%BD%99%E5%BC%A6%E8%A1%B0%E9%80%80/"/>
    
    <category term="梯度裁剪" scheme="https://hedon.top/tags/%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA/"/>
    
  </entry>
  
  <entry>
    <title>Redis 数据类型丨List丨从双向链表到 Listpack 的演进之路 (基于 Redis 8.2.1 源码)</title>
    <link href="https://hedon.top/2025/08/20/redis/redis-datatype-list/"/>
    <id>https://hedon.top/2025/08/20/redis/redis-datatype-list/</id>
    <published>2025-08-20T11:41:00.000Z</published>
    <updated>2025-08-20T11:50:37.452Z</updated>
    
    <content type="html"><![CDATA[<p>当你向 Redis 执行一条 <code>LPUSH mylist "hello"</code>命令时，你有没有想过，这个 "hello" 究竟被存放在了哪里？Redis为了让这次看似简单的操作尽可能快、尽可能省内存，在底层做了哪些令人惊叹的优化？</p><p>大多数开发者止步于 API的使用，但真正的技术专家，善于运用第一性原理，探究其设计背后的本质。今天，我们将从最基础的数据结构和计算机体系结构出发，层层剥茧，彻底解构Redis List 的进化史，并最终通过阅读 <ahref="https://github.com/redis/redis/blob/8.2.1/src/listpack.c#L505">Redis8.2.1 的源码</a>，来印证我们所有的推论。</p><h3 id="路线图">路线图</h3><p>我们的探索将遵循 Redis List 自身真实的进化路径：</p><ol type="1"><li><strong>创世纪：<code>linkedlist</code></strong> -教科书式的完美与现实的代价。</li><li><strong>激进探索：<code>ziplist</code></strong> -对内存的极致压榨与性能的隐患。</li><li><strong>伟大妥协：<code>quicklist</code></strong> -平衡空间与时间的工程奇迹。</li><li><strong>完美进化：<code>listpack</code></strong> -<code>quicklist</code> 的新内核，理论与现实的最终统一。</li></ol><hr /><h3 id="创世纪linkedlist-的优雅与代价">1.创世纪：<code>linkedlist</code> 的优雅与代价</h3><p>从计算机科学的角度看，List (列表)的最直观实现就是一个<strong>双向链表 (Doubly LinkedList)</strong>。早期的 Redis (2.0时代) 正是这样做的。</p><h4 id="第一性原理数据结构">第一性原理：数据结构</h4><p>一个双向链表由一系列独立的节点构成，每个节点除了保存数据外，还拥有两个指针，分别指向其前驱和后继节点。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">          +------+     +------+     +------+</span><br><span class="line">... &lt;---- | prev | &lt;-&gt; | prev | &lt;-&gt; | prev | ----&gt; ...</span><br><span class="line">          | data |     | data |     | data |</span><br><span class="line">... &lt;---- | next | &lt;-&gt; | next | &lt;-&gt; | next | ----&gt; ...</span><br><span class="line">          +------+     +------+     +------+</span><br></pre></td></tr></table></figure><p>优点：完美的 O(1) 头尾操作</p><ul><li>在链表的头部或尾部插入/删除一个节点，只需要修改相邻的 2-3个指针即可，这个过程消耗的时间是常数，与链表长度无关。这对于LPUSH/RPUSH/LPOP/RPOP 这样的操作来说，是理论上最完美的数据结构。</li></ul><p>缺点：现实世界的双重代价</p><ol type="1"><li><strong>高昂的内存开销</strong>：这是 <code>linkedlist</code>被淘汰的<strong>首要原因</strong>。在一个 64 位系统中，一个指针占用 8字节。这意味着每个节点，除了存储你的数据，仅 <code>prev</code> 和<code>next</code> 两个指针就要额外消耗 16字节！当你存储大量小数据时（比如整数），指针占用的空间会远超数据本身，这是对宝贵内存的巨大浪费。</li><li><strong>糟糕的 CPU缓存局部性</strong>：链表的节点在内存中是<strong>离散</strong>分布的。当CPU遍历链表时，它需要不断地从内存的不同区域加载节点数据，这种指针跳转的行为极易导致<strong>CPU Cache Miss (缓存未命中)</strong>。CPU无法有效利用其高速缓存来预读数据，导致遍历性能远不如连续内存的数组。</li></ol><hr /><h3 id="激进探索ziplist-对内存的极致压榨">2.激进探索：<code>ziplist</code> 对内存的极致压榨</h3><p>为了克服 <code>linkedlist</code> 的双重代价，Redis的设计者们创造了一种极其紧凑的数据结构：<strong>压缩列表(ziplist)</strong>。</p><h4 id="第一性原理连续内存布局">第一性原理：连续内存布局</h4><p><code>ziplist</code>的核心思想是，用一块<strong>连续的、完整的内存块</strong>来存储所有元素，从而彻底消除指针开销，并最大化利用CPU 缓存。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;zlbytes&gt; &lt;zltail&gt; &lt;zllen&gt; &lt;entry_1&gt; &lt;entry_2&gt; ... &lt;entry_N&gt; &lt;zlend&gt;</span><br></pre></td></tr></table></figure><ul><li><code>&lt;zlbytes&gt;</code>: 整个 <code>ziplist</code>占用的总字节数。</li><li><code>&lt;zltail&gt;</code>: 到最后一个 entry的偏移量，用于快速定位到表尾。</li><li><code>&lt;zllen&gt;</code>: entry 的数量。</li><li><code>&lt;entry&gt;</code>: 真正的列表元素，每个 entry也是变长的。</li><li><code>&lt;zlend&gt;</code>: 特殊的结束标记 <code>0xFF</code>。</li></ul><p><code>ziplist</code> 的精髓在于 <code>entry</code> 的设计。每个 entry的头部会记录<strong>前一个 entry</strong> 的长度(<code>prev-len</code>)，这使得 <code>ziplist</code>可以从后向前遍历。</p><p>优点：极致的内存效率</p><ul><li><code>ziplist</code> 是 Redis为了节省内存而设计的典范。它没有指针，并对小整数和短字符串使用变长编码，是内存使用最经济的序列型数据结构。同时，连续内存对CPU 缓存极为友好。</li></ul><p>缺点：连锁更新 (Cascading Updates)</p><ul><li>这是 <code>ziplist</code> 的<ahref="https://zh.wikipedia.org/w/index.php?title=%E9%98%BF%E5%96%80%E7%90%89%E6%96%AF">阿喀琉斯之踵</a>。由于每个<code>entry</code> 记录了前一个 <code>entry</code> 的长度，当在前一个<code>entry</code> 发生大小变化时，可能会导致当前 <code>entry</code>需要用更多的字节来存储 <code>prev-len</code>，这又可能导致当前<code>entry</code> 自身总长度变化，从而级联影响到下一个<code>entry</code>... 在最坏的情况下，一次插入可能导致后续所有<code>entry</code> 都需要重新分配空间，时间复杂度从 O(N) 退化到O(N2)。</li></ul><hr /><h3 id="伟大妥协quicklist-的平衡之道">3.伟大妥协：<code>quicklist</code> 的平衡之道</h3><p>既然 <code>linkedlist</code> 和 <code>ziplist</code>各有优劣，能否将它们结合起来，取其精华，去其糟粕？<strong>快速列表(quicklist)</strong> 应运而生，并从 Redis 3.2 开始成为 List的默认实现。</p><h4 id="第一性原理混合数据结构">第一性原理：混合数据结构</h4><p><code>quicklist</code> 的本质，就是一个由<code>ziplist</code>（或后来的<code>listpack</code>）节点组成的<strong>双向链表</strong>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+----------------+     +----------------+     +----------------+</span><br><span class="line">| quicklistNode  | &lt;-&gt; | quicklistNode  | &lt;-&gt; | quicklistNode  |</span><br><span class="line">| (ziplist/pack) |     | (ziplist/pack) |     | (ziplist/pack) |</span><br><span class="line">+----------------+     +----------------+     +----------------+</span><br><span class="line">         ^                    ^                      ^</span><br><span class="line">         |                    |                      |</span><br><span class="line">   [ e1, e2, e3 ]       [ e4, e5 ]           [ e6, e7, e8, e9 ]</span><br></pre></td></tr></table></figure><p>它在宏观上是一个 <code>linkedlist</code>，保持了 O(1)的头尾插入性能和灵活性。而在微观上，每个节点内部是一个<code>ziplist</code> 或<code>listpack</code>，存储了多个元素，极大地节省了内存，并提升了缓存局部性。<code>quicklist</code>通过将连锁更新的风险<strong>限制</strong>在一个个独立的小节点内部，完美地规避了<code>ziplist</code> 最大的风险。</p><hr /><h3 id="完美进化listpack-的最终形态">4. 完美进化：<code>listpack</code>的最终形态</h3><p><code>quicklist</code> 已经非常优秀，但它的内核 <code>ziplist</code>依然存在理论上的连锁更新风险。为了追求极致的理论完备性，Redis开发者设计了 <code>ziplist</code>的继任者：<strong>listpack</strong>。从 Redis 7.0开始，<code>quicklist</code> 的内部节点默认已由 <code>ziplist</code>替换为 <code>listpack</code>。</p><p><code>listpack</code> 的目标与 <code>ziplist</code>一样：用一块连续内存来存储数据。但它通过一个绝妙的设计，彻底根除了连锁更新。</p><h4id="第一性原理信息自包含与回溯机制">第一性原理：信息自包含与回溯机制</h4><p><code>ziplist</code>连锁更新的根源在于：<strong>后一个节点存储了前一个节点的信息(<code>prev-len</code>)</strong>。<code>listpack</code>的设计哲学是：<strong>每个节点只存储与自身相关的信息</strong>。</p><p>一个 <code>listpack</code> entry 的结构如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">+----------------+----------------+----------------+</span><br><span class="line">| encoding-type  | element-data   |    back-len    |</span><br><span class="line">+----------------+----------------+----------------+</span><br></pre></td></tr></table></figure><p>要理解 <code>listpack</code>的精髓，我们必须深度剖析其灵魂设计——<code>back-len</code> 字段。</p><ul><li><strong>命名</strong>：源码中称之为<code>backlen</code>，它最核心的功能是用于<strong>向后(Backward)</strong> 遍历。</li><li><strong>存储内容</strong>：<code>back-len</code>字段里物理存储的数值，等于该条目自身的<code>&lt;encoding-type&gt;</code> 和 <code>&lt;element-data&gt;</code><strong>两部分加起来的长度</strong>（我们称之为“部分长度”）。</li><li><strong>作用</strong>：当需要从后向前遍历时，解析器会从前一个条目的<strong>尾部</strong>，反向解析出这个“部分长度”，然后再动态计算出<code>&lt;back-len&gt;</code>字段自身的长度，两者相加得到前一个条目的<strong>总长度</strong>，从而实现精确的回溯跳转。</li></ul><h4id="源码佐证lpprev-函数及其秘术-redis-8.2.1">源码佐证：<code>lpPrev</code>函数及其"秘术" (Redis 8.2.1)</h4><p>让我们直接阅读 Redis <code>8.2.1</code> 版本的 <ahref="https://github.com/redis/redis/blob/8.2.1/src/listpack.c#L505">listpack.c</a>源码，看看 <code>lpPrev</code> 函数是如何实现回溯的。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* from: https://github.com/redis/redis/blob/8.2.1/src/listpack.c */</span></span><br><span class="line"><span class="type">unsigned</span> <span class="type">char</span> *<span class="title function_">lpPrev</span><span class="params">(<span class="type">unsigned</span> <span class="type">char</span> *lp, <span class="type">unsigned</span> <span class="type">char</span> *p)</span> &#123;</span><br><span class="line">    assert(p);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 边界检查：如果已经是第一个元素，无法再回溯 */</span></span><br><span class="line">    <span class="keyword">if</span> (p-lp == LP_HDR_SIZE) <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 关键一步(1)：从当前条目p的开头，后退一字节，来到前一个条目的末尾 */</span></span><br><span class="line">    p--; </span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 关键一步(2)：从前一个条目的末尾，反向解析出其“部分长度” */</span></span><br><span class="line">    <span class="type">uint64_t</span> prevlen = lpDecodeBacklen(p);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 关键一步(3)：计算&lt;back-len&gt;字段自身的长度，并加到“部分长度”上，得到“总长度” */</span></span><br><span class="line">    prevlen += lpEncodeBacklenBytes(prevlen);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 关键一步(4)：执行跳转。p指针当前在前一个条目的末尾，</span></span><br><span class="line"><span class="comment">     * 回退 (总长度 - 1) 的距离，就来到了前一个条目的开头 */</span></span><br><span class="line">    p -= prevlen<span class="number">-1</span>; </span><br><span class="line">    </span><br><span class="line">    lpAssertValidEntry(lp, lpBytes(lp), p);</span><br><span class="line">    <span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>要完全看懂这段代码，我们必须潜入它调用的两个核心函数：<code>lpDecodeBacklen</code>和 <code>lpEncodeBacklenBytes</code>。</p><p><strong><code>lpDecodeBacklen</code> - 优雅的"盲人摸象"</strong></p><p><code>lpDecodeBacklen</code> 的任务是，在不知道<code>&lt;back-len&gt;</code>字段有多长的情况下，从它的最后一个字节开始，反向、完整地把它读出来。这是如何做到的？答案是<strong>可变长度整数编码</strong>。</p><p><code>&lt;back-len&gt;</code> 的每个字节中，最高位 (MSB)是一个<strong>"延续位"</strong>：</p><ul><li><code>MSB = 1</code>：表示"我不是开头，前面还有字节"。</li><li><code>MSB = 0</code>：表示"我就是开头，到我为止"。</li></ul><p><code>lpDecodeBacklen</code> 的算法就像“盲人摸象”，但极其高效：</p><ol type="1"><li>从 <code>p</code> 指针（前一个条目的末尾）开始，读取 1 个字节。</li><li>检查它的最高位。如果是 <code>0</code>，说明<code>&lt;back-len&gt;</code> 只有 1 字节长，其余 7位就是长度值，任务完成。</li><li>如果是 <code>1</code>，说明这是多字节长度的一部分，记下其余 7位，然后<code>p--</code>，继续向前读下一个字节，重复此过程，直到找到那个最高位为<code>0</code> 的“领头”字节。</li><li>最后，将所有收集到的 7位数据块拼接起来，还原出完整的“部分长度”。</li></ol><p>以下是 <code>lpDecodeBacklen</code> 的核心源码片段：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* from: https://github.com/redis/redis/blob/8.2.1/src/listpack.c */</span></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">uint64_t</span> <span class="title function_">lpDecodeBacklen</span><span class="params">(<span class="type">unsigned</span> <span class="type">char</span> *p)</span> &#123;</span><br><span class="line">    <span class="type">uint64_t</span> val = <span class="number">0</span>;</span><br><span class="line">    <span class="type">uint64_t</span> shift = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        <span class="comment">/* 从 p 指针开始，向低地址（左）移动 */</span></span><br><span class="line">        val |= (<span class="type">uint64_t</span>)(p[<span class="number">0</span>] &amp; <span class="number">127</span>) &lt;&lt; shift;</span><br><span class="line">        <span class="comment">/* 如果最高位是 0，表示这是最后一个字节，循环终止 */</span></span><br><span class="line">        <span class="keyword">if</span> ((p[<span class="number">0</span>] &amp; <span class="number">128</span>) == <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">        shift += <span class="number">7</span>;</span><br><span class="line">        p--;</span><br><span class="line">        <span class="comment">/* 安全检查，防止无限循环 */</span></span><br><span class="line">        <span class="keyword">if</span> (shift &gt; <span class="number">63</span>) <span class="keyword">return</span> UINT64_MAX;</span><br><span class="line">    &#125; <span class="keyword">while</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> val;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong><code>lpEncodeBacklenBytes</code> -未卜先知的计算</strong></p><p><code>lpPrev</code> 在得到"部分长度" <code>prevlen</code>后，还需要知道 <code>&lt;back-len&gt;</code>字段本身占了几个字节，才能算出总长度。<code>lpEncodeBacklenBytes</code>的作用就是回答这个问题。</p><p>它的逻辑很简单，就是一系列的范围判断，这正是可变长度整数编码的逆过程。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* from: https://github.com/redis/redis/blob/8.2.1/src/listpack.c */</span></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> <span class="type">uint64_t</span> <span class="title function_">lpEncodeBacklenBytes</span><span class="params">(<span class="type">uint64_t</span> len)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (len &lt; <span class="number">128</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (len &lt; <span class="number">16384</span>) <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (len &lt; <span class="number">2097152</span>) <span class="keyword">return</span> <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (len &lt; <span class="number">268435456</span>) <span class="keyword">return</span> <span class="number">4</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">5</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>例如，如果 <code>lpDecodeBacklen</code> 返回的 <code>prevlen</code>是 <code>100</code>，<code>lpEncodeBacklenBytes(100)</code> 就会返回<code>1</code>。<code>lpPrev</code> 随即将两者相加得到总长度<code>101</code>，完成最终的回溯跳转。</p><h3 id="结论永不休止的优化之路">结论：永不休止的优化之路</h3><p>Redis List 的演进史，是软件工程领域追求极致性能和效率的缩影：</p><ol type="1"><li><strong><code>linkedlist</code></strong>：一个优雅的理论起点，但在现实的内存和CPU 面前显得脆弱。</li><li><strong><code>ziplist</code></strong>：一次激进的、向内存效率极限发起的冲锋，但留下了性能抖动的隐患。</li><li><strong><code>quicklist</code></strong>：一次伟大的工程妥协，在宏观与微观层面取得了精妙的平衡，成为稳定服务多年的基石。</li><li><strong><code>listpack</code></strong>：一次对理论完美的最终追求，通过改变节点内部的信息记录方式，彻底根除了历史遗留问题，让List 的实现达到了新的高度。</li></ol><p>作为 Redis 的使用者，我们享受着 <code>LPUSH</code>/<code>RPOP</code>的简洁与高效。但作为技术的探索者，我们更应欣赏这背后长达十余年的、对每一个字节、每一次CPU 缓存命中、每一种风险场景的极致思考与打磨。</p>]]></content>
    
    
    <summary type="html">本篇基于 Redis 8.2.1 源码，从双向链表到 Listpack 的演进之路，带你深入理解 Redis 的 List 数据类型。</summary>
    
    
    
    <category term="Redis" scheme="https://hedon.top/categories/Redis/"/>
    
    
    <category term="Redis" scheme="https://hedon.top/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>告别死记硬背：一份真正理解 PyTorch 核心设计的指南</title>
    <link href="https://hedon.top/2025/08/18/llm/pytorch/"/>
    <id>https://hedon.top/2025/08/18/llm/pytorch/</id>
    <published>2025-08-18T07:31:00.000Z</published>
    <updated>2025-08-19T07:44:51.302Z</updated>
    
    <content type="html"><![CDATA[<p>如果你正在学习 PyTorch，你很可能和我最初一样，有这样的困惑：PyTorch的 API 太多了，像一片望不到边的海洋。今天记住<code>view</code>，明天忘了 <code>permute</code>；刚学会<code>Dataset</code>，又对 <code>DataLoader</code> 的<code>num_workers</code>感到神秘。靠死记硬背来学习，不仅效率低下，而且无法真正建立起解决复杂问题的能力。</p><p>这篇博文的目的，就是为了打破这种困境。我们将不再孤立地看待API，而是从深度学习项目的<strong>第一性原理</strong>出发，去理解：</p><ul><li><strong>为什么会有这些 API？</strong>它们各自解决了什么核心问题？</li><li><strong>它们之间是什么关系？</strong>如何协同工作，共同完成一个任务？</li></ul><p>我们将从两个层面来构建你的 PyTorch 知识体系：</p><ol type="1"><li><strong>宏观篇：思维骨架</strong> -搭建一个完整的深度学习项目工作流，理解 PyTorch 的顶层设计。</li><li><strong>微观篇：数据血液</strong> - 深入模型内部，掌控作为“血液”的Tensor（张量）如何在其间流动和变换。</li></ol><h2 id="宏观篇搭建你的-pytorch-思维骨架">宏观篇：搭建你的 PyTorch思维骨架</h2><h3 id="pytorch-的核心设计哲学灵活与直观">1. PyTorch的核心设计哲学：灵活与直观</h3><p>要理解 PyTorch，首先要理解它的两个核心特点：</p><ol type="1"><li>动态计算图（Dynamic Computational Graph）</li><li>Python 优先（Python-First）</li></ol><p><strong>动态计算图</strong>：这是 PyTorch 与早期 TensorFlow(TensorFlow 1.x)最大的区别。传统的静态图是"先定义，后执行"，你必须先构建一个完整的计算图，然后才能送入数据。而PyTorch的动态图是"即时执行"(Define-by-Run)，计算图的构建和计算是同时发生的。</p><ul><li><strong>解决了什么问题？</strong>极大地增强了灵活性。对于处理动态输入（如长度可变的文本）的 NLP任务，或者需要复杂控制流（如循环、条件判断）的模型，动态图非常直观和方便。调试也变得异常简单，你可以像调试普通Python 代码一样，随时停下来查看中间变量的值。</li><li><strong>对应的 API 体现：</strong> 你写的每一行 PyTorch计算代码（例如<code>c = a + b</code>），都在动态地构建一个微小的计算图。你不需要任何特殊的session 或 placeholder。</li></ul><p><strong>Python 优先</strong>：PyTorch 深度整合在 Python生态中，其设计充满了 Pythonic的风格。它感觉不像是一个独立的程序，更像是一个 Python 的超强数学和 GPU计算库。</p><ul><li><strong>解决了什么问题？</strong>降低了学习门槛，提高了开发效率。研究人员和开发者可以用最熟悉的方式快速迭代想法。</li><li><strong>对应的 API 体现：</strong> 你会发现 PyTorch 的类（如<code>nn.Module</code>）、数据结构（如 <code>Tensor</code>的操作）和整体编程范式都与 NumPy 等常见 Python 库非常相似。</li></ul><h3 id="典型的深度学习流程与-pytorch-api-的映射">2. 典型的深度学习流程与PyTorch API 的映射</h3><p>我们可以将一个完整的深度学习项目分为几个核心阶段。PyTorch 的 API设计就是为了服务于这个流程中的每一步。</p><ol type="1"><li>数据准备（The Fuel）</li><li>模型构建（The Engine）</li><li>训练循环（The Driving Process）</li></ol><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250818155654972.png"alt="典型的深度学习流程与 PyTorch API 的映射" /><figcaption aria-hidden="true">典型的深度学习流程与 PyTorch API的映射</figcaption></figure><h4 id="阶段-1数据准备-the-fuel">阶段 1：数据准备 (The Fuel)</h4><p><strong>面临的问题：</strong></p><ol type="1"><li>原始数据格式各异，如何统一读取？</li><li>数据集可能非常大，无法一次性载入内存，怎么办？</li><li>训练时需要对数据进行批量 (batching)、打乱 (shuffling) 和预处理(preprocessing)，如何高效实现？</li><li>如何利用多核 CPU 来加速数据加载，避免 GPU 等待？</li></ol><p><strong>PyTorch 的解决方案 (核心 API):</strong><code>torch.utils.data.Dataset</code> 和<code>torch.utils.data.DataLoader</code></p><p><strong>API 关系与解析：</strong></p><ul><li><code>Dataset</code>：<strong>它定义了"数据集"是什么</strong>。这是一个抽象类，你只需要继承它并实现两个方法：<code>__len__</code>(返回数据集大小)和 <code>__getitem__</code> (根据索引 <code>idx</code>返回一条数据)。它解决了“如何获取单条数据”的问题，将数据访问的逻辑封装起来。</li><li><code>DataLoader</code>：<strong>它定义了"如何使用数据集"</strong>。它接收一个<code>Dataset</code> 对象，并在此基础上，优雅地解决了所有工程问题：<ul><li><code>batch_size</code>：自动将单条数据打包成一个 batch。</li><li><code>shuffle=True</code>：在每个 epoch开始时自动打乱数据顺序。</li><li><code>num_workers</code>：启动多个子进程并行加载数据，极大地提高了数据供给效率。</li><li><code>collate_fn</code>：自定义如何将多条样本合并成一个batch，对于处理非标准数据（如不同长度的句子）非常有用。</li></ul></li></ul><p><strong>一句话总结：<u><code>Dataset</code>负责“取”，<code>DataLoader</code>负责“送”。它们共同解决了数据供给的效率和标准化问题</u>。</strong></p><p><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250819154449744.png" /></p><h4 id="阶段-2模型构建-the-engine">阶段 2：模型构建 (The Engine)</h4><p><strong>面临的问题：</strong></p><ol type="1"><li>如何定义一个神经网络结构？</li><li>网络中包含大量需要学习的参数（权重 <code>weights</code> 和偏置<code>biases</code>），如何有效地管理它们？</li><li>如何实现前向传播 (forward pass) 的计算逻辑？</li><li>如何方便地在 CPU 和 GPU 之间切换模型？</li></ol><p><strong>PyTorch 的解决方案 (核心 API):</strong><code>torch.nn.Module</code></p><p><strong>API 关系与解析：</strong></p><ul><li><p><code>torch.Tensor</code>：<strong>这是 PyTorch的基石</strong>。它不仅仅是一个像 NumPy <code>ndarray</code>一样的多维数组，它还承载了另外两个至关重要的信息：</p><ul><li><code>grad_fn</code>：指向创建这个张量的函数，用于构建反向传播的计算图。</li><li><code>grad</code>：存储该张量的梯度。 你可以通过<code>tensor.to('cuda')</code> 轻松地将其移动到 GPU。</li></ul></li><li><p><code>torch.nn.Module</code>：<strong>所有神经网络层的基类</strong>。你可以把它想象成一个容器或一个零件。</p><ul><li>在 <code>__init__</code> 方法中，我们定义模型的"零件"，例如<code>self.conv1 = nn.Conv2d(...)</code>，<code>self.fc1 = nn.Linear(...)</code>。当你定义这些层时，PyTorch会自动将它们的参数注册到这个 <code>Module</code> 中。</li><li>在 <code>forward</code>方法中，我们定义这些"零件"如何连接起来，完成从输入到输出的计算。</li></ul></li><li><p><strong>为什么需要 <code>nn.Module</code>而不是直接用函数？</strong></p><p>因为 <code>nn.Module</code> 帮你自动处理了参数管理。你只需要调用<code>model.parameters()</code>就可以获取模型中所有需要训练的参数，而不需要手动去追踪每一个权重和偏置。它还提供了<code>model.train()</code> 和 <code>model.eval()</code>模式切换等便利功能，用于控制 <code>Dropout</code> 和<code>BatchNorm</code> 等层的行为。</p></li></ul><p><strong>一句话总结：<u>我们用 <code>Tensor</code> 作为数据流，用<code>nn.Module</code> 将神经网络的“骨架”和“参数”组织起来，并在<code>forward</code>方法中定义数据如何在这个骨架中流动。</u></strong></p><h4 id="阶段-3训练循环-the-driving-process">阶段 3：训练循环 (TheDriving Process)</h4><p>这是整个流程的核心，涉及到损失计算、反向传播和参数更新。</p><p><strong>面临的问题：</strong></p><ol type="1"><li>模型输出和真实标签之间的差距（损失）如何计算？</li><li>如何根据损失计算出模型中每个参数的梯度 (gradient)？</li><li>如何根据梯度来更新参数，以使损失变小？</li></ol><p><strong>PyTorch 的解决方案 (核心 API):</strong><code>torch.autograd</code>, <code>loss functions</code>,<code>torch.optim</code></p><p><strong>API 关系与解析：</strong></p><ol type="1"><li><strong>损失函数 (Loss Function)</strong> - 例如<code>nn.CrossEntropyLoss</code>, <code>nn.MSELoss</code><ul><li><strong>作用：</strong> 衡量模型预测值 <code>output</code> 和真实值<code>target</code> 之间的差距，计算出一个标量值 <code>loss</code>。这个<code>loss</code> 就是我们优化的目标，我们希望它越小越好。</li></ul></li><li><strong>自动求导系统 (Autograd)</strong> -<code>loss.backward()</code><ul><li><strong>作用：</strong> 这是 PyTorch 的魔法核心。当你对一个<code>requires_grad=True</code> 的 <code>Tensor</code>（我们的<code>loss</code> 就是）调用 <code>.backward()</code> 方法时，PyTorch会自动沿着计算图反向传播，计算出图中所有 <code>requires_grad=True</code>的叶子节点（也就是我们模型的参数 <code>model.parameters()</code>）相对于<code>loss</code>的梯度，并把结果累加到这些参数的 <code>.grad</code>属性上。</li><li><strong>它解决了什么？</strong>解决了深度学习中最复杂、最容易出错的数学问题——梯度计算。你不需要手动去推导和实现链式法则。</li></ul></li><li><strong>优化器 (Optimizer)</strong> - <code>torch.optim</code> (例如<code>optim.SGD</code>, <code>optim.Adam</code>)<ul><li><strong>作用：</strong> 它根据计算出的梯度来更新模型的参数。</li><li><strong>工作流程（三步曲）：</strong> a.<code>optimizer.zero_grad()</code>：清空上一轮迭代中累积的梯度。因为PyTorch 的梯度是累加的 (<code>+=</code>)，所以每轮更新前必须手动清零。b. <code>loss.backward()</code>：计算当前 batch 的梯度。 c.<code>optimizer.step()</code>：根据梯度更新参数。优化器会根据自身的算法（如SGD, Adam）来执行 <code>w = w - learning_rate * w.grad</code>这样的更新操作。</li></ul></li></ol><p><strong>一句话总结：<u><code>损失函数</code>告诉我们"错的有多离谱"，<code>loss.backward()</code>告诉我们"每个参数应该朝哪个方向改"，<code>optimizer.step()</code>负责"实际去改这些参数"。这三者构成了训练的核心闭环</u>。</strong></p><h3 id="代码示例">3. 代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 数据准备 (Data Preparation)</span></span><br><span class="line"><span class="comment"># 假设我们有 100 个样本，每个样本 10 个特征，标签是 0 或 1</span></span><br><span class="line">X_train = torch.randn(<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line">y_train = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">100</span>,)).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 Dataset 和 DataLoader 封装数据</span></span><br><span class="line"><span class="comment"># TensorDataset 是一个方便的包装器</span></span><br><span class="line">dataset = TensorDataset(X_train, y_train)</span><br><span class="line"><span class="comment"># DataLoader 负责批量、打乱等</span></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">16</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 模型构建 (Model Building)</span></span><br><span class="line"><span class="comment"># 继承 nn.Module 来定义我们自己的模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 在 __init__ 中定义模型的层（零件）</span></span><br><span class="line">        <span class="variable language_">self</span>.layer1 = nn.Linear(<span class="number">10</span>, <span class="number">5</span>) <span class="comment"># 输入 10 特征，输出 5 特征</span></span><br><span class="line">        <span class="variable language_">self</span>.activation = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.layer2 = nn.Linear(<span class="number">5</span>, <span class="number">1</span>)  <span class="comment"># 输入 5 特征，输出 1 特征</span></span><br><span class="line">        <span class="variable language_">self</span>.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 在 forward 中定义数据如何流动</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layer1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.activation(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.layer2(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = SimpleModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 定义损失函数和优化器 (Loss &amp; Optimizer)</span></span><br><span class="line">criterion = nn.BCELoss() <span class="comment"># 二分类交叉熵损失</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>) <span class="comment"># 随机梯度下降优化器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 训练循环 (Training Loop)</span></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloader: <span class="comment"># DataLoader 自动提供 batch</span></span><br><span class="line">        <span class="comment"># a. 前向传播</span></span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs.squeeze(), labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># b. 反向传播与优化（三步曲）</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 1. 梯度清零</span></span><br><span class="line">        loss.backward()        <span class="comment"># 2. 计算梯度</span></span><br><span class="line">        optimizer.step()       <span class="comment"># 3. 更新参数</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>或者可以参考笔者在学习 <ahref="https://github.com/rasbt/LLMs-from-scratch">Build a Large LanguageModel (From Scratch)</a> 一书时实践的训练 GPT-2 大模型的<ahref="https://github.com/hedon-ai-road/llm-from-scratch/blob/main/5-%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B.ipynb">代码</a>，会更复杂具体些。</p></blockquote><p>现在再回过头看 PyTorch 的众多API，你会发现它们都可以归入上述的框架中：</p><ul><li><strong>数据层(<code>torch.utils.data</code>)</strong>：一切为了高效、标准地提供数据。</li><li><strong>模型层(<code>torch.nn</code>)</strong>：一切为了灵活、方便地搭建和管理模型。<code>nn.Conv2d</code>,<code>nn.LSTM</code>, <code>nn.Transformer</code> 都是预先实现好的<code>nn.Module</code> "零件"。<code>nn.functional</code>里是对应的无状态函数版本（例如 <code>F.relu</code>），通常在<code>forward</code> 中使用。</li><li><strong>自动求导层(<code>torch.autograd</code>)</strong>：训练的幕后英雄，默默地处理最复杂的数学。</li><li><strong>优化层(<code>torch.optim</code>)</strong>：应用梯度的不同策略，决定了模型参数如何被更新。</li><li><strong>基础 (<code>torch</code>)</strong>：核心数据结构<code>Tensor</code> 以及大量的数学运算。</li></ul><h2 id="微观篇掌控-tensor-的七十二变">微观篇：掌控 Tensor的"七十二变"</h2><p>如果说理解工作流是掌握了"骨架"，那么理解 Tensor的形状变化就是掌握了"血液"在骨架中的流动方式。几乎 80% 的 PyTorch 新手bug 都和 Tensor shape（张量形状）不匹配有关。</p><p>延续之前的思路，我们依然不孤立地看 API，而是将它们放入<strong>"为什么需要变 -&gt; 在哪里变 -&gt; 如何变"</strong>的逻辑框架中，由浅入深地进行拆解。</p><h3 id="核心心智模型shape-is-semantics形状即语义">1. 核心心智模型：Shapeis Semantics（形状即语义）</h3><p>在深入 API 之前，请先建立一个最重要的心智模型：<u><strong>Tensor的每一个维度 (dimension) 都有其特定的语义含义</strong></u>。</p><p>一个典型的 4D Tensor <code>(B, C, H, W)</code> 在计算机视觉中，其形状<code>(16, 3, 224, 224)</code> 并不是一串孤立的数字，它的意思是：</p><ul><li><strong>B (Batch size) = 16</strong>: 这个 Tensor 里有 16张独立的图像。</li><li><strong>C (Channels) = 3</strong>: 每张图像有 3 个通道（R, G,B）。</li><li><strong>H (Height) = 224</strong>: 每张图像的高度是 224 像素。</li><li><strong>W (Width) = 224</strong>: 每张图像的宽度是 224 像素。</li></ul><p><img src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250818160244493.png" alt="Tensor 的每一个维度都有其特定的语义含义" style="zoom:50%;" /></p><p><strong>所有形状变换的根本原因，都是为了匹配下游操作（比如一个网络层）所期望的"语义"。</strong>当你遇到形状错误时，不要只想着"我要把这个 <code>(16, 512)</code> 变成<code>(16, 1, 512)</code>"，而应该去想："我当前的数据语义是<code>(批量, 特征)</code>，但下一层需要的是<code>(批量, 通道, 长度)</code>，所以我需要增加一个'通道'维"。</p><p>带着这个心智模型，我们来看 Tensor 的形状变换在整个流程中的角色。</p><h3 id="tensor-形状变换的场景与动机">2. Tensor 形状变换的场景与动机</h3><h4 id="阶段-1数据准备阶段-标准化">阶段 1：数据准备阶段 (标准化)</h4><p><strong>面临的问题：</strong> 原始数据（例如一张磁盘上的 JPEG图片）并不是 Tensor。即使转换成了Tensor，其维度也可能不符合模型训练的需要。</p><p><strong>核心动机：</strong><strong>标准化</strong>。将千差万别的单个数据点，统一成可以被模型批量处理的标准格式。</p><p><strong>关键变换：增加 Batch 维度</strong></p><ul><li><p><strong>为什么？</strong>深度学习训练是基于"小批量梯度下降"(Mini-batch Gradient Descent)的。我们不会一次只喂给模型一张图片，而是喂一批。这有两个好处：</p><ol type="1"><li>硬件（特别是 GPU）并行处理一个 batch 的数据效率极高；</li><li>一个 batch的平均梯度比单个样本的梯度更能代表整体数据，使训练更稳定。</li></ol></li><li><p><strong>如何实现？</strong></p><ul><li><p><strong>自动处理：</strong> <code>DataLoader</code> 在你从<code>Dataset</code> 取数据时，会自动帮你把多个单一样本堆叠 (stack)在一起，在最前面增加一个 Batch 维度。如果你从 <code>Dataset</code>取出的单张图片 Tensor 是 <code>(C, H, W)</code>，<code>DataLoader</code>会输出一个 <code>(B, C, H, W)</code> 的 Tensor。</p></li><li><p><strong>手动处理：</strong> 如果你只有一个样本，但模型需要一个batch 输入，你可以使用 <code>torch.unsqueeze(0)</code> 在第 0维增加一个维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一张图片，形状为 (3, 224, 224)</span></span><br><span class="line">single_image = torch.randn(<span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"><span class="comment"># 模型需要 batch 输入，手动增加 batch 维</span></span><br><span class="line"><span class="comment"># 形状变为 (1, 3, 224, 224)</span></span><br><span class="line">batched_image = single_image.unsqueeze(<span class="number">0</span>)</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="阶段-2模型内部-forward-传播-从一种形态到另一种形态">阶段2：模型内部 (<code>forward</code> 传播) (从一种形态到另一种形态)</h4><p>这是形状变换最频繁、最核心的区域。</p><ul><li><strong>面临的问题：</strong>数据在流经不同类型的神经网络层时，需要符合每一层对输入形状的特定要求。</li><li><strong>核心动机：</strong><strong>匹配接口</strong>。就像不同规格的管道需要转接头一样，不同网络层之间需要形状变换来“转接”。</li></ul><p>下面是几种最常见的变换场景：</p><p><strong>场景 A: "压平" - 从卷积到全连接</strong></p><ul><li><p><strong>为什么？</strong> 卷积层 (<code>nn.Conv2d</code>)非常擅长处理具有空间结构的数据（如图像），它的输出通常是 4D 的<code>(B, C_out, H_out, W_out)</code>，保留了空间信息。但是，全连接层(<code>nn.Linear</code>) 通常用于最后阶段的分类或回归，它期望的输入是 2D的<code>(B, num_features)</code>，即把每个样本的所有特征"拉平"成一个长向量。</p></li><li><p><strong>如何实现？</strong> <code>view</code>,<code>reshape</code>, <code>flatten</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设经过卷积和池化后，输出形状为 (16, 64, 7, 7)</span></span><br><span class="line">conv_output = torch.randn(<span class="number">16</span>, <span class="number">64</span>, <span class="number">7</span>, <span class="number">7</span>)</span><br><span class="line"><span class="comment"># 我们需要将其送入一个 nn.Linear(64 * 7 * 7, 100) 的层</span></span><br><span class="line"><span class="comment"># batch_size 维度需要保留</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法1: 使用 view (效率高，但不保证内存连续)</span></span><br><span class="line"><span class="comment"># -1 会自动计算该维度的大小</span></span><br><span class="line">linear_input = conv_output.view(<span class="number">16</span>, -<span class="number">1</span>) <span class="comment"># 形状变为 (16, 3136)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2: 使用 reshape (更安全，会自动处理内存问题)</span></span><br><span class="line">linear_input = conv_output.reshape(<span class="number">16</span>, -<span class="number">1</span>) <span class="comment"># 形状变为 (16, 3136)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法3: 使用 flatten (更语义化，推荐)</span></span><br><span class="line"><span class="comment"># start_dim=1 表示从第1个维度（Channels 维）开始压平</span></span><br><span class="line">linear_input = torch.flatten(conv_output, start_dim=<span class="number">1</span>) <span class="comment"># 形状变为 (16, 3136)</span></span><br></pre></td></tr></table></figure></li></ul><p><strong>场景 B: "换位" - 调整维度顺序</strong></p><ul><li><p><strong>为什么？</strong>不同的库或特定的层对维度的语义顺序有不同的要求。</p><ul><li><strong>经典案例 1 (图像)：</strong> Matplotlib 或 OpenCV处理图像时，通道维通常在最后 <code>(H, W, C)</code>。而 PyTorch的卷积层要求通道维在前 <code>(C, H, W)</code>。</li><li><strong>经典案例 2 (NLP)：</strong> PyTorch 的<code>nn.Transformer</code> 默认期望的输入是<code>(序列长度, 批量大小, 特征维度)</code>，而很多时候我们处理数据时更习惯<code>(批量大小, 序列长度, 特征维度)</code>。</li></ul></li><li><p><strong>如何实现？</strong> <code>permute</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 案例1: H, W, C -&gt; C, H, W</span></span><br><span class="line">image_hwc = torch.randn(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># permute 接收新的维度顺序</span></span><br><span class="line">image_chw = image_hwc.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>) <span class="comment"># 形状变为 (3, 224, 224)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 案例2: Batch-first -&gt; Seq-first for Transformer</span></span><br><span class="line">nlp_batch_first = torch.randn(<span class="number">16</span>, <span class="number">100</span>, <span class="number">512</span>) <span class="comment"># (B, Seq, Feat)</span></span><br><span class="line"><span class="comment"># 交换第 0 维和第 1 维</span></span><br><span class="line">nlp_seq_first = nlp_batch_first.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>) <span class="comment"># 形状变为 (100, 16, 512)</span></span><br></pre></td></tr></table></figure><p><code>transpose(dim1, dim2)</code> 是 <code>permute</code>的一个特例，它只能交换两个维度。</p></li></ul><p><strong>场景 C: "增删" - 增加或移除"占位"维度</strong></p><ul><li><p><strong>为什么？</strong> 有时为了进行广播 (broadcasting)计算，或者匹配一个需要特定维度数量的函数，我们需要临时增加或移除大小为 1的维度。</p></li><li><p><strong>如何实现？</strong> <code>unsqueeze</code> (增加) 和<code>squeeze</code> (移除)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 场景：给一个 2D 的 batch (B, F) 增加一个虚拟的“通道”维度</span></span><br><span class="line">x = torch.randn(<span class="number">16</span>, <span class="number">100</span>) <span class="comment"># (Batch, Features)</span></span><br><span class="line"><span class="comment"># 目标：变成 (16, 1, 100) 以便使用 1D 卷积 nn.Conv1d</span></span><br><span class="line">x_unsqueezed = x.unsqueeze(<span class="number">1</span>) <span class="comment"># 在第 1 维增加一个维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 场景：模型输出 (B, 1)，但 loss 函数需要 (B)</span></span><br><span class="line">model_output = torch.randn(<span class="number">16</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 移除所有大小为 1 的维度</span></span><br><span class="line">squeezed_output = model_output.squeeze() <span class="comment"># 形状变为 (16)</span></span><br><span class="line"><span class="comment"># 只移除第 1 维 (如果它的大小是 1)</span></span><br><span class="line">squeezed_output_dim1 = model_output.squeeze(<span class="number">1</span>) <span class="comment"># 形状变为 (16)</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="阶段-3损失计算阶段-对齐预测与真值">阶段 3：损失计算阶段(对齐"预测"与"真值")</h4><p><strong>面临的问题：</strong> 模型的输出 Tensor 和标签 (label) Tensor的形状可能不完全一致。</p><p><strong>核心动机：</strong><strong>对齐</strong>。使预测和真值的形状符合损失函数的要求。</p><p><strong>常见变换：</strong> <code>squeeze</code> 或<code>argmax</code></p><ul><li><code>nn.BCELoss</code> (二分类交叉熵) 通常要求模型输出和标签都是<code>(B)</code> 或 <code>(B, 1)</code>。如果你的模型输出了<code>(B, 1)</code> 而标签是 <code>(B)</code>，你可能需要<code>model_output.squeeze(1)</code> 来对齐。</li><li><code>nn.CrossEntropyLoss</code> (多分类交叉熵)很智能，它允许模型输出是 <code>(B, num_classes)</code> 的logits，而标签是 <code>(B)</code>的类别索引。它内部会自动处理对齐。在计算准确率时，你则需要用<code>torch.argmax(model_output, dim=1)</code> 来得到 <code>(B)</code>的预测类别，再和标签进行比较。</li></ul><h3 id="我应该用哪个-api">3. 我应该用哪个 API？</h3><p>当你需要改变 Tensor 形状时，可以按以下流程思考：</p><p><strong>我的目的是什么？</strong></p><ul><li>是为了<strong>"压平"</strong>多维特征给全连接层？ -&gt;<code>flatten</code> 或 <code>reshape/view</code>。</li><li>是为了<strong>"交换"</strong>维度的语义顺序（如 B,S,F -&gt;S,B,F）？ -&gt; <code>permute</code> 或 <code>transpose</code>。</li><li>是为了<strong>"增加"</strong>一个不存在的维度（如 batch 维，channel维）？ -&gt; <code>unsqueeze</code>。</li><li>是为了<strong>"移除"</strong>一个大小为 1 的多余维度？ -&gt;<code>squeeze</code>。</li></ul><blockquote><p><strong>一个黄金法则：<code>print(tensor.shape)</code></strong> 在<code>forward</code> 函数的每一行关键操作后，都加上<code>print(x.shape)</code>。这是调试 PyTorch模型形状问题的最简单、最有效的方法。它可以让你清晰地看到数据是如何一步步变换的。</p></blockquote><h3 id="代码示例-1">4. 代码示例</h3><p>让我们追踪一个 Tensor 在一个简单 CNN 中的完整旅程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ShapeJourneyCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">14</span> * <span class="number">14</span>, <span class="number">10</span>) <span class="comment"># 28x28 -&gt; 14x14 after pooling</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 初始输入 x: (B, 1, 28, 28) - 假设来自 MNIST</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Initial shape: \t\t<span class="subst">&#123;x.shape&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 经过第一个卷积层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        <span class="comment"># 形状变为 (B, 16, 28, 28) - 通道数从 1 变为 16</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;After Conv1: \t\t<span class="subst">&#123;x.shape&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        x = <span class="variable language_">self</span>.relu(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 经过最大池化层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.pool(x)</span><br><span class="line">        <span class="comment"># 形状变为 (B, 16, 14, 14) - H 和 W 都减半</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;After MaxPool: \t\t<span class="subst">&#123;x.shape&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># **关键变换：压平**</span></span><br><span class="line">        <span class="comment"># 为了送入 fc1，需要从 4D 变为 2D</span></span><br><span class="line">        <span class="comment"># 我们保留 batch 维度，将其余维度压平</span></span><br><span class="line">        x = torch.flatten(x, start_dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 形状变为 (B, 16*14*14) -&gt; (B, 3136)</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;After Flatten: \t\t<span class="subst">&#123;x.shape&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 经过全连接层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        <span class="comment"># 形状变为 (B, 10) - 10 是最终的类别数</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Final output shape: \t<span class="subst">&#123;x.shape&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 dummy input batch</span></span><br><span class="line">dummy_batch = torch.randn(<span class="number">64</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>) <span class="comment"># B=64</span></span><br><span class="line">model = ShapeJourneyCNN()</span><br><span class="line">model(dummy_batch)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Initial shape: torch.Size([64, 1, 28, 28])</span><br><span class="line">After Conv1: torch.Size([64, 16, 28, 28])</span><br><span class="line">After MaxPool: torch.Size([64, 16, 14, 14])</span><br><span class="line">After Flatten: torch.Size([64, 3136])</span><br><span class="line">Final output shape: torch.Size([64, 10])</span><br></pre></td></tr></table></figure><h2 id="总结">总结</h2><p>让我们回顾一下构建起的这张心智地图：</p><ol type="1"><li><strong>以工作流为纲</strong>：始终将 PyTorch 的 API 放入"数据准备-&gt; 模型构建 -&gt;训练循环"的框架中去理解其存在的意义。这构成了你的<strong>宏观骨架</strong>。</li><li><strong>以语义为轴</strong>：将 Tensor的形状变化理解为匹配不同模块语义接口的"翻译"过程。这让你能自如地掌控<strong>微观血液</strong>的流动。</li></ol><p>希望这篇指南能帮助你摆脱死记硬背的泥潭，从第一性原理出发，真正建立起对PyTorch 深刻而系统的理解，在"炼丹"之路上走得更远、更稳。</p>]]></content>
    
    
    <summary type="html">本文从 PyTorch 的核心设计出发，通过一个简单的例子，帮助读者理解 PyTorch 的核心设计，包括张量、自动求导、神经网络等。</summary>
    
    
    
    <category term="大模型" scheme="https://hedon.top/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="机器学习" scheme="https://hedon.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="深度学习" scheme="https://hedon.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="大模型" scheme="https://hedon.top/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="PyTorch" scheme="https://hedon.top/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>从 ECB 到 GCM：理解加密模式的演进</title>
    <link href="https://hedon.top/2025/08/15/encryption-mode/"/>
    <id>https://hedon.top/2025/08/15/encryption-mode/</id>
    <published>2025-08-15T09:31:00.000Z</published>
    <updated>2025-08-18T07:38:58.400Z</updated>
    
    <content type="html"><![CDATA[<p>在网络世界中，我们的数据需要被小心保护。对称加密算法，如AES，就是我们最常用的"保险箱"。但这个保险箱怎么用，却大有讲究。这就引出了我们今天讨论的主题：<strong>加密模式（EncryptionMode）</strong>。</p><p>本文将由浅入深地带你理解三种经典的分组加密模式：<strong>ECB、CBC</strong>和 <strong>GCM</strong>，并解释它们各自的优缺点和演进过程。</p><h3 id="简单的致命弱点ecbelectronic-codebook模式">1.简单的致命弱点：ECB（Electronic Codebook）模式</h3><p><strong>ECB模式</strong>是最简单的一种分组加密模式。它的工作原理非常直接：把明文数据切分成一个个固定大小的块，然后用同一个密钥，独立地加密每一个块。</p><p><strong>优点</strong>：</p><ul><li><strong>简单</strong>：原理清晰，易于实现。</li><li><strong>可并行</strong>：每个块的加密互不影响，可以并行处理，提高性能。</li><li><strong>可恢复</strong>：某个块损坏，只影响该块，不影响其他块的解密。</li></ul><p><strong>缺点</strong>：</p><ul><li><strong>不安全</strong>：这是 ECB模式的致命弱点。因为相同的明文块会产生相同的密文块，这使得攻击者可以通过分析密文中的重复模式来推断出原始数据的结构和内容。著名的“ECB企鹅”图片就是最好的例证。</li></ul><p>正是因为这个巨大的安全漏洞，ECB模式在大多数情况下都不被推荐使用。</p><hr /><h3 id="链式反应cbccipher-block-chaining模式">2. 链式反应：CBC（CipherBlock Chaining）模式</h3><p>为了解决 ECB 模式的重复性问题，工程师们设计了 <strong>CBC模式</strong>。它的核心思想是<strong>“链接”</strong>。</p><p>在 CBC模式中，每个明文块在加密前，都会先和<strong>前一个密文块</strong>进行异或运算。而第一个明文块则会和一个随机的<strong>初始化向量（IV）</strong>进行异或运算。</p><p><strong>优点</strong>：</p><ul><li><strong>更安全</strong>：由于引入了链式依赖和IV，即使有相同的明文块，它们加密后也会产生不同的密文，有效隐藏了数据模式，解决了ECB 的安全问题。</li></ul><p><strong>缺点</strong>：</p><ul><li><strong>无法并行</strong>：由于加密过程是链式的，每个块的加密都依赖于前一个块的结果，因此无法并行处理。</li><li><strong>错误传播</strong>：如果某个密文块在传输过程中损坏，它不仅会导致自身解密失败，还会影响后续所有块的解密，产生“多米诺骨牌效应”。</li></ul><p>CBC模式大大提高了安全性，在很长一段时间里都是行业标准。但是，它无法并行加密的缺点在面对海量数据时，成为了性能瓶颈。</p><hr /><h3 id="高性能与高安全gcmgaloiscounter-mode模式">3.高性能与高安全：GCM（Galois/Counter Mode）模式</h3><p>为了兼顾安全性和性能，<strong>GCM模式</strong>应运而生。它是一种<strong>认证加密（AuthenticatedEncryption）</strong>模式，完美结合了加密和数据完整性校验。</p><p>GCM 模式的核心思想是 <strong>CTR（CounterMode）</strong>。它不依赖于前面的密文块，而是通过一个<strong>不断递增的计数器</strong>，生成一个加密用的随机流，再将这个流和明文数据进行异或运算得到密文。</p><p><strong>优点</strong>：</p><ul><li><strong>可并行</strong>：每个加密块都是独立的，可以并行处理，极大地提高了加解密性能。</li><li><strong>认证加密</strong>：GCM模式除了加密，还内置了<strong>认证功能</strong>。它能生成一个<strong>认证标签（AuthenticationTag）</strong>，可以验证数据的完整性，确保数据在传输过程中没有被篡改。</li></ul><p><strong>缺点</strong>：</p><ul><li><strong>复杂度高</strong>：相对于 ECB 和 CBC，GCM的实现更复杂。</li></ul><hr /><h3 id="总结与展望">总结与展望</h3><p>从 ECB 的简单但危险，到 CBC 的安全但串行，再到 GCM的安全、高性能和认证，我们可以清晰地看到加密模式的演进。</p><table><thead><tr class="header"><th style="text-align: left;">特性</th><th style="text-align: left;">ECB</th><th style="text-align: left;">CBC</th><th style="text-align: left;">GCM</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><strong>工作模式</strong></td><td style="text-align: left;">独立</td><td style="text-align: left;">链接</td><td style="text-align: left;">计数器</td></tr><tr class="even"><td style="text-align: left;"><strong>安全性</strong></td><td style="text-align: left;">极低</td><td style="text-align: left;">较高</td><td style="text-align: left;">极高</td></tr><tr class="odd"><td style="text-align: left;"><strong>并行处理</strong></td><td style="text-align: left;">支持</td><td style="text-align: left;">不支持</td><td style="text-align: left;">支持</td></tr><tr class="even"><td style="text-align: left;"><strong>数据完整性</strong></td><td style="text-align: left;">不支持</td><td style="text-align: left;">不支持</td><td style="text-align: left;">支持</td></tr></tbody></table><p>在今天的网络世界中，<strong>GCM模式</strong>因其卓越的性能和安全性，已经成为最推荐使用的加密模式，广泛应用于TLS/SSL 等主流安全协议中。</p>]]></content>
    
    
    <summary type="html">加密模式 ECB、CBC、GCM</summary>
    
    
    
    <category term="加密模式" scheme="https://hedon.top/categories/%E5%8A%A0%E5%AF%86%E6%A8%A1%E5%BC%8F/"/>
    
    
    <category term="ECB" scheme="https://hedon.top/tags/ECB/"/>
    
    <category term="CBC" scheme="https://hedon.top/tags/CBC/"/>
    
    <category term="GCM" scheme="https://hedon.top/tags/GCM/"/>
    
  </entry>
  
  <entry>
    <title>一次由公网流出带宽飙升引发的服务器性能排查实录</title>
    <link href="https://hedon.top/2025/08/15/record-of-abnormal-investigation-of-public-network-traffic/"/>
    <id>https://hedon.top/2025/08/15/record-of-abnormal-investigation-of-public-network-traffic/</id>
    <published>2025-08-15T07:30:20.000Z</published>
    <updated>2025-08-15T08:04:49.686Z</updated>
    
    <content type="html"><![CDATA[<p>最近，我们的服务器监控系统发出了紧急警报：服务器的各项关键性能指标在<strong>2025 年 8 月 15 日 11:30左右</strong>出现了同步飙升。面对这一异常，我们并没有急于猜测，而是通过一个核心线索——公网流出流量，一步步揭开了问题的真相。本文将详细记录我们的排查过程，并深入解析每一步的工具应用与背后原理。</p><h4id="第一步从宏观监控入手锁定异常的核心">第一步：从宏观监控入手，锁定异常的核心</h4><p>故障排查的第一步，是细致分析监控图表，从中提取关键信息，从而圈定问题发生的精确时间。</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250815152916476.png"alt="异常现象" /><figcaption aria-hidden="true">异常现象</figcaption></figure><p>如上图所示，我们发现，在 <strong>2025 年 8 月 15 日 11:30左右</strong>，服务器的各项指标出现了显著异常：</p><ul><li><strong>公网流出带宽</strong>：在 11:37:00这个时间点，公网流出带宽达到了惊人的 <strong>110.899 M bit/s</strong>的峰值，远超正常水平。与此同时，公网流入带宽也有轻微增加，但量级远小于流出带宽。</li><li><strong>CPU 使用率</strong>：在带宽飙升的同时，CPU 使用率也从 25%左右的正常水平，迅速升高到接近 <strong>100%</strong> 的峰值。</li><li><strong>磁盘I/O</strong>：磁盘的读操作吞吐量和次数也出现了同步的峰值。</li></ul><p>此外，网络连接数的监控图也揭示了重要线索：</p><ul><li>在 11:30 左右，服务器的网络连接总数从约 2.5K 激增至 <strong>5.5K左右</strong>。</li><li>其中，<code>NON_ESTABLISHED</code>（非活跃）连接数急剧增加，最高达到了约<strong>2.475K</strong>，与<code>ESTABLISHED</code>（已建立）连接数几乎持平。</li></ul><p><strong>排查原理</strong>：多项关键指标在同一时间点同步异常，这强烈暗示着某个进程或任务正在大量消耗系统资源。公网带宽的异常是本次故障的核心线索，它将我们的排查方向聚焦于网络流量。同时，网络连接数中非活跃连接的激增，表明问题可能与高频率的连接建立与关闭有关，而非简单的持续高流量。这些宏观的监控数据，为我们后续深入排查提供了明确的起点和方向。</p><h4 id="第二步iftop-定位流量去向一剑封喉">第二步：<code>iftop</code>定位流量去向，一剑封喉</h4><p>既然问题是公网流出流量异常，那么这些流量究竟流向哪里？这是我们排查的下一个关键问题。我们运行了<code>iftop</code>工具，它能够实时监控网络流量的流向，结果令人震惊：</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250815153946932.png"alt="iftop 命令显示结果" /><figcaption aria-hidden="true">iftop 命令显示结果</figcaption></figure><ul><li><code>iftop</code>实时监控显示，服务器的公网流出流量（<code>=&gt;</code>）绝大部分都流向了IP 地址 <code>xxx</code>。</li><li>流出速率高达每秒 <strong>165Mbits/s</strong>，与监控图上的带宽峰值完全吻合。</li><li><code>iftop</code> 底部的 <code>TX</code>（发送）流量峰值达到了<strong>181M bits</strong>，进一步证实了带宽飙升的根源。</li></ul><p><strong>排查原理</strong>：<code>iftop</code>的强大之处在于它的<strong>实时性和直观性</strong>。它将服务器抽象的带宽数据，具象化为"本地IP A 到远端 IP B 的流量"。通过观察 <code>iftop</code>的输出，我们立刻将目光从"哪台服务器出了问题"转移到"这台服务器在向哪里发送数据""，从而大大缩短了排查路径。</p><h4 id="第三步nethogs-锁定应用进程确认元凶">第三步：<code>nethogs</code>锁定应用进程，确认元凶</h4><p>我们已经知道是服务器在向 <code>xxx</code>发送大量数据，但具体是哪个应用在做这件事？我们使用<code>nethogs</code>工具，它能够按进程实时监控流量，最终锁定了“元凶”：</p><ul><li><code>nethogs</code> 的输出明确显示，<strong><code>snakeweb_</code>应用</strong>是产生这些高流量的进程。</li><li>其发送（<code>SENT</code>）和接收（<code>RECEIVED</code>）流量都远超其他进程，证实了它是本次故障的直接“元凶”。</li></ul><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250815154151114.png"alt="nethogs" /><figcaption aria-hidden="true">nethogs</figcaption></figure><p><strong>排查原理</strong>：<code>nethogs</code> 将流量与具体的进程ID（PID）和程序路径关联起来，为我们提供了最终的、无可辩驳的证据。至此，我们已经完整地锁定了问题：<code>snakeweb_</code>应用向 IP <code>xxx</code> 发送大量数据。</p><h4 id="第四步发现-time_wait-堆积理解行为模式"><strong>第四步：发现<code>TIME_WAIT</code> 堆积，理解行为模式</strong></h4><p>在确认了应用和流量去向后，我们回过头来审视最初的一些异常现象。网络连接数的监控图显示，<code>NON_ESTABLISHED</code>（非活跃）连接数在11:30 左右急剧增加，最高达到了约 <strong>2.475K</strong>，与<code>ESTABLISHED</code>（已建立）连接数几乎持平。</p><p><strong>排查原理</strong>：大量的 <code>TIME_WAIT</code> 连接是 TCP连接在<strong>主动关闭后</strong>保持的一段等待时间。这一现象揭示了问题的另一面：<code>snakeweb_</code>应用在发送数据时，采用了<strong>高频率的短连接方式</strong>。每一次连接的建立和关闭，都在系统中留下了大量的<code>TIME_WAIT</code>状态连接，虽然不直接消耗带宽，但却占用了文件描述符等系统资源，成为了一个需要优化的次要问题。</p><h4id="第五步身份确认解决问题"><strong>第五步：身份确认，解决问题</strong></h4><p>通过 <code>whois</code> 查询，我们确认了流量流出的 IP属于阿里云，也是我们的一个服务之一。至此，整个问题链条已经完整。最后经过排查，内部的另外一个服务，新加了一个实时同步数据的功能，导致了流量的飙升。</p><h4 id="总结与反思">总结与反思</h4><p>这次排查完美地展示了工具在故障排查中的巨大作用。我们从公网流量飙升这个<strong>核心问题</strong>入手，利用<code>iftop</code> 快速将抽象的性能异常转化为清晰的网络通信流；再通过<code>nethogs</code>，我们锁定了具体进程；最后通过对<code>TIME_WAIT</code>等次要症状的分析，我们还原了应用的具体行为模式。整个过程环环相扣，最终成功定位并解决了问题。这提醒我们，在开发过程中，应时刻关注新功能对网络带宽、连接模式等底层资源的影响，避免因<strong>业务逻辑的改动</strong>而引发潜在的性能危机。</p>]]></content>
    
    
    <summary type="html">本文详细记录了一次由公网流出带宽飙升引发的服务器性能故障排查。我们从监控图表入手，利用 iftop 实时追踪流量去向，并最终通过 nethogs 锁定应用。该案例揭示了新功能配置对网络资源的巨大影响，为解决类似问题提供了宝贵经验。</summary>
    
    
    
    <category term="故障排查" scheme="https://hedon.top/categories/%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5/"/>
    
    
    <category term="网络" scheme="https://hedon.top/tags/%E7%BD%91%E7%BB%9C/"/>
    
    <category term="服务器" scheme="https://hedon.top/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
    <category term="故障排查" scheme="https://hedon.top/tags/%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5/"/>
    
    <category term="iftop" scheme="https://hedon.top/tags/iftop/"/>
    
  </entry>
  
  <entry>
    <title>大白话解释交叉熵损失</title>
    <link href="https://hedon.top/2025/08/13/llm/cross-entropy-loss/"/>
    <id>https://hedon.top/2025/08/13/llm/cross-entropy-loss/</id>
    <published>2025-08-13T11:30:20.000Z</published>
    <updated>2025-08-14T03:42:28.357Z</updated>
    
    <content type="html"><![CDATA[<h2 id="llm-训练过程概述">LLM 训练过程概述</h2><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250813193726179.png"alt="LLM 训练过程概述" /><figcaption aria-hidden="true">LLM 训练过程概述</figcaption></figure><p>在介绍交叉熵损失之前，我们先参考 <ahref="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/ch05.ipynb">Builda Large Language Model</a> 一书梳理一下训练 LLM的核心过程。笔者并非这个方向的专业人士，只能尝试从自己理解的角度来尽可能用大白话阐述这个过程在做什么、为什么这么做、能达到什么效果。</p><p>为了便于理解，我们可以把整个过程想象成<strong>教一个学徒如何写文章</strong>。</p><h3 id="文本生成text-generation">1. 文本生成（Text generation）</h3><blockquote><p>这就像让你的学徒开始写文章。一开始，它什么都不懂，只会胡乱地写一些词语。你给它一个开头，比如"从前有座山..."，它可能随便接上"...山里有只大象在跳舞。"完全不合逻辑。</p></blockquote><p>这是模型还没有训练好时，它根据一些输入，随机生成的一段文本。它生成的文本质量很差，毫无章法。</p><h3 id="文本评估text-evaluation">2. 文本评估（Text evaluation）</h3><blockquote><p>你现在需要一个<strong>"老师"</strong>来给这个学徒写的文章打分。你拿着学徒写的文章，和一篇<strong>标准答案（正确文章）</strong>进行对比。这个“老师”会告诉你，学徒写的文章和标准答案之间有多大的差距。这个打分的过程，就是我们本文将提到的<strong>交叉熵损失（Cross-EntropyLoss）</strong>。</p></blockquote><p>这个步骤是计算模型生成的文本与真实文本之间的损失值。模型会计算出它对下一个词的预测概率，并用交叉熵损失来衡量这个预测概率与真实词的“独热编码”概率有多大差距。<font color="red"><strong>损失值越大，说明模型预测得越差</strong></font>。</p><h3 id="训练集和验证集的损失training-set-and-validation-set-losses">3.训练集和验证集的损失（Training set and validation set losses）</h3><blockquote><p>你的学徒现在开始正式学习了。你给他一大堆文章（<strong>训练集</strong>）让他模仿学习，然后定期拿出一小部分它没看过的文章（<strong>验证集</strong>）给他做测试。</p><ul><li><strong>训练集损失：</strong>衡量学徒在学习过程中，对那些它看过的文章模仿得有多像。</li><li><strong>验证集损失：</strong>衡量学徒在面对新文章时，能不能把学到的东西举一反三，而不是只会死记硬背。</li></ul></blockquote><p>如果训练集损失一直下降，但验证集损失不降反升，那就说明学徒只会"死记硬背"了，这在机器学习里叫做<strong>过拟合（Overfitting）</strong>。</p><h3 id="大语言模型训练函数llm-training-function">4.大语言模型训练函数（LLM training function）</h3><p>这就是学徒的<strong>"大脑"</strong>，也是整个学习的核心。它根据"老师"给出的分数（损失值），调整自己的"大脑结构"（模型参数/权重）。如果某篇文章写得不好，它就会"反思"自己为什么写不好，然后调整下一次的写作方式，争取写得更好。这个调整的过程叫做<ahref="https://hedon.top/2025/07/27/llm/back-propagation/"><strong>反向传播（Backpropagation）</strong></a>和<strong>梯度下降（GradientDescent）</strong>。</p><h3id="训练模型生成类似人类的文本train-the-model-to-generate-human-like-text">5.训练模型生成类似人类的文本（Train the model to generate human-liketext）</h3><p>这就是整个训练的目的：通过不断地重复第 1-4步，让学徒的写作能力越来越强，最终写出来的文章，就像人类写的一样自然、流畅。</p><h3 id="文本生成策略text-generation-strategies">6. 文本生成策略（Textgeneration strategies）</h3><blockquote><p>学徒学得差不多了，但有时候会变得特别死板，只会把训练集里的东西原封不动地背出来。为了让它更有创意，更像人，你需要教它一些“写作技巧”。</p><p>例如：有时候，你不要总是选那个最有可能出现的词，可以偶尔选一些稍微不那么确定，但也很合理的词。</p></blockquote><p>这就是像<strong>Top-k采样</strong>、<strong>Top-p（核）采样</strong>、<strong>温度（Temperature）</strong>调节等技术。这些方法会让模型在生成文本时，增加一些随机性，避免总是生成重复、机械化的内容，减少过拟合的风险。</p><h3 id="权重保存和加载weight-saving-loading">7. 权重保存和加载（Weightsaving &amp; loading）</h3><p>学徒经过了长期的学习，终于成才了！现在你需要把它的"大脑"状态（也就是模型参数）保存下来。这样，下次再用的时候，就不用从头开始教了，直接把这个保存好的"大脑"拿出来用就行。</p><h3 id="来自-openai-的预训练权重pretrained-weights-from-openai">8. 来自OpenAI 的预训练权重（Pretrained weights from OpenAI）</h3><p>这就像你不是从一个零基础的学徒开始教，而是直接找一个已经很有经验的"天才学徒"来培养。OpenAI训练了海量的数据，已经把一个 GPT模型训练得非常强大了。我们直接拿来用，再结合自己的任务，在它的基础上继续微调。这样不仅省时省力，还能得到一个更好的模型。</p><h3 id="总结">总结</h3><p>GPT的训练过程就是，让一个初出茅庐的学徒（模型）写文章，找一个老师（损失函数）给它打分，然后根据分数调整它的大脑（参数）。反复这个过程，直到它写出来的文章像人类一样。为了让它更有创意，我们还教它一些写作技巧。最后，我们会把它的"大脑"保存下来，或者直接用一个"天才学徒"的大脑，在上面继续学习。</p><h2 id="交叉熵损失">交叉熵损失</h2><p>接下来我们回到本文的主题：<font color="red"><strong>交叉熵损失（Cross-EntropyLoss）</strong></font>。</p><blockquote><p>交叉熵损失是一种衡量模型预测结果与真实结果之间差异的指标。在分类任务中，模型通常会输出一个预测概率分布，而真实标签也可以被看作一个“理想”的概率分布。交叉熵损失的作用就是比较这两个概率分布的相似程度。<strong>如果模型的预测概率分布和真实概率分布越接近，交叉熵损失就越小，反之则越大。</strong>我们的目标就是通过训练，不断减小这个损失值，从而让模型学会做出更准确的预测。</p></blockquote><p>是不是一头雾水？哈哈，没关系，下面笔者将从概念、由来、原理和计算四个部分进行展开，尽可能以大白话的方式进行阐述，相信你阅读后回来再看一段定义的时候，会有不一样的理解~</p><h3 id="概念交叉熵损失就是给猜词打分">1.概念：交叉熵损失，就是给"猜词"打分</h3><p>想象一下，你正在教一个学徒写一句话。你告诉他句子的开头是："今天天气真..."，然后你让他猜下一个词应该是什么。</p><ul><li><p><strong>学徒的预测：</strong> 他可能会给出一些预测，比如：</p><ul><li>"好" （他觉得最可能）</li><li>"差" （也有一点可能）</li><li>"棒" （可能性更小）</li><li>"猫" （几乎不可能）</li></ul><p>这些预测，可以被看作一个<strong>概率分布</strong>。比如，他可能认为"好"的概率是80%，"差"的概率是 15%，"棒"的概率是 4%，"猫"的概率是 1%。</p></li><li><p><strong>正确的答案：</strong>实际上，正确的下一个词是<strong>"好"</strong>。</p></li><li><p><strong>交叉熵损失的作用：</strong>交叉熵损失就像一个严厉的老师，它只关注学徒对<strong>正确答案</strong>的预测。它会说："你对'好'这个词的预测概率是多少？<strong>这个概率越大，你这次的表现就越好，你的'惩罚'（损失）就越小。反之，你的表现越差，你的'惩罚'就越大。</strong>"</p></li></ul><p>简单来说，交叉熵损失的计算公式可以简化为： <spanclass="math display">\[损失值 = -log(模型对正确答案的预测概率)\]</span></p><ul><li>如果学徒对“好”的预测概率是 <strong>0.8</strong>，那么损失值大约是<span class="math inline">\(−log(0.8)≈0.223\)</span>。</li><li>如果学徒对“好”的预测概率是<strong>0.01</strong>（很差），那么损失值大约是 <spanclass="math inline">\(−log(0.01)≈4.605\)</span>。</li><li>如果学徒猜中率是 <strong>1.0</strong>（完美），那么损失值是 $−log(1)=0$。</li></ul><p>由此可见，交叉熵损失完美地实现了我们的教学目标：<strong>预测对了，损失就小；预测错了，损失就大。</strong></p><h3 id="由来从信息论到机器学习的迁移">2.由来：从信息论到机器学习的"迁移"</h3><p>要理解交叉熵损失的原理，我们需要追溯到它的老家：<strong>信息论</strong>。</p><h4 id="熵entropy">2.1 熵（Entropy）</h4><p>信息论中有一个概念叫"熵"，它衡量的是一个事件的<strong>不确定性</strong>。一个越不确定的事件，它的熵就越高，包含的信息量就越大。</p><ul><li>比如，我告诉您"太阳从东边升起"，这几乎是 100%确定的事，您没有获得任何新信息，所以它的熵很低。</li><li>但如果我告诉您"今天股市大涨"，这本身是一个不确定的事件，您就获得了新信息，所以它的熵很高。</li></ul><h4 id="交叉熵cross-entropy">2.2 交叉熵（Cross-Entropy）</h4><p>现在我们有两个概率分布：一个是真实的、完美的概率分布（记为 <spanclass="math inline">\(p\)</span>），另一个是我们模型的预测概率分布（记为<span class="math inline">\(q\)</span>）。</p><p>交叉熵衡量的就是，用我们模型的预测分布 <spanclass="math inline">\(q\)</span> 来表示真实的分布 <spanclass="math inline">\(p\)</span>，需要多少额外的"信息量"或者说"代价"。</p><p><strong>理论公式：</strong> 交叉熵的理论公式是 <spanclass="math inline">\(H(p,q)=−∑_ip_ilog(q_i)\)</span>。</p><ul><li>这里的 <span class="math inline">\(p_i\)</span>是真实事件的概率。</li><li><span class="math inline">\(q_i\)</span> 是我们模型预测的概率。</li></ul><p><strong>独热编码（One-hot）的简化</strong>：</p><p>在机器学习的分类任务中，我们的真实标签通常是独热编码的，比如正确答案是"猫''，那么真实分布<span class="math inline">\(p\)</span> 就是 <spanclass="math display">\[[0, 1, 0, ...]\]</span> 现在，让我们把独热编码的 <spanclass="math inline">\(p\)</span> 代入到上面的公式中： <spanclass="math display">\[H(p,q)=−(0⋅log(q_1)+1⋅log(q_2)+0⋅log(q_3)+...)\]</span>你会发现，求和公式里，只有<strong>正确类别（猫）</strong>对应的 <spanclass="math inline">\(p_i\)</span> 是 1，其他都是 <spanclass="math inline">\(0\)</span>。所以，整个求和公式就只剩下了一项：<span class="math display">\[H(p,q)=−log(q_{正确类别})\]</span>这就是交叉熵损失的最终形式。它之所以这样计算，完全是因为在分类任务中，我们<strong>只关心模型对正确答案的预测概率</strong>，而信息论中的交叉熵公式在遇到独热编码时，正好简化成了这个形式。</p><h3 id="原理为什么-logp-是一个好的损失函数">3. 原理：为什么 −log(p)是一个好的损失函数？</h3><p>让我们从数学和直觉两个角度来理解，为什么 <spanclass="math inline">\(−log(p)\)</span> 是一个完美的损失函数。</p><h4 id="数学角度">3.1 数学角度</h4><p><strong>梯度：</strong> 我们的目标是通过梯度下降法来最小化损失。对于<span class="math inline">\(−log(p)\)</span>，它的导数是 <spanclass="math inline">\(−1/p\)</span>。</p><ul><li>当 <span class="math inline">\(p\)</span> 接近 1时（预测得很准），<span class="math inline">\(1/p\)</span> 接近1，损失的梯度就很小。这意味着模型参数调整的幅度不大，因为它已经做得不错了。</li><li>当 <span class="math inline">\(p\)</span> 接近 0时（预测得很差），<span class="math inline">\(1/p\)</span>趋近于无穷大，损失的梯度就变得非常大。这意味着模型参数调整的幅度会非常大，因为它犯了一个严重的错误，需要大力纠正。</li></ul><p>这种特性使得模型在犯错时能快速学习，而在预测准确时则能稳定下来，这非常符合我们对训练过程的期望。</p><h4 id="直觉角度">3.2 直觉角度</h4><p><strong>不确定性：</strong> 让我们回到信息论。<spanclass="math inline">\(−log(p)\)</span> 实际上就是正确事件的信息量。</p><ul><li>如果模型预测正确事件的概率 <span class="math inline">\(p\)</span>很低，说明模型对正确答案非常不确定，那么这个正确答案的出现就包含了大量信息。交叉熵损失就用这个巨大的信息量来惩罚模型。</li><li>如果模型预测正确事件的概率 <span class="math inline">\(p\)</span>很高，说明模型很确定答案，那么这个正确答案的出现就包含很少信息。交叉熵损失就用这个很小的信息量来奖励模型。</li></ul><p>这种<strong>"用信息量来惩罚"</strong>的机制，确保了模型会努力去减少它对正确答案的不确定性，从而让它的预测结果越来越接近真实情况。</p><h3 id="计算">4. 计算</h3><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250813200501445.png"alt="交叉熵损失计算过程" /><figcaption aria-hidden="true">交叉熵损失计算过程</figcaption></figure><p>参考 <ahref="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/ch05.ipynb">Builda Large Language Model</a> 一书，交叉熵损失的计算过程大概分成上面所示的6 个步骤。</p><p><strong>步骤 1：Logits（对数几率）</strong></p><p>Logits 是模型在 Softmax层之前的原始输出值，它可以是任意实数。这些值代表了模型对每个类别的"置信度"，但还没有归一化为概率。图片中的<code>[[0.1113, -0.1057, -0.3666, ...]]</code> 就是一个样本的 Logits输出。</p><p><strong>步骤 2：Probabilities（概率）</strong></p><p>通过 Softmax 函数将 Logits转换为概率分布。这个函数的作用是将一组任意实数转换成一个概率分布，使得所有值都在0 到 1 之间，并且总和为 1。它的公式是 <spanclass="math inline">\(q_i=\frac{e^{z_i}}{∑_j^{e^{z_j}}}\)</span>， (其中<span class="math inline">\(z_i\)</span> 是第 <spanclass="math inline">\(i\)</span> 个类别的Logit)。<code>[[1.8849e-05, 1.5172e-05, 1.1687e-05, ...]]</code>就是经过 Softmax 转换后的概率分布。</p><p><strong>步骤 3：Target probabilities（目标概率）</strong></p><p>这一步的核心是从模型的预测中，提取出与真实答案相对应的概率值。在理论上，我们用独热编码（One-HotEncoding）来表示真实标签，例如 <code>[0, 1, 0, ...]</code>。图片中的<code>[7.4541e-05, ...]</code>正是模型根据这个独热编码所指示的正确索引，给出的预测概率。这些值通常很小，因为在训练初期，模型对正确答案的预测能力还很弱。在计算交叉熵时，我们只关心真实类别对应的预测概率。</p><p><strong>步骤 4：Log probabilities（对数概率）</strong></p><p>这一步是计算每个目标概率值的自然对数，即 <spanclass="math inline">\(log(q_i)\)</span>。例如，<code>[-9.5042, -10.3796, -11.3677, ...]</code>就是对目标概率取自然对数的结果。</p><p><strong>步骤 5：Average log probability（平均对数概率）</strong></p><p>这一步是计算<strong>所有对数概率的平均值</strong>。在步骤 4中，我们已经得到了模型对每个正确答案的预测概率的对数值。这一步就是将这些值加起来，然后除以样本或序列的长度，以得到一个平均值。</p><p><strong>步骤 6：Negative average logprobability（负平均对数概率）</strong></p><p><strong>这是计算</strong>最终损失值的步骤。在步骤 5的基础上，我们对平均对数概率取负号。这是为了<strong>将一个衡量模型错误程度的负数，转换成一个衡量模型错误程度的正数</strong>。这个操作没有复杂的数学含义，它只是为了让损失值的符号符合我们的直觉和约定。损失值越小代表模型表现越好。在图片中，对<code>-10.7940</code> 取负号后，得到的值是<code>10.7940</code>。这个值就是我们最终要最小化的损失（Loss）。在模型训练中，我们通过反向传播和梯度下降来不断减小这个损失值，从而迫使模型提高对正确答案的预测概率。</p><blockquote><p>上面 6 个步骤，可以直接使用 pytorch 的 <code>cross_entropy</code>计算，一步到位！</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)</span><br></pre></td></tr></table></figure><p>总结一下，整个计算流程可以概括为：</p><ol type="1"><li>模型输出原始分数（Logits）。</li><li>通过 Softmax 函数将分数转换为概率分布。</li><li>找出真实类别对应的预测概率。</li><li>对这个概率取负对数，得到损失值。</li><li>在训练时，我们会对所有样本的损失值求平均，然后进行反向传播更新模型参数。</li></ol><p>这个计算方式之所以合理，正是因为它完美地结合了信息论和机器学习的目标：<strong>通过最小化这个损失值，我们实际上是在最大化模型对正确类别的预测概率，从而让模型的预测分布越来越接近真实的分布。</strong>这是一种非常高效且理论基础坚实的训练方法。</p>]]></content>
    
    
    <summary type="html">本篇从 LLM 训练过程概述开始，通过&quot;教学徒写文章&quot;的生动比喻，帮助读者理解交叉熵损失在机器学习中的核心作用，以及如何用它来评估和优化模型的预测能力。</summary>
    
    
    
    <category term="大模型" scheme="https://hedon.top/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="机器学习" scheme="https://hedon.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="深度学习" scheme="https://hedon.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="大模型" scheme="https://hedon.top/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>大白话解释 GPT 架构中的权重共享</title>
    <link href="https://hedon.top/2025/08/13/llm/weight-typing/"/>
    <id>https://hedon.top/2025/08/13/llm/weight-typing/</id>
    <published>2025-08-13T08:30:20.000Z</published>
    <updated>2025-08-14T03:42:28.357Z</updated>
    
    <content type="html"><![CDATA[<p>在当今的大模型时代，GPT 架构以其强大的能力席卷了整个 AI领域。当你深入探究其内部结构时，会发现许多精妙的设计。其中一个看似简单、却能带来巨大效益的工程技巧，就是我们今天要讨论的——<strong>权重共享（WeightTying）</strong>。</p><h2 id="什么是权重共享">1. 什么是权重共享？</h2><p>想象一下你在学习一门外语。有两个过程：</p><ol type="1"><li><strong>听写</strong>：听到一个词后，你需要在脑海中构建它的意思。</li><li><strong>表达</strong>：你想表达一个意思时，需要从词库中挑出最合适的词。</li></ol><p>一个高效的学习者会发现，这两个过程是相辅相成的。你对一个词理解得越深（听写），就越能准确地使用它（表达）。反之亦然。</p><p>在 GPT 模型中，权重共享就是将这两个过程的"记忆"绑定在一起。</p><p>具体来说，模型有两个关键的权重矩阵：</p><ul><li><strong>输入嵌入（Input Embedding）</strong>：将输入的离散Token（如单词 "cat"）转换成连续的向量表示。这就像是你的"听写记忆"。</li><li><strong>输出线性层（Output LinearLayer）</strong>：将模型内部的向量表示转换回离散的Token，用于预测下一个词。这就像是你的"表达记忆"。</li></ul><p>更具体来说：</p><ul><li><strong>输入嵌入矩阵（Input Embedding Matrix）Wemb</strong>：这是一个将离散的 Token（词汇表中的ID）映射到连续向量空间（Token Embedding）的矩阵。它的维度是<code>[词汇表大小, 模型维度]</code>。当一个 Token ID 比如<code>5234</code> 进来时，模型会查找这个矩阵的第 <code>5234</code>行，将其作为这个 Token 的向量表示。</li><li><strong>输出词表线性层（Output Vocabulary LinearLayer）Wout</strong>：这是模型在最后一步用来预测下一个 Token的矩阵。它的维度是 <code>[模型维度, 词汇表大小]</code>。模型经过一系列Transformer Block 处理后，会得到一个 <code>[1, 模型维度]</code>的输出向量，这个向量会与 Wout 进行矩阵乘法，得到一个<code>[1, 词汇表大小]</code> 的 Logits向量。这个向量的每个值代表了词汇表中相应 Token 的概率分数，通过 Softmax归一化后，就可以得到下一个词的概率分布。</li></ul><p>权重共享的精髓在于，它<strong>将输出线性层的权重矩阵，设置为输入嵌入矩阵的转置</strong>。这意味着，模型在学习如何编码（理解）一个词时，也在同步学习如何解码（生成）这个词。</p><h2 id="为什么要这样做">2. 为什么要这样做？</h2><h3 id="浅层原因参数效率">浅层原因：参数效率</h3><p>这是最直观的好处。一个典型的 GPT 模型，词汇表大小可能达到 5万，模型维度（<code>d_model</code>）可能达到 4096。</p><ul><li><strong>不共享参数</strong>：<ul><li>输入嵌入矩阵参数量：<code>50000 * 4096</code></li><li>输出线性层参数量：<code>4096 * 50000</code></li><li>总参数量：<code>2 * 50000 * 4096 ≈ 4.1 亿</code></li></ul></li><li><strong>共享参数</strong>：<ul><li>总参数量：<code>50000 * 4096 ≈ 2.05 亿</code></li></ul></li></ul><p>通过共享参数，我们直接将这两部分的参数量减少了一半。这对于模型整体的参数规模来说，是一个显著的节省。在大规模模型中，这能有效降低显存占用，让训练和部署更具可行性。</p><h3id="深层原因泛化能力与语义对称性">深层原因：泛化能力与语义对称性</h3><p><strong>更好的梯度信号</strong>：当模型学习将一个 Token映射为有意义的向量时（输入嵌入），这些向量也会通过转置操作，影响到模型对下一个Token 的预测（输出线性层）。反之，当模型预测某个 Token概率的梯度回传时，也会同时更新输入嵌入矩阵。</p><p>这形成了一种"双向学习"的机制：模型在学习如何编码 Token的同时，也在学习如何解码Token，这两个过程相互强化。这就像一个人在学习如何说一个词（输出）时，也在不断加深对这个词的理解（输入）。</p><p><strong>增强泛化能力</strong>：</p><ul><li><strong>处理生僻词</strong>：对于训练语料中出现频率很低的词，模型可能没有足够的样本来学习其精确的向量表示。但通过权重共享，如果这个词作为"输出"被预测过，它的梯度也会回传到输入嵌入矩阵，让其向量表示得到更新。反之亦然。这使得模型对低频词的理解能力和预测能力都能得到提升，从而增强了模型的泛化能力。</li><li><strong>语义对称性</strong>：权重共享本质上假设了 Token的"编码"和"解码"过程应该具有某种对称性。一个 Token的向量表示，应该直接反映其作为输出时的"预测向量"。这可以看作是一种正则化，迫使模型学习更紧凑、更高效、更具语义一致性的向量空间。</li></ul><h2 id="落地实践要点与启示">3. 落地实践要点与启示</h2><p>在实际的 GPT 实现中，权重共享是一个常见的技巧。例如，OpenAI 的 GPT-2和许多基于其架构的开源模型都采用了这种做法。</p><ul><li><p><strong>实现细节</strong>：在 PyTorch等深度学习框架中，实现非常简单，通常只需要将<code>nn.Linear(d_model, vocab_size)</code>层的 <code>weight</code>参数设置为 <code>nn.Embedding(vocab_size, d_model)</code> 层的<code>weight.T</code> 即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设我们已经定义好了嵌入层</span></span><br><span class="line">embedding_layer = nn.Embedding(vocab_size, d_model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个线性层，用于预测下一个词</span></span><br><span class="line">output_layer = nn.Linear(d_model, vocab_size, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 权重共享的魔法就在这里：</span></span><br><span class="line"><span class="comment"># 将输出层的权重，设置为嵌入层权重的转置</span></span><br><span class="line">output_layer.weight = embedding_layer.weight</span><br></pre></td></tr></table></figure></li><li><p><strong>效果评估</strong>：在早期的研究中，例如在 Transformer架构中，研究人员就通过消融实验（ablation study）发现，权重共享能够带来约0.5 到 1个百分点的精度提升，同时大幅减少参数量。这证明了它在实践中的有效性。</p></li></ul><h2 id="总结">总结</h2><p>权重共享并非 GPT的"核心"创新，但它是一个非常精巧且有效的工程与理论结合。它通过一个简单的参数绑定，实现了：</p><ul><li><strong>工程上</strong>：显著减少模型参数量，提升训练和推理效率。</li><li><strong>理论上</strong>：建立输入和输出之间的双向学习机制，增强了模型对词汇表（特别是低频词）的泛化能力，并鼓励模型学习更具语义一致性的向量表示。</li></ul>]]></content>
    
    
    <summary type="html">本篇用外语学习的比喻，深入浅出地解释 GPT 架构中的权重共享技术，从听写记忆到表达记忆，帮助你理解这个提升大模型效率的核心优化策略</summary>
    
    
    
    <category term="大模型" scheme="https://hedon.top/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="机器学习" scheme="https://hedon.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="深度学习" scheme="https://hedon.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="大模型" scheme="https://hedon.top/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>Rust 多态的两种实现：Trait Bound 与 Trait Object 深度解析</title>
    <link href="https://hedon.top/2025/08/05/rust/rust-polymorphism/"/>
    <id>https://hedon.top/2025/08/05/rust/rust-polymorphism/</id>
    <published>2025-08-05T03:00:00.000Z</published>
    <updated>2025-08-05T05:05:00.215Z</updated>
    
    <content type="html"><![CDATA[<p>在 Rust编程中，实现多态（Polymorphism）主要有两种核心机制：<strong>TraitBound</strong> 和 <strong>Trait Object</strong>。虽然两者都基于<code>trait</code>，但它们的设计理念、底层实现和适用场景却截然不同。本文将带你从概念到具体的内存布局，深入探究这两种多态方式的本质。</p><h3 id="从一个基本问题说起">1. 从一个基本问题说起</h3><p>设想我们有一个<code>trait Draw</code>，它定义了绘制的方法。<code>Square</code>结构体实现了这个 <code>trait</code>。</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">trait</span> <span class="title class_">Draw</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">bounds</span>(&amp;<span class="keyword">self</span>) <span class="punctuation">-&gt;</span> (<span class="type">i32</span>, <span class="type">i32</span>, <span class="type">i32</span>, <span class="type">i32</span>); <span class="comment">// 假设定义了边界方法</span></span><br><span class="line">    <span class="comment">// ... 其他绘制相关方法</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Square</span> &#123;</span><br><span class="line">    top_left: Point,</span><br><span class="line">    size: <span class="type">i32</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Point</span> &#123;</span><br><span class="line">    x: <span class="type">i32</span>,</span><br><span class="line">    y: <span class="type">i32</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">Draw</span> <span class="keyword">for</span> <span class="title class_">Square</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">bounds</span>(&amp;<span class="keyword">self</span>) <span class="punctuation">-&gt;</span> (<span class="type">i32</span>, <span class="type">i32</span>, <span class="type">i32</span>, <span class="type">i32</span>) &#123;</span><br><span class="line">        (<span class="keyword">self</span>.top_left.x, <span class="keyword">self</span>.top_left.y, <span class="keyword">self</span>.size, <span class="keyword">self</span>.size)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>现在，我们如何编写一个函数来处理 <code>Square</code>，并调用它的<code>bounds</code> 方法呢？这就是 <strong>Trait Bound</strong> 和<strong>Trait Object</strong> 登场的时机。</p><h3 id="trait-bound编译期的静态多态">2. TraitBound：编译期的静态多态</h3><p><strong>Trait Bound</strong>的核心思想是<strong>编译期特化（Monomorphization）</strong>。它通过泛型参数<code>T</code> 来约束类型，确保该类型实现了某个 <code>trait</code>。</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">print_bounds</span>&lt;T: Draw&gt;(item: T) &#123;</span><br><span class="line">    <span class="keyword">let</span> (x, y, w, h) = item.<span class="title function_ invoke__">bounds</span>();</span><br><span class="line">    <span class="built_in">println!</span>(<span class="string">&quot;边界: x=&#123;&#125;, y=&#123;&#125;, width=&#123;&#125;, height=&#123;&#125;&quot;</span>, x, y, w, h);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> <span class="variable">square</span> = Square &#123; top_left: Point &#123; x: <span class="number">1</span>, y: <span class="number">2</span> &#125;, size: <span class="number">2</span> &#125;;</span><br><span class="line"><span class="title function_ invoke__">print_bounds</span>(square); <span class="comment">// T 被特化为 Square</span></span><br></pre></td></tr></table></figure><p><strong>底层原理：静态分发（Static Dispatch）</strong></p><p>在编译时，编译器会为 Square 类型生成一份 print_bounds函数的独立代码。当调用 print_bounds(square) 时，程序直接调用为 Square特化的版本，无需在运行时查找。</p><p><strong>优点与缺点</strong></p><ul><li><strong>零运行时开销</strong>：性能极致，与直接调用具体函数无异。</li><li><strong>代码膨胀（CodeBloat）</strong>：如果有很多不同的类型都实现了<code>Draw</code>，编译器就会生成多份 <code>print_bounds</code>的代码。</li><li><strong>语法糖</strong>：<code>fn print_bounds(item: impl Draw)</code>是 <code>fn print_bounds&lt;T: Draw&gt;(item: T)</code>的语法糖，两者在底层实现和性能上是完全等价的。</li></ul><hr /><h3 id="trait-object运行时的动态多态">3. TraitObject：运行时的动态多态</h3><p>现在，我们面临一个新问题：如果想把不同类型但都可绘制的对象放入同一个<code>Vec</code> 集合中怎么办？例如，我们有一个 <code>Square</code>和一个 <code>Circle</code>（假设 <code>Circle</code> 也实现了<code>Draw</code>），我们不能直接<code>vec![square, circle]</code>，因为 <code>Vec</code>要求所有元素是<strong>同一种具体类型</strong>。</p><p><strong>Trait Object</strong> 的核心思想是<strong>类型擦除（TypeErasure）</strong>，它允许我们将实现了相同 <code>trait</code>的不同类型实例统一处理。</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 假设 Circle 也实现了 Draw trait</span></span><br><span class="line"><span class="keyword">let</span> <span class="variable">circle</span> = Circle &#123; <span class="comment">/* ... */</span> &#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> <span class="variable">square</span> = Square &#123;</span><br><span class="line">  top_left: Point &#123; x: <span class="number">1</span>, y: <span class="number">2</span> &#125;,</span><br><span class="line">  size: <span class="number">2</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里的 `dyn` 关键字表示动态类型</span></span><br><span class="line"><span class="keyword">let</span> <span class="variable">draw_object</span>: <span class="type">Box</span>&lt;<span class="keyword">dyn</span> Draw&gt; = <span class="type">Box</span>::<span class="title function_ invoke__">new</span>(square);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以将不同类型但都实现了 Draw 的对象放入 Vec 中</span></span><br><span class="line"><span class="keyword">let</span> <span class="variable">drawable_items</span>: <span class="type">Vec</span>&lt;<span class="type">Box</span>&lt;<span class="keyword">dyn</span> Draw&gt;&gt; = <span class="built_in">vec!</span>[<span class="type">Box</span>::<span class="title function_ invoke__">new</span>(square), <span class="type">Box</span>::<span class="title function_ invoke__">new</span>(circle)];</span><br></pre></td></tr></table></figure><p><strong>底层原理：动态分发（Dynamic Dispatch）</strong></p><p>Box&lt;dyn Draw&gt; 是一个胖指针（Fat Pointer）。它包含两个部分：</p><ol type="1"><li><strong>数据指针</strong>：指向堆上实际的对象（例如<code>Square</code> 实例）。</li><li><strong>虚表指针</strong>：指向一张静态生成的<strong>虚函数表（vtable）</strong>。</li></ol><p>当调用 <code>draw_object.bounds()</code>时，程序会在<strong>运行时</strong>通过胖指针找到虚表，再从虚表中找到正确的方法地址并执行。</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250805124423615.png"alt="trait object layout" /><figcaption aria-hidden="true">trait object layout</figcaption></figure><p>上图展示了 <code>&amp;dyn Draw</code> 这个 <code>trait object</code>的内存布局：</p><p><strong>栈（Stack）</strong>：</p><ul><li><code>square</code>：原始的 <code>Square</code>实例，其数据（<code>top_left.x</code>, <code>top_left.y</code>,<code>size</code>）直接存储在栈上，大小在编译时可知。</li><li><code>draw</code>：这是一个 <code>&amp;dyn Draw</code> 类型的<strong>胖指针</strong>。它也存储在栈上，但其大小是固定的（两个指针的大小，通常是16 字节在 64 位系统上）。<ul><li>胖指针的<strong>第一个部分</strong>指向 <code>square</code>实例的实际数据地址。</li><li>胖指针的<strong>第二个部分</strong>指向<code>Draw for Square vtable</code>。</li></ul></li></ul><p><strong>虚表（Vtable）</strong>：</p><ul><li><code>Draw for Square vtable</code>：这是一个在编译时为<code>Square</code> 类型和 <code>Draw</code> <code>trait</code>的组合而生成的<strong>静态只读表</strong>。它包含了 <code>Square</code>实现 <code>Draw</code> <code>trait</code> 所需的所有信息，其中最重要的是<code>Square::bounds()</code> 方法的实际内存地址。</li></ul><p>通过 <code>draw</code> 胖指针调用 <code>draw.bounds()</code> 时，Rust运行时会：</p><ol type="1"><li>读取 <code>draw</code> 胖指针中的虚表指针。</li><li>通过虚表指针找到 <code>Draw for Square vtable</code>。</li><li>从虚表中找到 <code>bounds()</code> 方法的地址（即<code>Square::bounds()</code> 的地址）。</li><li>调用该地址处的函数，并将胖指针中的数据指针作为 <code>self</code>参数传递。</li></ol><blockquote><p><strong>虚表是与类型-trait 组合绑定的，而不是与实例绑定的。</strong>无论有多少个 <code>&amp;dyn Draw</code>类型的胖指针，只要它们都引用同一个 <code>Square</code> 实例，或者不同的<code>Square</code>实例，它们的虚表指针都会指向<strong>同一张</strong>静态生成的<code>Draw for Square vtable</code>。虚表是全局唯一的，为每种类型-trait组合只生成一份。</p></blockquote><h3 id="复杂场景下的内存布局组合-trait-object">4.复杂场景下的内存布局：组合 Trait Object</h3><p>当 <code>trait object</code> 组合多个 <code>trait</code> 时，比如<code>&amp;dyn Draw + Shape</code>，底层机制会更加精巧。</p><ul><li><strong>单 Trait Object</strong>：<code>&amp;dyn Draw</code> 和<code>&amp;dyn Shape</code> 是两个独立的胖指针，分别指向为<code>Square</code>-<code>Draw</code> 和<code>Square</code>-<code>Shape</code>组合生成的<strong>独立虚表</strong>。</li><li><strong>组合 TraitObject</strong>：<code>&amp;dyn Draw + Shape</code>是一个<strong>单一的胖指针</strong>。它指向一张包含了<strong>所有组合<code>trait</code> 方法地址的联合虚表</strong>。</li></ul><p>假如说我们定义的 <code>Shape</code> trait 如下：</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/// Anything that implements `Shape` must also implement `Draw`.</span></span><br><span class="line"><span class="keyword">trait</span> <span class="title class_">Shape</span>: Draw &#123;</span><br><span class="line">  <span class="comment">/// Render that portion of the shape that falls within `bounds`.</span></span><br><span class="line">  <span class="keyword">fn</span> <span class="title function_">render_in</span>(&amp;<span class="keyword">self</span>, bounds: Bounds);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/// Render the shape.</span></span><br><span class="line">  <span class="keyword">fn</span> <span class="title function_">render</span>(&amp;<span class="keyword">self</span>) &#123;</span><br><span class="line">      <span class="comment">// Default implementation renders that portion of the shape</span></span><br><span class="line">      <span class="comment">// that falls within the screen area.</span></span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">let</span> <span class="variable">Some</span>(visible) = <span class="title function_ invoke__">overlap</span>(SCREEN_BOUNDS, <span class="keyword">self</span>.<span class="title function_ invoke__">bounds</span>()) &#123;</span><br><span class="line">        <span class="keyword">self</span>.<span class="title function_ invoke__">render_in</span>(visible);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>现有如下代码：</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> <span class="variable">square</span> = Square &#123;</span><br><span class="line">  top_left: Point &#123; x: <span class="number">1</span>, y: <span class="number">2</span> &#125;,</span><br><span class="line">  size: <span class="number">2</span>,</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">let</span> <span class="variable">draw</span>: &amp;<span class="keyword">dyn</span> Draw = &amp;square;</span><br><span class="line"><span class="keyword">let</span> <span class="variable">shape</span>: &amp;<span class="keyword">dyn</span> Shape = &amp;square;</span><br></pre></td></tr></table></figure><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250805125330619.png"alt="组合 trait objects layout" /><figcaption aria-hidden="true">组合 trait objects layout</figcaption></figure><p><strong>栈（Stack）</strong>：</p><ul><li><code>square</code>：原始 <code>Square</code> 实例，不变。</li><li><code>draw</code>：<code>&amp;dyn Draw</code> 胖指针，指向<code>Square</code> 数据和 <code>Draw for Square vtable</code>。</li><li><code>shape</code>：这是一个<strong>新的、独立的</strong><code>&amp;dyn Shape</code> 胖指针。它同样指向 <code>Square</code>数据，但其虚表指针指向的是 <code>Shape for Square vtable</code>。</li></ul><p><strong>虚表（Vtable）</strong>：</p><ul><li><code>Draw for Square vtable</code>：为 <code>Square</code> 和<code>Draw</code> 组合生成的虚表，它包含了 <code>bounds()</code>方法的指针。</li><li><code>Shape for Square vtable</code>：为 <code>Square</code> 和<code>Shape</code> 组合生成的<strong>另一个独立的虚表</strong>。它包含了<code>Square::render_in()</code> 、<code>Square::bounds()</code>和<code>Shape::render()</code> 方法的地址。</li></ul><blockquote><p>总结：如果你有<strong>多个独立的 <code>trait object</code>类型</strong>（如 <code>&amp;dyn Draw</code> 和<code>&amp;dyn Shape</code>），即使它们引用的是<strong>同一个底层数据</strong>，它们各自的胖指针也会指向<strong>各自独立的虚表</strong>。</p></blockquote><h3 id="trait-object-的安全约束">5. Trait Object 的安全约束</h3><p>为了在实现动态多态的同时保证内存安全，Rust 对 <strong>traitobject</strong> 施加了严格的限制：</p><ul><li><strong><code>Sized</code> 约束</strong>：<code>dyn Trait</code>是一个DST，其大小在编译时未知。因此，它必须通过指针（<code>&amp;</code>、<code>Box</code>、<code>Rc</code>、<code>Arc</code>等）引用。</li><li><strong>方法限制</strong>：<code>trait object</code> 的<code>trait</code> 方法不能是泛型方法，也不能返回<code>Self</code>。这是因为编译器无法为泛型方法生成虚表条目，也无法确定返回<code>Self</code> 的返回值大小。例如，<code>Clone</code><code>trait</code> 因为其 <code>clone</code> 方法返回<code>Self</code>，所以不能直接作为 <code>trait object</code>。</li><li><strong>生命周期</strong>：<code>trait object</code>的生命周期会与它所引用的数据的生命周期绑定，防止悬空指针（<code>use-after-free</code>）问题。</li></ul><h3 id="总结">总结</h3><table><thead><tr class="header"><th>特性</th><th>Trait Bound (泛型)</th><th>Trait Object (动态)</th></tr></thead><tbody><tr class="odd"><td><strong>多态类型</strong></td><td><strong>静态多态</strong></td><td><strong>动态多态</strong></td></tr><tr class="even"><td><strong>分发方式</strong></td><td><strong>静态分发</strong> (编译时)</td><td><strong>动态分发</strong> (运行时)</td></tr><tr class="odd"><td><strong>性能开销</strong></td><td><strong>零开销</strong></td><td><strong>轻微开销</strong> (虚表查找)</td></tr><tr class="even"><td><strong>底层原理</strong></td><td><strong>编译期特化</strong></td><td><strong>类型擦除 + 胖指针/虚表</strong></td></tr><tr class="odd"><td><strong>大小类型</strong></td><td><code>Sized</code></td><td><code>Unsized</code> (必须通过指针引用)</td></tr><tr class="even"><td><strong>典型应用</strong></td><td>极致性能、类型已知</td><td>异构集合、插件化、通用接口</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">Rust 多态的两种实现：Trait Bound 与 Trait Object 深度解析</summary>
    
    
    
    <category term="rust" scheme="https://hedon.top/categories/rust/"/>
    
    
    <category term="rust" scheme="https://hedon.top/tags/rust/"/>
    
  </entry>
  
  <entry>
    <title>大白话解释反向传播算法</title>
    <link href="https://hedon.top/2025/07/27/llm/back-propagation/"/>
    <id>https://hedon.top/2025/07/27/llm/back-propagation/</id>
    <published>2025-07-27T04:30:20.000Z</published>
    <updated>2025-08-14T03:42:28.356Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一核心思想一个-分锅-大会">一、核心思想：一个 “分锅” 大会</h3><p>想象一下，你是一个大公司的CEO，你的公司有一个很长的流水线，用来生产一个精密的产品。这条流水线有很多道工序，每道工序都有一个工人负责。</p><ol type="1"><li><strong>最终产品出问题了</strong>：产品下线后，你发现最终的成品和设计图纸有偏差(比如，要求重 100 克，结果做出来重 110 克)。这个 “10 克的偏差” 就是<strong>误差 (Error)</strong>。</li><li><strong>你作为 CEO开始追责</strong>：你肯定不会把所有人都骂一顿，或者随机开除一个工人。最科学的方法是<strong>从后往前</strong>追查。</li><li><strong>追责第一步</strong>：你首先找到<strong>最后一道工序</strong>的工人。因为他是直接影响成品的人。你对他说：“产品重了 10克，你的操作对最终重量影响最大，你先调整一下你的机器参数。”</li><li><strong>追责第二步</strong>：这个工人会说：“老板，我这道工序的产出，也受到<strong>上一道工序</strong>给我的半成品的影响啊。根据我的机器参数，我可以计算出，上一个工人交给我的半成品大概是重了8 克导致的。”</li><li><strong>追责第三步</strong>：于是，你又拿着这个 “8 克的偏差” 去找<strong>倒数第二个工人</strong>。这个工人也同样会计算他受到了他上游工序的影响。</li><li><strong>一路向前追溯</strong>：就这样，这个 “锅” (误差)从最后一个工人开始，一层一层地<strong>向前传递</strong>，每个工人都根据自己的 “责任” 大小，领走一部分“锅”，并对自己的机器参数做出微小的调整。</li></ol><p>这个从后往前追责、分锅、调整的过程，就是 <strong>反向传播</strong>的核心思想。</p><h3 id="二从比喻到神经网络">二、从比喻到神经网络</h3><p>现在，我们把上面的比喻翻译成神经网络的术语：</p><table><colgroup><col style="width: 11%" /><col style="width: 20%" /><col style="width: 68%" /></colgroup><thead><tr class="header"><th>大白话比喻</th><th>神经网络术语</th><th>解释</th></tr></thead><tbody><tr class="odd"><td><strong>流水线</strong></td><td><strong>神经网络 (Neural Network)</strong></td><td>由多个层级组成，数据从输入层流向输出层。</td></tr><tr class="even"><td><strong>工人</strong></td><td><strong>神经元 (Neuron)</strong></td><td>网络中的计算单元。</td></tr><tr class="odd"><td><strong>工人的机器参数</strong></td><td><strong>权重 (Weights) 和 偏置 (Biases)</strong></td><td>每个神经元里需要学习和调整的参数，就像机器的旋钮。</td></tr><tr class="even"><td><strong>最终产品</strong></td><td><strong>网络的预测输出 (Prediction)</strong></td><td>比如，给一张猫的图片，网络输出 “90% 是狗”。</td></tr><tr class="odd"><td><strong>设计图纸</strong></td><td><strong>真实标签 (True Label)</strong></td><td>正确答案，比如 “100% 是猫”。</td></tr><tr class="even"><td><strong>产品偏差</strong></td><td><strong>损失/误差 (Loss / Error)</strong></td><td>预测输出和真实标签之间的差距。由 <strong>损失函数 (LossFunction)</strong> 计算得出。</td></tr><tr class="odd"><td><strong>从后往前追责分锅</strong></td><td><strong>反向传播 (Backpropagation)</strong></td><td>将总误差从输出层开始，一层层向输入层传播，计算出每一层权重对总误差的“贡献度”。</td></tr><tr class="even"><td><strong>调整机器参数</strong></td><td><strong>权重更新 (Weight Update)</strong></td><td>使用一种叫做 <strong>梯度下降 (Gradient Descent)</strong>的方法，根据计算出的“贡献度”来微调网络中所有的权重，目的是让总误差变小。</td></tr></tbody></table><h3 id="三核心工具微积分里的-链式法则">三、核心工具：微积分里的“链式法则”</h3><p>你可能会问，每个工人是怎么精确计算出他应该背多大的“锅”呢？</p><p>这里的“锅”在数学上，就是 <strong>梯度(Gradient)</strong>，简单理解就是 <strong>导数</strong>。导数衡量的是“如果我稍微动一下这个参数，最终的误差会改变多少”。</p><ul><li>如果导数很大(无论是正还是负)，说明这个参数对最终误差的影响很大，是“主要责任人”，需要大幅调整。</li><li>如果导数很小，接近0，说明它基本没啥影响，是“吃瓜群众”，基本不用动。</li></ul><p>反向传播算法的数学精髓，就是应用了微积分里的 <strong>链式法则 (ChainRule)</strong>。</p><p><strong>链式法则通俗解释</strong>：如果 C 的变化依赖于 B，而 B的变化又依赖于 A，那么链式法则可以帮助我们计算出 A 的微小变化最终会对 C产生多大的影响。</p><p>在神经网络里，最终的误差 (Loss) 是输出层 (Output Layer)的函数，输出层又是前一个隐藏层 (Hidden Layer)的函数，以此类推，直到输入层。反向传播正是利用链式法则，高效地计算出<strong>总误差</strong> 相对于 <strong>网络中每一个权重</strong>的梯度(导数)。它就像一套完美的公式，能精确地把“锅”不多不少、恰如其分地分配给每一个相关的参数。</p><h3 id="四总结反向传播的完整流程">四、总结：反向传播的完整流程</h3><p>所以，神经网络的学习过程（训练）可以总结为以下循环往复的步骤：</p><ol type="1"><li><strong>正向传播 (Forward Pass)</strong>：<ul><li>给网络一个输入数据 (例如一张图片)。</li><li>数据从输入层开始，经过每一层神经元的计算(乘以权重，加上偏置，再通过激活函数)，最后到达输出层，得到一个预测结果。</li><li>这就像把原材料放上传送带，走完整条流水线，得到最终产品。</li></ul></li><li><strong>计算损失 (Calculate Loss)</strong>：<ul><li>用损失函数比较网络的预测结果和真实的正确答案，计算出它们之间的差距，即总误差(Loss)。</li><li>这就像质检员检查最终产品，看它和设计图纸差了多少。</li></ul></li><li><strong>反向传播 (Backward Pass / Backpropagation)</strong>：<ul><li>这是最关键的一步。从总误差出发，利用链式法则，从输出层开始，反向逐层计算出网络中<strong>每一个权重</strong> 对这个总误差的“贡献度”(梯度)。</li><li>这就像 CEO 拿着质检报告，从后往前追责，精确地给每个工序“分锅”。</li></ul></li><li><strong>更新权重 (Update Weights)</strong>：<ul><li>根据反向传播计算出的“贡献度”(梯度)，使用梯度下降等优化算法，对网络中所有的权重进行微小的调整。调整的方向是<strong>让总误差变小</strong> 的方向。</li><li>这就像每个工人接到“整改通知”后，都去微调自己的机器旋钮。</li></ul></li></ol><p>通过成千上万次地重复以上 4个步骤，网络中的所有权重会逐渐被调整到最优状态，使得网络在接收新的输入时，能够做出非常准确的预测。</p><p>简单来说，<strong>反向传播就是神经网络高效学习的秘诀，它通过一个巧妙的“从后往前分锅”机制，告诉网络里的每一个参数应该如何自我调整，才能让最终的预测结果越来越准。</strong></p>]]></content>
    
    
    <summary type="html">本篇用 CEO 追责分锅的比喻，深入浅出地解释反向传播算法的工作原理，从流水线管理到神经网络训练，帮助你理解这个深度学习的核心算法</summary>
    
    
    
    <category term="大模型" scheme="https://hedon.top/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="机器学习" scheme="https://hedon.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="深度学习" scheme="https://hedon.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="大模型" scheme="https://hedon.top/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>读书笔记丨《Fundamentals of Software Architecture》</title>
    <link href="https://hedon.top/2025/07/24/note-fosa/"/>
    <id>https://hedon.top/2025/07/24/note-fosa/</id>
    <published>2025-07-24T03:01:24.000Z</published>
    <updated>2025-07-28T02:38:44.430Z</updated>
    
    <content type="html"><![CDATA[<h1id="聊架构设计的时候我们在谈什么">聊架构设计的时候，我们在谈什么？</h1><p><strong>第一步：理解商业与组织上下文 (Understand Business &amp;Organizational Context)</strong></p><ul><li><strong>利益相关方 (Stakeholders)</strong>:他们的核心诉求和期望是什么？</li><li><strong>用户视角 (User Perspective)</strong>:我们要为用户解决什么核心痛点？</li><li><strong>商业目标 (Business Goals)</strong>:这个项目要达成什么商业指标？（例如：降低成本、提升转化率）</li><li><strong>组织能力 (Organizational Capabilities)</strong>:<ul><li>公司文化 (Company Culture): 我们的文化是拥抱变化还是追求稳定？</li><li>团队现状 (Team Status): 团队的技术栈、技能水平和规模如何？</li></ul></li></ul><p><strong>第二步：定义架构特性与约束 (Define ArchitecturalCharacteristics &amp; Constraints)</strong></p><p>这一步的目标是将第一步中模糊的需求，转化为具体、可度量的技术目标。</p><ul><li><strong>识别架构特性 (Identify Architectural Characteristics /-ilities)</strong>:<ul><li>从性能、可伸缩性、可用性、容错性、可维护性、安全性、成本等特性中，识别出本次设计<strong>最关键</strong>的3-5 个。</li><li><strong>对它们进行排序</strong>。例如，对于一个后台管理系统，“可维护性”的优先级可能就高于“性能”。</li></ul></li><li><strong>明确约束条件 (Define Constraints)</strong>:<ul><li>有哪些不可逾越的红线？例如：预算上限、上线日期 (Time toMarket)、必须使用公司内某技术平台、法律合规要求等。</li></ul></li></ul><p><strong>第三步：探索方案与决策 (Explore Solutions &amp; MakeDecisions)</strong></p><p>有了第二步清晰的目标和边界，我们现在可以带着这些标准去评估方案。</p><ul><li><strong>探索可选方案 (Explore Options)</strong>: 至少寻找 2-3个备选方案。</li><li><strong>进行权衡分析 (Analyze Trade-offs)</strong>:基于第二步定义的<strong>架构特性优先级</strong>，系统地对比各方案的优劣。</li><li><strong>评估风险 (Assess Risks)</strong>:每个方案可能引入哪些短期或长期的技术、成本、人员风险？</li><li><strong>记录决策 (Document Decisions)</strong>: 使用 ADR(Architecture Decision Record) 记录最终选择和放弃的原因。</li></ul><p><strong>第四步：设计实施路径与验证机制 (Design Implementation Path&amp; Verification)</strong></p><p>在真正开始大规模编码前，设计好如何走，以及如何验证我们走在正确的路上。</p><ul><li><strong>实施计划 (Implementation Plan)</strong>:<ul><li>是否需要技术原型 (PoC) 来验证关键难点？</li><li>如何进行任务拆解和里程碑规划？</li></ul></li><li><strong>构建适应度函数 (Build Fitness Functions)</strong>:<ul><li>针对第二步定义的关键架构特性，设计具体的“检验尺”。</li><li>例如：为保证“模块解耦”，设计一个静态代码检查规则，禁止模块间的非法调用。</li></ul></li><li><strong>知识沉淀 (Knowledge Sedimentation)</strong>:准备好核心的架构图、设计文档等。</li></ul><p><strong>第五步：部署、观测与效果衡量 (Deploy, Observe &amp; MeasureEffectiveness)</strong></p><p>将架构推向真实世界，并通过数据验证其价值。</p><ul><li><strong>持续交付 (CI/CD)</strong>:作为将设计快速、可靠地部署到生产环境的手段。</li><li><strong>系统监控 (System Monitoring)</strong>:观测系统的健康状况（CPU、内存、延迟、错误率等）。</li><li><strong>业务指标验证 (Business Metrics Verification)</strong>:<strong>（闭环关键）</strong>验证是否达成了第一步定义的商业目标？例如，新架构上线后，用户转化率是否真的提升了？</li></ul><p><strong>第六步：复盘、沉淀与演进 (Retrospect, Internalize &amp;Evolve)</strong></p><ul><li><strong>问题记录与根因分析 (Problem Record &amp; Root CauseAnalysis)</strong>: 发生了什么？为什么会发生？</li><li><strong>流程与原则改进 (Process &amp; PrincipleImprovement)</strong>:如何优化我们的设计流程、技术原则，避免未来再犯？</li><li><strong>人员与组织成长 (Personnel &amp; OrganizationalGrowth)</strong>: 团队通过这次项目学到了什么？需要组织哪些培训？</li></ul><h1 id="fundamentals-of-software-architectrue-笔记梳理">Fundamentals ofSoftware Architectrue 笔记梳理</h1><blockquote><p>本章笔者将打散 FOSA书中的各个知识点，并将它们贯穿在我们上面提到的整个架构设计闭环中，同时会添加一些书中没有的内容进行补充扩展。</p></blockquote><h2 id="理解商业与组织上下文">1. 理解商业与组织上下文</h2><blockquote><p>利益相关方：他们的核心诉求和期望是什么？</p><p>用户视角：我们要为用户解决什么核心痛点？</p><p>商业目标：这个项目要达成什么商业指标？</p><p>组织能力：我们的文化是拥抱变化还是追求稳定？团队的技术栈、技能水平和规模如何？</p></blockquote><h3 id="谈判技巧">1.1 谈判技巧</h3><p>FOSA指出，架构师必须理解并驾驭企业的<strong>政治环境</strong>。几乎每一个架构决策都会受到挑战，这可能来自产品负责人、项目经理、业务利益相关方（因为成本或时间增加），甚至是开发者（认为有更好的方法）。</p><p>因此，架构师需要具备卓越的<strong>谈判和引导技能</strong>(Negotiation andFacilitation)，以理解各方诉求，并在分歧出现时达成共识。</p><p>FOSA 给出了几种谈判思路：</p><ol type="1"><li><strong>利用语法和流行语更好地理解情况。</strong>软件架构师应注意业务利益相关者在沟通中使用的短语和流行语。例如，像“我们需要零停机时间”或“我昨天就需要这些功能”这样的表述，虽然可能不精确，但却能揭示出对可用性或上市时间等方面的真正关注。通过利用这些“废话语法”，架构师可以更好地理解对方真正的担忧和需求，从而在谈判中占据优势。</li><li><strong>在进入谈判之前收集尽可能多的信息。</strong>在谈判之前，架构师应尽可能多地收集相关信息。例如，如果业务利益相关者坚持“五个九”的可用性（99.999%），架构师应提前研究这意味着什么，并将其转化为实际的停机时间（例如，每年约31.5秒的计划外停机时间）。充分掌握事实和数据有助于进行基于现实的讨论。</li><li><strong>当一切都失败时，说明成本和时间。</strong>这是最后的谈判策略。尽管成本和时间（投入的工作量）是任何谈判中的关键因素，但应作为最后的手段使用。过早提及这些可能会使谈判陷入僵局，因为它们可能会被视为阻止或拒绝的借口。</li><li><strong>利用“分而治之”的原则来限定需求。</strong>这一策略借鉴了孙子兵法中的思想，即“其力合者，离之”。当面临不合理或范围过大的要求时（例如，整个系统都需要“五个九”的可用性），架构师可以通过提问来缩小范围，确定哪些特定部分或功能真正需要这种高水平的特性。这样做可以减少困难且昂贵需求的范围，从而简化谈判。</li><li><strong>永远记住演示胜于讨论。</strong>当与同事或开发人员在技术方法上存在分歧时，与其争论不休，不如通过实际的演示来证明你的观点。例如，如果你认为消息队列比REST 更适合特定的服务间通信，可以在模拟生产环境中进行 A/B测试，用数据和实际结果来说服对方。实际操作的证据通常比理论争论更有说服力。</li><li><strong>在谈判中避免过于争辩或让事情变得过于个人化——冷静的领导力结合清晰简洁的推理总能赢得谈判。</strong>在讨论中，如果气氛变得过于激烈或个人化，最好的做法是暂停谈判，待双方冷静后再重新进行。作为领导者，保持冷静和专业的态度，并用清晰、简洁的逻辑进行推理，往往能够有效化解冲突，促使对方退让，最终达成共识。</li><li><strong>在说服开发人员采纳架构决策或执行特定任务时，提供理由而不是“高高在上地发号施令”。</strong>架构师不应凭借职位来命令开发人员，而应通过提供充分的理由来说明为什么需要某个架构决策或任务。例如，解释“所有数据库调用都需要通过业务层”是为了“更好地控制变更”，这比单纯命令“你必须通过业务层”更容易被接受。理解背后的原因能促使开发人员更积极地接受并实施决策。</li><li><strong>如果开发人员不同意某个决策，让他们自己找到解决方案。</strong>当开发人员对某个技术决策有异议时，与其直接反驳，不如挑战他们，让他们自己去探索并证明他们的替代方案。例如，如果开发人员坚持使用某个框架但你认为它不符合安全要求，可以让他们自行研究并展示如何解决安全问题。这不仅能促进开发人员的学习和思考，也能让架构师在最终解决方案上获得团队的认可和支持，形成双赢局面。</li></ol><h3 id="业务理解">1.2 业务理解</h3><p>架构决策必须<strong>提供业务价值</strong>。如果一个架构决策没有业务价值，它可能就不是一个好的决策，需要重新考虑。</p><p>FOSA强调，架构决策的<strong>商业合理性</strong>至关重要。常见的商业合理性包括：<strong>成本</strong>(Cost)、<strong>上市时间</strong> (Time toMarket)、<strong>用户满意度</strong> (User Satisfaction)和<strong>战略定位</strong> (StrategicPositioning)。在与业务利益相关方谈判时，要重点关注他们最看重的指标。</p><p>这里面的一大难点就是：<strong>业务方与开发方使用的不是同一种"语言"</strong>。双方对同一件事情的关注点是不一样的，所以表述出来的述求，也是不同的。所以架构师的职责就是需要将业务领域的关注点和架构特性进行对应。</p><p>比如：</p><table><colgroup><col style="width: 36%" /><col style="width: 63%" /></colgroup><thead><tr class="header"><th>Domain Concern</th><th>Architecture characteristics</th></tr></thead><tbody><tr class="odd"><td>Mergers and acquisitions 合并与收购</td><td>互操作性 interoperability<br>可扩展性 scalability<br>适配性adaptability<br>可扩展性 extensibility</td></tr><tr class="even"><td>Time to market 上市时间</td><td>灵活性 agility<br/>可测试性 testability<br/>可部署性deployability</td></tr><tr class="odd"><td>User satisfaction 用户满意度</td><td>性能 performance<br/>可用性 availability<br/>容错性 faulttolerance<br/>可测试性 testability<br/>可部署性 deployability<br/>灵活性agility<br/>安全性 security</td></tr><tr class="even"><td>Competitive advantage 竞争优势</td><td>灵活性 agility<br/>可测试性 testability<br/>可部署性deployability<br/>可扩展性 scalability<br/>可用性availability<br/>容错性 fault tolerance</td></tr><tr class="odd"><td>Time and budget 时间和预算</td><td>简单性 simplicity<br/>可行性 feasibility</td></tr></tbody></table><p>另外，随着业务的发展，关注点也是在不断发生变化的，这个时候，架构所侧重的架构特性也是随之改变。</p><h2 id="定义架构特性与约束">2. 定义架构特性与约束</h2><blockquote><p>识别架构特性：从性能、可伸缩性、可用性、容错性、可维护性、安全性、成本等特性中，识别出本次设计最关键的3-5 个。</p><p>明确约束条件：有哪些不可逾越的红线？</p></blockquote><h3 id="架构特性定义">2.1 架构特性定义</h3><p>架构师的核心职责之一就是识别和定义系统的<strong>架构特性</strong>(ArchitectureCharacteristics)。这些特性定义了系统的<strong>成功标准</strong>，并且通常与系统的<strong>功能性</strong>(Functionality) 正交。</p><p>一个属性要成为架构特性（Architecture Characteristics），需至少满足 3个条件：</p><ol type="1"><li><strong>指定非领域设计考量</strong>：架构特性关注的是应用程序"如何"实现需求以及做出某些选择"为何"的原因，而不是应用程序"应该做什么"的业务需求。例如，性能水平通常不会出现在需求文档中，但却是重要的架构特性。</li><li><strong>影响设计的某个结构方面</strong>：如果一个架构特性需要特殊结构考虑才能成功，那么它就会上升到架构特性的层面。例如，一般的安全性对于几乎所有项目都是必需的，但当需要设计特定的模块、组件或服务来隔离关键安全问题时，安全才成为一个架构特性。</li><li><strong>对应用程序的成功至关重要</strong>：应用程序可以支持大量的架构特性，但并非所有都应该被支持。支持每个架构特性都会增加设计的复杂性，因此，架构师的关键任务是选择最少的、对应用程序成功至关重要或重要的架构特性，而不是尽可能多的。</li></ol><h3 id="架构特性类型">2.2 架构特性类型</h3><ul><li><strong>显性架构特性</strong>：是在需求规范中明确列出的，作为必要设计的一部分。它们通常直接出现在需求文档或其他具体说明中。</li><li><strong>隐性架构特性</strong>：很少出现在需求文档中，但它们对于项目的成功是必需的。架构师必须利用他们对问题领域的知识，在分析阶段发现这些特征。</li></ul><p>可进一步细分为：操作特性、结构特性和交叉特性。</p><p>操作性架构特性涵盖了系统的<strong>运行能力</strong>，例如性能、可伸缩性、弹性、可用性和可靠性等。这些特性通常与运营和DevOps 关注点高度重叠。</p><table><colgroup><col style="width: 23%" /><col style="width: 76%" /></colgroup><thead><tr class="header"><th>特性</th><th>说明</th></tr></thead><tbody><tr class="odd"><td>Availability</td><td>系统需要保持可用的时间长度；例如，如果需要 24/7可用，则需要采取措施确保系统始终可用。它指的是软件可操作和可访问的程度。</td></tr><tr class="even"><td>Continuity</td><td>灾难恢复能力。</td></tr><tr class="odd"><td>Performance</td><td>衡量应用程序请求和响应周期所需的时间。它包括压力测试、高峰分析、功能使用频率分析、所需容量和响应时间。它也可以是更具体的度量，例如首屏渲染时间，即网页首次可见的时间。</td></tr><tr class="even"><td>Recoverability</td><td>业务连续性要求（例如，发生灾难时，系统需要多快才能重新上线？）这将影响备份策略和对复制硬件的要求。它也指软件从故障中恢复的能力，通过恢复任何受影响的数据并重新建立系统的所需状态。</td></tr><tr class="odd"><td>Reliability/Safety</td><td>评估系统是否需要具备故障安全能力，或者其任务关键性是否影响生命。如果系统发生故障，是否会给公司带来巨额损失。它指系统在指定条件下和指定时间内运行的程度。</td></tr><tr class="even"><td>Robustness</td><td>在互联网连接中断、断电或硬件故障时，处理错误和边界条件的能力。</td></tr><tr class="odd"><td>Scalability</td><td>系统随着用户或请求数量的增加而执行和运行的能力。这意味着处理大量并发用户而不会出现严重的性能下降。</td></tr></tbody></table><p>结构性架构特性关注<strong>代码结构</strong>。在许多情况下，架构师对代码质量问题负有独立或共同的责任，例如良好的模块化、组件间的受控耦合、可读性强的代码以及其他内部质量评估。</p><table><colgroup><col style="width: 25%" /><col style="width: 74%" /></colgroup><thead><tr class="header"><th>特性</th><th>说明</th></tr></thead><tbody><tr class="odd"><td>Configurability</td><td>最终用户通过可用界面轻松更改软件配置方面的能力。</td></tr><tr class="even"><td>Extensibility</td><td>系统的可扩展性。</td></tr><tr class="odd"><td>Installability</td><td>系统在所有必要平台上安装的便捷性。它指软件在指定环境中安装和/或卸载的程度。</td></tr><tr class="even"><td>Leverageability/Reuse</td><td>跨多个产品利用通用组件的能力。它指开发人员在多个系统或构建其他资产中重复使用资产的程度。</td></tr><tr class="odd"><td>Maintainability</td><td>开发人员修改、纠正或使其适应环境和/或需求变化的有效性和效率程度。</td></tr><tr class="even"><td>Portability</td><td>系统是否需要在多个平台上运行。它指开发人员将系统、产品或组件从一个硬件、软件或其他操作或使用环境转移到另一个环境的程度。</td></tr><tr class="odd"><td>Supportability</td><td>应用程序所需的技术支持级别。系统中调试错误所需的日志记录及其他设施的级别。</td></tr><tr class="even"><td>Upgradeability</td><td>从该应用程序/解决方案的旧版本轻松/快速升级到新版本的能力。</td></tr></tbody></table><p>交叉架构特性指的是那些难以归类或超出传统类别，但却形成重要设计约束和考虑的特性。</p><table><colgroup><col style="width: 27%" /><col style="width: 72%" /></colgroup><thead><tr class="header"><th>特性</th><th>说明</th></tr></thead><tbody><tr class="odd"><td>Accessibility</td><td>确保所有用户（包括色盲或听力障碍等残障用户）能够访问系统。它指使软件可供具有最广泛特征和能力的人使用。</td></tr><tr class="even"><td>Archivability</td><td>数据是否需要在一段时间后归档或删除。</td></tr><tr class="odd"><td>Authentication</td><td>确保用户是其所声称的身份的安全要求。</td></tr><tr class="even"><td>Authorization</td><td>确保用户只能访问应用程序内特定功能（按用例、子系统、网页、业务规则、字段级别等）的安全要求。</td></tr><tr class="odd"><td>Legal</td><td>系统在哪些法律约束下运行（数据保护、萨班斯-奥克斯利法案、GDPR等）？公司需要哪些保留权利？关于应用程序构建或部署方式的任何规定。</td></tr><tr class="even"><td>Privacy</td><td>隐藏内部公司员工交易信息的能力（加密交易，甚至数据库管理员和网络架构师都无法查看）。</td></tr><tr class="odd"><td>Security</td><td>数据是否需要在数据库中加密？内部系统之间网络通信是否需要加密？远程用户访问需要何种类型的认证？它指软件保护信息和数据的程度，以便人员或其他产品或系统具有与其授权类型和级别相称的数据访问程度。</td></tr><tr class="even"><td>Supportability</td><td>应用程序所需的技术支持级别。系统中调试错误所需的日志记录及其他设施的级别。</td></tr><tr class="odd"><td>Usability/Achievability</td><td>用户使用应用程序/解决方案实现目标所需的培训水平。它指用户可以有效、高效、满意地使用系统达到预期目的。</td></tr></tbody></table><h3 id="架构特性选择">2.3 架构特性选择</h3><p>架构特性不是越多越好：</p><ul><li><strong>增加系统设计的复杂性</strong>：每增加一个架构特性，都会使整个系统设计变得更加复杂。支持过多的架构特性会导致在架构师和开发人员开始解决核心业务问题之前，系统就变得越来越复杂。</li><li><strong>分散对核心问题的关注</strong>：架构特性定义了系统的成功标准，通常与系统的功能性正交，关注的是“如何”实现需求以及“为什么”做出某些选择。然而，如果过度追求特性数量，可能会导致偏离原始的业务问题，即开发软件的最初动机。</li><li><strong>每个特性都涉及权衡</strong>：软件架构中的每一个方面都存在权衡，有优点也有缺点。例如，在拍卖系统中，选择使用主题（topic）进行通信可能带来架构可扩展性的优势和服务的解耦，但会引入数据访问和数据安全方面的潜在问题，并且不支持异构契约。而使用队列（queue）则允许每个消费者拥有自己的契约，但不具备可扩展性，并且会增加服务间的耦合。架构师需要分析这些权衡，并根据业务驱动因素和环境选择最重要的特性。</li><li><strong>过度规范的危害</strong>：架构师过度规范架构特性是常见的陷阱，其破坏性不亚于规范不足，因为它会使系统设计过于复杂。历史案例“瓦萨号”战舰的失败就是一个例证，它是因为过度追求建造最宏伟的战舰（即过度规范架构特性）而最终导致沉没。</li><li><strong>陷入“意外复杂性”陷阱</strong>：架构师有时会为解决方案、图表和文档添加不必要的复杂性。正如一位作者所言，“开发者被复杂性吸引，就像飞蛾扑火一样——结果往往相同”。这种“意外复杂性”是由于人为地使问题复杂化，而不是问题本身固有的复杂性。通过识别子领域类型并根据其业务逻辑的复杂性选择合适的实现模式（例如，事务脚本和活动记录适用于简单业务逻辑，而领域模型和事件溯源领域模型适用于复杂的核心子领域），可以避免引入不必要的复杂性。</li><li><strong>设计应由业务驱动</strong>：领域驱动设计（DDD）的核心思想在于让业务领域驱动软件设计决策。这意味着设计决策应该基于业务领域的需求和战略，而非盲目地堆砌所有可能的架构特性。</li></ul><p>因此，与领域利益相关者合作时，架构师应努力使最终的架构特性列表尽可能短，因为每个特性都会增加总体系统设计的复杂性。</p><h2 id="探索方案与决策">3. 探索方案与决策</h2><blockquote><p>探索可选方案 ：至少寻找 2-3 个备选方案。</p><p>进行权衡分析：基于第二步定义的架构特性优先级，系统地对比各方案的优劣。</p><p>评估风险：每个方案可能引入哪些短期或长期的技术、成本、人员风险？</p><p>记录决策：使用 ADR (Architecture Decision Record)记录最终选择和放弃的原因。</p></blockquote><h3 id="架构风格">3.1 架构风格</h3><h4 id="分层架构-layered-architecture">3.1.1 分层架构 LayeredArchitecture</h4><figure><img src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/layer.png"alt="3.1.1 分层架构" /><figcaption aria-hidden="true">3.1.1 分层架构</figcaption></figure><p>分层架构的<strong>核心驱动力</strong>是<strong>关注点分离（SeparationofConcerns）</strong>。它将一个复杂的系统按照不同的职责或技术关注点，垂直地划分成若干个水平的“层（Layer）”。</p><p>这些层之间存在一个至关重要的约束：<strong>依赖关系是单向的</strong>。通常来说，上层可以依赖下层，但下层绝对不能依赖上层。例如，表现层可以调用业务逻辑层，但业务逻辑层不应该知道任何关于表现层的具体实现细节。</p><p>优点：</p><ul><li><strong>简单性（Simplicity）和低成本（Cost）</strong>：分层架构模式非常成熟，广为人知，开发团队的学习成本极低。对于中小型项目、预算有限的初创公司或内部管理系统，它是一个"足够好"的、性价比极高的起点。</li><li><strong>可维护性（Maintainability）</strong>：如前所述，只要遵循了隔离层原则，系统的维护和迭代会非常清晰。对于那些业务逻辑相对稳定、变更不频繁的系统，这是一个巨大的优势。</li><li><strong>整体可部署性（Deployability）</strong>：分层架构天然倾向于构建<strong>单体应用（Monolith）</strong>。整个应用被打包成一个单元（例如一个WAR包或一个可执行文件）进行部署。这极大地简化了部署和运维的复杂度，尤其是在项目早期或运维能力有限的团队中。</li></ul><p>缺点：</p><ul><li><strong>技术分区而非领域分区</strong>：分层架构是一种技术分区架构。这意味着它的组件是根据其在架构中的技术角色（如表示层、业务层、持久层），而不是根据业务领域（如客户、订单）进行分组的。这会导致任何特定的业务领域（例如“客户”领域）的逻辑都会分散在架构的所有层中。同时，当需要对特定业务领域的需求进行更改时，由于其逻辑分散在多个技术层中，开发人员必须在所有相关层中进行修改，这降低了开发的敏捷性。</li><li><strong>部署风险高</strong>：在分层架构中，即使是对少量代码的更改（例如，一个类文件中简单的三行更改），也需要重新部署整个部署单元。这种部署往往会捆绑数十个其他更改，从而显著增加了部署风险，且部署频率受到限制。</li><li><strong>测试范围大且不完整</strong>：由于整个应用程序是作为一个大型单体单元部署的，开发人员通常不会为简单的三行更改花费数小时执行完整的回归测试套件。这导致测试覆盖范围不完整，并且难以确保更改不会影响看似不相关的部分。</li></ul><h4 id="管道架构-pipeline-architecture">3.1.2 管道架构 PipelineArchitecture</h4><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250715105907327.png"alt="3.1.2 管道架构" /><figcaption aria-hidden="true">3.1.2 管道架构</figcaption></figure><p>管道架构，又称为管道与过滤器架构（Pipes and FiltersArchitecture），是一种用于处理数据流的强大模式。它的核心思想非常直观，就像一条工厂的流水线：原材料从一端进入，经过一系列独立工站的加工、处理、检验，最终在另一端形成成品。</p><p>要理解管道架构，首先要理解它的两个基本构件：</p><ul><li><strong>过滤器(Filter)</strong>：它是一个独立的、可执行的处理单元，负责接收数据、执行单一任务（例如转换格式、过滤内容、扩充信息），然后将处理后的数据传递出去。关键在于，每个过滤器都是<strong>自包含（Self-Contained）</strong>和<strong>无状态（Stateless）</strong>的，它不关心上一个过滤器是谁，也不关心下一个过滤器是谁。</li><li><strong>管道(Pipe)</strong>：代表流水线上的"传送带"。它是一个<strong>单向</strong>的数据通道，负责将一个过滤器处理完的数据传递给下一个过滤器。</li></ul><p>过滤器一般又分为 4 种：</p><ul><li><strong>生产者 (Producer /Source)</strong>：作为整条管道的<strong>起点</strong>。它不接收来自管道的数据，而是负责创建数据，并将这些初始数据泵入管道。</li><li><strong>转换器(Transformer)</strong>：它从上游管道接收数据，对其进行某种形式的<strong>修改或转换</strong>，然后将结果发送到下游管道。</li><li><strong>测试器(Tester)</strong>：它接收数据，并根据一个或多个条件对数据进行<strong>检验</strong>。如果数据满足条件，就将其传递到下游管道；如果不满足，则数据流在此处被中断（或被导向另一条错误处理管道）。</li><li><strong>消费者 (Consumer /Sink)</strong>：作为整条管道的<strong>终点</strong>。它从上游管道接收最终处理好的数据，并将其消费掉，通常不会再将数据传递出去。</li></ul><p>优点:</p><ul><li><strong>成本低且简单</strong>：作为一种单体架构，管道架构不具备分布式架构风格所带来的复杂性，因此它简单易懂，并且构建和维护成本相对较低。</li><li><strong>高模块化</strong>：通过不同过滤器类型之间关注点的分离，实现了架构的模块化。任何过滤器都可以修改或替换而不影响其他过滤器。</li><li><strong>部署性和可测试性较好</strong>：由于其模块化程度较高，部署性和可测试性略优于分层架构，但仍受单体应用固有的部署仪式、风险和测试完整性等因素的影响。</li></ul><p>缺点:</p><ul><li><strong>单体特性带来的限制</strong>：尽管在模块化方面有所改进，但它仍然是一种单体应用。这意味着部署的仪式感、风险、部署频率以及测试的完整性都会受到单体特性的影响。例如，对任何更改都需要测试和部署整个单体应用。</li><li><strong>弹性低</strong>：由于其单体部署和缺乏架构模块化，管道架构的弹性评级非常低（一星）。尽管可以在单体内部实现某些功能的伸缩，但这通常需要复杂的设计技术，而管道架构并不擅长此道。</li><li><strong>可伸缩性差</strong>：与弹性类似，由于是单体架构且缺乏模块化，可伸缩性也评级很低。应用程序的伸缩能力受限于单一系统量子。</li><li><strong>性能一般</strong>：管道架构不适合高性能系统，因为它缺乏并行处理能力、存在闭合分层（closedlayering）以及可能出现"架构下沉"（sinkhole anti-pattern）问题。</li></ul><h4 id="微核架构-microkernel-architecture">3.1.3 微核架构 MicrokernelArchitecture</h4><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250716105151041.png"alt="3.1.3 微核架构" /><figcaption aria-hidden="true">3.1.3 微核架构</figcaption></figure><p>微核架构，也被称为<strong>插件化架构（Plug-inArchitecture）</strong>，是一种能够提供极高扩展性、灵活性和演化能力的系统设计模式。它的核心思想是将系统功能划分为两部分：一个最小化的、稳定的<strong>核心系统（CoreSystem）</strong>和一个由独立<strong>插件组件（Plug-inComponents）</strong>构成的可扩展生态。</p><ul><li><strong>核心系统 (CoreSystem)</strong>：这是架构的"微核"。它的职责被严格限制在最小且必要的范围内，通常只包含：<ol type="1"><li>系统运行所必需的通用业务逻辑（例如，一个 IDE的文件管理和基础编辑器）。</li><li>一个至关重要的<strong>插件管理机制</strong>，包括插件的注册、发现、生命周期管理等。这是连接核心与插件的桥梁。</li></ol></li><li><strong>插件组件 (Plug-inComponents)</strong>：这些是独立的、可插拔的模块，用于实现<strong>扩展功能或特定业务逻辑</strong>。每个插件都通过一个由核心系统定义的<strong>标准契约（StandardContract）</strong>来与核心交互。这个契约通常是一个接口或一组 API。</li></ul><p>优点：</p><ul><li><strong>高模块化与扩展性</strong>：微内核架构通过插件组件实现了高度模块化和扩展性。应用程序逻辑被划分为核心系统和独立的插件组件，从而提供了可扩展性、适应性以及应用程序特性和自定义处理逻辑的隔离。任何插件都可以修改或替换而不影响其他组件，例如，添加一个新的电子设备评估逻辑只需添加一个新的插件组件并更新注册表。</li><li><strong>成本较低且相对简单</strong>：作为一种单体架构，微内核架构避免了分布式架构风格所带来的复杂性，因此它简单易懂，并且构建和维护成本相对较低。</li><li><strong>部署性和可测试性较好</strong>：由于其模块化程度较高，功能可以隔离到独立的插件组件中。如果做得好，这可以减少整体测试范围并降低部署风险，尤其是在运行时部署插件组件的情况下。因此，可部署性和可测试性略优于分层架构。</li><li><strong>领域与架构的同构性</strong>：微内核架构可以<strong>同时进行领域分区和技术分区</strong>。对于需要针对每个位置或客户端进行不同配置的问题，或者那些强调用户定制和功能扩展性的产品（例如Jira 或Eclipse IDE），这种架构风格非常适用。</li></ul><p>缺点：</p><ul><li><strong>单体特性带来的限制</strong>：尽管在模块化方面有所改进，但它<strong>仍然是一种单体应用</strong>。这意味着部署的仪式感、风险、部署频率以及测试的完整性都会受到单体特性的影响。</li><li><strong>弹性低</strong>：由于其单体部署和缺乏架构模块化，微内核架构的<strong>弹性评级非常低</strong>（一星）。尽管可以在单体内部实现某些功能的伸缩，但这通常需要复杂的设计技术。</li><li><strong>可伸缩性差</strong>：与弹性类似，由于是单体架构且缺乏模块化，可伸缩性也<strong>评级很低</strong>（一星）。所有请求都必须<strong>通过核心系统才能到达独立的插件组件</strong>。</li></ul><h4 id="基于服务的架构-service-based-architecture">3.1.4 基于服务的架构Service-Based Architecture</h4><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250717114456233.png"alt="3.1.4 基于服务的架构 SBA" /><figcaption aria-hidden="true">3.1.4 基于服务的架构 SBA</figcaption></figure><p>如果说单体（Monolith）和微服务（Microservices）是两个广为人知的端点，那么基于服务的架构（Service-BasedArchitecture,SBA）就是它们之间那个常常被忽略，却又极具现实意义的"务实中间派"。它既非庞大到笨拙，也非精细到繁杂，为许多成长中的系统提供了一条平滑的演进路径。</p><p>SBA的本质是一种将一个大型的单体应用，<strong>分解为少数几个、逻辑独立的、可独立部署的"服务"</strong>的架构风格。SBA 的服务数量通常不多，一般在 <strong>4 到 12个</strong>之间。它不像微服务那样追求极致的拆分（可能会有几十上百个服务），而是将应用按照<strong>核心的业务领域（Domain）</strong>进行划分。</p><p>与微服务不同的是 SBA的典型实现是，所有服务共享<strong>同一个数据库</strong>。这种设计的初衷是为了在享受独立部署带来的好处的同时，最大限度地<strong>降低数据层面的复杂性</strong>。共享数据库可以：</p><ul><li><strong>简化开发</strong>：开发者无需处理复杂的分布式事务和跨服务数据同步问题。</li><li><strong>保证数据一致性</strong>：传统的 ACID事务可以在数据库层面轻松实现。</li><li><strong>降低技术门槛</strong>：团队无需掌握复杂的分布式数据管理技术。</li></ul><p>随着业务发展，共享数据库的弊端会逐渐显现。在以下情况下，拆分数据库就成了合理的选择：</p><ol type="1"><li><strong>服务资源争用 (ServiceContention)</strong>：某个服务（如高流量的商品浏览服务）对数据库产生巨大压力，影响了其他关键服务（如订单服务）的性能。</li><li><strong>数据隔离与安全 (Data Isolation andSecurity)</strong>：某个服务处理的数据高度敏感（如支付服务中的金融信息），需要从主数据库中物理隔离出来，以满足合规性或安全要求。</li><li><strong>技术栈不匹配 (TechnologyMismatch)</strong>：某个服务有特殊的数据存储需求。例如，搜索服务最适合使用Elasticsearch，而核心业务数据则存储在关系型数据库中。</li></ol><p>当这些情况发生时，SBA允许你"渐进式"地将某个服务连同其数据一起剥离出去，赋予它独立的数据库。</p><p>优点：</p><ul><li><strong>可部署性(Deployability)</strong>：这是最大的优势之一。每个服务都可以独立部署，使得发布更加频繁、风险更低。</li><li><strong>模块化(Modularity)</strong>：通过按领域划分服务，实现了清晰的业务模块边界。</li><li><strong>可维护性(Maintainability)</strong>：每个服务的代码库规模远小于整个单体，更易于理解、修改和维护。</li><li><strong>容错性 (FaultTolerance)</strong>：一个服务的崩溃不会导致整个应用程序宕机（尽管共享数据库可能成为共同的故障点）。</li><li><strong>保留ACID事务</strong>：这是其相对于其他细粒度分布式架构（如微服务）的一大优势。由于领域服务是粗粒度的，事务通常限制在一个服务内部，可以利用传统的ACID 事务来保证<strong>数据完整性和一致性</strong>。</li></ul><p>缺点：</p><ul><li><strong>弹性低</strong>：尽管可以在单体内部实现某些功能的伸缩，但由于其单体部署和缺乏架构模块化，弹性评级仍然较低。</li><li><strong>可伸缩性受限</strong>：虽然可以扩展，但由于服务粒度较粗，与微服务等细粒度服务相比，在机器资源方面效率不高，成本效益也较低。</li><li><strong>部署风险</strong>：虽然比传统单体应用有所改进，但由于部署的代码量仍然较大，其<strong>部署风险</strong>仍然高于微服务架构。</li></ul><h4 id="事件驱动架构-event-driven-architecture">3.1.5 事件驱动架构Event-Driven Architecture</h4><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250718110824820.png"alt="3.1.5 事件驱动架构" /><figcaption aria-hidden="true">3.1.5 事件驱动架构</figcaption></figure><p>在传统的<strong>请求驱动模型</strong>中，系统接收请求后会确定性地、同步地将请求路由到各个请求处理器来处理数据。而事件驱动模型则不同，它<strong>对特定情况做出反应，并根据该事件采取行动</strong>。</p><p>EDA 的力量源泉来自于异步通信，它有以下优点：</p><ol type="1"><li><strong>极高的系统韧性与可用性 (Resiliency andAvailability)</strong>：在同步调用中，如果服务 B 宕机，服务 A的调用会立刻失败，导致整个链路中断。但在异步模式下，服务 A将事件发送给一个中间人（消息代理），然后就可继续自己的工作。即使服务 B此时宕机，事件也会被安全地存放在代理中，待 B恢复后再进行处理。这使得系统能够优雅地处理局部故障，整体可用性大大提高。</li><li><strong>卓越的可伸缩性与弹性 (Scalability andElasticity)</strong>：生产者和消费者被完全解耦，可以独立进行伸缩。如果事件产生的速度突然加快，我们只需要增加消费者实例的数量即可，而无需对生产者做任何改动。这种按需、独立伸缩的能力是构建高弹性系统的关键。</li></ol><p>典型的 EDA 有 2种拓扑，分别为代理模式（broker）和中介者模式（mediator），二者最大的区别在于后者具有一个统一的协调者，这会对异常处理、全局统筹有很好的管控手段，当同时也牺牲了系统的解耦程度、灵活度和性能。</p><p>在 EDA 中，有几个典型的问题需要关注：</p><ul><li><p><strong>异常处理</strong>：可采用 workflow event pattern工作流事件模式。事件处理后，如果失败了，就告知<code>workflow process</code>。<code>workflow processor</code>识别错误，如果能自动处理，就自动处理，并丢回原始队列中，重新执行。如果不能处理，就放到dashbord 上，人工检查、校正或重试。</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250725163511396.png"alt="workflow event pattern 工作流事件模式" /><figcaption aria-hidden="true">workflow event pattern工作流事件模式</figcaption></figure></li><li><p><strong>数据丢失</strong>：发送事件到 channel 的路上、channel转发事件到处理器的路上和处理器处理完持久化到 db的路上都有可能发生数据的丢失。可以通过同步发送、持久化队列、ACK机制和事务型 DB 来解决这个问题。</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250725163644273.png"alt="防止 EDA 数据丢失的思路" /><figcaption aria-hidden="true">防止 EDA 数据丢失的思路</figcaption></figure></li><li><p><strong>返回响应</strong>：如果希望在事件驱动架构中实现请求-响应的能力，可以消息的两个元数据字段：<strong>回复地址(Reply-To)</strong> 和 <strong>关联标识 (Correlation ID)</strong>来通过回传通道返回响应数据。</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250718132839093.png"alt="EDA 返回响应数据的处理思路" /><figcaption aria-hidden="true">EDA 返回响应数据的处理思路</figcaption></figure></li></ul><p>优点：</p><ul><li><strong>可伸缩性与弹性 (Scalability &amp;Elasticity)</strong>：独立伸缩组件的能力是其核心优势。</li><li><strong>可扩展性(Extensibility)</strong>：系统极易扩展。当需要增加新功能时，只需开发一个新的服务来订阅感兴趣的现有事件即可，完全无需改动已有服务。</li><li><strong>响应性(Responsiveness)</strong>：对于需要快速响应用户的系统，可以将耗时任务异步化。例如，用户提交视频后，系统立即返回"上传成功，正在处理中"，然后通过事件驱动后台的转码、审核等一系列复杂流程。</li></ul><p>缺点：</p><ul><li><strong>简单性 (Simplicity)</strong>：EDA显著增加了系统的复杂性。你需要管理消息代理，处理异步编程的挑战（如调试、错误处理），并应对最终一致性带来的心智负担。</li><li><strong>事务性(Transactional)</strong>：实现跨多个服务的原子性操作（即分布式事务）变得异常困难。虽然可以通过Saga等模式来模拟长事务，但其实现复杂，且只能保证最终一致性而非强一致性。</li><li><strong>工作流的可观测性 (Observability ofWorkflow)</strong>：尤其是在代理拓扑中，业务流程被分散到各个独立的处理器中，没有一个集中的地方可以让你直观地看到一个完整的业务流程是如何执行的，这给监控和排错带来了巨大挑战。</li></ul><h4 id="空间架构-space-based-architecture">3.1.6 空间架构 Space-BasedArchitecture</h4><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250721175147426.png"alt="3.1.6 空间架构" /><figcaption aria-hidden="true">3.1.6 空间架构</figcaption></figure><p>传统三层 Web 拓扑在用户量剧增时呈倒三角：Web层易横向扩容，数据库层最难扩容，最终成为性能上限。为削弱数据库瓶颈，业界先用本地缓存，再出现集中式分布式缓存，但网络跳转仍是热点。把数据直接放到每个处理节点的<strong>复制型内存网格</strong>并实时同步，才真正让数据库从"同步路径"上消失，空间架构由此成形。</p><p>空间架构的名称来源于<strong>元组空间（TupleSpace）</strong>多个并行处理器通过共享内存进行通信。SBA的核心理念便是将应用数据保存在内存中（in-memory），并在所有活跃的处理单元（ProcessingUnits）复制，从而移除中心数据库作为同步约束，实现近乎无限的伸缩性。</p><p>空间架构由以下几个部分组成：</p><ul><li><p><strong>处理单元 Processing Unit：</strong></p><ul><li><p>处理单元包含了<strong>应用逻辑</strong>（包括基于 Web的组件和后端业务逻辑）。</p></li><li><p>它还包含一个<strong>内存数据网格</strong>和<strong>复制引擎</strong>，通常由Hazelcast、Apache Ignite 或 Oracle Coherence 等产品实现。</p></li><li><p>处理单元可以包含小型、单一用途的服务，类似于微服务</p></li></ul></li><li><p><strong>虚拟化中间件 VirtualizedMiddleware：</strong>虚拟化中间件负责处理架构中的基础设施问题，控制数据同步和请求处理。它由以下四个关键组件组成：</p><ul><li><p><strong>消息网格（MessagingGrid）</strong>：它负责将请求转发到任何可用的处理单元。</p></li><li><p><strong>数据网格（Data Grid）</strong>：它是 SBA中最重要和关键的组件，通常在处理单元内部以复制缓存的形式实现。它确保每个处理单元都包含完全相同的数据，数据复制是异步且快速的。</p></li><li><p><strong>处理网格（ProcessingGrid）</strong>：这是一个可选组件，用于管理<strong>协调请求处理</strong>，当一个业务请求涉及多个处理单元时，它会协调这些处理单元之间的请求。</p></li><li><p><strong>部署管理器（DeploymentManager）</strong>：该组件根据负载条件管理处理单元实例的<strong>动态启动和关闭</strong>，对于实现应用的弹性伸缩至关重要。</p></li></ul></li><li><p><strong>数据泵 DataPumps：</strong>数据泵是<strong>将数据发送到另一个处理器，然后该处理器更新数据库</strong>的方式。它们总是<strong>异步</strong>的，提供内存缓存与数据库之间的<strong>最终一致性（EventualConsistency）</strong>。消息机制是数据泵的常用实现方式，因为它支持异步通信、保证消息传递和维护消息顺序。</p></li><li><p><strong>数据写入器 Data Writers：</strong>数据写入器（DataWriters）负责接收来自数据泵的消息，并用消息中包含的信息更新数据库。它们可以是服务、应用或数据中心（如AbInitio）。写入器的粒度可以根据数据泵和处理单元的范围而变化，例如，领域驱动的数据写入器可以处理特定领域（如客户）内的所有更新。</p></li><li><p><strong>数据读取器 DataReaders：</strong>负责从数据库读取数据，并通过反向数据泵将其发送到处理单元。服务需要通过数据读取器访问数据的情况有三种：</p><ol type="1"><li>所有相同命名缓存的处理单元实例都崩溃时。</li><li>所有相同命名缓存的处理单元需要重新部署时。</li><li>需要检索复制缓存中不包含的归档数据时。</li></ol></li></ul><p>空间架构最大的一个问题就是<strong>数据冲突</strong>，不同的processing unit处理同一个业务逻辑相关的数据时，由于数据同步存在时序问题，所以很容易出现数据不一致的情况。</p><p>可以从以下几个因素进行冲突概率的评估：</p><ul><li>N：处理相同缓存的 processing unit 的数量</li><li>UR：缓存更新频率</li><li>S：缓存大小</li><li>RL：缓存复制的延迟</li></ul><blockquote><p>CollisitionRate = N × (UR<sup>2</sup>/S) × RL</p></blockquote><p>如果估算出来的冲突概率无法接受，或者需要缓存在内存中的业务数据过多而超过单机负载时，也可以使用<strong>分布式缓存</strong>来替代复制缓存。</p><p>优点：</p><ul><li><strong>弹性（Elasticity）</strong>：处理单元可以根据负载动态启停，实现高度弹性。</li><li><strong>伸缩性（Scalability）</strong>：通过内存数据缓存和移除数据库约束，支持处理数百万并发用户。</li><li><strong>性能（Performance）</strong>：移除了数据库瓶颈，提供了极高的性能。</li></ul><p>缺点：</p><ul><li><strong>简洁性（Simplicity）</strong>：SBA是一种<strong>非常复杂的架构风格</strong>，因为它涉及到缓存、最终一致性以及众多动态组件。</li><li><strong>可测试性（Testability）</strong>：由于需要模拟极高的伸缩性和弹性负载，<strong>测试复杂且成本高昂</strong>，许多高负载测试甚至需要在生产环境中进行，带来巨大风险。</li><li><strong>成本（Cost）</strong>：由于缓存产品许可费和高资源利用率，SBA通常相对昂贵。</li></ul><h4id="面向服务架构-orchestration-driven-service-oriented-architecture">3.1.7面向服务架构 Orchestration-Driven Service-Oriented Architecture</h4><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250722110031795.png"alt="3.1.7 面向服务架构" /><figcaption aria-hidden="true">3.1.7 面向服务架构</figcaption></figure><p>编排驱动的面向服务架构（Orchestration-Driven Service-OrientedArchitecture，简称SOA）是一种在特定时代背景下演变而来的软件架构风格。它在 20 世纪 90年代末企业快速扩张、需要更复杂的 IT 系统来适应增长的背景下出现。</p><ul><li><strong>资源稀缺性</strong>：在开源操作系统尚未被认为足够可靠用于严肃工作之前，操作系统和商业数据库服务器的许可费用昂贵且按机器收费。这导致架构师们被要求尽可能地实现<strong>重用</strong>，以优化成本。</li><li><strong>企业级重用</strong>：SOA的一个主要目标是实现服务层面的重用，即逐步构建可随时间增量重用的业务行为。大型公司厌倦了重复编写软件，因此采取了逐步解决这个问题的策略。</li><li><strong>技术分层</strong>：这种架构风格也将<strong>技术分层</strong>理念推向了极致。其驱动哲学围绕着企业级的重用展开。</li></ul><p>这个架构在历史进程中是一个反面教材，它是核心思想就俩字：<strong>复用</strong>！</p><p>失败的最核心原因：过度重视技术，以技术为导向进行模块划分和复用尝试，而业务是不断演进变化的，最终技术与业务之间的隔阂无法弥补，功亏一篑。</p><p>其他原因还有：</p><ul><li>过度追求复用导致的高度耦合</li><li>编排引擎成为巨大的耦合点和瓶颈</li><li>技术分区带来的业务流程碎片化</li></ul><h4 id="微服务架构-microservice-architecture">3.1.8 微服务架构Microservice Architecture</h4><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250723111539943.png"alt="3.1.8 微服务架构" /><figcaption aria-hidden="true">3.1.8 微服务架构</figcaption></figure><p>微服务架构的核心在于<strong>高度解耦</strong>。它<strong>倾向于复制而非耦合</strong>。这意味着，如果架构师的目标是高度解耦，那么他们会选择复制而不是重用。微服务通过物理上建模限界上下文（BoundedContext）的逻辑概念来实现高度解耦。</p><p>限界上下文（BoundedContext）是微服务设计理念的核心驱动力。这是一个愿与领域驱动设计（DDD）的概念。限界上下文代表了一种<strong>解耦</strong>风格。在限界上下文内，与特定领域相关的所有内部组件（如代码和数据库模式）都是紧密耦合的，但它们与外部限界上下文的任何内容（如其他数据库或类定义）是<strong>解耦</strong>的。</p><p>这种隔离使得每个服务可以<strong>独立演进</strong>，定义其自身所需的一切，而不必适应其他部分的约束。它<strong>避免了传统单体架构中常见的共享类和数据库作为集成点导致的紧密耦合问题</strong>。</p><p>所以微服务也是一个典型的领域分区架构，并且它倾向于将领域分区推到极致。</p><p>在划分微服务粒度时，以下三个方面是需要重点考虑的：</p><ol type="1"><li><strong>目的（Purpose）</strong>：微服务的首要目的应该是<strong>捕获一个领域或工作流</strong>。理想情况下，每个微服务都应该具有<strong>极高的功能内聚性</strong>，为整个应用程序贡献一个<strong>重要的行为</strong>。这意味着，服务应该专注于一个单一的、明确的业务功能。</li><li><strong>事务（Transactions）</strong>：限界上下文是业务工作流，通常需要<strong>在事务中协作的实体</strong>可以为服务边界提供良好的指示。由于分布式事务在分布式架构中会带来复杂性，架构师应尽量设计系统以<strong>避免跨服务的事务</strong>。如果需要跨服务事务，这可能表明服务粒度过细。事务边界通常是服务粒度的常见指标。</li><li><strong>通信（Communication）</strong>：如果一组服务为了完成功能而需要<strong>大量通信</strong>，那么将这些服务捆绑成一个更大的服务可能有助于<strong>避免过度的通信开销</strong>。换句话说，如果服务变得过于“多话”（chatty），频繁地相互调用，那么它们的边界可能需要重新评估，以减少不必要的<strong>全局复杂性</strong>。</li></ol><p>此外，业界也有一些其他的常用的判断方法：</p><ol type="1"><li><strong>变更频率</strong>：把一起变更/部署的东西放在一个服务，频率不同的拆开。</li><li><strong>耦合指标</strong>：如果拆分后跨服务调用暴增，说明拆太细；反之，如果内部复杂度过高且团队协作困难，可能太粗。</li><li><strong>认知负荷</strong>：一个团队能完全理解并独立维护的范围通常就是一个合理服务边界。</li></ol><p>在微服务架构中，有几个典型的问题需要关注：</p><ul><li><p><strong>基础设施复用</strong>：虽然微服务倾向于复制而非耦合，不过这更多是在业务层面，对于运维层面的基础设施，包括但不限于：<strong>监控（Monitoring）</strong>、<strong>日志记录（Logging）</strong>、<strong>断路器（CircuitBreakers）</strong>和<strong>服务发现（ServiceDiscovery）</strong>，微服务是主张进行统一建设和复用的。</p></li><li><p><strong>服务协作方式</strong>：一般有编舞和编排 2种协作方式：</p><ul><li><strong>编舞（Choreography）</strong>：是指多个服务<strong>相互之间直接通信</strong>，而<strong>没有中央协调器</strong>。服务（如同舞者）根据彼此发出的事件或信息自主响应和行动。</li><li><strong>编排（Orchestration）</strong>：是指通过一个<strong>单独的协调器服务</strong>来管理和控制工作流中多个服务的协调。协调器（如同乐队指挥）负责指导每个服务的执行顺序，并处理整个业务流程的状态和错误。在微服务中，架构师可以创建<strong>局部化的协调器服务</strong>来处理复杂的业务流程。</li></ul><p>微服务两者都支持。不过编舞方式更符合微服务的高度解耦哲学，因为它不依赖于中央协调器，而是通过解耦的事件来实现通信，使用起来更简便。当然，在复杂的业务流程中，<strong>编舞环境下的错误处理和协调会变得更加复杂</strong>。如果业务流程<strong>本质上是耦合的</strong>，此时编排可能更为适合。</p></li><li><p><strong>数据一致性：</strong>微服务主张尽可能避免分布式事务的问题，如果多个服务经常需要处理分布式事务问题，那最好将它们合而为一，直接在一个ACID 事务中完成。在万不得已的时候，也可以采用如 saga和最终一致性、人工补偿等方式来缓解数据一致性问题。</p></li></ul><p>优点：</p><ul><li><strong>高度解耦与小部署单元</strong>：微服务架构极力推崇<strong>高度解耦</strong>。每个服务都是<strong>极小的部署单元</strong>，且具备<strong>高度的独立性</strong>。这种解耦使得团队可以独立地开发、测试和部署服务，大大减少了对其他服务的依赖，从而提高了敏捷性。</li><li><strong>DevOps 革命与自动化</strong>：微服务架构的成功离不开<strong>DevOps革命和对操作关注点的自动化</strong>。自动化部署、自动化测试等现代工程实践是微服务存在的基础，它们极大地提高了部署频率、降低了部署风险，并保证了测试的完整性。</li><li><strong>更快的变更响应速度</strong>：由于服务范围小且高度解耦，当业务需求发生变化时，团队只需修改受影响的少量服务，而不是整个大型单体。这种<strong>增量式的演进</strong>能力使得组织能够<strong>更快地响应市场变化，提高时间到市场（time-to-market）的速度</strong>。</li><li><strong>单一职责与清晰边界</strong>：每个微服务都专注于一个<strong>单一的业务功能或领域</strong>。这种清晰的职责边界使得开发人员更容易理解、测试和维护代码，因为他们不必处理与服务无关的复杂性</li></ul><p>缺点：</p><ul><li><strong>网络调用开销（Network CallOverhead）</strong>：微服务是分布式架构。这意味着服务之间（乃至用户界面与服务之间）的通信需要通过网络进行。网络调用比本地方法调用耗时更长。当一个业务请求需要链式调用多个微服务时，累积的网络延迟会显著影响整体响应时间。</li><li><strong>安全验证开销（Security VerificationOverhead）</strong>：在微服务架构中，由于每个服务都是独立的部署单元，因此每个服务端点都需要进行安全验证。这增加了额外的处理时间。这种“在每个入口处进行安全检查”的模式进一步降低了同步、高度分布式架构（如微服务）的性能。</li><li><strong>高复杂性（Complexity）</strong>：作为一种分布式架构，微服务固有的缺点在于运行时连接各个部分所带来的复杂性，为了解决由此带来了一系列问题，需要学习、使用甚至开发一系列的组件，会给团队带来更大的心智负担和运维难度。</li><li><strong>数据一致性（DataConsistency）</strong>：如上所述，但无法避免分布式事务时，为了处理数据一致性问题，会引入很大的非业务复杂性。</li></ul><h3 id="架构选择">3.2 架构选择</h3><p>软件架构第一原理：<font color="red"><strong>一切都是权衡</strong></font>。</p><p>软件架构第二原理：<font color="red"><strong>为什么比如何更重要</strong></font>。</p><p>在选择架构时，最典型的 3 个问题：</p><ol type="1"><li>单体还是分布式架构？</li><li>数据存在哪里？</li><li>异步还是同步通信？</li></ol><h4 id="单体-vs-分布式">3.2.1 单体 vs 分布式</h4><p>当团队规模有限、需求节奏温和，而且必须尽快交付可用版本时，单体依旧是上市速度最快且认知成本最低的形态：所有模块共用同一进程，Debug、部署、回滚都异常直接。</p><p>然而，随着业务子域越来越多、发布节奏愈发碎片化，巨石应用往往演变成"所有人都必须一起上线或一起停机"的瓶颈。此时把系统拆成若干服务，允许各自独立发布，能显著缓解排期冲突；同时也可以针对流量热点的子域单独扩容，而非整包扩容。</p><p>带来的复杂度在于网络调用、链路追踪、容错和 DevOps自动化，一旦这些配套不到位，分布式的优势就会被运维复杂度和认知成本抵消。换言之，拆分前要先确认组织是否具备持续交付、自动化监控、故障演练等能力，否则分布式只会把"技术债"换成"组织债"。</p><h4 id="数据存储">3.2.2 数据存储</h4><p>如果系统只处理核心交易并且对强一致性要求极高，一体化的关系数据库依旧能提供最成熟、最易掌控的事务保障。随着并发数和存储量攀升，分库分表成为横向扩展的常规做法，但需要额外的分布式事务模式或Saga 来保证业务完整性。</p><p>如果读写模式呈现极强的峰谷或结构多变，就非常适合引入键值、文档、列式乃至时序、图数据库等多模型共存策略。这样做的关键在于为每一类数据访问场景挑选最经济的存储形式，同时在数据治理层面清晰定义数据主权、法务合规和生命周期。</p><h4 id="同步-vs-异步">3.2.3 同步 vs 异步</h4><blockquote><p><strong>一般原则：优先使用同步通信，必要时使用异步通信。</strong></p></blockquote><p><strong>同步调用</strong>（如 REST 或gRPC）带来的是即时反馈和易于调试的调用链，适用于用户交互需要立刻响应的场合。然而它也拉高了两个服务在时间维度上的耦合：只要任意环节超时或故障，整个链路都会受影响。</p><p><strong>异步消息</strong>则通过中间件把调用方与被调用方解耦，让系统可以削峰填谷并获得天然的弹性缓冲区；代价是业务体验不再“即时”，而且需要额外处理幂等、重复消费、消息顺序、死信等问题。通常情况下，读取或修改单一资源这一类“命令/查询”仍倾向同步；任务排队、事件通知、工作流编排与数据集成则更适合异步。若核心场景必须保证强一致性，仍可采用同步事务或锁；而能够容忍短暂的不一致时，则转而采用事件驱动的最终一致模式。</p><h3 id="风险评估">3.3 风险评估</h3><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250725170843538.png"alt="架构风险评估矩阵" /><figcaption aria-hidden="true">架构风险评估矩阵</figcaption></figure><p><strong>风险的影响面（Impact）</strong>：这个维度主要评估一旦风险发生，会对系统、业务、用户产生多大程度的负面影响。</p><ul><li><strong>低影响（Low）：</strong>影响范围小，可控性强。例如，某个非核心模块的性能略有下降，只影响少量用户，且有明确的降级方案。或者，故障恢复时间短，对整体业务影响微乎其微。</li><li><strong>中影响（Medium）：</strong>影响范围较大，但可控。例如，系统某个核心功能出现短暂不可用，影响部分用户，但可以通过人工干预或备用方案快速恢复。业务运营会受到一定影响，但不会造成灾难性的损失。</li><li><strong>高影响（High）：</strong>影响范围广，失控性强。例如，系统核心服务大面积宕机，导致业务全面停止。或者，数据出现严重损坏，造成不可挽回的损失。</li></ul><p><strong>风险出现的可能性（Likelihood）</strong>：这个维度主要评估风险发生的概率。</p><ul><li><strong>低可能性（Low）：</strong>发生概率很小。例如，系统依赖的某个成熟、稳定的第三方服务，过去几年从未出现过故障。或者，经过充分的测试和验证，某个技术方案的潜在问题已经被基本排除。</li><li><strong>中可能性（Medium）：</strong>发生概率一般。例如，某个新技术或新组件，虽然经过了小规模测试，但在大规模生产环境下的表现还未得到充分验证。或者，架构依赖的某个外部系统，其SLA（服务等级协议）历史记录显示偶尔会出现短暂的抖动。</li><li><strong>高可能性（High）：</strong>发生概率很高。例如，在高峰期对数据库进行无主键大批量更新操作，必然会导致锁表和性能问题。或者，系统设计存在明显的单点故障，一旦该节点出现问题，整个系统就会瘫痪。</li></ul><p>在分析时，不要企图一次性对所有的架构特性进行分析，拆开了，逐一击破，避免一次性关注点太多，从而不知所向。</p><h3 id="架构决策">3.4 架构决策</h3><h4 id="anti-pattern1-covering-your-assets">3.4.1 Anti-Pattern1:Covering Your Assets</h4><blockquote><p>害怕承担责任，总是希望有更高级别的人来拍板。决策过程变得极其缓慢，甚至为了规避风险而选择最保守、最平庸的技术方案，而不是最合适的方案。</p></blockquote><p>应对方案：</p><ul><li><strong>Fact（事实）:</strong>聚焦于客观事实和数据。在做技术选型或架构决策时，不要只凭感觉或经验，而是要基于事实，如性能测试报告、技术预研结果、业界最佳实践、开源社区活跃度等。当所有人都基于事实说话时，决策的对错就更容易被评估和追溯，而非个人责任。</li><li><strong>Options（可选方案）:</strong>明确列出所有可行的备选方案，并分析它们的优缺点、成本、风险和收益。当一个决策有多个清晰的选项时，团队可以共同讨论和权衡，而不是只盯着一个保守方案不放。</li></ul><p>实践建议：</p><ul><li><strong>建立决策评审机制：</strong> 明确谁是最终的决策者（DRI -Directly ResponsibleIndividual），并设立评审环节。评审会上，每个人都应基于数据和事实来论证自己的观点。</li><li><strong>鼓励小步快跑和 PoC：</strong>对于有争议的技术方案，可以先用小规模的PoC（概念验证）项目来验证其可行性。用实际结果说话，而不是让大家停留在理论争辩。</li></ul><h4 id="anti-pattern2-groundhog-day">3.4.2 Anti-Pattern2: GroundhogDay</h4><blockquote><p>团队成员在每次会议上都重复同样的讨论，无法达成共识。由于没有明确的决策记录或决策依据，导致下一次讨论又回到原点。</p></blockquote><p>应对方案：</p><ul><li><strong>Subject（主题）:</strong>在每次讨论前，都必须有一个明确的、聚焦的<strong>Subject</strong>。这次会议要讨论什么？目标是什么？是决定数据库选型？还是讨论消息队列的方案？有了明确的主题，才能避免讨论跑偏。</li><li><strong>Decision（决策）:</strong> 讨论结束后，必须得出一个明确的<strong>Decision</strong>。决策是什么？为什么做出这个决策？这个决策有哪些局限性？明确记录下来，并让所有人都知晓。</li></ul><p>实践建议：</p><ul><li><strong>会议纪要：</strong>每次关键的架构讨论后，都必须有正式的会议纪要。纪要中要包含：<strong>讨论主题、所有备选方案、最终决策、决策依据以及未被采纳方案的理由</strong>。</li><li><strong>设立时间限制：</strong>在讨论时，可以为每个议题设定一个时间限制。如果超过时间仍无法达成一致，可以先暂停，让大家会后去搜集更多数据，再进行下一轮讨论。</li></ul><h4 id="anti-pattern3-email-driven-architecture">3.4.3 Anti-Pattern3:Email-Driven Architecture</h4><blockquote><p>重要的架构决策都散落在团队成员的邮件、聊天记录或者 Wiki的各个角落，没有一个集中的、可检索的知识库。当新成员加入或需要回顾历史决策时，很难找到完整的信息。</p></blockquote><p>应对方案：</p><ul><li><strong>Subject（主题） 和 Decision（决策）:</strong>这两个元素是解决这个问题的核心。架构决策不应该只是一个口头或邮件的结论，而是一个完整的<strong>ADR（Architecture Decision Record）</strong>。ADR本身就是一个以主题和决策为核心的文档。</li></ul><p>实践建议：</p><ul><li><strong>建立 ADR 制度：</strong> 强烈建议引入 ADR 机制。</li><li><strong>使用统一的知识管理平台：</strong> 将所有 ADR存放在一个统一的、可检索的知识管理平台（如飞书文档, Wiki 或Git）。这样，团队成员可以轻松地查阅历史决策，新成员也能快速理解系统的演进过程。</li></ul><h4 id="架构决策记录-adr">3.4.4 架构决策记录 ADR</h4><p><img src="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/fosa-adr.png" alt="架构决策记录 ADR" style="zoom:33%;" /></p><p><strong>TITLE（标题）</strong></p><ul><li><strong>解释：</strong> 标题应该简短、清晰地描述这个 ADR的核心决策是什么。</li><li><strong>示例：</strong> "使用 RabbitMQ 替代 Kafka 作为消息队列" 或"将数据库从 MySQL 切换到 PostgreSQL"。</li><li><strong>作用：</strong>让读者一眼就能明白这份文档的主题。一个好的标题本身就包含了<strong>Subject</strong>。</li></ul><p><strong>STATUS（状态）</strong></p><ul><li><strong>解释：</strong> ADR 的生命周期状态。通常包括以下几种：<ul><li><strong>Proposed（提案中）：</strong>决策还在讨论阶段，尚未被团队接受。</li><li><strong>Accepted（已接受）：</strong>决策已经通过，可以开始实施。</li><li><strong>Superseded（已废弃）：</strong> 这个决策已经被新的 ADR替代。这对于追踪架构演变历史非常重要。这样要<u><strong>链接到新的ADR</strong></u>，方便追溯！</li></ul></li><li><strong>作用：</strong>帮助团队成员了解该决策的当前状态，避免对过时或仍在讨论中的方案产生误解。</li></ul><p><strong>CONTEXT（背景）</strong></p><ul><li><strong>解释：</strong>为什么要做出这个决策？它试图解决什么问题？这里应该详细描述问题的来龙去脉、约束条件以及技术或业务驱动因素。</li><li><strong>示例：</strong> "我们现有的系统在处理高并发订单时，MySQL数据库的写入性能出现了瓶颈，导致订单处理延迟。"</li><li><strong>作用：</strong> 提供决策的<strong>Fact</strong>（事实），让读者理解决策背后的原因，而不是孤立地看待决策本身。</li></ul><p><strong>DECISION（决策）</strong></p><ul><li><strong>解释：</strong>明确描述最终的决策是什么，并给出相应的理由。这个部分是整个 ADR的核心。</li><li><strong>示例：</strong>"我们决定将订单处理服务从同步调用改为异步消息队列。备选方案是采用Kafka，但我们最终选择了 RabbitMQ，原因是 RabbitMQ具有更完善的路由机制和更稳定的交付保障，更适合我们对消息可靠性的高要求。"</li><li><strong>作用：</strong> 记录决策的 <strong>Decision</strong> 和<strong>Options</strong>。它清晰地表明我们做了什么选择，以及为什么没有选择其他方案。</li></ul><p><strong>CONSEQUENCES（影响）</strong></p><ul><li><strong>解释：</strong>这个决策会带来什么后果？包括积极的和消极的。</li><li><strong>示例：</strong><ul><li><strong>积极影响：</strong>"订单处理性能将得到显著提升，系统的可扩展性增强。"</li><li><strong>消极影响：</strong> "引入 RabbitMQ会增加运维复杂性，团队需要学习新的技术栈。需要额外投入人力进行开发和部署。"</li></ul></li><li><strong>作用：</strong>帮助团队全面评估决策的利弊，提前预见潜在的风险和挑战。这与我们之前讨论的风险评估中的「风险的影响面」有异曲同工之妙。</li></ul><p><strong>COMPLIANCE（遵循）</strong></p><ul><li><strong>解释：</strong>如何确保团队会遵循这个决策？这个部分更多是关于实践和治理。</li><li><strong>示例：</strong> "新开发的订单服务必须通过 RabbitMQ进行异步通信。代码评审时，需要检查是否遵守此规范。运维团队需要负责RabbitMQ 集群的部署和监控。"</li><li><strong>作用：</strong>将抽象的决策转化为具体的行动和规范，确保决策能够真正落地。</li></ul><p><strong>NOTES（备注）</strong></p><ul><li><strong>解释：</strong>用于记录一些额外的元数据，例如：文档的作者、创建日期、链接到相关的 Jira工单或会议记录等。</li><li><strong>作用：</strong> 便于管理和追溯文档。</li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"><span class="strong">**ADR #001 - 使用 RabbitMQ 替代 Kafka 作为消息队列**</span></span></span><br><span class="line"><span class="section">---</span></span><br><span class="line"><span class="strong">**TITLE（标题）**</span></span><br><span class="line"></span><br><span class="line">将消息队列从 Kafka 切换至 RabbitMQ</span><br><span class="line"></span><br><span class="line"><span class="strong">**STATUS（状态）**</span></span><br><span class="line"></span><br><span class="line">Accepted（已接受）</span><br><span class="line"></span><br><span class="line"><span class="strong">**CONTEXT（背景）**</span></span><br><span class="line"></span><br><span class="line">我们现有的订单服务在业务高峰期时，订单创建和扣减库存的同步处理流程出现了严重的性能瓶颈。MySQL 数据库的写入操作成为单点瓶颈，导致订单处理延迟增加，甚至出现超时。为了解决这一问题，我们决定引入消息队列，将订单创建的后续流程（如库存扣减、积分发放）改为异步处理。</span><br><span class="line"></span><br><span class="line">在技术选型阶段，团队提出了两个主要的备选方案：Kafka 和 RabbitMQ。我们希望找到一个能满足以下需求的消息队列：</span><br><span class="line"></span><br><span class="line"><span class="bullet">1.</span>  <span class="strong">**高可靠性：**</span> 消息不能丢失，即使在消费者故障或重启时。</span><br><span class="line"><span class="bullet">2.</span>  <span class="strong">**消息时效性：**</span> 消息需要被及时处理，不接受长时间的延迟。</span><br><span class="line"><span class="bullet">3.</span>  <span class="strong">**灵活的路由：**</span> 能够根据不同的业务场景，将消息发送到不同的消费者。</span><br><span class="line"><span class="bullet">4.</span>  <span class="strong">**易于运维：**</span> 团队需要能快速上手，运维成本不能过高。</span><br><span class="line"></span><br><span class="line"><span class="strong">**DECISION（决策）**</span></span><br><span class="line"></span><br><span class="line">我们决定采用 <span class="strong">**RabbitMQ**</span> 作为新的消息队列，用于实现订单处理流程的异步化。</span><br><span class="line"></span><br><span class="line"><span class="strong">**核心理由：**</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">*</span> <span class="strong">**消息路由的灵活性：**</span> RabbitMQ 提供了多种 exchange 类型（如 direct, fanout, topic），可以实现非常灵活的消息路由。这使得我们可以轻松地根据不同的订单类型或业务事件（例如，秒杀订单、普通订单）将消息发送到不同的消费者队列，满足未来的业务扩展需求。</span><br><span class="line"><span class="bullet">*</span> <span class="strong">**消息的可靠性：**</span> RabbitMQ 提供了成熟的持久化机制（Durable Queues）和消息确认机制（Publisher Confirms），能确保即使在 RabbitMQ 本身或消费者故障时，消息也不会丢失。这对订单处理这种核心业务至关重要。</span><br><span class="line"><span class="bullet">*</span> <span class="strong">**团队学习曲线：**</span> 团队成员在内部技术分享中对 RabbitMQ 的概念（exchange, queue, binding）有了一定的了解，学习成本相对可控。</span><br><span class="line"></span><br><span class="line"><span class="strong">**备选方案的局限性：**</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">*</span> <span class="strong">**Kafka：**</span> Kafka 的核心设计思想是基于日志和分区，其路由能力相对较弱，主要通过 topic 和分区来实现消息分发。虽然可以通过消费者组来实现负载均衡，但在某些复杂路由场景下，需要额外的开发工作来适配。同时，Kafka 在保证单条消息的精确可靠投递方面，实现起来比 RabbitMQ 复杂一些，而这正是我们当前业务最关注的点。</span><br><span class="line"></span><br><span class="line"><span class="strong">**CONSEQUENCES（影响）**</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">*</span> <span class="strong">**积极影响：**</span></span><br><span class="line"><span class="bullet">    *</span> 显著提升订单处理的并发能力和吞吐量，缓解数据库写入瓶颈。</span><br><span class="line"><span class="bullet">    *</span> 提升系统的可扩展性，未来可以方便地增加更多异步消费者服务。</span><br><span class="line"><span class="bullet">    *</span> 系统的响应时间将大大缩短，提升用户体验。</span><br><span class="line"></span><br><span class="line"><span class="bullet">*</span> <span class="strong">**消极影响：**</span></span><br><span class="line"><span class="bullet">    *</span> 引入 RabbitMQ 会增加系统的运维复杂性，需要额外的监控和维护工作。</span><br><span class="line"><span class="bullet">    *</span> 团队需要投入时间学习和掌握 RabbitMQ 的相关知识，尤其是如何处理消费者故障、消息死信等问题。</span><br><span class="line"><span class="bullet">    *</span> 系统架构复杂度增加，需要重新设计和实现订单服务与消息队列的集成部分。</span><br><span class="line"></span><br><span class="line"><span class="strong">**COMPLIANCE（遵循）**</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">*</span> 所有与订单相关的异步化处理流程，必须通过 RabbitMQ 进行通信。</span><br><span class="line"><span class="bullet">*</span> 新的服务代码必须严格遵循消息持久化和确认机制，以确保消息不丢失。</span><br><span class="line"><span class="bullet">*</span> 运维团队负责 RabbitMQ 集群的部署、监控和维护，并确保其高可用性。</span><br><span class="line"><span class="bullet">*</span> 在代码评审时，需要确保新引入的异步化服务遵循此 ADR 的设计规范。</span><br><span class="line"></span><br><span class="line"><span class="strong">**NOTES（备注）**</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">*</span> <span class="strong">**作者：**</span> Gemini AI</span><br><span class="line"><span class="bullet">*</span> <span class="strong">**创建日期：**</span> 2025-07-26</span><br><span class="line"><span class="bullet">*</span> <span class="strong">**关联工单：**</span> PROJECT-1234 - 订单服务高并发性能优化</span><br><span class="line"><span class="bullet">*</span> <span class="strong">**相关会议记录：**</span> 架构评审会议 [2025-07-25]</span><br></pre></td></tr></table></figure><h2 id="设计实施路径与验证机制">4. 设计实施路径与验证机制</h2><blockquote><p>实施计划：是否需要技术原型 (PoC)来验证关键难点？如何进行任务拆解和里程碑规划？</p><p>构建适用度函数：针对第二步定义的关键架构特性，设计具体的检验尺。</p><p>知识沉淀：准备好核心的架构图、设计文档等。</p></blockquote><h3 id="实施计划">4.1 实施计划</h3><p><strong>技术原型 (PoC)来验证关键难点</strong>：架构师应频繁进行概念验证(PoC)，以验证架构决策的可行性，并深入了解实施细节。PoC有助于比较不同解决方案，并评估性能、可伸缩性等架构特性。建议架构师在进行PoC时编写生产质量的代码，这是架构师可以用于保持编码手感的有效手段，同时一次性的PoC 代码往往会成为团队的参考架构。</p><p><strong>任务拆解和里程碑规划</strong>：组件识别和架构设计是一个迭代过程，通过反馈不断优化。<strong>敏捷方法论</strong>支持迭代开发和快速反馈，有助于架构师在实践中调整决策。架构师还需要平衡架构工作和实际编码，通过<strong>委派核心路径代码</strong>，避免成为团队瓶颈。</p><h3 id="适应度函数">4.2 适应度函数</h3><p><strong>适应度函数是架构治理的核心工具</strong>。它是一种<strong>客观的函数</strong>，用于衡量代码复杂度和架构特性，并<strong>自动化验证</strong>开发团队是否遵循了架构决策和设计原则。适应度函数应<strong>集成到CI/CD流程中</strong>，在代码集成时自动检查合规性，从而避免问题积累。</p><ul><li><strong>检测循环依赖</strong>：可编写适应度函数来检测并防止组件之间的循环依赖，因为这会损害模块化（例如，使用<strong>JDepend</strong>工具）。这有助于维护架构中“重要但不紧急”的实践。</li><li><strong>分层架构合规性</strong>：利用<strong>ArchUnit</strong>（Java）或<strong>NetArchTest</strong>（.NET）等工具，可以确保分层架构中各层之间的访问限制被遵守。例如，限制表现层不能直接访问数据库，而必须通过业务层和持久层。</li><li><strong>验证距主序列距离</strong>：通过适应度函数验证代码抽象性与不稳定性之间的平衡。</li><li><strong>自动化编码标准合规性</strong>：例如，检查特定类是否包含必需的注解。</li></ul><h3 id="知识沉淀">4.3 知识沉淀</h3><ul><li><strong>ADR</strong>：将每一次关键决策及其动机、权衡、后果记录下来，形成可检索的决策日志。</li><li><strong><a href="https://c4model.com/">C4架构图</a></strong>：在每个里程碑输出更新后的系统上下文、容器、组件图，配合ADR 链接。</li></ul><h3 id="管理松紧度">4.4 管理松紧度</h3><p>架构师需要根据团队实际情况采用恰到好处的管理松紧度，才能发挥团队的最大潜力。</p><p>采取哪种管理松紧度，可以从几个方面进行考量：</p><ul><li><strong>teamfamiliarity</strong>：团队内部的熟悉程度，越不熟悉，越需要更多投入。</li><li><strong>team size</strong>：团队大小，团队越大， 越需要投入。</li><li><strong>overallexperience</strong>：团队经验，新人越多，越需要投入。</li><li><strong>project complexity</strong>：项目越复杂，越需要投入。</li><li><strong>projectduration</strong>：项目周期，周期越长，越需要投入。</li></ul><p>按照这 5 个方面，极限 tight 是 20 分，极限 loose 是 -20分，进行综合评价，看看自己是应该扮演什么角色。</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250727144525753.png"alt="管理松紧度计算表盘" /><figcaption aria-hidden="true">管理松紧度计算表盘</figcaption></figure><h2 id="部署观测与效果衡量">5. 部署、观测与效果衡量</h2><blockquote><p>持续交付：作为将设计快速、可靠地部署到生产环境的手段。</p><p>系统监控：观测系统的健康状况（CPU、内存、延迟、错误率等）。</p><p>业务指标验证：验证是否达成了第一步定义的商业目标？例如，新架构上线后，用户转化率是否真的提升了？</p></blockquote><h3 id="持续交付与部署自动化">5.1 持续交付与部署自动化</h3><p>持续交付 (CI/CD) 是架构落地的关键环节。FOSA强调，现代软件架构的成功离不开 DevOps革命和对操作关注点的自动化。持续交付不仅仅是技术实践，更是组织文化的体现。</p><p>核心要素：</p><ul><li><p>自动化构建与测试：每次代码提交都触发自动化的构建、单元测试、集成测试流程，确保代码质量。</p></li><li><p>环境一致性：通过容器化技术（如Docker）和基础设施即代码（IaC）确保开发、测试、生产环境的一致性。</p></li><li><p>渐进式部署：采用蓝绿部署、金丝雀发布等策略，降低部署风险，实现零停机时间。</p></li><li><p>快速回滚机制：当新版本出现问题时，能够快速回滚到上一个稳定版本。</p></li></ul><p>架构师职责：</p><ul><li><p>设计适合团队规模的 CI/CD 流水线</p></li><li><p>确保架构决策能够通过自动化流程得到验证</p></li><li><p>平衡部署频率与系统稳定性</p></li></ul><h3 id="系统监控与可观测性">5.2 系统监控与可观测性</h3><p>可观测性 (Observability) 是现代分布式系统的生命线。FOSA指出，在分布式架构中，一个请求可能会流经数十个甚至上百个服务，要诊断一个问题，需要建立复杂的可观测性体系。</p><p>三大支柱：</p><ol type="1"><li><p>指标 (Metrics)：量化系统性能的关键指标</p><ul><li><p>RED方法：Rate（请求率）、Error（错误率）、Duration（延迟）</p></li><li><p>USE方法：Utilization（利用率）、Saturation（饱和度）、Errors（错误）</p></li><li><p>业务指标：用户转化率、订单成功率、收入增长率</p></li></ul></li><li><p>日志 (Logs)：记录系统运行时的详细信息</p><ul><li><p>结构化日志：使用 JSON 格式，便于机器解析</p></li><li><p>日志聚合：集中收集、存储和分析日志</p></li><li><p>日志级别：根据重要性设置不同的日志级别</p></li></ul></li><li><p>追踪 (Tracing)：追踪请求在分布式系统中的完整路径</p><ul><li><p>分布式追踪：为每个请求分配唯一ID，追踪其在整个调用链中的路径</p></li><li><p>链路追踪：记录服务间的调用关系和耗时</p></li><li><p>性能分析：识别系统瓶颈和性能热点</p></li></ul></li></ol><p>监控策略：</p><ul><li><p>分层监控：从基础设施层到应用层，建立完整的监控体系</p></li><li><p>告警策略：设置合理的告警阈值，避免告警疲劳</p></li><li><p>可视化仪表板：为不同角色提供定制化的监控视图</p></li></ul><h3 id="业务指标验证与闭环反馈">5.3 业务指标验证与闭环反馈</h3><p>业务指标验证是架构设计的闭环关键。FOSA强调，技术架构的最终目标是服务于业务价值，因此必须验证是否达成了第一步定义的商业目标。</p><p>验证流程：</p><ol type="1"><li>建立基线：在架构变更前，记录关键业务指标的当前状态</li><li>设定目标：基于第一步的商业目标，设定具体的量化指标</li><li>持续监控：在架构部署后，持续跟踪业务指标的变化</li><li>效果评估：定期评估架构变更对业务指标的实际影响</li></ol><p>常见业务指标：</p><ul><li><p>用户相关：日活跃用户数、用户留存率、用户满意度</p></li><li><p>业务相关：订单转化率、客单价、复购率</p></li><li><p>技术相关：系统可用性、响应时间、错误率</p></li></ul><p>A/B 测试策略：</p><ul><li><p>在架构变更时，可以考虑 A/B 测试来验证效果</p></li><li><p>对比新旧架构在相同条件下的业务表现</p></li><li><p>基于数据做出是否全面推广的决策</p></li></ul><h3 id="性能监控与容量规划">5.4 性能监控与容量规划</h3><p>性能监控是架构健康度的重要指标。FOSA指出，架构师需要持续监控系统的性能表现，并基于趋势进行容量规划。</p><p>关键性能指标：</p><ul><li><p>响应时间：P50、P95、P99 延迟</p></li><li><p>吞吐量：每秒处理的请求数</p></li><li><p>资源利用率：CPU、内存、磁盘、网络使用率</p></li><li><p>错误率：4xx、5xx 错误的比例</p></li></ul><p>容量规划方法：</p><ul><li><p>趋势分析：基于历史数据预测未来需求</p></li><li><p>压力测试：通过模拟高负载验证系统极限</p></li><li><p>弹性规划：设计自动扩缩容机制应对流量波动</p></li></ul><h2 id="复盘沉淀与演进">6. 复盘、沉淀与演进</h2><blockquote><p>问题记录与根因分析：发生了什么？为什么会发生？</p><p>流程与原则改进：如何优化我们的设计流程、技术原则，避免未来再犯？</p><p>人员与组织成长：团队通过这次项目学到了什么？需要组织哪些培训？</p></blockquote><h3 id="问题记录与根因分析">6.1 问题记录与根因分析</h3><p>根因分析 (Root Cause Analysis, RCA) 是架构演进的基础。FOSA强调，架构师需要建立系统性的问题记录和分析机制，避免同样的问题重复发生。</p><p>分析框架：</p><ol type="1"><li><p>5W1H 分析法</p><ul><li><p>What：发生了什么问题？</p></li><li><p>When：什么时候发生的？</p></li><li><p>Where：在哪个组件/服务中发生的？</p></li><li><p>Who：谁发现了这个问题？</p></li><li><p>Why：为什么会发生？</p></li><li><p>How：如何避免再次发生？</p></li></ul></li><li><p>鱼骨图分析</p><ul><li><p>人员因素：技能不足、沟通不畅</p></li><li><p>流程因素：流程缺陷、决策不当</p></li><li><p>技术因素：架构设计问题、技术选型错误</p></li><li><p>环境因素：基础设施问题、外部依赖故障</p></li></ul></li><li><p>时间线分析</p><ul><li><p>按时间顺序记录事件发展过程</p></li><li><p>识别关键决策点和转折点</p></li><li><p>分析因果关系链</p></li></ul></li></ol><p>问题分类：</p><ul><li><p>架构设计问题：组件划分不当、接口设计不合理</p></li><li><p>技术选型问题：技术栈不匹配、性能瓶颈</p></li><li><p>流程管理问题：决策流程不清晰、沟通机制缺失</p></li><li><p>人员技能问题：团队技能不足、知识传递不畅</p></li></ul><h3 id="流程与原则改进">6.2 流程与原则改进</h3><p>持续改进是架构师的核心职责。FOSA指出，架构师需要基于实践经验，不断优化设计流程和技术原则。</p><p>流程改进方法：</p><ol type="1"><li><p>回顾会议 (Retrospective)</p><ul><li><p>定期组织团队回顾会议</p></li><li><p>识别流程中的痛点和改进机会</p></li><li><p>制定具体的改进行动计划</p></li></ul></li><li><p>架构评审机制</p><ul><li><p>建立正式的架构评审流程</p></li><li><p>邀请相关方参与评审</p></li><li><p>记录评审决策和后续行动</p></li></ul></li><li><p>决策记录 (ADR) 更新</p><ul><li><p>定期回顾和更新 ADR</p></li><li><p>记录决策的后续影响和教训</p></li><li><p>为未来类似决策提供参考</p></li></ul></li></ol><p>原则演进：</p><ul><li><p>技术原则：基于实践经验更新技术选型原则</p></li><li><p>设计原则：优化组件划分和接口设计原则</p></li><li><p>流程原则：改进决策流程和沟通机制</p></li><li><p>质量原则：更新代码质量和测试策略</p></li></ul><h3 id="持续学习与团队领导">6.3 持续学习与团队领导</h3><p>组织学习是架构成功的关键。FOSA强调，架构师不仅要关注技术架构，更要关注团队和组织的成长。优秀的架构师通过培养团队能力和建立学习型组织，实现技术债务的持续偿还和架构能力的持续提升。</p><h4 id="分钟法则">6.3.1 20 分钟法则</h4><p>架构师需要持续学习以保持技术广度。FOSA指出，技术发展日新月异，架构师必须建立系统化的学习机制，避免技术视野的固化。</p><p><strong>20分钟法则</strong>：建议每天至少投入20分钟学习新知识或深入特定主题，以系统化地拓展技术广度。这种持续的小剂量学习比偶尔的集中学习更有效，能够保持技术敏锐度。</p><p>学习策略：</p><ul><li><p>技术深度与广度平衡：在保持一个技术领域的深度基础上，系统性地拓展技术广度</p></li><li><p>问题驱动学习：将实际工作中遇到的问题作为学习的起点</p></li><li><p>理论与实践结合：通过概念验证（PoC）验证新技术的适用性</p></li><li><p>跨领域学习：不仅学习技术，还要了解业务、管理、心理学等相关领域</p></li></ul><h4 id="个人技术雷达">6.3.2 个人技术雷达</h4><p>建立"个人雷达"可以帮助架构师系统化地评估和追踪新兴技术和实践，类似于ThoughtWorks 的技术雷达。</p><p>雷达分类：</p><ul><li><p>采用 (Adopt)：经过验证的技术，可以安全地在生产环境中使用</p></li><li><p>试用 (Trial)：有前景的技术，可以在非关键项目中尝试</p></li><li><p>评估 (Assess)：值得关注的技术，需要进一步研究和评估</p></li><li><p>保持 (Hold)：暂时不推荐使用的技术，但保持关注</p></li></ul><p>雷达维护：</p><ul><li><p>定期更新：每季度更新一次技术雷达</p></li><li><p>团队共享：与团队分享技术雷达，促进集体学习</p></li><li><p>决策参考：将技术雷达作为技术选型的重要参考</p></li></ul><h4 id="知识分享">6.3.3 知识分享</h4><p>架构师应通过以身作则而非仅仅凭借头衔来领导团队。他们可以通过主持"午餐分享会"(brown-bag lunches)来分享技术知识和经验，从而提升在团队中的领导力和影响力。</p><h4 id="团队健康监控与预警">6.3.4 团队健康监控与预警</h4><p>当出现以下 3个问题时，意味着团队已经开始进入不健康状态了，作为架构师，需要及时发现和解决团队协作中的问题。</p><p><strong>ProcessLoss（过程损失）</strong>：随着人数的增加，团队效率却在降低。</p><ul><li><p>表现：团队规模扩大后，沟通成本激增，决策效率下降</p></li><li><p>原因：信息传递链条过长，协调成本超过协作收益</p></li><li><p>解决方案：</p><ul><li><p>建立清晰的信息传递机制</p></li><li><p>采用敏捷方法，保持小团队结构</p></li><li><p>定期评估团队规模与效率的关系</p></li></ul></li></ul><p><strong>PluralisticIgnorance（多元无知）</strong>：当团队成员因为觉得自己没掌握某些信息的时候，对提出的方案不好提出拒绝，而只能在表面进行同意。</p><ul><li><p>表现：会议上大家都点头同意，但会后执行时遇到各种问题</p></li><li><p>原因：团队成员缺乏安全感，不敢提出质疑</p></li><li><p>解决方案：</p><ul><li><p>营造安全的讨论环境，鼓励质疑和提问</p></li><li><p>建立"魔鬼代言人"机制，专门负责提出反对意见</p></li><li><p>定期进行匿名反馈收集</p></li></ul></li></ul><p><strong>Diffusion ofResponsibility（责任扩散）</strong>：职责混乱，大家不知道谁应该为哪些东西负责任。</p><ul><li><p>表现：任务推诿，问题无人负责，决策无人执行</p></li><li><p>原因：角色定义不清晰，责任边界模糊</p></li><li><p>解决方案：</p><ul><li><p>建立明确的 RACI 矩阵（Responsible, Accountable, Consulted,Informed）</p></li><li><p>定期回顾和更新团队职责分工</p></li><li><p>建立问责机制，确保每个决策都有明确的责任人</p></li></ul></li></ul><h2 id="总结">总结</h2><p>架构设计一个系统性的六步工程过程，从商业理解到组织成长形成闭环。它强调"为什么"比"怎么做"更重要，要求架构师在理解利益相关方诉求和用户痛点的基础上，将模糊需求转化为可度量的技术目标，通过多方案权衡分析选择"最不差"而非"最佳"的架构方案，并建立持续交付、监控验证和复盘演进的机制，最终实现技术债务的持续偿还和团队能力的持续提升。整个方法论的核心是权衡取舍的艺术，以及架构师在技术决策中始终提供技术和业务双重理由的能力。</p>]]></content>
    
    
    <summary type="html">基于《Fundamentals of Software Architecture》内容，梳理出六步架构设计方法论，从商业理解到组织成长形成闭环，探讨架构师如何在权衡取舍中做出&quot;最不差&quot;的决策，以及如何通过持续交付、监控验证和复盘演进构建可持续的架构能力。</summary>
    
    
    
    <category term="读书笔记" scheme="https://hedon.top/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="读书笔记" scheme="https://hedon.top/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="fosa" scheme="https://hedon.top/tags/fosa/"/>
    
  </entry>
  
  <entry>
    <title>FOSA丨17丨微服务架构</title>
    <link href="https://hedon.top/2025/07/23/fosa/fosa-ch17/"/>
    <id>https://hedon.top/2025/07/23/fosa/fosa-ch17/</id>
    <published>2025-07-23T03:02:00.000Z</published>
    <updated>2025-07-23T03:34:46.645Z</updated>
    
    <content type="html"><![CDATA[<p>本系列文章通过逐章回答<ahref="https://fundamentalsofsoftwarearchitecture.com/">《Fundamentals ofSoftware Architecture》</a>（下文简称FOSA）一书中的课后思考题，来深入理解书中的核心概念和理论，从而提升我们的软件架构设计能力。本篇为<u>第十七章</u>内容。</p><p>本章的课后题是：</p><ol type="1"><li><p>Why is the bounded context concept so critical for microservicesarchitecture?</p><p>为什么限界上下文的概念对于微服务来说如此重要？</p></li><li><p>What are three ways of determining if you have the right level ofgranularity in a microservice?</p><p>在划分微服务粒度的时候，哪三个方面是你需要重点考虑的？</p></li><li><p>What functionality might be contained within a sidecar?</p><p>sidecar 有哪些功能？</p></li><li><p>What is the difference between orchestration and choreography?Which does microservices support? Is one communication style easier inmicroservices?</p><p>编舞（orchestration）和编排（choreography）的区别是什么？微服务支持哪种模式？在微服务中，哪种通信方式更简便？</p></li><li><p>What is a saga in microservices?</p><p>在微服务中，saga 是什么?</p></li><li><p>Why are agility, testability, and deployability so well supportedin microservices?</p><p>为什么敏捷性、可测试性和可部署性在微服务架构中表现良好？</p></li><li><p>What are two reasons performance is usually an issue inmicroservices?</p><p>在微服务中，性能问题的两个核心因素是什么？</p></li><li><p>Is microservices a domain-partitioned architecture or atechnically partitioned one?</p><p>微服务架构是领域分区还是技术分区？</p></li><li><p>Describe a topology where a microservices ecosystem might be onlya single quantum.</p><p>描述一种拓扑结构，其中微服务生态系统可能仅有一个架构量子。</p></li><li><p>How was domain reuse addressed in microservices? How wasoperational reuse addressed?</p><p>微服务中是如何解决领域复用问题的？又是如何解决运维复用问题的呢？</p></li></ol><hr /><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250723111539943.png"alt="FOSA Figure 17-1. The topologu of the microservices architecture style" /><figcaption aria-hidden="true">FOSA Figure 17-1. The topologu of themicroservices architecture style</figcaption></figure><h2 id="业务边界">业务边界</h2><blockquote><ol type="1"><li><p>Why is the bounded context concept so critical for microservicesarchitecture?</p><p>为什么限界上下文的概念对于微服务来说如此重要？</p></li><li><p>Is microservices a domain-partitioned architecture or atechnically partitioned one?</p><p>微服务架构是领域分区还是技术分区？</p></li></ol></blockquote><p>限界上下文（BoundedContext）是微服务设计理念的核心驱动力。微服务架构与领域驱动设计（DDD）紧密相关，尤其是受限界上下文概念的深刻影响。限界上下文代表了一种<strong>解耦</strong>风格。在限界上下文内，与特定领域相关的所有内部组件（如代码和数据库模式）都是紧密耦合的，但它们与外部限界上下文的任何内容（如其他数据库或类定义）是<strong>解耦</strong>的。</p><p>微服务架构的首要目标是<strong>高度解耦</strong>。它通过物理地建模限界上下文的逻辑概念来实现这一目标。这意味着，微服务架构鼓励将系统分解为<strong>独立的、自包含的服务</strong>，每个服务都对应一个特定的限界上下文。</p><p>这种隔离使得每个服务可以<strong>独立演进</strong>，定义其自身所需的一切，而不必适应其他部分的约束。它<strong>避免了传统单体架构中常见的共享类和数据库作为集成点导致的紧密耦合问题</strong>。</p><p>所以微服务也是一个典型的领域分区架构，并且它倾向于将领域分区推到极致。</p><h2 id="服务粒度">服务粒度</h2><blockquote><ol start="2" type="1"><li><p>What are three ways of determining if you have the right level ofgranularity in a microservice?</p><p>在划分微服务粒度的时候，哪三个方面是你需要重点考虑的？</p></li></ol></blockquote><p>在划分微服务粒度时，以下三个方面是需要重点考虑的：</p><ol type="1"><li><strong>目的（Purpose）</strong>：微服务的首要目的应该是<strong>捕获一个领域或工作流</strong>。理想情况下，每个微服务都应该具有<strong>极高的功能内聚性</strong>，为整个应用程序贡献一个<strong>重要的行为</strong>。这意味着，服务应该专注于一个单一的、明确的业务功能。</li><li><strong>事务（Transactions）</strong>：限界上下文是业务工作流，通常需要<strong>在事务中协作的实体</strong>可以为服务边界提供良好的指示。由于分布式事务在分布式架构中会带来复杂性，架构师应尽量设计系统以<strong>避免跨服务的事务</strong>。如果需要跨服务事务，这可能表明服务粒度过细。事务边界通常是服务粒度的常见指标。</li><li><strong>通信（Communication）</strong>：如果一组服务为了完成功能而需要<strong>大量通信</strong>，那么将这些服务捆绑成一个更大的服务可能有助于<strong>避免过度的通信开销</strong>。换句话说，如果服务变得过于“多话”（chatty），频繁地相互调用，那么它们的边界可能需要重新评估，以减少不必要的<strong>全局复杂性</strong>。</li></ol><p>书中还强调，<strong>迭代</strong>是确保良好服务设计的唯一途径，架构师很少能在第一次尝试时就发现完美的粒度、数据依赖和通信风格，只有不断适配业务发展、不断思考改善，才能设计出良好的架构。</p><p>此外，业界也有一些其他的常用的判断方法：</p><ol type="1"><li><strong>变更与部署频率一致性</strong>：把一起变更/部署的东西放在一个服务，频率不同的拆开。</li><li><strong>耦合/通信“积分器 vs.解耦器”指标</strong>：如果拆分后跨服务调用暴增（“chattiness”），说明拆太细；反之，如果内部复杂度过高且团队协作困难，可能太粗。</li><li><strong>团队/认知负荷</strong>：一个团队能完全理解并独立维护的范围通常就是一个合理服务边界。</li></ol><h2 id="基础设施">基础设施</h2><blockquote><ol start="3" type="1"><li><p>What functionality might be contained within a sidecar?</p><p>sidecar 有哪些功能？</p></li><li><p>How was domain reuse addressed in microservices? How wasoperational reuse addressed?</p><p>微服务中是如何解决领域复用问题的？又是如何解决运维复用问题的呢？</p></li></ol></blockquote><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250723111700037.png"alt="FOSA Figure 17-3. The service plane connects the sidecars in a service mesh" /><figcaption aria-hidden="true">FOSA Figure 17-3. The service planeconnects the sidecars in a service mesh</figcaption></figure><p>Sidecar 模式用于处理微服务中的<strong>通用运维关注点（operationalconcerns）</strong>，包括但不限于：<strong>监控（Monitoring）</strong>、<strong>日志记录（Logging）</strong>、<strong>断路器（CircuitBreakers）</strong>和<strong>服务发现（ServiceDiscovery）</strong>。这些功能由一个独立的组件处理，该组件可以由单个团队拥有，也可以由共享的基础设施团队拥有，从而实现了运维方面的复用。</p><p>而在领域复用中，由于微服务架构的主要目标是<strong>高度解耦</strong>。为了实现这一目标，微服务<strong>倾向于复制（duplication）而不是传统意义上的复用（reuse）</strong>。这意味着，对于通用实体（如<code>Address</code>类），微服务会<strong>避免共享公共类或数据库模式</strong>。相反，每个服务会在其自己的限界上下文内定义和管理其所需的数据和行为，即使这意味着某些概念的重复实现。这种策略牺牲了代码级别的复用，以换取服务之间更高的解耦度和独立演进的能力。</p><h2 id="服务协作">服务协作</h2><blockquote><ol start="4" type="1"><li><p>What is the difference between orchestration and choreography?Which does microservices support? Is one communication style easier inmicroservices?</p><p>编舞（orchestration）和编排（choreography）的区别是什么？微服务支持哪种模式？在微服务中，哪种通信方式更简便？</p></li></ol></blockquote><ul><li><p><strong>编舞（Choreography）</strong>：是指多个服务<strong>相互之间直接通信</strong>，而<strong>没有中央协调器</strong>。服务（如同舞者）根据彼此发出的事件或信息自主响应和行动。</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250723112521618.png"alt="FOSA Figure 17-7. Using choreography in microservices to manage coordination" /><figcaption aria-hidden="true">FOSA Figure 17-7. Using choreography inmicroservices to manage coordination</figcaption></figure></li><li><p><strong>编排（Orchestration）</strong>：是指通过一个<strong>单独的协调器服务</strong>来管理和控制工作流中多个服务的协调。协调器（如同乐队指挥）负责指导每个服务的执行顺序，并处理整个业务流程的状态和错误。在微服务中，架构师可以创建<strong>局部化的协调器服务</strong>来处理复杂的业务流程。</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250723112551443.png"alt="FOSA Figure 17-8. Using orchestration in microservices" /><figcaption aria-hidden="true">FOSA Figure 17-8. Using orchestration inmicroservices</figcaption></figure></li></ul><p>微服务两者都支持。不过编舞方式更符合微服务的高度解耦哲学，因为它不依赖于中央协调器，而是通过解耦的事件来实现通信，使用起来更简便。当然，在复杂的业务流程中，<strong>编舞环境下的错误处理和协调会变得更加复杂</strong>。如果业务流程<strong>本质上是耦合的</strong>，此时编排可能更为适合。</p><h2 id="一致性">一致性</h2><blockquote><ol start="5" type="1"><li><p>What is a saga in microservices?</p><p>在微服务中，saga 是什么?</p></li></ol></blockquote><p>在微服务中，<strong>Saga</strong>是一种<strong>分布式事务模式</strong>，用于管理跨多个服务的业务事务，因为在微服务中，跨服务边界的传统ACID 事务是不推荐的（甚至不可能的）。</p><p>Saga模式通过将一个业务流程分解为一系列<strong>本地事务</strong>来实现，每个本地事务由一个服务执行。</p><ul><li>如果某个本地事务失败，Saga 会通过执行<strong>补偿事务（compensatingtransactions）</strong>来撤销之前已成功的本地事务所做的更改，从而确保数据的一致性。</li><li>Saga 可以通过<strong>事件溯源（eventsourcing）</strong>或<strong>有限状态机（finite statemachines）</strong>来管理事务的状态。</li></ul><p>虽然 Saga 可以用于解决分布式事务问题，但也应<strong>谨慎使用 Saga模式</strong>，因为它会增加系统的复杂性，并且如果它成为架构中的主导特性，则可能表明服务粒度划分不当，违反了微服务解耦的核心原则。</p><h2 id="优点">优点</h2><blockquote><ol start="6" type="1"><li><p>Why are agility, testability, and deployability so well supportedin microservices?</p><p>为什么敏捷性、可测试性和可部署性在微服务架构中表现良好？</p></li></ol></blockquote><p>敏捷性（Agility）、可测试性（Testability）和可部署性（Deployability）在微服务架构中得到良好支持的原因主要有以下几点：</p><ul><li><strong>高度解耦与小部署单元</strong>：微服务架构极力推崇<strong>高度解耦</strong>。每个服务都是<strong>极小的部署单元</strong>，且具备<strong>高度的独立性</strong>。这种解耦使得团队可以独立地开发、测试和部署服务，大大减少了对其他服务的依赖，从而提高了敏捷性。</li><li><strong>DevOps 革命与自动化</strong>：微服务架构的成功离不开<strong>DevOps革命和对操作关注点的自动化</strong>。自动化部署、自动化测试等现代工程实践是微服务存在的基础，它们极大地提高了部署频率、降低了部署风险，并保证了测试的完整性。</li><li><strong>更快的变更响应速度</strong>：由于服务范围小且高度解耦，当业务需求发生变化时，团队只需修改受影响的少量服务，而不是整个大型单体。这种<strong>增量式的演进</strong>能力使得组织能够<strong>更快地响应市场变化，提高时间到市场（time-to-market）的速度</strong>。</li><li><strong>单一职责与清晰边界</strong>：每个微服务都专注于一个<strong>单一的业务功能或领域</strong>。这种清晰的职责边界使得开发人员更容易理解、测试和维护代码，因为他们不必处理与服务无关的复杂性</li></ul><h2 id="缺点">缺点</h2><blockquote><ol start="7" type="1"><li><p>What are two reasons performance is usually an issue inmicroservices?</p><p>在微服务中，性能问题的两个核心因素是什么？</p></li></ol></blockquote><p>在微服务中，性能问题通常由以下两个核心因素导致：</p><ol type="1"><li><strong>网络调用开销（Network CallOverhead）</strong>：微服务是分布式架构。这意味着服务之间（乃至用户界面与服务之间）的通信需要通过网络进行。网络调用比本地方法调用耗时更长。当一个业务请求需要链式调用多个微服务时，累积的网络延迟会显著影响整体响应时间。</li><li><strong>安全验证开销（Security VerificationOverhead）</strong>：在微服务架构中，由于每个服务都是独立的部署单元，因此每个服务端点都需要进行安全验证。这增加了额外的处理时间。这种“在每个入口处进行安全检查”的模式进一步降低了同步、高度分布式架构（如微服务）的性能。</li></ol><p>尽管性能是微服务常见的问题，但可以通过<strong>数据缓存（caching）和数据复制（replication）</strong>等模式来减少不必要的网络调用，从而提高性能。</p><h2 id="架构量子">架构量子</h2><blockquote><ol start="9" type="1"><li><p>Describe a topology where a microservices ecosystem might be onlya single quantum.</p><p>描述一种拓扑结构，其中微服务生态系统可能仅有一个架构量子。</p></li></ol></blockquote><p>通常来讲，微服务架构都意味着存在多个架构量子。但如果其部署或通信模型导致了上述的紧密耦合，例如<strong>共享数据库</strong>或<strong>中央同步协调器</strong>，那么整个微服务生态系统仍可能被归类为一个单一量子。</p><p><strong>1. 共享单一数据库</strong>：</p><ul><li>如果<strong>所有微服务都共享一个单一的、中央化的数据库实例</strong>，那么整个系统很可能构成一个单一量子。在这种情况下，尽管服务是独立的部署单元，但数据库模式的任何更改都可能影响所有服务，导致它们<strong>无法独立演进和部署</strong>。这使得系统在部署和数据一致性方面表现得像一个整体。</li><li>例如，传统的<strong>分层单体（layeredmonolith）</strong>即使有多个逻辑层，但由于共享一个数据库，它也是一个单一量子。</li></ul><p><strong>2. 强制同步通信与中央协调器</strong>：</p><ul><li>在某些情况下，即使服务是分离的，如果它们之间存在<strong>大量强制的同步通信依赖（synchronousconnascence）</strong>，或者存在一个<strong>中央编排引擎（orchestrationengine）</strong>作为所有行为的巨大耦合点，那么整个系统也可能被视为一个单一量子。在这种拓扑中，如果一个服务调用另一个服务是同步的，那么这些服务的操作架构特性（例如，性能和可用性）必须在调用期间保持一致。中央协调器会限制架构中任何部分具有不同架构特性的能力。</li><li>例如，<strong>编排驱动的服务导向架构（Orchestration-DrivenService-Oriented Architecture,SOA）</strong>，即使是分布式架构，也通常只有一个量子，因为它普遍使用单一或少量数据库，并且其编排引擎作为巨大的耦合点，阻止了各个部分独立拥有不同的架构特性。</li></ul><h2 id="架构全貌">架构全貌</h2><p><strong>边界设计</strong>：限界上下文、团队边界。</p><p><strong>粒度与组件识别</strong>：功能内聚 vs.通信复杂度；量子范围思维。</p><p><strong>数据拥有权</strong>：每服务独立数据存储（数据库多样性）；避免共享表。</p><p><strong>通信风格</strong>：同步 vs. 异步；编排 vs. 编舞。</p><p><strong>一致性策略</strong>：最终一致性、Saga、补偿事务。</p><p><strong>弹性与可观测性</strong>：Sidecar/ServiceMesh、熔断、限流、Tracing。</p><p><strong>部署与运营</strong>：CI/CD、容器编排（K8s）、自动化测试策略。</p><p><strong>性能与成本权衡</strong>：网络开销、数据复制、缓存策略。</p><p><strong>治理与演化</strong>：契约测试、架构健身函数、可观测指标驱动重构。</p>]]></content>
    
    
    <summary type="html">本篇通过回答《Fundamentals of Software Architecture》第十七章的课后思考题，深入探讨微服务架构中限界上下文的核心作用、服务粒度划分的三大原则、sidecar模式的功能特性，以及编排与编舞的通信机制差异、saga分布式事务模式、微服务的敏捷性优势与性能挑战，帮助理解微服务架构的领域驱动设计理念和分布式系统复杂性，提升架构师在构建现代分布式系统时的微服务拆分能力和架构治理水平。</summary>
    
    
    
    <category term="架构设计" scheme="https://hedon.top/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    
    
    <category term="读书笔记" scheme="https://hedon.top/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="fosa" scheme="https://hedon.top/tags/fosa/"/>
    
    <category term="软件架构" scheme="https://hedon.top/tags/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>FOSA丨16丨面向服务架构</title>
    <link href="https://hedon.top/2025/07/22/fosa/fosa-ch16/"/>
    <id>https://hedon.top/2025/07/22/fosa/fosa-ch16/</id>
    <published>2025-07-22T03:02:00.000Z</published>
    <updated>2025-07-23T02:54:15.715Z</updated>
    
    <content type="html"><![CDATA[<p>本系列文章通过逐章回答<ahref="https://fundamentalsofsoftwarearchitecture.com/">《Fundamentals ofSoftware Architecture》</a>（下文简称FOSA）一书中的课后思考题，来深入理解书中的核心概念和理论，从而提升我们的软件架构设计能力。本篇为<u>第十六章</u>内容。</p><p>本章的课后题是：</p><ol type="1"><li><p>What was the main driving force behind service-orientedarchitecture?</p><p>SOA 的主要驱动力是什么？</p></li><li><p>What are the four primary service types within a service-orientedarchitecture?</p><p>SOA 的四种主要服务类型是什么？</p></li><li><p>List some of the factors that led to the downfall ofservice-oriented architecture.</p><p>列举一些导致 SOA 衰落的因素。</p></li><li><p>Is service-oriented architecture technically partitioned ordomain partitioned?</p><p>SOA 是技术分层还是领域分层？</p></li><li><p>How is domain reuse addressed in SOA? How is operational reuseaddressed?</p><p>SOA 中如何解决领域复用和操作复用问题？</p></li></ol><hr /><h2 id="背景">背景</h2><blockquote><ol type="1"><li><p>What was the main driving force behind service-orientedarchitecture</p><p>SOA 的主要驱动力是什么？</p></li><li><p>Is service-oriented architecture technically partitioned ordomain partitioned?</p><p>SOA 是技术分层还是领域分层？</p></li></ol></blockquote><p>编排驱动的面向服务架构（Orchestration-Driven Service-OrientedArchitecture，简称SOA）是一种在特定时代背景下演变而来的软件架构风格。它在 20 世纪 90年代末企业快速扩张、需要更复杂的 IT 系统来适应增长的背景下出现。</p><ul><li><strong>资源稀缺性</strong>：在开源操作系统尚未被认为足够可靠用于严肃工作之前，操作系统和商业数据库服务器的许可费用昂贵且按机器收费。这导致架构师们被要求尽可能地实现<strong>重用</strong>，以优化成本。</li><li><strong>企业级重用</strong>：SOA的一个主要目标是实现服务层面的重用，即逐步构建可随时间增量重用的业务行为。大型公司厌倦了重复编写软件，因此采取了逐步解决这个问题的策略。</li><li><strong>技术分层</strong>：这种架构风格也将<strong>技术分层</strong>理念推向了极致。其驱动哲学围绕着企业级的重用展开。</li></ul><h2 id="拓扑">拓扑</h2><blockquote><ol start="2" type="1"><li><p>What are the four primary service types within a service-orientedarchitecture?</p><p>SOA 的四种主要服务类型是什么？</p></li></ol></blockquote><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250722110031795.png"alt="FOSA Figure 16-1. Topology of orchestration-driven service-oriented architecture" /><figcaption aria-hidden="true">FOSA Figure 16-1. Topology oforchestration-driven service-oriented architecture</figcaption></figure><p>围绕企业级复用的目标，SOA 定义了以下几种服务类型：</p><ul><li><strong>业务服务（BusinessServices）</strong>：位于架构顶层，提供入口点，代表领域行为（例如ExecuteTrade 或PlaceOrder）。这些服务定义通常不包含代码，只包含输入、输出和模式信息，并由业务用户定义。</li><li><strong>企业服务（EnterpriseServices）</strong>：包含细粒度的共享实现，是构成粗粒度业务服务的构建块，并通过编排引擎连接起来（例如CreateCustomer、CalculateQuote）。其目标是构建可复用的原子行为，从而逐步建立可复用的企业资产集合。</li><li><strong>应用服务（ApplicationServices）</strong>：一次性、单一实现的服务，不要求与企业服务同等程度的复用和粒度。通常由单一应用团队拥有，用于解决特定应用需求。</li><li><strong>基础设施服务（InfrastructureServices）</strong>：提供操作层面的关注点，如监控、日志记录、身份验证和授权。这些服务通常是具体的实现，由共享的基础设施团队与运维团队紧密协作拥有。</li><li><strong>编排引擎（OrchestrationEngine）</strong>：作为分布式架构的核心，负责将业务服务实现通过编排串联起来，包括事务协调和消息转换等功能。它还充当集成中心，允许集成自定义代码、软件包和传统软件系统。由于这个机制是架构的核心，负责这个引擎的集成架构团队往往会成为组织内部的政治力量和官僚瓶颈<strong>...</strong>。</li></ul><h2 id="失败原因">失败原因</h2><blockquote><ol start="3" type="1"><li><p>List some of the factors that led to the downfall ofservice-oriented architecture.</p><p>列举一些导致 SOA 衰落的因素。</p></li><li><p>How is domain reuse addressed in SOA? How is operational reuseaddressed?</p><p>SOA 中如何解决领域复用和操作复用问题？</p></li></ol></blockquote><p>这个架构在历史进程中是一个反面教材，它是核心思想就俩字：复用！reuse。</p><p>失败的最核心原因：过度重视技术，以技术为导向进行模块划分和复用尝试，而业务是不断演进变化的，最终技术与业务之间的隔阂无法弥补，功亏一篑。其他原因还有：</p><ul><li>过度追求复用导致的高度耦合</li><li>编排引擎成为巨大的耦合点和瓶颈</li><li>技术分区带来的业务流程碎片化</li></ul><p>这里谈到了一对矛盾：复用和耦合。复用必定会带来耦合，解耦，会带来更多的重复。</p><p>在 SOA中，复用是其核心目标，但其实现方式也导致了架构的显著副作用：<strong>紧密耦合</strong>。</p><ul><li><strong>领域复用（Domain Reuse）</strong>：SOA通过抽象共享的业务概念（例如 <code>Customer</code>客户）为可重用服务来解决领域重用问题。其他服务会引用这些"规范的（canonical）"客户服务。</li><li><strong>操作复用（OperationalReuse）</strong>：通过基础设施服务（InfrastructureServices）尽可能地重用所有功能，无论是领域功能还是操作功能。</li></ul><p>然而，这种设计也带来了负面影响：当一个系统主要围绕重用构建时，组件之间也会产生大量的耦合。例如，对规范客户服务的更改可能会波及到所有其他引用该服务的服务，使得变更变得高风险和复杂。</p>]]></content>
    
    
    <summary type="html">本篇通过回答《Fundamentals of Software Architecture》第十六章的课后思考题，深入探讨面向服务架构的历史驱动力与核心理念、四种主要服务类型的特征与职责、SOA衰落的关键因素分析，以及技术分层与领域分层的架构特性、领域复用与操作复用的实现机制，帮助理解面向服务架构的企业级设计原理和服务编排思想，提升架构师在构建大型企业系统时的服务化架构选择能力和SOA设计水平。</summary>
    
    
    
    <category term="架构设计" scheme="https://hedon.top/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    
    
    <category term="读书笔记" scheme="https://hedon.top/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="fosa" scheme="https://hedon.top/tags/fosa/"/>
    
    <category term="软件架构" scheme="https://hedon.top/tags/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>FOSA丨15丨空间架构</title>
    <link href="https://hedon.top/2025/07/21/fosa/fosa-ch15/"/>
    <id>https://hedon.top/2025/07/21/fosa/fosa-ch15/</id>
    <published>2025-07-21T03:02:00.000Z</published>
    <updated>2025-07-21T12:37:08.795Z</updated>
    
    <content type="html"><![CDATA[<p>本系列文章通过逐章回答<ahref="https://fundamentalsofsoftwarearchitecture.com/">《Fundamentals ofSoftware Architecture》</a>（下文简称FOSA）一书中的课后思考题，来深入理解书中的核心概念和理论，从而提升我们的软件架构设计能力。本篇为<u>第十五章</u>内容。</p><p>本章的课后题是：</p><ol type="1"><li><p>Where does space-based architecture get its name from?</p><p>空间架构的名字从何而来？</p></li><li><p>What is a primary aspect of space-based architecture thatdifferentiates it from other architecture styles?</p><p>空间架构区别与其他架构的主要方面是什么？</p></li><li><p>Name the four components that make up the virtualized middlewarewithin a space-based architecture.</p><p>说出空间架构的虚拟化中间层的 4 个组成结构。</p></li><li><p>What is the role of the messaging grid?</p><p>消息网格的作用是什么？</p></li><li><p>What is the role of a data writer in space-basedarchitecture?</p><p>数据写入器在空间架构中的作用是什么？</p></li><li><p>Under what conditions would a service need to access data throughthe data reader?</p><p>一个服务在什么情况下需要通过数据读取器去获取数据？</p></li><li><p>Does a small cache size increase or decrease the chances for adata collision?</p><p>缓存越小，数据冲突概率是增大还是减小？</p></li><li><p>What is the difference between a replicated cache and adistributed cache? Which one is typically used in space-basedarchitecture?</p><p>复制缓存和分布式缓存的区别是什么？空间架构更倾向于使用哪个？</p></li><li><p>List three of the most strongly supported architecturecharacteristics in space- based architecture.</p><p>列出 3 个空间架构中非常优秀的架构特性。</p></li><li><p>Why does testability rate so low for space-basedarchitecture?</p><p>为什么空间架构的可测性较差？</p></li></ol><hr /><h2 id="背景">背景</h2><p>基于空间的架构（SBA）是一种专门为解决<strong>高伸缩性（Scalability）</strong>、<strong>高弹性（Elasticity）</strong>、<strong>高并发（HighConcurrency）</strong>、<strong>变动剧烈且不可预测</strong>的应用场景，例如在线票务系统或在线拍卖系统。</p><p>传统三层 Web 拓扑在用户量剧增时呈倒三角：Web层易横向扩容，数据库层最难扩容，最终成为性能上限。为削弱数据库瓶颈，业界先用本地缓存，再出现集中式分布式缓存，但网络跳转仍是热点。把数据直接放到每个处理节点的<strong>复制型内存网格</strong>并实时同步，才真正让数据库从"同步路径"上消失，空间架构由此成形。</p><p>空间架构的名称来源于<strong>元组空间（TupleSpace）</strong>多个并行处理器通过共享内存进行通信。SBA的核心理念便是将应用数据保存在内存中（in-memory），并在所有活跃的处理单元（ProcessingUnits）复制，从而移除中心数据库作为同步约束，实现近乎无限的伸缩性。</p><h2 id="拓扑">拓扑</h2><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250721175147426.png"alt="FOSA Figure 15-2. Space-based architecture basic topology" /><figcaption aria-hidden="true">FOSA Figure 15-2. Space-basedarchitecture basic topology</figcaption></figure><h3 id="处理单元-processing-unit">处理单元 Processing Unit</h3><ul><li>处理单元包含了<strong>应用逻辑</strong>（包括基于 Web的组件和后端业务逻辑）。</li><li>它还包含一个<strong>内存数据网格</strong>和<strong>复制引擎</strong>，通常由Hazelcast、Apache Ignite 或 Oracle Coherence 等产品实现。</li><li>处理单元可以包含小型、单一用途的服务，类似于微服务</li></ul><h3 id="虚拟化中间件-virtualized-middleware">虚拟化中间件 VirtualizedMiddleware</h3><p>虚拟化中间件负责处理架构中的基础设施问题，控制数据同步和请求处理。它由以下四个关键组件组成：</p><ul><li><strong>消息网格（MessagingGrid）</strong>：它负责将请求转发到任何可用的处理单元。</li><li><strong>数据网格（Data Grid）</strong>：它是 SBA中最重要和关键的组件，通常在处理单元内部以复制缓存的形式实现。它确保每个处理单元都包含完全相同的数据，数据复制是异步且快速的。</li><li><strong>处理网格（ProcessingGrid）</strong>：这是一个可选组件，用于管理<strong>协调请求处理</strong>，当一个业务请求涉及多个处理单元时，它会协调这些处理单元之间的请求。</li><li><strong>部署管理器（DeploymentManager）</strong>：该组件根据负载条件管理处理单元实例的<strong>动态启动和关闭</strong>，对于实现应用的弹性伸缩至关重要。</li></ul><h3 id="数据泵-data-pumps">数据泵 Data Pumps</h3><p>数据泵是<strong>将数据发送到另一个处理器，然后该处理器更新数据库</strong>的方式。它们总是<strong>异步</strong>的，提供内存缓存与数据库之间的<strong>最终一致性（EventualConsistency）</strong>。消息机制是数据泵的常用实现方式，因为它支持异步通信、保证消息传递和维护消息顺序。</p><h3 id="数据写入器-data-writers">数据写入器 Data Writers</h3><p>数据写入器（DataWriters）负责接收来自数据泵的消息，并用消息中包含的信息更新数据库。它们可以是服务、应用或数据中心（如AbInitio）。写入器的粒度可以根据数据泵和处理单元的范围而变化，例如，领域驱动的数据写入器可以处理特定领域（如客户）内的所有更新。</p><h3 id="数据读取器-data-readers">数据读取器 Data Readers</h3><p>负责从数据库读取数据，并通过反向数据泵将其发送到处理单元。服务需要通过数据读取器访问数据的情况有三种：</p><ol type="1"><li>所有相同命名缓存的处理单元实例都崩溃时。</li><li>所有相同命名缓存的处理单元需要重新部署时。</li><li>需要检索复制缓存中不包含的归档数据时。</li></ol><h2 id="数据冲突">数据冲突</h2><blockquote><p>不同的 processing unit处理同一个业务逻辑相关的数据时，由于数据同步存在时序问题，所以很容易出现数据不一致的情况。</p></blockquote><p>可以从以下几个因素进行冲突概率的评估：</p><ul><li>N：处理相同缓存的 processing unit 的数量</li><li>UR：缓存更新频率</li><li>S：缓存大小</li><li>RL：缓存复制的延迟</li></ul><p>CollisitionRate = N* (UR<sup>2</sup>/S) *RL</p><p>其中<strong>缓存大小越小，意味着缓存能够容纳的数据量越少，因此在给定的更新速率和复制延迟下，数据被频繁覆盖和发生冲突的几率就越高。</strong></p><h2 id="分布式缓存">分布式缓存</h2><p><strong>复制缓存</strong>：每个处理单元包含一个自己的内存数据网格，与其他共享相同命名缓存的处理单元同步。这是SBA通常采用的缓存模式，因为它提供高性能和高容错性。适用于小缓存大小（&lt;100MB）、低更新率和相对静态数据。</p><p><strong>分布式缓存</strong>：需要一个外部服务器或服务专门用于存放集中式缓存。它支持高水平的数据一致性，但性能较低（需要远程访问），且容错性存在问题（如果缓存服务器宕机）。适用于大缓存大小（&gt;500MB）、高度动态数据和高更新率。</p><h2 id="优点">优点</h2><ul><li><strong>弹性（Elasticity）</strong>：处理单元可以根据负载动态启停，实现高度弹性。</li><li><strong>伸缩性（Scalability）</strong>：通过内存数据缓存和移除数据库约束，支持处理数百万并发用户。</li><li><strong>性能（Performance）</strong>：移除了数据库瓶颈，提供了极高的性能。</li></ul><h2 id="缺点">缺点</h2><ul><li><strong>简洁性（Simplicity）</strong>：SBA是一种<strong>非常复杂的架构风格</strong>，因为它涉及到缓存、最终一致性以及众多动态组件。</li><li><strong>可测试性（Testability）</strong>：由于需要模拟极高的伸缩性和弹性负载，<strong>测试复杂且成本高昂</strong>，许多高负载测试甚至需要在生产环境中进行，带来巨大风险。</li><li><strong>成本（Cost）</strong>：由于缓存产品许可费和高资源利用率，SBA通常相对昂贵。</li></ul>]]></content>
    
    
    <summary type="html">本篇通过回答《Fundamentals of Software Architecture》第十五章的课后思考题，深入探讨空间架构的命名来源与核心特征、虚拟化中间层的组件构成、消息网格与数据读写器的协作机制，以及缓存策略选择、数据冲突管理和架构特性评估分析，帮助理解空间架构的分布式计算原理和高可扩展性设计思路，提升架构师在构建高性能分布式系统时的架构选择能力和空间化设计水平。</summary>
    
    
    
    <category term="架构设计" scheme="https://hedon.top/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    
    
    <category term="读书笔记" scheme="https://hedon.top/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="fosa" scheme="https://hedon.top/tags/fosa/"/>
    
    <category term="软件架构" scheme="https://hedon.top/tags/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>FOSA丨14丨事件驱动架构</title>
    <link href="https://hedon.top/2025/07/18/fosa/fosa-ch14/"/>
    <id>https://hedon.top/2025/07/18/fosa/fosa-ch14/</id>
    <published>2025-07-18T02:30:00.000Z</published>
    <updated>2025-07-18T05:34:41.927Z</updated>
    
    <content type="html"><![CDATA[<p>本系列文章通过逐章回答<ahref="https://fundamentalsofsoftwarearchitecture.com/">《Fundamentals ofSoftware Architecture》</a>（下文简称FOSA）一书中的课后思考题，来深入理解书中的核心概念和理论，从而提升我们的软件架构设计能力。本篇为<u>第十四章</u>内容。</p><p>本章的课后题是：</p><ol type="1"><li><p>What are the primary differences between the broker and mediatortopologies?</p><p>代理拓扑（broker）和中介者拓扑（mediator）两种拓扑的根本区别是什么？</p></li><li><p>For better workflow control, would you use the mediator or brokertopology?</p><p>为了更好的流程控制，你会选择代理拓扑还是中介者拓扑？</p></li><li><p>Does the broker topology usually leverage a publish-and-subscribemodel with topics or a point-to-point model with queues?</p><p>在代理拓扑中，是经常使用基于主题的发布订阅模式还是基于队列的点到点模式？</p></li><li><p>Name two primary advantage of asynchronous communications.</p><p>列出 2 个异步通信的主要优势。</p></li><li><p>Give an example of a typical request within the request-basedmodel.</p><p>举一个 request-based 模式的典型例子。</p></li><li><p>Give an example of a typical request in an event-based model.</p><p>举一个 event-based 模式的典型例子。</p></li><li><p>What is the difference between an initiating event and aprocessing event in event-driven architecture?</p><p>在事件驱动架构中，初始事件和处理中事件二者有什么不同？</p></li><li><p>What are some of the techniques for preventing data loss whensending and receiving messages from a queue?</p><p>有哪些技术可以防止在从队列发送和接收消息时丢失数据？</p></li><li><p>What are three main driving architecture characteristics forusing event-driven architecture?</p><p>使用事件驱动架构的三个主要驱动架构特性是什么？</p></li><li><p>What are some of the architecture characteristics that are notwell supported in event-driven architecture?</p><p>事件驱动架构不能很好地支持哪些架构特性？</p></li></ol><hr /><p>传统的软件设计如同一个等级森严的组织，组件 A 直接向组件 B<strong>下达命令</strong>（例如，调用一个函数或API）。而事件驱动架构则更像一个现代化的、扁平的协作网络。组件 A只是<strong>发布一个事实</strong>（嘿，我这里发生了一件事！），而其他对此事感兴趣的组件（B,C,D...）可以自行决定如何<strong>响应</strong>。这种从命令到响应的范式革命，是事件驱动架构（Event-DrivenArchitecture, EDA）的灵魂所在。</p><h2 id="异步通信">异步通信</h2><p>EDA 的力量源泉来自于异步通信，它有以下优点：</p><ol type="1"><li><strong>极高的系统韧性与可用性 (Resiliency andAvailability)</strong>：在同步调用中，如果服务 B 宕机，服务 A的调用会立刻失败，导致整个链路中断。但在异步模式下，服务 A将事件发送给一个中间人（消息代理），然后就可继续自己的工作。即使服务 B此时宕机，事件也会被安全地存放在代理中，待 B恢复后再进行处理。这使得系统能够优雅地处理局部故障，整体可用性大大提高。</li><li><strong>卓越的可伸缩性与弹性 (Scalability andElasticity)</strong>：生产者和消费者被完全解耦，可以独立进行伸缩。如果事件产生的速度突然加快，我们只需要增加消费者实例的数量即可，而无需对生产者做任何改动。这种按需、独立伸缩的能力是构建高弹性系统的关键。</li></ol><h2 id="拓扑">拓扑</h2><p>典型的 EDA 有 2 种拓扑，分别为：</p><ul><li>代理拓扑（broker）</li><li>中介者拓扑（mediator）</li></ul><h3 id="broker">broker</h3><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250718110824820.png"alt="FOSA Figure 14-2. Broker topology" /><figcaption aria-hidden="true">FOSA Figure 14-2. Brokertopology</figcaption></figure><p>一个典型的 broker 拓扑如上图所示，它包含以下几个部分：</p><ul><li><code>initiating event</code>：初始事件，它用于<strong>启动整个事件流</strong>，一般来源于系统外部。</li><li><code>event channel</code>：事件通道，用于传递事件，比如 Go 的channel，或者分布式系统中的消息队列，如 RabbitMQ、Kafka等。一个事件通道一般对应一个订阅主题（topic）。</li><li><code>event processor</code>：事件处理器，它们会根据需求，订阅自己感兴趣的topic，从 <code>event channel</code> 中获取事件进行处理。</li><li><code>processing event</code>：处理事件，是由<strong>事件处理器生成并异步广播的事件</strong>，用于广告它刚刚完成了什么操作。这些事件是事件流的中间步骤，通知其他事件处理器某个操作已经完成，以便它们可以继续后续的处理。无论是否有其他的<code>event processor</code>关心这些事件，最佳实践中还是建议一直发布这些事件，这对于后续的扩展性非常良好。</li></ul><p>它具有以下特点：</p><ul><li><strong>核心思想</strong>：它的唯一职责就是高效、可靠地分发事件。所有的业务逻辑和处理步骤都存在于各个独立的事件处理器（服务）中。</li><li><strong>工作流</strong>：工作流是<strong>分散且隐式</strong>的。一个事件可能被多个消费者同时处理，触发多个并行的、互不相关的后续流程。</li><li><strong>通信模型</strong>：利用<strong>基于主题的发布/订阅（Publish-Subscribe）模型</strong>。一个事件被发布到特定主题（Topic）上，所有订阅了该主题的消费者都能收到一份该事件的副本并进行处理。这使得系统具有极强的扩展性，可以随时增加新的订阅者来响应现有事件，而无需修改任何已有代码。</li><li><strong>优点</strong>：事件生产者和事件消费者之间是<strong>完全解耦</strong>的。生产者不知道谁会消费它的事件，消费者也不知道是谁生产了它所消费的事件。它们唯一的共同依赖是<strong>消息代理</strong>以及<strong>事件的契约（Schema）</strong>。</li><li><strong>缺点</strong>：端到端的工作流是<strong>隐式</strong>的，缺乏全局视图。如果流程出了问题，很难追踪到底是哪个环节的协同出了错，这对于异常处理和数据一致性要求较高的系统不是很友好。</li></ul><p>完整例子可参考下图：</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250718113630056.png"alt="FOSA Figure 14-4. Example of the broker topology" /><figcaption aria-hidden="true">FOSA Figure 14-4. Example of the brokertopology</figcaption></figure><h3 id="mediator">mediator</h3><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250718111638117.png"alt="FOSA Figure 14-5. Mediator topology" /><figcaption aria-hidden="true">FOSA Figure 14-5. Mediatortopology</figcaption></figure><p>一个典型的 mediator 拓扑如上图所示，它跟 broker 有些许不同：</p><ul><li><code>event queue</code>：事件队列，它跟 <code>event channel</code>有所不同，专门用于 <code>event mediator</code> 接收<code>initiating event</code>。</li><li><code>event mediator</code>：事件中介者，了解处理事件所需的步骤，并生成相应的处理事件，这些事件被发送到专用事件通道（eventchannel），采用<strong>点对点消息传递</strong>方式。在一些复杂的场景中，也可以设置多个<code>event mediator</code>，并分配到不同的层次中，以更好的管理复杂业务流程。</li></ul><p>它具有以下特点：</p><ul><li><strong>核心思想</strong>：它像一个流程编排引擎，包含了实现复杂业务流程的核心逻辑。</li><li><strong>工作流</strong>：工作流是<strong>集中且显式</strong>的。中介者接收一个初始事件，然后根据预设的逻辑，一步步地调用不同的服务来完成一个完整的、有状态的业务流程。</li><li><strong>通信模型</strong>：利用<strong>基于队列的点对点（Point-to-Point）模型</strong>。</li><li><strong>优点</strong>：工作流是<strong>显式</strong>的，易于理解、监控和管理。复杂的错误处理、重试、补偿逻辑都可以在中介者中集中处理。</li><li><strong>缺点</strong>：中介者本身可能成为一个<strong>复杂的单点</strong>（但通常是高可用的集群），所有流程的修改都必须在其中进行，降低了系统的灵活性。</li></ul><p>完整例子可参考下图：</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250718113522146.png"alt="FOSA Figure 14-9. Step 2 of the mediator example" /><figcaption aria-hidden="true">FOSA Figure 14-9. Step 2 of the mediatorexample</figcaption></figure><h3 id="对比">对比</h3><table><colgroup><col style="width: 12%" /><col style="width: 41%" /><col style="width: 45%" /></colgroup><thead><tr class="header"><th>对比维度</th><th>代理拓扑 (Broker Topology)</th><th>中介者拓扑 (Mediator Topology)</th></tr></thead><tbody><tr class="odd"><td><strong>核心组件</strong></td><td>轻量级、无状态的消息代理</td><td>重量级、有状态的流程中介者</td></tr><tr class="even"><td><strong>智能位置</strong></td><td>分散在各个事件处理器中</td><td>集中在中介者中</td></tr><tr class="odd"><td><strong>工作流</strong></td><td><strong>协同式 (Choreography)</strong>，隐式，涌现式</td><td><strong>编排式 (Orchestration)</strong>，显式，集中式</td></tr><tr class="even"><td><strong>流程控制</strong></td><td>弱，难以进行全局控制</td><td>强，易于进行精细控制和监控</td></tr><tr class="odd"><td><strong>耦合模型</strong></td><td>极致解耦（仅依赖代理和事件契约）</td><td>轮轴式耦合（所有服务都依赖中-介者）</td></tr><tr class="even"><td><strong>灵活性</strong></td><td>极高，易于增加新的事件响应者</td><td>较低，流程变更需修改中介者</td></tr><tr class="odd"><td><strong>典型技术</strong></td><td>消息队列、流平台 (Kafka, RabbitMQ)</td><td>工作流引擎、ESB (AWS Step Functions, Camel)</td></tr><tr class="even"><td><strong>适用场景</strong></td><td>简单通知、数据广播、高度可扩展的系统</td><td>复杂、多步、有状态的业务流程，Saga 模式</td></tr></tbody></table><h2 id="request-reply">Request-Reply</h2><blockquote><ol start="5" type="1"><li><p>Give an example of a typical request within the request-basedmodel.</p><p>举一个 request-based 模式的典型例子。</p></li><li><p>Give an example of a typical request in an event-based model.</p><p>举一个 event-based 模式的典型例子。</p></li></ol></blockquote><h3 id="request-based-vs-event-based">request-based vs event-based</h3><table><colgroup><col style="width: 9%" /><col style="width: 45%" /><col style="width: 45%" /></colgroup><thead><tr class="header"><th>对比维度</th><th>基于请求的模型 (Request-Based)</th><th>基于事件的模型 (Event-Based)</th></tr></thead><tbody><tr class="odd"><td><strong>核心意图</strong></td><td><strong>命令 (Command)</strong></td><td><strong>通知 (Notification / Fact)</strong></td></tr><tr class="even"><td><strong>详细说明</strong></td><td>请求方必须知道接收方的确切地址和接口（例如，一个 URL 端点和其 API契约）。它们之间是点对点的、强依赖的关系。</td><td>发布方和消费方互相完全不知道对方的存在。它们唯一的共同依赖是消息中间件和事件的格式。这种解耦是其最大优势。</td></tr><tr class="odd"><td><strong>通信模式</strong></td><td><strong>通常是同步的 (Synchronous)</strong></td><td><strong>总是异步的 (Asynchronous)</strong></td></tr><tr class="even"><td><strong>详细说明</strong></td><td>请求方发送请求后，会<strong>阻塞并等待</strong>一个响应。从请求方的视角看，整个调用是一个连续、不间断的操作。</td><td>发布方发送事件后，<strong>立即继续</strong>自己的工作（“发后即忘”Fire-and-Forget）。它不等待任何结果。</td></tr><tr class="odd"><td><strong>例子</strong></td><td><strong>打电话</strong></td><td><strong>发布社交动态</strong></td></tr></tbody></table><h3 id="event-based-实现-reply">event-based 实现 reply</h3><p>虽然事件驱动架构的核心是异步和解耦，但在很多业务场景中，请求方确实需要得到一个明确的回复。例如，一个Web前端请求处理一个复杂的计算，它不能永远等待，而是需要在一个合理的时间内得到计算结果。</p><p>在事件模型之上实现请求-响应模式，关键在于解决两个核心问题：</p><ol type="1"><li><strong>响应应该发往何处？</strong>（因为接收方并不知道请求方是谁）</li><li><strong>收到的响应如何与当初的请求对应起来？</strong>（因为请求方可能同时发出了多个请求）</li></ol><p>解决方案是巧妙地利用消息的两个元数据字段：<strong>回复地址(Reply-To)</strong> 和 <strong>关联标识 (Correlation ID)</strong>。</p><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250718132839093.png"alt="FOSA Figure 14-20. Request-reply message processing using a correlation ID" /><figcaption aria-hidden="true">FOSA Figure 14-20. Request-reply messageprocessing using a correlation ID</figcaption></figure><p><strong>Step 1: 请求方 (Requester) 发起请求</strong></p><ol type="1"><li><strong>创建临时回复队列</strong>：请求方首先为自己创建一个唯一的、临时的、专用于接收本次响应的队列。这个队列的生命周期通常与本次请求-响应过程绑定。</li><li><strong>生成关联 ID</strong>：请求方生成一个全局唯一的字符串，作为<code>Correlation ID</code>。</li><li><strong>构造请求消息</strong>：请求方创建请求消息，其内容是业务数据。在消息的<strong>属性（Properties）或头信息（Headers）</strong>中，设置两个关键字段：<ul><li><code>Reply-To</code>: 填入刚才创建的临时回复队列的名称。</li><li><code>Correlation ID</code>: 填入刚才生成的唯一 ID。</li></ul></li><li><strong>发送并等待</strong>：请求方将这个构造好的消息发送到一个众所周知的<strong>请求队列</strong>（例如<code>calculation-request-queue</code>）。然后，它开始<strong>监听</strong>自己的那个<strong>临时回复队列</strong>，等待一个包含相同<code>Correlation ID</code> 的消息出现。通常还会设置一个超时时间。</li></ol><p><strong>Step 2: 响应方 (Replier) 处理请求并回复</strong></p><ol type="1"><li><strong>接收请求</strong>：响应方服务从<strong>请求队列</strong>中消费一条消息。</li><li><strong>处理业务逻辑</strong>：执行消息内容所要求的业务计算或操作。</li><li><strong>提取元数据</strong>：从收到的请求消息的属性中，提取出<code>Reply-To</code> 和 <code>Correlation ID</code> 的值。</li><li><strong>构造响应消息</strong>：响应方创建响应消息，其内容是业务处理的结果。</li><li><strong>设置并发送响应</strong>：在响应消息的属性中，<strong>必须</strong>将从请求中收到的那个<code>Correlation ID</code><strong>原封不动地设置回去</strong>。然后，将此响应消息发送到请求消息中<code>Reply-To</code> 字段所指定的那个队列地址。</li></ol><p><strong>Step 3: 请求方 (Requester) 接收响应</strong></p><ol type="1"><li><strong>接收消息</strong>：请求方在其临时回复队列上收到了一个消息。</li><li><strong>匹配关联 ID</strong>：它检查收到的响应消息中的<code>Correlation ID</code> 是否与它当初发送的那个 ID 相匹配。</li><li><strong>完成闭环</strong>：如果 ID匹配，则证明这就是它所等待的响应。请求-响应的流程至此完成。请求方可以处理响应结果，然后安全地删除那个临时的回复队列。</li></ol><h2 id="可靠性">可靠性</h2><blockquote><p>What are some of the techniques for preventing data loss when sendingand receiving messages from a queue?</p><p>有哪些技术可以防止在从队列发送和接收消息时丢失数据？</p></blockquote><p>这是一个生产者、消费者和代理三方共同的责任：</p><p><strong>1. 代理端 (Broker Side)</strong>：</p><ul><li><strong>持久化(Persistence)</strong>：代理在将事件放入队列或主题时，会先将其写入磁盘，确保即使代理重启，事件也不会丢失。</li><li><strong>集群与复制 (Clustering andReplication)</strong>：通过将代理部署为集群，并将事件在多个节点间进行复制，可以防止单点故障导致的数据丢失。</li></ul><p><strong>2. 客户端 (Client Side)</strong>：</p><ul><li><strong>消费者确认(ACK)</strong>：消费者在<strong>成功处理完</strong>一个事件后，必须向代理发送一个ACK 信号。如果消费者在处理过程中崩溃而未发送ACK，代理会认为该事件未被成功处理，并会将其重新投递给其他消费者。</li><li><strong>事务性发件箱(Transactional)</strong>：这是一个非常关键的高级模式。为了确保"写入业务数据库"和"发送事件"这两个操作的原子性，开发者会将待发送的事件与业务数据变更<strong>放在同一个本地数据库事务中</strong>，写入一个发件箱（Outbox）表。然后由一个独立的轮询进程负责读取发件箱表，并将事件可靠地发送给代理。这彻底解决了"业务成功但事件未发出"的问题。</li></ul><h2 id="架构权衡">架构权衡</h2><blockquote><p>What are three main driving architecture characteristics for usingevent-driven architecture?</p><p>使用事件驱动架构的三个主要驱动架构特性是什么？</p></blockquote><ul><li><strong>可伸缩性与弹性 (Scalability &amp;Elasticity)</strong>：如前所述，独立伸缩组件的能力是其核心优势。</li><li><strong>可扩展性(Extensibility)</strong>：系统极易扩展。当需要增加新功能时，只需开发一个新的服务来订阅感兴趣的现有事件即可，完全无需改动已有服务。</li><li><strong>响应性(Responsiveness)</strong>：对于需要快速响应用户的系统，可以将耗时任务异步化。例如，用户提交视频后，系统立即返回"上传成功，正在处理中"，然后通过事件驱动后台的转码、审核等一系列复杂流程。</li></ul><blockquote><p>What are some of the architecture characteristics that are not wellsupported in event-driven architecture?</p><p>事件驱动架构不能很好地支持哪些架构特性？</p></blockquote><ul><li><strong>简单性 (Simplicity)</strong>：EDA显著增加了系统的复杂性。你需要管理消息代理，处理异步编程的挑战（如调试、错误处理），并应对最终一致性带来的心智负担。</li><li><strong>事务性(Transactional)</strong>：实现跨多个服务的原子性操作（即分布式事务）变得异常困难。虽然可以通过Saga等模式来模拟长事务，但其实现复杂，且只能保证最终一致性而非强一致性。</li><li><strong>工作流的可观测性 (Observability ofWorkflow)</strong>：尤其是在代理拓扑中，业务流程被分散到各个独立的处理器中，没有一个集中的地方可以让你直观地看到一个完整的业务流程是如何执行的，这给监控和排错带来了巨大挑战。</li></ul>]]></content>
    
    
    <summary type="html">本篇通过回答《Fundamentals of Software Architecture》第十四章的课后思考题，深入探讨事件驱动架构中代理拓扑与中介者拓扑的设计差异、异步通信的优势机制、请求模式与事件模式的应用场景，以及事件类型分类、消息可靠性保障技术和架构特性支持分析，帮助理解事件驱动架构的核心设计原理和实施策略，提升架构师在构建响应式系统时的架构选择能力和事件化设计水平。</summary>
    
    
    
    <category term="架构设计" scheme="https://hedon.top/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    
    
    <category term="读书笔记" scheme="https://hedon.top/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="fosa" scheme="https://hedon.top/tags/fosa/"/>
    
    <category term="软件架构" scheme="https://hedon.top/tags/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>FOSA丨13丨基于服务的架构</title>
    <link href="https://hedon.top/2025/07/17/fosa/fosa-ch13/"/>
    <id>https://hedon.top/2025/07/17/fosa/fosa-ch13/</id>
    <published>2025-07-17T02:30:00.000Z</published>
    <updated>2025-07-18T02:24:55.131Z</updated>
    
    <content type="html"><![CDATA[<p>本系列文章通过逐章回答<ahref="https://fundamentalsofsoftwarearchitecture.com/">《Fundamentals ofSoftware Architecture》</a>（下文简称FOSA）一书中的课后思考题，来深入理解书中的核心概念和理论，从而提升我们的软件架构设计能力。本篇为<u>第十三章</u>内容。</p><p>本章的课后题是：</p><ol type="1"><li><p>How many services are there in a typical service-basedarchitecture?</p><p>在一个经典的基于服务的架构中通常有多少个服务？</p></li><li><p>Do you have to break apart a database in service-basedarchitecture?</p><p>在基于服务的架构中，你是否必须将数据库进行拆分？</p></li><li><p>Under what circumstances might you want to break apart adatabase?</p><p>在什么场景下你会对数据库进行拆分？</p></li><li><p>What technique can you use to manage database changes within aservice-based architecture?</p><p>在基于服务的架构中，你会使用什么样的技术来管理数据库变更？</p></li><li><p>Do domain services require a container (such as Docker) torun?</p><p>领域服务需要在容器（如 Docker）中运行吗？</p></li><li><p>Which architecture characteristics are well supported by theservice-based architecture style?</p><p>基于服务的架构在哪些架构特性表现很优异？</p></li><li><p>Why isn’t elasticity well supported in a service-basedarchitecture?</p><p>为什么基于服务的架构的架构弹性不是很好？</p></li><li><p>How can you increase the number of architecture quanta in aservice-based architecture?</p><p>在基于服务的架构中，你如何增加架构量子的数量？</p></li></ol><hr /><h2 id="简介">简介</h2><p>在软件架构的演进光谱中，如果说单体（Monolith）和微服务（Microservices）是两个广为人知的端点，那么基于服务的架构（Service-BasedArchitecture,SBA）就是它们之间那个常常被忽略，却又极具现实意义的"务实中间派"。它既非庞大到笨拙，也非精细到繁杂，为许多成长中的系统提供了一条平滑的演进路径。</p><p>SBA的本质是一种将一个大型的单体应用，<strong>分解为少数几个、逻辑独立的、可独立部署的"服务"</strong>的架构风格。SBA 的服务数量通常不多，一般在 <strong>4 到 12个</strong>之间。它不像微服务那样追求极致的拆分（可能会有几十上百个服务），而是将应用按照<strong>核心的业务领域（Domain）</strong>进行划分。</p><h2 id="拓扑">拓扑</h2><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250717114456233.png"alt="FOSA Figure 13-8. Electronics recycling example using service-based architecture" /><figcaption aria-hidden="true">FOSA Figure 13-8. Electronics recyclingexample using service-based architecture</figcaption></figure><h2 id="数据库">数据库</h2><p>SBA最具标志性，也是与微服务最根本的区别之一，就在于它对数据库的处理方式。这直接引出了接下来的两个问题。</p><blockquote><p>Do you have to break apart a database in service-basedarchitecture?</p><p>在基于服务的架构中，你是否必须将数据库进行拆分？</p></blockquote><p>答案是：<strong>通常不，而且默认不拆分是其主要特征。</strong></p><p>SBA的典型实现是，所有服务共享<strong>同一个数据库</strong>。这种设计的初衷是为了在享受独立部署带来的好处的同时，最大限度地<strong>降低数据层面的复杂性</strong>。共享数据库可以：</p><ul><li><strong>简化开发</strong>：开发者无需处理复杂的分布式事务和跨服务数据同步问题。</li><li><strong>保证数据一致性</strong>：传统的 ACID事务可以在数据库层面轻松实现。</li><li><strong>降低技术门槛</strong>：团队无需掌握复杂的分布式数据管理技术。</li></ul><p>在共享数据库的模式下，如何管理这个公共资产成了一个关键的治理问题。</p><blockquote><p>What technique can you use to manage database changes within aservice-based architecture?</p><p>在基于服务的架构中，你会使用什么样的技术来管理数据库变更？</p></blockquote><p>当多个团队开发的服务都依赖同一个数据库时，随意的 Schema变更会引发灾难。因此，必须采用严格的数据库治理技术。</p><p>核心方法是成立一个跨团队的数据库治理小组，或者由一个专职的数据库管理员（DBA）团队来担当此任。这个团队的职责是：</p><ul><li><strong>守护数据库 Schema的所有权</strong>：任何对数据库结构的修改（增删改表、字段等）都必须通过该团队的评审。</li><li><strong>执行数据库迁移脚本</strong>：使用专业的数据库迁移工具（如<strong>Flyway</strong> 或<strong>Liquibase</strong>）来统一管理和执行所有的变更脚本，确保变更的可追溯性、版本化和一致性。</li><li><strong>保证向后兼容性</strong>：确保数据库的变更不会破坏现有服务的正常运行。</li></ul><p>然而，这种共享模式并非一成不变，这就引出了下一个问题：</p><blockquote><p>Under what circumstances might you want to break apart adatabase?</p><p>在什么场景下你会对数据库进行拆分？</p></blockquote><p>随着业务发展，共享数据库的弊端会逐渐显现。在以下情况下，拆分数据库就成了合理的选择：</p><ol type="1"><li><strong>服务资源争用 (ServiceContention)</strong>：某个服务（如高流量的商品浏览服务）对数据库产生巨大压力，影响了其他关键服务（如订单服务）的性能。</li><li><strong>数据隔离与安全 (Data Isolation andSecurity)</strong>：某个服务处理的数据高度敏感（如支付服务中的金融信息），需要从主数据库中物理隔离出来，以满足合规性或安全要求。</li><li><strong>技术栈不匹配 (TechnologyMismatch)</strong>：某个服务有特殊的数据存储需求。例如，搜索服务最适合使用Elasticsearch，而核心业务数据则存储在关系型数据库中。</li></ol><p>当这些情况发生时，SBA允许你"渐进式"地将某个服务连同其数据一起剥离出去，赋予它独立的数据库。</p><h2 id="部署">部署</h2><blockquote><p>Do domain services require a container (such as Docker) to run?</p><p>领域服务需要容器（例如 Docker）来运行吗？</p></blockquote><p>答案是：<strong>不需要，但强烈推荐。</strong></p><p>从技术上讲，你可以将每个服务单独部署服务器上。但是，容器技术（如Docker）和容器编排工具（如 Kubernetes）与 SBA的理念天然契合。使用容器可以带来巨大好处：</p><ul><li>环境一致性</li><li>部署简化</li><li>资源利用率</li></ul><h2 id="架构权衡">架构权衡</h2><blockquote><p>Which architecture characteristics are well supported by theservice-based architecture style?</p><p>基于服务的架构在哪些架构特性表现很优异？</p></blockquote><p>相比于单体架构，SBA 在以下方面有显著提升：</p><ol type="1"><li><strong>可部署性(Deployability)</strong>：这是最大的优势之一。每个服务都可以独立部署，使得发布更加频繁、风险更低。</li><li><strong>模块化(Modularity)</strong>：通过按领域划分服务，实现了清晰的业务模块边界。</li><li><strong>可维护性(Maintainability)</strong>：每个服务的代码库规模远小于整个单体，更易于理解、修改和维护。</li><li><strong>容错性 (FaultTolerance)</strong>：一个服务的崩溃不会导致整个应用程序宕机（尽管共享数据库可能成为共同的故障点）。</li></ol><p>然而，SBA 并非银弹，它也有其固有的局限性。</p><blockquote><p>Why isn’t elasticity well supported in a service-basedarchitecture?</p><p>为什么基于服务的架构的架构弹性不是很好？</p></blockquote><p><strong>弹性</strong>指的是根据实时负载，自动、精细地伸缩应用<strong>特定部分</strong>的能力。</p><p>SBA对弹性的支持不佳，根源在于其服务的<strong>粗粒度</strong>。假设"订单服务"包含了"浏览历史订单"、"创建新订单"和"订单退款"三个功能。如果"创建新订单"功能因为促销活动而流量激增，你无法只针对这一个功能进行扩容。你必须将整个庞大的"订单服务"进行水平扩展，复制出多个实例。这不仅造成了资源浪费（其他两个功能并未承压），也远不如微服务那样能够对具体功能点进行精准、高效的弹性伸缩。</p><h2 id="架构量子">架构量子</h2><blockquote><p>How can you increase the number of architecture quanta in aservice-based architecture?</p><p>在基于服务的架构中，你如何增加架构量子的数量？</p></blockquote><p>首先要明确，在<strong>典型的、共享数据库的 SBA</strong>中，整个系统只有<strong>一个架构量子</strong>。因为所有服务都与同一个数据库紧密耦合，它们无法被真正独立地部署和演化，形成了一个不可分割的整体。</p><p>增加架构量子的数量，唯一的途径就是<strong>打破这种共享依赖</strong>。具体方法是：<strong>将某个服务连同其数据一起拆分出来，为其分配一个独立的、专用的数据库。</strong></p><p>每完成一次这样的拆分，这个被分离出去的服务就演变成了一个独立的架构量子。因此，增加架构量子的过程，就是<strong>逐步从共享数据库模型向"每个服务一个数据库"模型演进的过程</strong>，也就逐渐趋向于微服务架构了。</p>]]></content>
    
    
    <summary type="html">本篇通过回答《Fundamentals of Software Architecture》第十三章的课后思考题，深入探讨基于服务的架构中服务数量的设计考量、数据库分解策略与变更管理机制、领域服务的容器化部署模式，以及基于服务架构的特性支持分析、弹性限制因素和架构量子扩展方案，帮助理解基于服务架构的核心设计原则和实施要点，提升架构师在构建分布式系统时的架构选择能力和服务化设计水平。</summary>
    
    
    
    <category term="架构设计" scheme="https://hedon.top/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    
    
    <category term="读书笔记" scheme="https://hedon.top/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="fosa" scheme="https://hedon.top/tags/fosa/"/>
    
    <category term="软件架构" scheme="https://hedon.top/tags/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>FOSA丨12丨微核架构</title>
    <link href="https://hedon.top/2025/07/16/fosa/fosa-ch12/"/>
    <id>https://hedon.top/2025/07/16/fosa/fosa-ch12/</id>
    <published>2025-07-16T02:20:00.000Z</published>
    <updated>2025-07-17T02:31:33.098Z</updated>
    
    <content type="html"><![CDATA[<p>本系列文章通过逐章回答<ahref="https://fundamentalsofsoftwarearchitecture.com/">《Fundamentals ofSoftware Architecture》</a>（下文简称FOSA）一书中的课后思考题，来深入理解书中的核心概念和理论，从而提升我们的软件架构设计能力。本篇为<u>第十二章</u>内容。</p><p>本章的课后题是：</p><ol type="1"><li><p>What is another name for the microkernel architecture style?</p><p>微核架构风格的别名是什么？</p></li><li><p>Under what situations is it OK for plug-in components to bedependent on other plug-in components?</p><p>在什么情况下，插件组件之间可以相互依赖？</p></li><li><p>What are some of the tools and frameworks that can be used tomanage plug-ins?</p><p>有哪些工具和框架可用于管理插件？</p></li><li><p>What would you do if you had a third-party plug-in that didn’tconform to the standard plug-in contract in the core system?</p><p>如果一个第三方插件不遵循核心系统的标准插件契约，你会怎么做？</p></li><li><p>Provide two examples of the microkernel architecture style.</p><p>举 2 个微核架构的例子。</p></li><li><p>Is the microkernel architecture style technically partitioned ordomain partitioned?</p><p>微核架构是技术分区还是领域分区？</p></li><li><p>Why is the microkernel architecture always a single architecturequantum?</p><p>为什么微核架构总是单一的架构量子？</p></li><li><p>What is domain/architecture isomorphism?</p><p>什么是领域/架构同构性？</p></li></ol><hr /><h2 id="拓扑">拓扑</h2><p>微核架构，也被称为<strong>插件化架构（Plug-inArchitecture）</strong>，是一种能够提供极高扩展性、灵活性和演化能力的系统设计模式。它的核心思想是将系统功能划分为两部分：一个最小化的、稳定的<strong>核心系统（CoreSystem）</strong>和一个由独立<strong>插件组件（Plug-inComponents）</strong>构成的可扩展生态。</p><p>我们可以将其想象成一个智能手机：操作系统是其微核，提供最基础的功能（通信、电源管理、应用商店接口），而我们安装的每一个App 就是一个插件，为手机赋予了无穷无尽的新功能。</p><p>核心构成：</p><ul><li><strong>核心系统 (CoreSystem)</strong>：这是架构的“微核”。它的职责被严格限制在<strong>最小且必要</strong>的范围内，通常只包含：<ol type="1"><li>系统运行所必需的通用业务逻辑（例如，一个 IDE的文件管理和基础编辑器）。</li><li>一个至关重要的<strong>插件管理机制</strong>，包括插件的注册、发现、生命周期管理等。这是连接核心与插件的桥梁。</li></ol></li><li><strong>插件组件 (Plug-inComponents)</strong>：这些是独立的、可插拔的模块，用于实现<strong>扩展功能或特定业务逻辑</strong>。每个插件都通过一个由核心系统定义的<strong>标准契约（StandardContract）</strong>来与核心交互。这个契约通常是一个接口或一组 API。</li></ul><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250716105151041.png"alt="FOSA Figure 12-1. Basic components of the microkernel architecture style" /><figcaption aria-hidden="true">FOSA Figure 12-1. Basic components of themicrokernel architecture style</figcaption></figure><p>典型例子：</p><ul><li>Chrome</li><li>VS Code</li></ul><h2 id="插件生态">插件生态</h2><p>理想情况下，插件应该只依赖于核心系统，保持彼此的独立性，以获得最大的灵活性。然而，在复杂的现实世界中，插件间的依赖是不可避免的。</p><h3 id="插件依赖">插件依赖</h3><blockquote><p>Under what situations is it OK for plug-in components to be dependenton other plug-in components?</p><p>在什么情况下，插件组件之间可以相互依赖？</p></blockquote><p>允许插件间依赖的<strong>合理情况</strong>是：当一个插件的功能是另一个插件功能的<strong>逻辑扩展或前提</strong>时。</p><blockquote><p>例如：一个 <code>PDF 导出</code> 插件，可能需要依赖一个通用的<code>报表生成</code> 插件。<code>PDF 导出</code> 插件负责将<code>报表生成</code> 插件产生的数据模型渲染成 PDF 文件。</p></blockquote><h3 id="插件管理">插件管理</h3><blockquote><p>What are some of the tools and frameworks that can be used to manageplug-ins?</p><p>有哪些工具和框架可用于管理插件？</p></blockquote><p>管理插件的复杂性催生了许多优秀的框架和标准：</p><ol type="1"><li><strong>OSGi (Open Service Gateway initiative)</strong>：这是 Java平台中最著名、最强大的插件化框架。它提供了一整套完善的模块层（Bundle）和生命周期管理机制，是构建大型、复杂微核系统的首选。EclipseIDE 就是基于 OSGi 构建的。</li><li><strong>Eclipse Rich Client Platform (RCP)</strong>：基于OSGi，专门用于构建桌面富客户端应用的框架，其本身就是一个微核。</li><li><strong>Java Platform Module System (JPMS)</strong>：从 Java 9开始引入的官方模块化系统，也可以作为实现插件化的基础。</li><li><strong>Java ServiceLoader</strong>：Java内置的一个简单的服务发现机制，适用于较轻量级的插件化场景。</li><li><strong>其他生态</strong>：在 .NET 中有 MEF (Managed ExtensibilityFramework)；在 Web 应用中，可以通过 Webhooks机制实现一种分布式的插件化思想，允许第三方服务作为“插件”来响应核心系统的事件。</li></ol><h3 id="插件适配">插件适配</h3><blockquote><p>What would you do if you had a third-party plug-in that didn’tconform to the standard plug-in contract in the core system?</p><p>如果一个第三方插件不遵循核心系统的标准插件契约，你会怎么做？</p></blockquote><p>如果一个第三方插件不遵循核心系统的标准插件契约，最佳解决方案是引入<strong>适配器模式(Adapter Pattern)</strong>。</p><p>具体做法是：</p><ul><li><p>创建一个新的、我们自己控制的<strong>适配器插件 (AdapterPlug-in)</strong>，这个适配器插件<strong>遵循</strong>我们核心系统的标准契约。</p></li><li><p>在适配器插件的内部，它负责将核心系统发来的请求<strong>翻译</strong>成第三方插件能够理解的格式，并调用第三方插件。</p></li><li><p>反之，它也负责将第三方插件的返回结果<strong>翻译</strong>回核心系统期望的格式。</p></li></ul><h2 id="分区风格">分区风格</h2><blockquote><p>Is the microkernel architecture style technically partitioned ordomain partitioned?</p><p>微核架构是技术分区还是领域分区？</p></blockquote><p>微核架构是一种<strong>混合分区 (Hybrid Partitioning)</strong>的风格，这也是它独特的地方。</p><ul><li><strong>核心系统本身</strong>通常是<strong>技术分区</strong>的。它关注的是底层、通用的技术能力，如插件生命周期管理、安全、通信等，而不涉及具体的业务领域。</li><li><strong>插件组件</strong>则通常是<strong>领域分区</strong>的。每一个插件都封装了一个特定的业务功能或领域（例如<code>税务计算插件</code>、<code>保单审批插件</code>、<code>Git 版本控制插件</code>）。</li></ul><p>这种混合模式使得系统既有坚实的技术底座，又能灵活地按业务领域进行扩展。</p><h2 id="架构量子">架构量子</h2><blockquote><p>Why is the microkernel architecture always a single architecturequantum?</p><p>为什么微核架构总是单一的架构量子？</p></blockquote><p>首先，我们需要定义<strong>架构量子 (ArchitectureQuantum)</strong>：一个高功能内聚、可独立部署的组件，它包含了所有使其能够正常工作所需的元素（包括数据）。</p><p>微核架构在其经典实现中之所以是单一架构量子，是因为它在两个主要维度上表现出强烈的内聚和耦合：</p><ul><li><strong>在运行时维度上</strong>：组件共享同一个进程和内存空间，通过进程内调用紧密耦合，形成了一个"共同命运共同体"，缺乏独立的容错能力。</li><li><strong>在数据维度上</strong>：组件通常共享同一个物理数据库实例和事务上下文，导致在数据管理和演化上紧密耦合。</li></ul><h2 id="同构性">同构性</h2><blockquote><p>What is domain/architecture isomorphism?</p><p>什么是领域/架构同构性？</p></blockquote><p><strong>同构性 (Isomorphism)</strong>是一个源于数学的概念，意为"结构上的相似性"或"一对一的映射关系"。</p><p><strong>领域/架构同构性 (Domain/Architecture Isomorphism)</strong>指的是<strong>软件的架构结构与它所要解决的问题领域（业务领域）的结构高度一致</strong>。一个具备良好同构性的架构，其模块划分、组件关系能够清晰地反映出业务领域的划分和业务流程。</p><p>微核架构是展现领域/架构同构性的一个绝佳范例。</p><ul><li><strong>问题领域</strong>可以被分解为一个"通用基础平台"和多个"特定业务功能"。</li><li><strong>微核架构</strong>恰好与之对应：<strong>核心系统</strong>映射了''通用基础平台"，而每一个<strong>插件</strong>则精确地映射了一个"特定业务功能"。</li></ul><p>这种一一对应的关系使得系统非常容易被业务人员和开发人员共同理解，需求变更也能快速定位到需要修改的插件，极大地提升了系统的可维护性和演化能力。</p>]]></content>
    
    
    <summary type="html">本篇通过回答《Fundamentals of Software Architecture》第十二章的课后思考题，深入探讨微核架构中插件组件间依赖关系的设计原则、插件管理工具与框架的选择策略、第三方插件契约兼容性处理方案，以及微核架构的分区特性、架构量子特征和领域同构性概念分析，帮助理解微核架构的核心设计模式和扩展机制，提升架构师在构建可扩展系统时的架构选择能力和插件化设计水平。</summary>
    
    
    
    <category term="架构设计" scheme="https://hedon.top/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    
    
    <category term="读书笔记" scheme="https://hedon.top/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="fosa" scheme="https://hedon.top/tags/fosa/"/>
    
    <category term="软件架构" scheme="https://hedon.top/tags/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>FOSA丨11丨管道架构</title>
    <link href="https://hedon.top/2025/07/15/fosa/fosa-ch11/"/>
    <id>https://hedon.top/2025/07/15/fosa/fosa-ch11/</id>
    <published>2025-07-15T02:20:00.000Z</published>
    <updated>2025-07-15T03:18:25.911Z</updated>
    
    <content type="html"><![CDATA[<p>本系列文章通过逐章回答<ahref="https://fundamentalsofsoftwarearchitecture.com/">《Fundamentals ofSoftware Architecture》</a>（下文简称FOSA）一书中的课后思考题，来深入理解书中的核心概念和理论，从而提升我们的软件架构设计能力。本篇为<u>第十一章</u>内容。</p><p>本章的课后题是：</p><ol type="1"><li><p>Can pipes be bidirectional in a pipeline architecture?</p><p>在管道架构中管道可以是双向的吗？</p></li><li><p>Name the four types of filters and their purpose.</p><p>说出 4 种类型的过滤器及它们的作用。</p></li><li><p>Can a filter send data out through multiple pipes?</p><p>一个过滤器能否通过多条管道将数据发送出去？</p></li><li><p>Is the pipeline architecture style technically partitioned ordomain partitioned?</p><p>管道架构是技术分区还是领域分区？</p></li><li><p>In what way does the pipeline architecture supportmodularity?</p><p>管道架构是如何支持模块化的呢？</p></li><li><p>Provide two examples of the pipeline architecture style.</p><p>举 2 个管道架构的例子。</p></li></ol><hr /><h2 id="拓扑">拓扑</h2><figure><imgsrc="https://hedonspace.oss-cn-beijing.aliyuncs.com/img/image-20250715105907327.png"alt="FOSA Figure 11-2. Pipeline architecture example" /><figcaption aria-hidden="true">FOSA Figure 11-2. Pipeline architectureexample</figcaption></figure><p>管道架构，又称为管道与过滤器架构（Pipes and FiltersArchitecture），是一种用于处理数据流的强大模式。它的核心思想非常直观，就像一条工厂的流水线：原材料从一端进入，经过一系列独立工站的加工、处理、检验，最终在另一端形成成品。</p><p>要理解管道架构，首先要理解它的两个基本构件：</p><ul><li><strong>过滤器(Filter)</strong>：它是一个独立的、可执行的处理单元，负责接收数据、执行单一任务（例如转换格式、过滤内容、扩充信息），然后将处理后的数据传递出去。关键在于，每个过滤器都是<strong>自包含（Self-Contained）</strong>和<strong>无状态（Stateless）</strong>的，它不关心上一个过滤器是谁，也不关心下一个过滤器是谁。</li><li><strong>管道(Pipe)</strong>：代表流水线上的"传送带"。它是一个<strong>单向</strong>的数据通道，负责将一个过滤器处理完的数据传递给下一个过滤器。</li></ul><p>在管道架构中，每个<strong>过滤器</strong>通常代表一个具体的技术操作，而不是一个完整的业务领域。整个管道将这些技术步骤串联起来，以完成一个业务流程，但其划分的单元（过滤器）是技术性的。</p><h2 id="管道">管道</h2><p>管道的<strong>单向性（Unidirectional）</strong>是该架构风格的基石。原因在于：</p><ol type="1"><li><strong>维持简单性与解耦</strong>：单向流动保证了数据处理的顺序性和可预测性。每个过滤器只需关注自己的输入和输出，无需处理复杂的双向通信或回调逻辑。</li><li><strong>避免状态依赖</strong>：如果管道是双向的，就意味着过滤器之间可能存在请求-响应（Request-Response）式的交互。这会引入状态和时间上的耦合，破坏了过滤器作为独立、无状态组件的核心原则。一个需要双向通信的场景，更适合采用其他架构风格（如客户端-服务器模式），而非管道架构。</li></ol><p>因此，严格意义上的管道架构，其管道必须是单向的。同时，管道也可以支持强大的分支（Forking）和扇出（Fan-out）能力，一个过滤器可以根据处理结果，将数据发送到不同的下游管道，这个过程依旧保持了其单向性。</p><h2 id="过滤器">过滤器</h2><ul><li><p><strong>生产者 (Producer /Source)</strong>：作为整条管道的<strong>起点</strong>。它不接收来自管道的数据，而是负责创建数据，并将这些初始数据泵入管道。</p></li><li><p><strong>转换器(Transformer)</strong>：它从上游管道接收数据，对其进行某种形式的<strong>修改或转换</strong>，然后将结果发送到下游管道。</p></li><li><p><strong>测试器(Tester)</strong>：它接收数据，并根据一个或多个条件对数据进行<strong>检验</strong>。如果数据满足条件，就将其传递到下游管道；如果不满足，则数据流在此处被中断（或被导向另一条错误处理管道）。</p></li><li><p><strong>消费者 (Consumer /Sink)</strong>：作为整条管道的<strong>终点</strong>。它从上游管道接收最终处理好的数据，并将其消费掉，通常不会再将数据传递出去。</p></li></ul><h2 id="模块化">模块化</h2><ul><li><strong>高内聚、低耦合（High Cohesion, LowCoupling）</strong>：每个过滤器都是一个高内聚的模块，只专注于完成一件定义明确的任务。同时，过滤器之间通过管道这一标准接口进行通信，实现了极低的耦合，它们互相不知道对方的存在。</li><li><strong>可组合性（Composability）</strong>：过滤器就像乐高积木。我们可以通过不同的排列组合，快速地搭建出全新的数据处理流程，而无需修改过滤器本身的代码。</li><li><strong>可复用性（Reusability）</strong>：一个通用的过滤器（例如<code>GzipCompressor</code>）可以被用在任何需要数据压缩的管道中，实现了代码的高度复用。</li><li><strong>可替换性（Replaceability）</strong>：只要遵守管道中的数据格式约定，我们可以轻易地用一个性能更好的新过滤器来替换掉一个旧的过滤器，而不会影响到管道的其他部分。</li></ul><h2 id="例子">例子</h2><h3 id="unixlinux-命令行">1. UNIX/Linux 命令行</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat access.log | grep &quot;ERROR&quot; | sort | uniq -c</span><br></pre></td></tr></table></figure><ul><li><code>cat access.log</code>：生产者，读取日志文件并产生数据流。</li><li><code>|</code>：管道，将标准输出连接到下一个命令的标准输入。</li><li><code>grep "ERROR"</code>：测试器/转换器，过滤出包含 "ERROR"的行。</li><li><code>sort</code>：转换器，对错误日志进行排序。</li><li><code>uniq -c</code>：转换器/消费者，统计重复行并输出最终结果。</li></ul><h3 id="eltextract-transform-load-流程">2. ELT(Extract, Transform, Load)流程</h3><ul><li><strong>Extract（抽取）</strong>：生产者过滤器，从各种源系统（如业务数据库、日志文件、API）中读取原始数据。</li><li><strong>Transform（转换）</strong>：一系列转换器和测试器过滤器，对数据进行清洗（去除无效值）、转换（统一格式）、扩充（关联其他数据）、聚合（计算统计值）等操作。</li><li><strong>Load（加载）</strong>：消费者过滤器，将最终处理好的、高质量的数据加载到目标数据仓库或数据湖中，供后续分析使用。</li></ul>]]></content>
    
    
    <summary type="html">本篇通过回答《Fundamentals of Software Architecture》第十一章的课后思考题，深入探讨管道架构中管道双向性的可能性与限制、过滤器类型的分类与作用机制、数据流向的设计原则，以及管道架构的分区特性、模块化支持方式和典型应用场景分析，帮助理解管道与过滤器架构的核心概念和设计模式，提升架构师在处理数据流应用时的架构选择能力和系统设计水平。</summary>
    
    
    
    <category term="架构设计" scheme="https://hedon.top/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    
    
    <category term="读书笔记" scheme="https://hedon.top/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="fosa" scheme="https://hedon.top/tags/fosa/"/>
    
    <category term="软件架构" scheme="https://hedon.top/tags/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
</feed>
