[{"title":"FOSA丨06丨衡量和管理架构特性","path":"/2025/07/07/fosa-ch6/","content":"本系列文章通过逐章回答《Fundamentals of Software Architecture》（下文简称 FOSA）一书中的课后思考题，来深入理解书中的核心概念和理论，从而提升我们的软件架构设计能力。本篇为第六章内容。 本章的课后题是： Why is cyclomatic complexity such an important metric to analyze for architecture? 为什么圈复杂度是架构分析的重要指标？ What is an architecture fitness function? How can they be used to analyze an architecture? 什么是架构适应度函数？它们如何用于分析架构？ Provide an example of an architecture fitness function to measure the scalability of an architecture. 提供一个衡量架构可伸缩性的架构适应度函数示例。 What is the most important criteria for an architecture characteristic to allow architects and developers to create fitness functions? 允许架构师和开发人员创建适应度函数的最重要标准是什么？ 评估架构特性 评估架构特性一般可以从 3 个方面入手： 运维性指标 (operational measures)：主要关注系统在运维层面的能力，涵盖性能、可扩展性、弹性、可用性、可靠性等能力。 结构性指标 (structural measures)：关注代码结构，如模块化、组件间受控的耦合、可读代码以及其他内部质量评估。常用工具有： 圈复杂度 (Cyclomatic Complexity，CC)：一个代码层面的度量标准，由 Thomas McCabe, Sr. 于 1976 年开发，通过分析代码的决策点（如 if 语句）来量化代码的复杂性。高圈复杂度可能表明代码难以理解和测试。公式为 CC = E - N + 2（针对单个函数），或 CC = E - N + 2P（针对扇出调用）。行业普遍认为 CC 值低于 10 是可接受的，但更倾向于低于 5。 距主序列距离 (Distance from the Main Sequence，D)：一个基于抽象性（A）和不稳定性（I）的综合指标，公式为 D = |A + I - 1|。它反映了抽象性和不稳定性之间的理想关系。远离理想线的类可能落入\"无用区\"（过于抽象难以使用）或\"痛苦区\"（过于具体且难以维护） 流程性指标 (process measures)：关注软件开发过程中的特性，如敏捷性、可测试性和可部署性。常用工具有： 代码覆盖率（code coverage） 管理架构特性 管理架构特性主要通过 4 个方面： 架构适应性函数 (Architecture Fitness Functions)：这是评估系统输出质量的客观函数，用于衡量架构特性。它们将重要的架构原则编码到软件基础中，并自动验证这些原则是否得到遵守。例如： 检测组件之间的循环依赖 (Cyclic Dependencies) 验证分层架构中的层间依赖关系 衡量距主序列的距离 混沌工程 架构决策记录 (Architecture Decision Records, ADRs)：ADR 是一种有效的文档化架构决策的方式，通常是一到两页的短文本文件。每个 ADR 应包含标题、状态（例如“已接受”、“已取代”）、上下文、决策（使用肯定性语言）和结果（包括决策的正面和负面影响，以及权衡分析）。ADR 使得架构师能够清晰记录决策的技术和业务理由，避免重复讨论和误解。ADR 中的“合规性 (Compliance)”部分可以强制架构师思考如何衡量和管理决策的合规性，无论是手动还是通过适应性函数自动化。 风险风暴 (Risk Storming)：这是一种协作活动，用于识别、达成共识并减轻架构风险。它包括识别（个体非协作活动）、共识（协作活动，讨论并统一风险评估）和缓解（协作活动，寻找减少或消除风险的方法）三个主要阶段。风险风暴通常使用风险矩阵 (Risk Matrix)，通过“影响”和“可能性”两个维度来量化风险。风险评估报告还可以显示特定风险类别或领域随时间的改进或恶化，使用加号 (+) 和减号 (-) 表示方向。 持续沟通与协作：有效的沟通对于知识共享和项目成功至关重要。架构师应与产品负责人、项目经理、业务干系人以及开发人员进行谈判和协商，以获得架构决策的批准。倡导通用语言 (Ubiquitous Language)，确保所有项目相关方使用相同的业务领域术语，从而减少信息丢失和误解。 回答问题 基于上述对本章的概括回顾，回到本章的 4 个课后题，笔者梳理了一下自己的理解，供读者们参考。 圈复杂度： 圈复杂度是一种评估代码复杂性的工具。 如果一个函数（方法）中，条件分支和语句越多，则说明越复杂，一方面可能是业务逻辑本身就足够复杂，另外一方面，也很可能是代码的模块拆分没有做好，逻辑没有梳理清晰，写成了一坨。 这背后体现了模块化、可测试性、可部署性、可扩展性、可迭代性等多种代码结构层面的架构特性。 架构适应度函数： 架构适应度函数是一种用于持续评估当前架构是否满足需求的机制，可以理解为\"架构的单元测试\"。 不同的组织、不同的团队、不同的职责对同一个架构特性的理解、定义和需求都是不尽相同的。通过协商、建立其符合具体需求的架构适应度函数，在达成共识的基础上，可以持续对某些架构特性进行达标检测，避免偏离。比如代码测试覆盖率可以用来检测架构的可测试性、部署耗时可以用来横向架构的可部署性、可迭代性等。 要编写 fitness function，最重要也是唯一最重要的标准是架构特性必须能够被客观地衡量和定义。通过鼓励客观定义，团队可以拆解复合特性，从而发现可以被客观衡量的功能。一旦特性被具体定义，就可以更容易地建立相应的适应度函数来验证其完整性。 衡量系统的可伸缩性： 可伸缩性，指的是系统在用户或请求数量增加时，仍然能够维持性能和运行的能力。 假如说我们现在有一个订单服务，如果我们希望它具备良好的可伸缩性，当我们将服务实例从 2 个增加到 4 个时，系统在相同响应时间基准下，应用能处理接近翻倍的请求吞吐量。","tags":["读书笔记","软件架构","fosa"],"categories":["架构设计"]},{"title":"FOSA丨05丨识别架构特征","path":"/2025/07/04/fosa-ch5/","content":"本系列文章通过逐章回答《Fundamentals of Software Architecture》（下文简称 FOSA）一书中的课后思考题，来深入理解书中的核心概念和理论，从而提升我们的软件架构设计能力。本篇为第五章内容。 本章的课后题是： Give a reason why it is a good practice to limit the number of characteristics (“-ilities”) an architecture should support. 举一个例子来说明为什么限制系统中支持的架构特性的数量的有必要的。 True or false: most architecture characteristics come from business requirements and user stories. 大多数的架构特性都来自于业务需求和用户故事，这对吗？ If a business stakeholder states that time-to-market (i.e., getting new features and bug fixes pushed out to users as fast as possible) is the most important business concern, which architecture characteristics would the architecture need to support? 如果一个业务利益相关者将上市时间（比如以最快的速度实现新功能和修复 BUG）视为最重要的需求点，这个时候需要支持什么样的架构特性？ What is the difference between scalability and elasticity? 可伸缩性（scalability） 和弹性（elasticity）的区别是什么？ You find out that your company is about to undergo several major acquisitions to significantly increase its customer base. Which architectural characteristics should you be worried about? 如果你发现了你的公司进行了几次重大收购以大幅增加其客户群，这个时候你应该考虑什么架构特性？ 架构特性不是越多越好 增加系统设计的复杂性：每增加一个架构特性，都会使整个系统设计变得更加复杂。支持过多的架构特性会导致在架构师和开发人员开始解决核心业务问题之前，系统就变得越来越复杂。 分散对核心问题的关注：架构特性定义了系统的成功标准，通常与系统的功能性正交，关注的是“如何”实现需求以及“为什么”做出某些选择。然而，如果过度追求特性数量，可能会导致偏离原始的业务问题，即开发软件的最初动机。 每个特性都涉及权衡：软件架构中的每一个方面都存在权衡，有优点也有缺点。例如，在拍卖系统中，选择使用主题（topic）进行通信可能带来架构可扩展性的优势和服务的解耦，但会引入数据访问和数据安全方面的潜在问题，并且不支持异构契约。而使用队列（queue）则允许每个消费者拥有自己的契约，但不具备可扩展性，并且会增加服务间的耦合。架构师需要分析这些权衡，并根据业务驱动因素和环境选择最重要的特性。 过度规范的危害：架构师过度规范架构特性是常见的陷阱，其破坏性不亚于规范不足，因为它会使系统设计过于复杂。历史案例“瓦萨号”战舰的失败就是一个例证，它是因为过度追求建造最宏伟的战舰（即过度规范架构特性）而最终导致沉没。 陷入“意外复杂性”陷阱：架构师有时会为解决方案、图表和文档添加不必要的复杂性。正如一位作者所言，“开发者被复杂性吸引，就像飞蛾扑火一样——结果往往相同”。这种“意外复杂性”是由于人为地使问题复杂化，而不是问题本身固有的复杂性。通过识别子领域类型并根据其业务逻辑的复杂性选择合适的实现模式（例如，事务脚本和活动记录适用于简单业务逻辑，而领域模型和事件溯源领域模型适用于复杂的核心子领域），可以避免引入不必要的复杂性。 设计应由业务驱动：领域驱动设计（DDD）的核心思想在于让业务领域驱动软件设计决策。这意味着设计决策应该基于业务领域的需求和战略，而非盲目地堆砌所有可能的架构特性。 因此，与领域利益相关者合作时，架构师应努力使最终的架构特性列表尽可能短，因为每个特性都会增加总体系统设计的复杂性。 如何识别架构特性 从领域焦点中识别架构特性 从业务需求中识别架构特性 这里面的一大难点就是：业务方与开发方使用的不是同一种\"语言\"。双方对同一件事情的关注点是不一样的，所以表述出来的述求，也是不同的。 所以在识别架构特性的时候，架构师的职责就是需要将业务领域的关注点和架构特性进行对应。比如： Domain Concern Architecture characteristics Mergers and acquisitions 合并与收购 互操作性 interoperability可扩展性 scalability适配性 adaptability可扩展性 extensibility Time to market 上市时间 灵活性 agility可测试性 testability可部署性 deployability User satisfaction 用户满意度 性能 performance可用性 availability容错性 fault tolerance可测试性 testability可部署性 deployability灵活性 agility安全性 security Competitive advantage 竞争优势 灵活性 agility可测试性 testability可部署性 deployability可扩展性 scalability可用性 availability容错性 fault tolerance Time and budget 时间和预算 简单性 simplicity可行性 feasibility 另外， 随着业务的发展，关注点也是在不断发生变化的，这个时候，架构所侧重的架构特性也是随之改变的。 可扩展性 vs 弹性 可伸缩性（Scalability）：指的是系统在用户或请求数量增加时，仍然能够维持性能和运行的能力。它衡量的是系统在负载线性增加时，性能是否能够保持相应的线性增长。例如，如果一个系统在用户增加一倍时，其性能也能线性提升，那么它就是可伸缩的。这通常通过增加资源（如服务器实例）来实现，以应对持续增长的用户数量。 弹性（Elasticity）：指的是系统处理请求突发性增长的能力。它关注的是系统如何有效地应对不可预测和可变的用户流量高峰。例如，音乐会售票系统在门票开售时会经历用户流量的突然飙升，这需要高弹性的支持。一个具有弹性的系统能够在流量高峰时动态地启动新的处理单元（Processing Units），并在负载降低时关闭它们 简而言之，可伸缩性是关于处理增加的负载并保持性能，而弹性是关于处理突发性、不可预测的负载波动。","tags":["读书笔记","软件架构","fosa"],"categories":["架构设计"]},{"title":"FOSA丨04丨架构特性定义","path":"/2025/07/03/fosa-ch4/","content":"本系列文章通过逐章回答《Fundamentals of Software Architecture》（下文简称 FOSA）一书中的课后思考题，来深入理解书中的核心概念和理论，从而提升我们的软件架构设计能力。本篇为第四章内容。 本章的课后题是： What three criteria must an attribute meet to be considered an architecture characteristic? 一个属性要被认定为架构特性，必须满足哪三个条件？ What is the difference between an implicit characteristic and an explicit one? Provide an example of each. 隐性特性和显性特性的区别是什么？请分别举一个例子。 Provide an example of an operational characteristic. 提供一个操作特性的例子。 Provide an example of a structural characteristic. 提供一个结构特性的例子。 Provide an example of a cross-cutting characteristic. 提供一个交叉特性的例子。 Which architecture characteristic is more important to strive for—availability or performance? 在架构特性中，更应努力追求的是可用性还是性能？ 架构特性定义 一个属性要成为架构特性（Architecture Characteristics），需至少满足 3 个条件： 指定非领域设计考量：架构特性关注的是应用程序\"如何\"实现需求以及做出某些选择\"为何\"的原因，而不是应用程序\"应该做什么\"的业务需求。例如，性能水平通常不会出现在需求文档中，但却是重要的架构特性。 影响设计的某个结构方面：如果一个架构特性需要特殊结构考虑才能成功，那么它就会上升到架构特性的层面。例如，一般的安全性对于几乎所有项目都是必需的，但当需要设计特定的模块、组件或服务来隔离关键安全问题时，安全才成为一个架构特性。 对应用程序的成功至关重要：应用程序可以支持大量的架构特性，但并非所有都应该被支持。支持每个架构特性都会增加设计的复杂性，因此，架构师的关键任务是选择最少的、对应用程序成功至关重要或重要的架构特性，而不是尽可能多的。 显性 隐性 显性架构特性 (Explicit Architecture Characteristics) 是在需求规范中明确列出的，作为必要设计的一部分。它们通常直接出现在需求文档或其他具体说明中。 在书中的 Silicon Sandwiches 的案例中，用户数量（“当前数千，未来可能数百万”）就隐含地要求了可伸缩性 (Scalability)，即在不严重降低性能的情况下处理大量并发用户的能力。虽然需求没有明确提出可伸缩性，但从用户数量的描述中可以推断出来 隐性架构特性 (Implicit Architecture Characteristics) 很少出现在需求文档中，但它们对于项目的成功是必需的。架构师必须利用他们对问题领域的知识，在分析阶段发现这些特征。 可用性 (Availability) 是一种隐性特征，它确保用户可以访问系统。与可用性密切相关的是可靠性 (Reliability)，它确保网站在交互过程中保持正常运行，不会出现连接中断等问题。这些特征通常不会在设计文档中明确指定，但对于几乎所有应用程序都至关重要。 操作特性 操作性架构特性涵盖了系统的运行能力，例如性能、可伸缩性、弹性、可用性和可靠性等。这些特性通常与运营和 DevOps 关注点高度重叠。 特性 说明 Availability 系统需要保持可用的时间长度；例如，如果需要 24/7 可用，则需要采取措施确保系统始终可用。它指的是软件可操作和可访问的程度。 Continuity 灾难恢复能力。 Performance 衡量应用程序请求和响应周期所需的时间。它包括压力测试、高峰分析、功能使用频率分析、所需容量和响应时间。它也可以是更具体的度量，例如首屏渲染时间，即网页首次可见的时间。 Recoverability 业务连续性要求（例如，发生灾难时，系统需要多快才能重新上线？）这将影响备份策略和对复制硬件的要求。它也指软件从故障中恢复的能力，通过恢复任何受影响的数据并重新建立系统的所需状态。 Reliability/Safety 评估系统是否需要具备故障安全能力，或者其任务关键性是否影响生命。如果系统发生故障，是否会给公司带来巨额损失。它指系统在指定条件下和指定时间内运行的程度。 Robustness 在互联网连接中断、断电或硬件故障时，处理错误和边界条件的能力。 Scalability 系统随着用户或请求数量的增加而执行和运行的能力。这意味着处理大量并发用户而不会出现严重的性能下降。 结构特性 结构性架构特性关注代码结构。在许多情况下，架构师对代码质量问题负有独立或共同的责任，例如良好的模块化、组件间的受控耦合、可读性强的代码以及其他内部质量评估。 特性 说明 Configurability 最终用户通过可用界面轻松更改软件配置方面的能力。 Extensibility 系统的可扩展性。 Installability 系统在所有必要平台上安装的便捷性。它指软件在指定环境中安装和/或卸载的程度。 Leverageability/Reuse 跨多个产品利用通用组件的能力。它指开发人员在多个系统或构建其他资产中重复使用资产的程度。 Maintainability 开发人员修改、纠正或使其适应环境和/或需求变化的有效性和效率程度。 Portability 系统是否需要在多个平台上运行。它指开发人员将系统、产品或组件从一个硬件、软件或其他操作或使用环境转移到另一个环境的程度。 Supportability 应用程序所需的技术支持级别。系统中调试错误所需的日志记录及其他设施的级别。 Upgradeability 从该应用程序/解决方案的旧版本轻松/快速升级到新版本的能力。 交叉特性 交叉架构特性指的是那些难以归类或超出传统类别，但却形成重要设计约束和考虑的特性。 特性 说明 Accessibility 确保所有用户（包括色盲或听力障碍等残障用户）能够访问系统。它指使软件可供具有最广泛特征和能力的人使用。 Archivability 数据是否需要在一段时间后归档或删除。 Authentication 确保用户是其所声称的身份的安全要求。 Authorization 确保用户只能访问应用程序内特定功能（按用例、子系统、网页、业务规则、字段级别等）的安全要求。 Legal 系统在哪些法律约束下运行（数据保护、萨班斯-奥克斯利法案、GDPR 等）？公司需要哪些保留权利？关于应用程序构建或部署方式的任何规定。 Privacy 隐藏内部公司员工交易信息的能力（加密交易，甚至数据库管理员和网络架构师都无法查看）。 Security 数据是否需要在数据库中加密？内部系统之间网络通信是否需要加密？远程用户访问需要何种类型的认证？它指软件保护信息和数据的程度，以便人员或其他产品或系统具有与其授权类型和级别相称的数据访问程度。 Supportability 应用程序所需的技术支持级别。系统中调试错误所需的日志记录及其他设施的级别。 Usability/Achievability 用户使用应用程序/解决方案实现目标所需的培训水平。它指用户可以有效、高效、满意地使用系统达到预期目的。 可用性 vs 性能 在架构特性中，没有绝对的\"更重要\"之分，只有权衡取舍 (trade-offs)。这是软件架构的第一定律。架构师很少能够设计一个系统来最大化每一个架构特性。 可用性和性能之间的选择取决于具体的业务驱动因素、环境和一系列其他因素。例如： 业务需求： 如果一个系统对用户来说必须始终可用，即使偶尔慢一点也可以接受，那么可用性可能是更高的优先级。例如，一个紧急服务系统或金融交易系统，即使延迟增加，也必须确保服务不中断。 如果业务目标是提供极快的响应时间，即使偶尔停机也能接受，那么性能可能更重要。例如，高频交易系统，毫秒级的延迟都可能导致巨大损失。 相互影响与权衡：提升某一个架构特性往往会对其他特性产生负面影响。例如，为了提高安全性，可能需要进行更多的加密和间接操作，这几乎肯定会负面影响性能。 上下文决定：不同的系统部分可能需要不同的优先级。例如，在一个在线拍卖系统中，拍卖师界面的可用性和可靠性可能比单个竞拍者界面的可用性更关键。 因此，架构师的工作是分析这些权衡，并根据特定情况选择\"最不差的架构\"（least worst architecture）。这要求架构师深入理解业务领域，并与所有相关利益方（包括开发人员、业务分析师、产品负责人等）进行协作，而不是孤立地做出决策。","tags":["读书笔记","软件架构","fosa"],"categories":["架构设计"]},{"title":"RAG 全栈技术","path":"/2025/07/03/ai-rag-tech-complete/","content":"1. 文档加载 langchain-document_loaders langchian Document page_content: 文档内容 metadata: 文档元信息 from langchain.schema import Documentdocument = Document( page_content=Hello, world!, metadata=source: https://example.com) html 在线网页：from langchaincommunity.document_loaders _import WebBaseLoader 本地文件：from langchaincommunity.document_loaders _import BSHTMLLoader 解析代码：from bs4 import BeautifulSoup from bs4 import BeautifulSoup# 读取 HTML 文件内容html_txt = with open(./file_load/test.html, r) as f: for line in f.readlines(): html_txt += line# 解析 HTMLsoup = BeautifulSoup(html_txt, lxml)# 代码块 td class=codecode_content = soup.find_all(td, class_=code)for ele in code_content: print(ele.text) print(+*100) 这里对代码块解析时的 class 需要根据具体网页的元素定义进行更换，不过大体思路都一样（也不局限于代码块）。 PDF 加载文件：from langchaincommunity.document_loaders _import PyMuPDFLoader 解析表格：import fitz import fitzdoc = fitz.open(./file_load/fixtures/zhidu_travel.pdf)table_data = []text_data = []doc_tables = []for idx, page in enumerate(doc): text = page.get_text() text_data.append(text) tabs = page.find_tables() for i, tab in enumerate(tabs): ds = tab.to_pandas() table_data.append(ds.to_markdown())for tab in table_data: print(tab) print(=*100) Unstructured Unstructured 是由 Unstructured.IO 开发的开源 Python 库，专为处理非结构化数据（如 PDF、Word、HTML、XML 等）设计。在 LangChain 中，它作为文档加载的核心工具，实现以下功能： 格式支持广泛：解析 PDF、DOCX、PPTX、HTML、XML、CSV 等格式，甚至支持扫描件中的 OCR 文本提取。 元素分区（Partitioning）：将文档拆分为结构化元素（标题、段落、表格、列表），保留原始布局和元数据。 数据清洗：自动清理文档中的无关符号、页眉页脚，生成纯净文本。 使用 langchain_unstructured 需要安装： uv add unstructureduv add langchain_unstructureduv add unstructured_inferenceuv add unstructured_pytesseract# 系统依赖（macOS）brew install popplerbrew install tesseractbrew install libmagicbrew install ghostscriptbrew install pandoc PDF 需要额外安装： uv remove camelot-py # 如果有 camelot 需要先移出，在一些版本上存在冲突uv add unstructured[pdf] 使用时导入包： from langchain_unstructured import UnstructuredLoader PPT 需要安装额外依赖： uv add python-pptx 使用时导入包： from langchain_community.document_loaders import UnstructuredPowerPointLoader 解析 PPT 中的表格及其他特殊类型，可以使用原始的 python-pptx 库： from pptx import Presentationfrom pptx.enum.shapes import MSO_SHAPE_TYPEppt = Presentation(./file_load/fixtures/test_ppt.pptx)for slide_number, slide in enumerate(ppt.slides, start=1): print(fSlide slide_number:) for shape in slide.shapes: if shape.has_text_frame: # 文本信息 print(shape.text) if shape.has_table: # 表格信息 table = shape.table for row_idx, row in enumerate(table.rows): for col_idx, cell in enumerate(row.cells): cell_text = cell.text print(fRow row_idx + 1, Column col_idx + 1: cell_text) if shape.shape_type == MSO_SHAPE_TYPE.PICTURE: # 图片信息 imgae = shape.image image_filename = ./file_load/fixtures/pic_from_ppt.jpg with open(image_filename, wb) as f: f.write(imgae.blob) Word 需要安装额外依赖： uv add docx2txtuv add python-docx 使用时导入包： from langchain_community.document_loaders import Docx2txtLoader 解析 Word 中的表格及其他特殊类型，可以使用原始的 python-docx 库： from docx import Documentdef read_docx(file_path): doc = Document(file_path) for para in doc.paragraphs: print(para.text) for table in doc.tables: for row in table.rows: for cell in row.cells: print(cell.text, end= | ) print()file_path = ./file_load/fixtures/test_word.docxread_docx(file_path=file_path) Excel 需要安装额外依赖： uv add openpyxl ragflow.deepdoc RAGFlow 是一个开源的、基于\"深度文档理解\"的 RAG 引擎。 RAGFlow 的主要特点： 开箱即用： 提供 Web UI 界面，用户可以通过简单的几次点击，无需编写代码，就能完成知识库的建立和问答测试。通过 Docker 可以一键部署，非常方便。 工作流自动化 (Automated Workflow)： RAGFlow 将复杂的 RAG 流程（文档解析、切块、向量化、存储、检索、生成）模板化。用户可以选择不同的模板来适应不同的数据和任务需求，整个过程高度自动化。 可视化与可解释性： 在处理文档时，RAGFlow 会生成一个可视化的解析结果图，让用户能清晰地看到文档是如何被理解和切分的，大大增强了系统的透明度和可调试性。 企业级特性： 它支持多种文档格式，能够生成可溯源的答案（即答案会附上来源出处），并且兼容多种 LLM 和向量数据库，易于集成到现有企业环境中。 如果说 RAGFlow 是一个高效的问答“工厂”，那么 DeepDoc 就是这个工厂里最核心、最先进的“原材料加工车间”。所有外部文档在进入知识库之前，都必须经过 DeepDoc 的精细处理。 DeepDoc 的全称是 Deep Document Understanding（深度文档理解），它是 RAGFlow 实现高质量检索的基石。它并非简单地提取文本，而是试图像人一样“看”和“理解”文档的版面布局和内在逻辑。 DeepDoc 的工作原理与核心能力： 视觉版面分析 (Vision-based Layout Analysis)： 理论： DeepDoc 首先会利用计算机视觉（CV）模型，像人眼一样扫描整个文档页面。它不是逐行读取字符，而是先识别出页面上的宏观结构，例如：这是标题、那是段落、这是一个表格、这是一张图片、这是一个页眉/页脚。 实践： 对于一个两栏布局的 PDF 报告，传统的文本提取工具可能会把左边一行的结尾和右边一行的开头错误地拼在一起。而 DeepDoc 的视觉分析能准确识别出两个独立的栏目，并按照正确的阅读顺序（先读完左栏，再读右栏）来处理文本。 智能分块 (Intelligent Chunking)： 理论： 这是 DeepDoc 最具价值的一点。在理解了文档布局之后，它会进行“语义分块”而非“物理分块”。传统的 RAG 会把文档切成固定长度（如 500 个字符）的块，这常常会将一个完整的表格或一段逻辑连贯的话拦腰截断。 实践： DeepDoc 会将一个完整的表格识别出来并视为一个独立的“块”（Chunk）。一个标题和它紧随其后的段落也会被智能地划分在一起。这样做的好处是，当用户提问与表格相关的问题时，系统检索到的就是这个包含完整上下文的表格块，而不是表格的某几行碎片。这极大地保证了提供给 LLM 的上下文信息的完整性和逻辑性。 高质量光学字符识别 (OCR)： 理论： 对于扫描的 PDF 文件或者文档中嵌入的图片，DeepDoc 内置了高质量的 OCR 引擎。 实践： 即便文档是扫描的复印件，它也能尽可能准确地提取出其中的文字内容，并将其融入到上述的版面分析中，确保信息不丢失。 表格解析与转译： 理论： 识别出表格只是第一步，更关键的是让 LLM 能“读懂”表格。 实践： DeepDoc 能够提取出表格的结构化数据，并将其转换为 LLM 更容易理解的格式，例如 Markdown 格式。一个复杂的表格图片，在经过 DeepDoc 处理后，可能会变成一个 Markdown 文本表格，这样 LLM 就能轻松地理解其行列关系，并回答诸如“请总结一下表格中第三季度销售额最高的产品是哪个？”这类的问题。 笔者在实践过程中，通过精简 ragflow.deepdoc 中的 pdfparser，抽出了一个组件 deepdoc_pdfparser. 2. 分块策略 可视化工具：ChunkViz RAG 五大分块策略 参考：5-chunking-strategies-for-rag 3. 向量嵌入 3.1 嵌入模型评测 Hugging Face 的 MTEB (Massive Text Embedding Benchmark) 是一个大规模的文本嵌入模型评测基准。它的核心作用是为各种文本嵌入模型提供一个统一、全面、客观的性能衡量标准。 涵盖了文本嵌入在现实世界中最常见的 8 种应用场景，共计 58 个数据集和 112 种语言。这 8 大任务分别是： Bitext Mining (双语文本挖掘): 在不同语言的句子中找出翻译对。 Classification (分类): 将文本划分到预定义的类别中。 Clustering (聚类): 将相似的文本分组在一起。 Pair Classification (句子对分类): 判断两个句子是否具有某种关系 (如释义、矛盾等)。 Reranking (重排序): 对一个已经排好序的列表 (如搜索结果) 进行重新排序，以提升质量。 Retrieval (检索): 从一个大规模的文档语料库中找出与查询最相关的文档。这是目前文本嵌入最核心和最热门的应用之一。 Semantic Textual Similarity (STS, 语义文本相似度): 判断两个句子的语义相似程度，通常给出一个从 0 到 5 的分数。 Summarization (摘要): 评估生成的摘要与原文的语义相似度。 MTEB 3.2 稀疏嵌入（Sparse Embedding） 特征 说明 维度 通常等于完整词表或特征集合的大小，可达 10⁵ – 10⁶；大多数维度为 0，只有少数位置有权重 构造方式 基于词频或词频-逆文档频率（TF-IDF）、BM25 等统计方法，不依赖深度学习 权重含义 每个非零维可直观解释为某个词或特征的重要度，具有高度可解释性 检索/存储 用倒排索引即可实现 O(1) 级精确匹配；在线增量更新代价低 优势 对长文档、术语精确匹配友好 易于调参（停用词、词根化） 资源消耗小、无推理延迟 劣势 维度极高，逐向量暴力计算代价大 只捕获词面共现，无法理解语义或同义词 对拼写/语序变化鲁棒性差 TF-IDF(Term Frequency - Inverse Document Frequency，词频-逆文档频率) 一种经典的加权方案，用来衡量 词语 t 对 文档 d 在 语料库 D 中的重要程度。 一句话：词在整个语料库中出现得越少，但在本篇文档中出现得越多，那它就越重要。 公式：\\(TF-IDF(t,d,D) = TF(t,d) × IDF(t,D)\\) TF（局部权重）： 计数：tf = #t 出现次数 频率：tf = #t / |d| 对数平滑：tf = 1 + log(#t) ID（全局权重）：\\[IDF(t) = log\\frac{N-df(t)+0.5}{df(t)+0.5} \\] N = 语料中文档总数 df(t) = 含词 t 的文档数 加 1 或 0.5 可以避免分母为 0，并抑制长尾噪声。 BM25(Best Matching 25) 可视为 TF-IDF 的扩展版，进一步引入： k₁ 控制 TF 饱和：TF 越大，增益递减。 b 长度归一化：文档越长，单词 TF 权重被抑制。 公式：\\[w(t,d)=IDF(t)⋅\\frac{TF(k_{1} +1)}{TF+k_{1}·（1-b+b·\\frac{文档长度}{平均文档长度} ）} \\] 角色 控制对象 常见区间 极值行为 直觉比喻 k₁ (saturation factor) TF 饱和曲线斜率——同一个词在同一文档中重复出现到第 n 次时，还能再加多少分 1.0 – 2.0 k₁ → 0：完全不计重复词；k₁ → ∞：线性计数，退化为 TF-IDF 沾一滴酱油 vs. 倒一瓶酱油：味道总有极限，不会永远 1 → 2 → 3 倍变浓 b (length normalizer) 文档长度惩罚强度——长文能否用“大块头”刷分 0.3 – 0.9 b = 0：不考虑长度（BM15）b = 1：长度全量归一化（BM11） 打篮球按身高加分：b=0 不管身高；b=1 按身高严格扣分；中间值折中 3.3 密集嵌入（Dense Embedding） 特征 说明 维度 兼顾效率与表达力，常见 128 – 1536；每一维几乎都非零。 构造方式 由深度模型（BERT、Sentence-BERT、OpenAI text-embedding-3-small 等）端对端学习，捕获上下文语义 权重含义 单维难以直观解释，但整体向量在低维空间中编码了丰富的语义相似度 检索/存储 需专门的 ANN（HNSW、Faiss IVF-PQ 等）索引；向量更新需重新编码 优势 具备语义泛化能力，能跨同义词、拼写、语序可跨语言、跨模态（图文）在 RAG/问答场景提升召回率 劣势 训练与推理成本高（GPU/CPU 向量化计算）结果可解释性弱 在线增量写入需再编码、重建索引 3.4 ColBERT ColBERT 是一种让 BERT 用“词级小向量”做快速、精准文本检索的方法 —— 既不像传统 TF-IDF 那样粗糙，也不像跨编码器那样慢。 ColBERT = “把 BERT 的句向量拆成 token 向量，再用 Late Interaction 重新拼起来做检索”的工程化改造版 BERT。 Late Interaction 就是把查询（Q）和文档（D）先独立编码，等到最后打分时再让它们在 token 级别 做一次“小范围、轻量级”的互动——既不像 Cross-Encoder 那样“一上来就深度交互”，也不像 Bi-Encoder 那样“全程零交互”。 换言之，BERT 提供语言理解底座，ColBERT 在此之上加了面向检索的输出格式与打分逻辑，二者既同宗又分工明确。 预训练阶段（同一个 BERT 权重）┌───────────────┐│ Google BERT │ ← 海量文本上做 MLM/NSP└───────────────┘│ （加载相同参数）╭───────────┴───────────╮│ ││ ↓ 普通微调 │ ↓ ColBERT 微调│ （分类、NER…） │ （稠密检索）│ ││ 取 [CLS] 整句向量 │ 保留 每个 token 向量│ + 任务特定头 │ + Late-Interaction 打分│ │╰───────────┬───────────╯│下游推理/检索 3.5 BGE-M3 BGE-M3 是由智源研究院（BAAI）开发的新一代旗舰文本嵌入模型，它开创性地在单一模型内集成了多语言（支持超过 100 种语言）、长文本 （支持 8192 词符）和多功能检索（同时支持稠密、稀疏和多向量检索）的强大能力。 M 含义 具体能力 参考 Multi-Functionality 多功能 同时产出 稠密向量（dense）、多向量/ColBERT（colbert） 和 稀疏向量（sparse），一套模型即可覆盖混合检索需求。 huggingface.bge-m3 Multi-Linguality 多语种 覆盖 100+ 语言，是目前公开数据集中多语检索任务的 SOTA。 arXiv.bge-m3 Multi-Granularity 多粒度 最长输入 8 192 token，既能编码短句也能处理长文档。 huggingface.bge-m3 4. 查询增强技术 4.1 查询构建 4.1.1 Text-to-SQL Text-to-SQL 构建 DDL 知识库：schema 提取与切片； 构建 Q-SQL 知识库：示例对注入； 构建 DB 描述知识库：业务描述补充； 提供 RAG 检索上下文； 调用 LLM 进行 SQL 生成； 执行 SQL 并反馈结果； 迭代直到正确解决问题。 常用框架： vanna Chat2DB DB-GPT 4.1.2 Text-to-Cypher 跟 Text-to-SQL 一样，只不过是生成图数据库（neo4j）查询语句。 Text-to-Cypher 构建图元模型（Graph Metamodel）知识库； 构建 Q-Cypher 知识库（示例对注入）； 构建图描述（Graph Description）知识库； 提供 RAG 检索上下文； 调用 LLM 进行 SQL 生成； 执行 SQL 并反馈结果； 迭代直到正确解决问题。 4.1.3 从查询中提取元数据构建过滤器 从查询中提取元数据构建过滤器 将自然语言转为向量查询语句； 利用 LLM 推断出元数据过滤条件； 在查询检索时，根据过滤条件进行文档过滤； 返回过滤后的相似文档； 实战案例： https://ragflow.io/blog/implementing-text2sql-with-ragflow https://medium.com/neo4j/generating-cypher-queries-with-chatgpt-4-on-any-graph-schema-a57d7082a7e7 4.2 查询翻译 通过对用户查询进行改造和扩展，使其更加清晰、具体，从而提高检索精度。 常用工具： 方案 链接 说明 ragbear GitHub - lexiforest/ragbear rewrite= 参数多种改写模式 LangChain Query Transformations 内置链式改写 LlamaIndex Query Transform Cookbook ¶ 多策略组合 Haystack Advanced RAG: Automated Structured Metadata Enrichment | Haystack pipeline node 4.2.1 Query2Doc Query2Doc 是指将 query 直接交给 LLM 去生成一份相关文档，然后将 query 和生成的文档一起去进行检索。虽然 LLM 生成的文档可能不对，但是提供了更丰富的信息、丰富了问题的语义，有助于提高检索时的精度。 def query2doc(query): prompt = f你是一名公司员工制度的问答助手，熟悉公司规章制度，请简短回答以下问题：query doc_info = llm(prompt) context_query = fquery, doc_info return context_query 4.2.2 HyDE HyDE（Hypothetical Document Embeddings，假设文档向量）让 LLM 根据 query 去生成一系列假设性文档，然后将这些文档跟 query 一起做向量化，取向量均值去进行检索。 def hyde(query, include_query=True): prompt_template = 你是一名公司员工制度的问答助手，熟悉公司规章制度，请简短回答以下问题： Question: question Answer: prompt = PromptTemplate(input_variables=[question], template=prompt_template) embeddings = HypotheticalDocumentEmbedder(llm_chain= prompt | llm, base_embeddings=embedding_model.get_embedding_fun()) hyde_embedding = embeddings.embed_query(query) if include_query: query_embeddings = embedding_model.get_embedding_fun().embed_query(query) result = (np.array(query_embeddings) + np.array(hyde_embedding)) / 2 result = list(result) else: result = hyde_embedding result = list(map(float, result)) return result 4.2.3 子问题查询 当问题比较复杂时，可以利用 LLM 将问题拆解成子问题，每个子问题都生成检索上下文，可以根据合并后总的上下文回答，也可以每个上下文独立回答后汇总。 def sun_question(query): prompt_template = 你是一名公司员工制度的问答助手，熟悉公司规章制度。 你的任务是对复杂问题继续拆解，以便理解员工的意图。 请根据以下问题创建一个子问题列表： 复杂问题：question 请执行以下步骤： 1. 识别主要问题：找出问题中的核心概念或主题。 2. 分解成子问题：将主要问题分解成可以独立理解和解决的多个子问题。 3. 只返回子问题列表，不包含其他解释信息，格式为： 1. 子问题1 2. 子问题2 3. 子问题3 ... prompt = PromptTemplate(input_variables=[question], template=prompt_template) llm_chain = prompt | llm sub_queries = llm_chain.invoke(query).split( ) return sub_queries 4.2.4 查询改写 当问题表达不清、措辞差、缺少关键信息时，使用 LLM 根据用户问题多角度重写问题，增加额外的信息，提高检索质量。 def question_rewrite(query): prompt_template = 你是一名公司员工制度的问答助手，熟悉公司规章制度。 你的任务是需要为给定的问题，从不同层次生成这个问题的转述版本，使其更易于检索，转述的版本增加一些公司规章制度的关键词。 问题：question 请直接给出转述后的问题列表，不包含其他解释信息，格式为： 1. 转述问题1 2. 转述问题2 3. 转述问题3 ... prompt = PromptTemplate(input_variables=[question], template=prompt_template) llmchain = prompt | llm rewrote_question = llmchain.invoke(query) return rewrote_question 4.2.5 查询抽象 查询抽象（Take a Step Back）是指当问题包含太多的细节，可能导致检索时忽略了关键的信息，降低检索质量。可以将用户的具体问题转化为一个更高层次的抽象问题，一个更广泛的问题，关注于高级概念或原则，从而提高检索质量。 from langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts.chat import ChatPromptTemplatefrom langchain_core.prompts.few_shot import FewShotChatMessagePromptTemplate# 将复杂问题抽象化，使其更聚焦在本质问题上def take_step_back(query): examples = [ input: 我祖父去世了，我要回去几天, output: 公司丧葬假有什么规定？, , input: 我去北京出差，北京的消费高，有什么额外的补助？, output: 员工出差的交通费、住宿费、伙食补助费的规定是什么？ , ] example_prompt = ChatPromptTemplate.from_messages( [ (human, input), (ai, output), ] ) few_shot_prompt = FewShotChatMessagePromptTemplate( example_prompt=example_prompt, examples=examples, ) prompt = ChatPromptTemplate.from_messages( [ ( system, 你是一名公司员工制度的问答助手，熟悉公司规章制度。 你的任务是将输入的问题通过归纳、提炼，转换为关于公司规章制度制定相关的一般性问题，使得问题更容易捕捉问题的意图。 请参考下面的例子，按照同样的风格直接返回一个转述后的问题： ), # few shot exmaples, few_shot_prompt, # new question (user, question) ] ) question_gen = prompt | llm | StrOutputParser() res = question_gen.invoke(question: query).removeprefix(AI: ) return res 4.3 查询路由 查询路由（Query Routing）是指根据用户问题的具体意图，自动判断应该将该问题导向最合适的数据源（例如向量知识库、SQL 数据库、图数据库或特定 API）以获取最精准信息的决策过程。 思路 图示 逻辑路由 语义路由 5. 索引优化技术 基本思路： 父子文档索引 分层索引 多表示索引 5.1 从小块到大上下文 向量检索的时候，检索到的是一个小文档，但是通过小文档的 metadata，返回给 LLM 的是查询出来的大文档。 节点-句子滑动窗口检索 SentenceWindowNodeParser SentenceWindowNodeParser 父子文本块检索：ParentDocumentRetriever ParentDocumentRetriever 前后串连、自动扩展上下文：PrevNextNodePostprocessor、AutoPrevNextPostprocessor PrevNextNodePostprocessor 5.2 构建有层次的索引 构建两个向量数据库（Summary 和 Details），通过 Metadata 进行连接； 通过 llamaindex 的 indexnode 和 PandasQueryEngine； 通过查询先检索相关表名，然后做 Text2SQL。 Summary Detail 5.3 构建多表示索引 构建混合索引：EnsembleRetriever EnsembleRetriever 构建多表示索引：MultiVectorRetriever MultiVectorRetriever 6. 检索后优化技术 6.1 重排 rerank 传统的搜索或推荐系统通常分为两步： 召回（Recall）: 从海量的候选集中，快速、粗略地筛选出几百或几千个可能相关的项目。此阶段追求速度和查全率，常用技术包括基于关键词的搜索（如 BM25）或向量相似度搜索（ANN）。 排序/重排（Ranking/Reranking）: 对召回的结果进行更精细、更复杂的计算，以确定最终呈现给用户的顺序。此阶段追求精准度和查准率。我们今天讨论的技术就属于这个范畴。 可以把这个过程比作“海选”和“决赛”。召回是海选，快速淘汰掉明显不相关的选手；重排是决赛，评委（重排模型）对入围选手进行全方位的严格评审，最终给出排名。 6.1.1 RRF 重排 民主投票式的融合 RRF 是一种简单、高效、无需训练的“结果融合”策略。想象一下，你有多个独立的搜索系统（比如一个关键词搜索，一个向量搜索），它们各自对同一批文档给出了自己的排名。RRF 的作用就是将这些不同的排名列表“民主地”融合成一个最终的、可能更优的排名列表。 RRF 的核心思想是：一个文档在多个列表中的排名越靠前，它在最终列表中的排名就应该越靠前。 与简单地将不同系统的得分相加不同，RRF 采用“倒数排名”（Reciprocal Rank）来计算每个文档的最终分数。 RRF Rerank 优势： 无需训练: 即插即用，非常方便。 性能稳健: 在不同召回源质量参差不齐时，表现通常比简单的分数相加（如 sum fusion）更稳定。 计算开销极低: 几乎不增加额外的计算负担。 缺点： 效果上限不高: 它只利用了“排名”这一个信息，忽略了不同系统给出的原始“分数”中蕴含的置信度信息。 依赖召回质量: 如果所有召回源的质量都很差，RRF 也无力回天。 适应场景： 混合搜索（Hybrid Search）：融合关键词搜索（BM25）和向量搜索的结果。 多模态搜索：融合文本、图片等不同模态的搜索结果。 6.1.2 Cross-Encoder 重排 query 和文档的\"深度阅读理解\" 如果说召回阶段的向量搜索是\"看标题识文章\"，那么 Cross-Encoder 重排就是\"把 query 和每篇文档放在一起，逐字逐句地做一篇完整的阅读理解\"。 它通过深度学习模型（通常是 Transformer 架构，如 BERT）来判断一个 query 和一个文档之间的相关性到底有多强。 Cross-Encoder Rerank 基本流程： 输入构建: 将查询（Query）和待排序的文档（Document）用一个特殊的分隔符（如 [SEP]）拼接在一起，形成一个单一的输入序列。例如：[CLS] 我的问题是什么？[SEP] 这是候选文档的内容... [SEP] 模型计算: 将这个拼接后的序列输入到一个预训练好的 Transformer 模型（如 BERT）中。模型内部的自注意力机制（Self-Attention）会让 Query 中的每个词和 Document 中的每个词都进行充分的交互和信息比对。 分数输出: 模型在处理完整个序列后，通常会利用起始位置 [CLS] Token 对应的输出向量，接一个简单的线性层，最终输出一个单一的分数（logit），这个分数就代表了 Query 和 Document 之间的相关性得分。 排序: 根据所有文档得到的相关性得分，从高到低进行排序，得到最终结果。 优点： 效果极佳: 由于对 query 和文档进行了深度的、非对称的交互分析，其精度通常是所有重排方法中最高的。 缺点： 计算成本极高: 对于每个 (query, document) 对，都需要进行一次完整的、重量级的模型前向传播。如果召回了 500 个文档，就需要进行 500 次 BERT 模型的计算，这在实时性要求高的场景下是巨大的挑战。 无法提前索引: 文档的表示不是独立的，必须在查询时与 query 结合才能计算，因此无法像向量搜索那样提前为所有文档建立索引。 适用场景： 对精度要求极高，且可以容忍较高延迟的场景，如某些法律或医疗文献的精确查找。 作为“黄金标准”来生成高质量的标注数据，用于训练更轻量的召回模型（如 Bi-Encoder）。 6.1.3 ColBERT 重排 Contextualized Late Interaction over BERT。 介于“看标题”和“深度阅读”之间的“划重点式”阅读 ColBERT 试图在 Cross-Encoder 的高精度和 Bi-Encoder (召回阶段常用) 的高效率之间找到一个平衡点。它的核心思想是：不需要逐字逐句地进行完整对比，而是先把 query 和文档各自的\"重点\"（关键词向量）划出来，然后再计算这些重点之间的匹配程度。 ColBERT Rerank 基本流程： 独立编码: 首先，使用一个 BERT 类的模型（但稍作修改）分别独立地处理 Query 和 Document。它不再是输出一个单一的 [CLS] 向量，而是为 Query 和 Document 中的每一个 Token 都生成一个上下文相关的向量。 Query 端向量: 对于 Query，我们保留所有 Token 的输出向量。 Document 端向量: 对于 Document，我们也保留所有 Token 的输出向量。这些向量可以提前计算并存储，这是它比 Cross-Encoder 高效的关键。 延迟交互计算: 在查询时，进行“延迟交互”。具体来说，对于 Query 中的每一个 Token 向量，我们都去 Document 的所有 Token 向量中寻找一个最相似的（通过最大内积MaxSim操作）。 分数聚合: 最后，将 Query 中每个 Token 找到的最大相似度分数相加，得到最终的相关性总分。 这个过程好比： Query: \"best deep learning framework\" Document: \"PyTorch is a popular framework for deep learning...\" ColBERT 会分别计算： \"best\" 和文档中所有词向量的最大相似度。 \"deep\" 和文档中所有词向量的最大相似度。 \"learning\" 和文档中所有词向量的最大相似度。 \"framework\" 和文档中所有词向量的最大相似度。 然后把这四个最大相似度值加起来作为总分。 优点： 性能优越: 精度远超传统的 Bi-Encoder，并且在很多任务上能逼近 Cross-Encoder。 效率较高: 由于文档向量可以预计算和索引，查询时的计算开销远低于 Cross-Encoder，只涉及向量的相似度计算。 缺点： 存储开销大: 需要为文档中的每个 Token 都存储一个高维向量，存储成本远高于只存一个文档向量的 Bi-Encoder。 实现相对复杂: 其索引和查询逻辑比标准向量搜索更复杂。 适用场景： 需要高精度但又对延迟有一定要求的现代搜索引擎，如微软的 Bing 就在使用类似的技术。 作为 RAG 系统中的高质量重排器。 6.1.4 Cohere 和 Jina 重排 商业化的\"重排即服务\"（Reranking-as-a-Service） Cohere 和 Jina AI 都是提供 AI 模型和服务的公司。它们都将高质量的重排模型封装成了简单易用的 API 服务。本质上，它们提供的重排器很可能就是基于类似 Cross-Encoder 架构的、在海量高质量数据上训练和优化的专有模型。 优点： 使用简单: 只需几行代码调用 API 即可，无需关心模型训练、部署和维护。 效果保证: 通常能获得非常好的开箱即用效果，因为这些模型经过了大量数据的锤炼。 缺点： 成本: 按调用量或 token 数量计费，对于大流量应用可能是一笔不小的开销。 数据隐私: 需要将你的 query 和文档数据发送给第三方服务商，对于数据敏感的应用需要仔细评估其隐私政策。 灵活性受限: 无法像自建模型那样进行深度定制或调优。 适用场景： 快速原型验证（MVP）。 中小型企业或开发者，希望以最小的工程代价获得最好的排序质量。 大型企业中非核心但又需要高质量排序的业务场景。 6.1.5 RankGPT 和 RankLLM 这是最新的重排范式，直接利用 LLM 强大的语言理解和推理能力来进行排序。它的思路是：不再让模型输出一个简单的相关性分数，而是让 LLM 直接对召回的文档列表进行\"思考\"和\"比较\"，然后输出一个排序好的列表。 基本思路： 构建 Prompt: 将 query 和召回的文档列表（通常是文档的标题和摘要）格式化成一个复杂的 Prompt。这个 Prompt 会明确指示 LLM 作为一个排序专家，对给定的文档列表根据与 query 的相关性进行排序，并按指定的格式输出结果。 LLM 推理: 将这个 Prompt 发送给 LLM。LLM 会利用其强大的上下文理解能力，分析 query 的深层意图，并比较不同文档之间的细微差别（例如，一个内容更全面，另一个更新颖）。 解析输出: LLM 会返回一个文本结果，比如一个重新排序好的文档 ID 列表。程序需要解析这个文本输出来获取最终的排序。 优点： 理解复杂意图: LLM 能够理解非常复杂和模糊的 query，并能进行一定程度的推理，这是传统模型难以做到的。 零样本/少样本能力强: 无需针对特定任务进行微调，就能在很多场景下取得惊人的效果。 可解释性: 有时可以引导 LLM 给出排序的理由，增加了透明度。 缺点： 成本和延迟极高: 调用大型 LLM API 的成本和时间开销是目前所有方法中最高的，通常只能用于非实时或小批量任务。 上下文长度限制: LLM 的上下文窗口大小有限，一次能处理的文档数量和文档长度都受限。 稳定性问题: 输出格式可能不稳定，需要设计鲁棒的解析逻辑。结果也可能有一定的随机性。 适用场景： 对召回结果的“最后一公里”进行精加工，例如对前 10 名结果进行最终排序。 作为生成高质量排序标注数据的强大工具。 对成本不敏感、但对排序质量有极致要求的特定应用。 6.1.6 时效加权重排 这是一种业务逻辑驱动的重排策略，而非特定的模型或算法。其核心思想是：对于某些类型的查询，最新的信息比旧的信息更有价值。 基本思路： 时间衰减函数 (Time Decay Function): 设计一个函数，使得文档的分数随着其发布时间的流逝而衰减。最常用的函数是指数衰减或高斯衰减。 分桶加权: 将文档按发布时间分到不同的桶里，如\"24 小时内\"、\"一周内\"、\"一月内\"、\"更早\"。为每个桶设置一个固定的权重或加分项。例如，\"24 小时内\"的文档分数乘以 1.5，\"一周内\"的乘以 1.2 等。 优点： 实现简单: 逻辑清晰，容易实现和调整。 效果显著: 对于新闻、社交媒体、产品更新等时效性强的查询，能极大提升用户体验。 缺点： \"一刀切\"风险: 如果不加区分地对所有查询都增强时效性，可能会伤害那些寻求\"\"永恒\"知识的查询（如\"什么是牛顿第一定律\"）。 参数难调: 衰减函数的形状、权重 w 等参数需要根据经验和 A/B 测试来仔细调整。 适用场景： 新闻搜索: 用户总是想看最新的报道。 电商新品: 用户搜索\"手机\"时，可能更想看到最新款。 社交媒体 Feed: 最新的帖子通常排在最前面。 需要与查询意图识别结合: 一个优秀的系统应该能识别出哪些 query 是具有时效性意图的，然后动态地应用时效性加权。 6.2 压缩 compression 传统的 RAG 流程是“检索-增强-生成”。系统首先根据用户问题从知识库中检索出若干相关文档片段（Chunks），然后将这些片段作为上下文（Context）连同用户问题一起提交给大语言模型（LLM），由 LLM 生成最终答案。 这里面潜藏着几个挑战： 上下文窗口限制 (Context Window Limit)：每个 LLM 都有其上下文长度上限（如 GPT-4 是 128k tokens）。如果检索出的文档过多，会超出窗口限制，导致无法处理。 成本与延迟 (Cost Latency)：LLM 的 API 调用费用通常与输入的 Token 数量成正比。上下文越长，费用越高，同时模型的推理时间也越长，导致用户等待时间增加。 “大海捞针”问题 (Lost in the Middle)：研究表明，当 LLM 的上下文中包含大量信息时，它对位于上下文中间部分信息的注意力会下降。如果关键信息被大量无关或次要信息包围，LLM 可能无法有效利用它，从而影响生成答案的准确性。 噪声干扰 (Noise Interference)：检索出的文档片段虽然“相关”，但并非每个字、每句话都对回答当前问题至关重要。这些无关信息就是“噪声”，会干扰 LLM 的判断。 因此，RAG 压缩技术的核心目标，就是在将检索到的信息送入 LLM 之前，对其进行“精炼”——去除无关信息、保留核心内容，从而在降低成本、提升效率的同时，提高最终答案的质量。 6.2.1 上下文压缩检索器 上下文压缩检索器是指 LangChain 提供的 Contextual Compression Retriever。 它不是一个独立的检索器，而是一个\"包装器\"（Wrapper）。它首先使用一个常规的检索器（如 VectorStoreRetriever）获取一批文档，然后通过一个嵌入的\"文档压缩器\"（Document Compressor）对这些文档进行筛选或重写，最后只返回那些真正重要的信息。 Contextual Compression Retriever LangChain 提供了两种主流的压缩器： LLMChainExtractor：这个压缩器内部会运行一个 LLM（通常是一个小模型）。它会遍历每个检索到的文档，并向 LLM 提出一个问题，例如：\"请从以下文档中抽取出与'[用户原始问题]'相关的句子。\" LLM 会根据指令抽取出关键句子，丢弃无关部分，从而实现压缩。这是一种基于 LLM 的抽取式压缩 EmbeddingsFilter：这个压缩器不依赖 LLM。它会计算用户问题和每个检索文档（或文档内更小的句子片段）的嵌入向量（Embedding）之间的相似度。只有当相似度超过预设的阈值（e.g., similarity_threshold=0.8）时，该文档或句子才会被保留。这是一种基于嵌入相似度的过滤式压缩。 优势： 提升信噪比：直接过滤掉与问题无关的整个文档或文档中的无关部分。 灵活性高：可以根据需求选择计算成本低但效果略粗糙的EmbeddingsFilter，或选择成本高但更智能的LLMChainExtractor。 模块化：与 LangChain 生态无缝集成，易于实现。 6.2.2 句子嵌入优化器 句子嵌入优化器是指 LlamaIndex 提供的 Sentence Embedding Optimizer。 与 LangChain 的 EmbeddingsFilter 思想非常相似，但它在 LlamaIndex 的生态系统内，并专注于句子级别的精细化过滤。 在检索到相关的文档块（Node）之后，不是将整个文档块都丢给 LLM，而是深入到文档块内部，逐一分析每个句子，只保留与用户问题最相关的句子。 基本原理： 初始节点检索：查询引擎首先从索引中检索出 Top-K 个最相关的节点（Nodes，相当于 LangChain 的 Documents）。 句子级分析：SentenceEmbeddingOptimizer（或类似功能的SimilarityPostprocessor）接收这些节点。它会： 将每个节点分解成单独的句子。 为每个句子计算一个嵌入向量。 计算每个句子的嵌入向量与用户原始问题嵌入向量之间的相似度得分。 阈值过滤：它会根据一个预设的相似度阈值（similarity_cutoff）来决定保留哪些句子。只有得分高于阈值的句子才会被保留下来，组合成新的、更精简的节点内容。 合成响应：最后，只有这些经过精炼的、包含高相关度句子的节点才会被送入响应合成器（Response Synthesizer），由 LLM 生成最终答案。 优势： 粒度极细：相比于过滤整个文档，句子级过滤能最大程度地保留一个文档块中的相关信息，同时剔除无关句子，精度更高。 减少上下文割裂：有时一个文档块整体相关度可能不高，但其中有一两句关键信息。这种方法可以精准地把这两句\"捞\"出来，避免整个文档块被丢弃。 6.2.3 LLMLingua 在将包含检索文档的冗长提示词（Prompt）发送给昂贵的大模型（如 GPT-4）之前，先用一个更小、更便宜的语言模型（如 GPT-2 或一个微调过的 Llama）来对这个提示词进行\"有损压缩\"。这个压缩过程会识别并删除那些对 LLM 理解问题和生成答案不太重要的词语或句子。 LLMLingua: Innovating LLM efficiency with prompt compression - Microsoft Research 基本原理： 构建完整提示词：将用户问题和所有检索到的文档拼接成一个完整的、非常长的提示词。 小模型介入：LLMLingua 使用一个小模型来分析这个长提示词。它会评估如果从提示词中删除某个词或某段话，对大模型理解原始提示词的“困惑度”会产生多大影响。 智能删除：它会优先删除那些对困惑度影响最小的词语和句子，因为这些内容被认为是信息量较低或冗余的。这个过程被设计得非常精巧，旨在保留关键的实体、术语和逻辑关系。 生成压缩提示词：经过这个过程，原始的长提示词被压缩成一个更短的版本，其中包含了原始上下文的\"精华\"。 提交大模型：最后，这个压缩后的、短小精悍的提示词被发送给目标大模型进行处理。 6.2.4 RECOMP 压缩 RECOMP (REtrieval-and-COMPression) 是一种面向复杂问题的、多步骤的 RAG 策略，它将压缩思想融入到了一个更宏大的框架中。 当面对一个需要综合多个信息源才能回答的复杂问题时，传统的 RAG 一次性检索出的文档可能包含大量不相关细节。RECOMP 通过\"分而治之\"和\"先抽取再合成\"的方式来创建高度浓缩和相关的上下文。 基本原理： 问题分解（可选）：对于一个非常复杂的问题，可能首先会将其分解为几个更简单的子问题。 检索与抽取 (Retrieve and Extract)：针对（每个子）问题，执行以下操作： 检索：从知识库中检索相关文档。 抽取：这是关键步骤。它不是直接使用这些文档，而是向 LLM 发出指令，要求 LLM 阅读每个文档，并从中抽取出与当前（子）问题直接相关的简明摘要或关键事实点。例如：\"请阅读以下关于 A 公司的财报，并抽取出其 2023 年第四季度的收入和利润数字。\" 压缩与合成 (Compress and Synthesize)： 将从所有文档中抽取出的摘要或事实点收集起来。 再次调用 LLM，将这些零散但高度相关的信息点合成成一段连贯、流畅、无冗余的文本。这段文本就是最终为原始复杂问题量身定制的“完美上下文”。 最终生成：将这个合成好的、高度浓缩的上下文连同原始问题一起提交给 LLM，生成最终答案。 优势： 极高的信息密度：最终生成的上下文几乎不含任何与问题无关的噪声，每一句话都是为了回答问题而存在的。 处理复杂问题的能力强：非常适合需要整合来自不同文档、不同主题信息的“多跳（multi-hop）”问题。 可解释性：由于中间步骤生成了摘要和事实点，这个过程比黑盒方法更易于调试和理解。 6.2.5 Prompt Caching 记忆上下文 它是一种性能优化技术，而非内容压缩技术，但常在处理长上下文时被提及。 在 Transformer 模型（所有现代 LLM 的基础）中，当模型处理一个序列时，它会为每个 Token 计算一个键（Key）和值（Value）向量，这个计算过程非常耗时。Prompt Caching（或称 KV Cache）技术的核心就是：将已经处理过的 Prompt 部分的 KV 向量缓存起来，下次请求时如果 Prompt 前缀相同，则直接复用缓存，无需重新计算。 基本原理： 首次请求：用户发送一个长 Prompt（例如，一篇需要总结的文章）。模型在处理这个长 Prompt 时，会计算其中每个 Token 的 KV 向量，并将它们存储在 GPU 的内存中（即 KV Cache）。 后续交互：现在，用户基于这篇文章提问（例如，“文章的作者是谁？”）。这个新的请求实际上是 [原始长Prompt] + [新问题]。 缓存命中：当模型收到这个新请求时，它会发现请求的前半部分（[原始长Prompt]）与上一次完全相同。它会立即从 KV Cache 中加载这部分的 KV 向量，而只需为新的部分（[新问题]）计算 KV 向量。 加速生成：这样一来，模型省去了重复计算长 Prompt 部分的巨大开销，从而极大地加快了对新问题的响应速度。 优势： 大幅提升多轮对话或连续查询的性能：对于聊天机器人、文档问答等需要保持长上下文的场景，效果极其显著。 降低总计算成本：虽然不减少送入的 Token 数，但通过复用计算结果，降低了处理相同前缀的实际计算成本和时间。 6.3 校正 correction C-RAG 的核心思想是在检索模块和生成模块之间，引入一个轻量级的“检索评估器” (Retrieval Evaluator)，并根据评估结果采取不同的校正措施。这项技术主要在学术论文 arXiv:2401.15884 中被系统性地提出和阐述。 C-RAG C-RAG 的精髓在于其动态的、差异化的处理策略。 当评估为“不正确”时：C-RAG 会果断地抛弃所有从内部知识库检索到的文档。因为它判断这些文档只会误导 LLM。取而代之，它会重写 (Rewrite) 用户的查询，使其更适合通用搜索引擎，然后触发网络搜索 (Web Search)，从更广阔的、实时更新的互联网中获取信息。这极大地扩展了 RAG 系统的知识边界，尤其适用于回答关于近期事件或内部知识库未覆盖领域的问题。 当评估为“正确”时：即便文档是相关的，也可能包含大量与问题无关的“噪音”段落。为了让 LLM 更专注于核心信息，C-RAG 采用了一种“分解-再重组” (Decompose-then-Recompose)*的知识精炼算法。 分解 (Decompose)：将相关的文档分解成更小的、独立的知识片段 (Knowledge Strips)。 重组 (Recompose)：再次使用评估器对每个知识片段进行打分，过滤掉无关的片段，只保留最核心、最相关的知识点，然后将这些精华片段“重组”起来，作为最终的上下文。 当评估为“模糊”时：C-RAG 会采取一种混合策略。它会同时对内部检索到的模糊文档进行上述的“分解-再重组”精炼，并启动网络搜索获取外部信息。最后，将两方面的信息合并，为 LLM 提供一个更全面、更鲁棒的上下文。 实战案例：LangGraph-CRAG 7. 响应生成 Output parsers | 🦜️🔗 LangChain LlamaIndex 丨 Output Parsing Modules OpenAI 丨 Structured Outputs 8. 系统性优化 系统性优化指的是从系统层面上，通过优化整个 RAG 流程来达到一个更好的检索效果。 8.1 自我修正与反思型 RAG 业界标杆：self-rag 笔者实践：self-rag.py 此架构模拟了人类“先思考、再审视、后修正”的决策过程。系统首先生成一个初步答案，然后启动一个内部的\"批评家\"来评估这个答案的质量。如果发现问题（如信息不完整、逻辑不通顺），系统会生成修正指令，并基于新指令进行迭代优化，直到产出高质量的最终答案。 Self-RAG 思想简化 8.2 迭代式检索 RAG RA-ISF 此架构专门应对信息不足的问题。当一次检索无法获取回答复杂问题所需的全部信息时，系统会进入一个迭代循环。它会分析已获取的内容，智能地生成新的、更深入的查询，然后再次进行检索。这个过程不断重复，直到收集到足够全面的上下文，最后再进行综合生成。 Iterative Retrieval RAG 原理简化 8.3 自适应/智能体 RAG AgenticRAG-Survey 此架构将 RAG 提升到了一个智能体 （Agent）的高度。系统核心是一个作为大脑的 LLM，它能自主分析用户问题，并决策采取何种行动：是进行知识库检索、上网搜索、调用计算器，还是直接回答。它能制定多步计划并调用不同工具，展现出更高的灵活性和解决复杂问题的能力。 Agentic RAG 原理简化 9. 评估 实践案例：eval.ipynb 9.1 三大标准 Context Relevance：系统检索到的上下文是否紧密围绕用户的问题展开，是否包含了解答问题所需的关键信息。 Faithfulness：生成的答案与给定的上下文之间的事实一致性。 Answer Relevance：关注答案是否直接回答了问题，还关注答案是否完整、是否包含冗余信息。 RAG 评估三大标准 9.2 三大步骤 RAG 评估三大步骤 9.3 Ragas Ragas 评估指标： Faithfulness: 生成的答案与给定的上下文之间的事实一致性。 Answer relevancy: 关注答案是否直接回答了问题，还关注答案是否完整、是否包含冗余信息。 Context Precision: 衡量检索上下文的信噪比。 Context Recall: 判断是否能检索到回答问题所需的全部相关信息。 优点： 优点： 轻量易用。 指标专业性：专为 RAG 设计四大核心指标：上下文相关性（Context Relevance）、上下文召回率（Context Recall）、答案忠实度（Faithfulness）、答案相关性（Answer Relevance）。 无参考标签评估：不依赖参考答案即可完成评估，降低标注成本。 缺点： 结果可解释性弱：仅输出分数，不提供得分原因。 本地化支持不足：主要优化英文场景，对中文等语言支持有限。 功能扩展性弱：不支持自定义指标，灵活性较。 代码示例： 1. 构建数据集 from datasets import Datasetquestions = [ 伙食补助费标准是什么?, 出差可以买意外保险吗？需要自己购买吗,]ground_truths = [ 伙食补助费标准: 西藏、青海、新疆 120元/人、天 其他省份 100元/人、天, 出差可以购买交通意外保险，由单位统一购买，不再重复购买,]answers = []contexts = []for query in questions: response, context_list = run_rag_pipeline_without_stream(query=query, k=3) answers.append(response) contexts.append(context_list)data = question: questions, answer: answers, contexts: contexts, ground_truth: ground_truthsdataset = Dataset.from_dict(data) 2. 定义评估指标 from ragas.metrics import( faithfulness, answer_relevancy, context_recall, context_precision,) 3. 执行评估 from ragas import evaluatefrom ragas import RunConfigeval_llm = RagLLM()embedding_model = RagEmbedding()eval_embedding_fn = embedding_model.get_embedding_fun()result = evaluate( dataset=dataset, llm=eval_llm, embeddings=eval_embedding_fn, metrics=[ context_precision, context_recall, faithfulness, answer_relevancy, ], raise_exceptions=True, run_config=config)df = result.to_pandas() 4. 评估结果 Ragas 评估结果示例 9.4 TruLens 提供一个交互式的仪表板（Dashboard），用于可视化评估结果、比较不同版本的实验并追踪性能变化。它不仅支持 LangChain 和 LlamaIndex 等主流框架，还支持对完全自定义的 RAG 应用进行封装和评估。 典型流程： 定义反馈函数（如 Groundedness，AnswerRelevance，ContextRelevance）； 然后用 TruApp 包装 RAG 应用； 再一个 with 上下文管理器中运行查询； run_dashboard 启动仪表盘查看结果。 优点： 全链路追踪：记录 RAG 全流程（检索、上下文、生成），支持根本原因分析，精准定位故障点（如检索错误或生成偏差）。 可视化与集成：内置 Web 仪表盘，实时展示评估结果；深度集成 LangChain 和 LlamaIndex。 反馈函数组合：支持自定义反馈函数（如毒性检测、语言匹配），灵活适配业务需求。 缺点： 指标覆盖面窄：核心仅三大指标（上下文相关性、答案忠实度、答案相关性），缺乏上下文召回率等关键维度。 依赖人工标注：答案正确性等指标需参考答案（Ground Truth），增加标注成本。 调试门槛高：全链路追踪需额外配置，对新手不够友好。 代码示例： from trulens_eval import TruApp, Feedback, OpenAI, Selectfrom trulens_eval.app import App# 初始化反馈函数提供者provider = OpenAI()# 定义 RAG 三元组反馈函数f_groundedness = Feedback(provider.groundedness_measure_with_cot_reasons).on(Select.RecordCalls.retrieve.rets.collect()).on_output()f_answer_relevance = Feedback(provider.relevance_with_cot_reasons).on_input().on_output()f_context_relevance = Feedback(provider.context_relevance_with_cot_reasons).on_input().on(Select.RecordCalls.retrieve.rets[:]).aggregate(np.mean)# 包装 RAG 应用tru_rag_app = TruApp(rag_query_engine, app_id=RAG_v1, feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance])# 运行并记录评估with tru_rag_app as recording: rag_query_engine.query(What did the author do growing up?)# 启动仪表板tru.run_dashboard() 9.5 DeepEval 将自身定位为 LLM 应用的\"单元测试\"框架，理念非常现代化。提供超过 14 种评估指标，不仅覆盖 RAG，还包括微调等场景。其一大亮点是指标具有自我解释能力，即在给出分数的同时，会提供具体的理由来解释为何得分不高，极大地便利了调试过程。此外，它与流行的测试框架 Pytest 深度集成，可以无缝地融入 CI/CD 流程。 优点： 工程化与自动化：原生支持 pytest，可集成 CI/CD 流水线，实现自动化测试与报告生成。 指标丰富且可定制：内置 30+ 指标（如忠实度、毒性、偏见检测），支持 DAG 自定义指标（决策树结构）满足复杂逻辑。独创上下文召回率计算（基于关键陈述覆盖比例）。 结果可解释性强：提供分数原因及改进建议，支持与 RAGAS 结果联动分析 缺点： 部分指标非 RAG 专属：如摘要质量、知识保留等指标更通用，需筛选适用场景。 依赖评估模型：默认使用 OpenAI 模型，替换自定义模型需额外开发。 配置复杂：DAG 指标需设计节点逻辑（任务节点、裁决节点等），学习曲线陡峭。 示例代码： import deepevalimport deepeval.evaluatefrom deepeval.metrics import ( FaithfulnessMetric, AnswerRelevancyMetric, ContextualPrecisionMetric, ContextualRecallMetric)from deepeval.test_case import LLMTestCasetest_cases = []for i, question in enumerate(questions): test_cases.append(LLMTestCase( input=question, actual_output=answers[i], retrieval_context=contexts[i], expected_output=ground_truths[i], ))evaluation_metrics = [ FaithfulnessMetric(threshold=0.7), AnswerRelevancyMetric(threshold=0.8), ContextualPrecisionMetric(threshold=0.7), ContextualRecallMetric(threshold=0.9)]results = deepeval.evaluate( test_cases=test_cases, metrics=evaluation_metrics)for i, result in enumerate(results.test_results): print(f--- TestCase i+1 ---) print(fQuery: result.input) if result.success: print(f✅ Overall Result: Passed ) else: print(f❌ Overall Result: Failed ) # 打印每个指标的详细得分和原因 for metric_result in result.metrics_data: print(f 📊 Metric: metric_result.__class__.__name__) print(f - Score: metric_result.score:.2f (Threshold: metric_result.threshold)) reason = getattr(metric_result, reason, N/A) print(f - Reason: reason) print(- * 25 + ) 结合单元测试： import pytestfrom deepeval import assert_test@pytest.mark.parametrize(test_case, test_cases)def regualation_rag_eval(test_case: LLMTestCase): print(fTesting Input: test_case.input) assert_test( test_case=test_case, metrics=[ FaithfulnessMetric(threshold=0.7), AnswerRelevancyMetric(threshold=0.8), ContextualPrecisionMetric(threshold=0.7), ContextualRecallMetric(threshold=0.9) ] ) 输出示例： --- TestCase 1 ---Query: 伙食补助费标准是什么?✅ Overall Result: Passed 📊 Metric: MetricData - Score: 1.00 (Threshold: 0.7) - Reason: The score is 1.00 because there are no contradictions, indicating a perfect alignment between the actual output and the retrieval context. Great job maintaining accuracy and consistency! 📊 Metric: MetricData - Score: 1.00 (Threshold: 0.8) - Reason: The score is 1.00 because the response perfectly addresses the question about the standard for meal allowances without any irrelevant information. Great job! 📊 Metric: MetricData - Score: 1.00 (Threshold: 0.7) - Reason: The score is 1.00 because the relevant nodes in the retrieval contexts are perfectly ranked above the irrelevant node. The first node provides a clear table with the 伙食补助费标准 for different regions, directly answering the input question. The second node further explains the concept and provides the same standards, reinforcing the relevance. The third node, which discusses hotel recommendations and accommodation fees, is unrelated and correctly ranked last. 📊 Metric: MetricData - Score: 1.00 (Threshold: 0.9) - Reason: The score is 1.00 because the expected output perfectly aligns with the information in the nodes in the retrieval context, showcasing a flawless match. Great job! 10. Graph RAG 10.1 图数据库 10.1.1 neo4j neo4j 使用的是语言是 cypher。Cypher 的核心是 MATCH（模式匹配） + RETURN（结果返回），辅以 CREATE/MERGE（数据操作）、WHERE（过滤）、WITH（管道传递）。 1. 节点与关系语法 节点：用圆括号 () 表示，可包含变量、标签和属性。 ()：匿名节点 (p:Person)：变量 p + 标签 Person (p:Person name: 'Alice', age: 30)：带属性的节点。 关系：用方括号 [] 表示，放在两个短横线中间（--），方向用箭头（→ 或 ←）指定。 -[:KNOWS]-：无变量、类型为 KNOWS 的无向关系 -[r:ACTED_IN roles: ['Neo']]-→：变量 r + 类型 ACTED_IN + 属性 2. 模式匹配（MATCH） 核心是通过路径模式描述图结构： MATCH (p:Person)-[r:ACTED_IN]-(m:Movie title: The Matrix)RETURN p, r.roles 可选匹配：OPTIONAL MATCH 处理可能不存在的关系。 3. 数据操作语句 创建： CREATE (p:Person name: 'Alice')：创建节点。 CREATE (a)-[:FRIEND]-(b)：创建关系（需先匹配 a, b） 更新：SET 修改属性 MATCH (p:Person) SET p.age = 31 合并：MERGE 存在则匹配，不存在则创建 MERGE (p:Person name: Alice)ON CREATE SET p.created_at = timestamp() 删除： DELETE n：删除节点（需先断开关系） DETACH DELETE n：删除节点及关联关系 4. 查询控制条件 过滤：WHERE 条件筛选 MATCH (p:Person) WHERE p.age 30 OR p.name STARTS WITH A 返回：RETURN 指定输出 连接查询：WITH 传递中间结果 MATCH (p)-[:FRIEND]-(f)WITH p, count(f) AS friendCountWHERE friendCount 10RETURN p.name 聚合与排序： COUNT(), COLLECT()：聚合函数 ORDER BY p.age DESC LIMIT 10：排序和分页 5. 索引与约束 索引：加速节点查找 CREATE INDEX FOR (p:Person) ON (p.name) 约束：确保数据唯一性 CREATE CONSTRAINT ON (m:Movie) ASSERT m.title IS UNIQUE 10.1.2 nebula graph nebula graph 使用的语言是 nGQL。 10.2 典型流程 将问题提交给 LLM，让其提取（总结）关键词； 通过关键词来地毯式查询节点，尝试命中图数据库中定义的节点； 如果有命中的，则通过节点来查询关联的关系和节点信息； 将查询到的信息组织上上下文提交给 LLM，解答最初的问题。 10.3 实战案例 learn-neo4j ftt_graph_rag 11. ReAct RAG ReAct = Reasoning + Acting = 推理 + 行动 核心理念：让大型语言模型像人一样，在解决复杂问题时，能够先思考分析（推理），然后根据思考结果采取行动（行动），再观察行动结果，接着进行新一轮的思考，如此循环，直到问题解决。 核心流程： 思考（Thought） 行动（Action） 观察（Observation） 思考（Thought） ... 最终答案（Final Answer） 11.1 Prompt 1. 明确的规则制定（Rule Formulation） 循环结构：强制模型遵循 \"Thought - Action - Observation\" 的循环。 输出格式：严格规定每一个环节的输出格式，便于程序解析。 终止条件：明确告诉模型何时任务算完成，以及如何提交最终答 2. 精确的工具授权（Tool Granting） 功能单一：每个工具最好只做一件事，这让模型更容易选择。 描述清晰：工具的描述 (description) 是模型决定使用哪个工具的唯一依据。描述要用自然语言写得清晰、准确，说明白“这个工具能干什么”。 参数明确：工具的输入参数 (parameters) 必须定义清楚，包括名称、类型和用途。 有了 MCP 后，这一步可以用 MCP 来替代。 3. 高质量的示例引导（Example Guidance） 展示思维链：清晰地展示从问题到第一个思考，再到行动的逻辑。 覆盖典型场景：展示如何使用不同的工具，甚至是如何组合使用工具。 处理异常情况：最好能包含一个处理错误的示例 (比如搜索不到结果时该怎么办)，这能极大地提升模型的鲁棒性。 Prompt 示例骨架： You are an expert assistant capable of solving complex problems by breaking them down into a sequence of thought and action. You must strictly follow the format of Thought, Action, Observation to solve the problem.You have access to the following tools:[tool_definitions]Use the following format:Question: The users question you need to answer.Thought: Your internal reasoning and plan for the next action.Action: The tool you will use. It must be one of [list_of_tool_names].Observation: The result returned by the action.... (this Thought/Action/Observation cycle can repeat N times)Thought: I now have the final answer.Action: finish(the final answer) Few Shot 示例： Question: 苹果公司的现任 CEO 是谁？他的前任是谁？Thought: 我需要分两步解决这个问题。第一，找出苹果的现任 CEO。第二，找出他的前任。我先执行第一步。Action: search(苹果公司现任 CEO)Observation: 苹果公司现任 CEO 是蒂姆·库克 (Tim Cook)。Thought: 我已经知道了现任 CEO 是蒂姆·库克。现在我需要找出他的前任是谁。Action: search(蒂姆·库克的前任是谁)Observation: 蒂姆·库克的前任是苹果公司的创始人史蒂夫·乔布斯 (Steve Jobs)。Thought: 我已经获得了所有需要的信息：现任 CEO 是蒂姆·库克，前任是史蒂夫·乔布斯。我可以给出最终答案了。Action: finish(苹果公司的现任 CEO 是蒂姆·库克，他的前任是史蒂夫·乔布斯。) 11.2 实战案例 react-rag 12. RAG 相关思考 企业大模型落地的现实解法：为什么 RAG 是绕不开的技术路径？ 不需要 RAG！手把手教你构建问答 Agent","tags":["ai","rag"],"categories":["ai","rag"]},{"title":"FOSA丨03丨模块化","path":"/2025/07/02/fosa-ch3/","content":"本系列文章通过逐章回答《Fundamentals of Software Architecture》（下文简称 FOSA）一书中的课后思考题，来深入理解书中的核心概念和理论，从而提升我们的软件架构设计能力。本篇为第三章内容。 本章的课后题是： What is meant by the term connascence? \"连接性\"这个术语是什么意思？ What is the difference between static and dynamic connascence? 静态连接性和动态连接性有什么区别？ What does connascence of type mean? Is it static or dynamic connascence? 类型连接性是什么意思？它是静态的还是动态的？ What is the strongest form of connascence? 连接性最强的形式是什么？ What is the weakest form of connascence? 连接性最弱的形式是什么？ Which is preferred within a code base—static or dynamic connascence? 在一个代码库中，静态连接性和动态连接性哪个更受青睐？ 本篇的主题是 Modularity（模块化），谈到模块化，我们最常提到的一句话就是：\"高内聚（cohesion）、低耦合（coupling）\"。 内聚是指模块内部各元素之间关联的紧密程度。高内聚意味着模块中的所有元素都紧密相关，并且共同为一个单一的、明确定义的目的服务。高内聚使模块的功能职责更加集中和明确，易于理解和修改。 耦合是指不同模块之间相互依赖的程度。低耦合意味着模块之间的依赖关系很弱，一个模块的改变对其他模块的影响尽可能小。低耦合降低了系统修改、测试和部署的风险。当系统的一个部分发生变化时，由于依赖关系较少，需要修改的其他部分也较少。 课后题中提到的 connascence（连接性）跟耦合的概念很像。 当一个组件的修改需要修改另外一个组件才能保持系统整体的正确性时，这两个组件就处于连接性状态。它衡量了软件组件之间相互依赖的程度。 连接性是对耦合度量的一种细化。虽然传统的耦合度量（如内向耦合和外向耦合）关注的是连接的方向，但连接性更进一步，关心组件之间如何耦合在一起。 连接性可以分为 2 大类： 静态连接性：指的是源代码级别的耦合，它可以通过简单的源代码分析来确定。 命名连接性：多个组件必须就某个实体的名称达成一致。这是最弱的连接性形式，也是代码库中最理想的耦合方式，因为现代重构工具可以轻松实现系统范围内的名称更改。 类型连接性：多个组件必须就某个实体的类型达成一致。这在许多静态类型语言中很常见，例如变量和参数被限制为特定类型。 意义连接性 / 约定连接性：多个组件必须就某个值的含义或约定达成一致。 位置连接性：多个组件必须就某个实体在列表、记录或参数中的位置达成一致。 算法连接性：多个组件必须就某个特定算法达成一致。 执行连接性：多个组件必须就某个执行顺序或流程达成一致。 动态连接性：指的是运行时的调用，这种很难确定，因为缺乏有效的运行时分析工具。 执行连接性：不同语句的执行顺序很重要。例如一段代码中某些属性必须按特定顺序设置才能正确运行。 时序连接性：多个组件的执行时序很重要。通常发生在竞态条件中，即两个线程同时执行并影响联合操作的结果 值连接性：多个值之间相互关联，必须一起改变。 身份连接性：多个组件必须引用同一个实体。例如，两个独立的组件必须共享和更新一个公共数据结构。 连接性的强弱如下图所示： FOSA Figure 3-5. The strength on connascence provides a good refactoring guide 越往左上角，表示连接性越弱，也是我们编写代码时更希望看到的。 Page-Jones 提出了 3 个使用连接性来改善系统模块化的指导原则： 通过将系统分解为封装的元素来最小化整体连接性； 最小化任何剩余的跨越封装边界的连接性； 最大化封装边界内的连接性。 Jim Weirich 进一步给出了 2 条建议： 程度规则（Rule of Degree）：将强形式的连接性转换为弱形式的连接性。例如，可以通过重构将意义连接性（CoM）转换为命名连接性（CoN），即创建命名常量而不是使用\"魔法值\"。 局部性规则（Rule of Locality）：随着软件元素之间距离的增加，使用弱形式的连接性。这意味着，如果组件彼此远离，则它们之间的耦合应尽可能松散，避免强连接性形式。","tags":["读书笔记","软件架构","fosa"],"categories":["架构设计"]},{"title":"FOSA丨02丨架构思维","path":"/2025/07/01/fosa-ch2/","content":"本系列文章通过逐章回答《Fundamentals of Software Architecture》（下文简称 FOSA）一书中的课后思考题，来深入理解书中的核心概念和理论，从而提升我们的软件架构设计能力。本篇为第二章内容。 本章的课后题是： Describe the traditional approach of architecture versus development and explain why that approach no longer works. 描述传统意义上的架构与开发的方法，并解释为什么这种方法不再适用。 List the three levels of knowledge in the knowledge triangle and provide an example of each. 列出知识三角中的三个层次，并为每个层次提供一个示例。 Why is it more important for an architect to focus on technical breadth rather than technical depth? 为什么对于一位架构师而言，注重技术的广度而非深度会显得更为重要呢？ What are some of the ways of maintaining your technical depth and remaining hands-on as an architect? 作为架构师，有哪些方法可以保持技术深度并保持动手能力呢？ 1. 架构与开发方法 在传统方法中，架构师与开发人员的职责是分离的，如下图所示： FOSA Figure 2-1. Traditional view of architecture versus design 架构师的职责： 分析业务逻辑，以提取和定义架构特征 选择适合问题领域的架构模式和风格 创建系统的构建块 —— 组件 开发者的职责： 根据架构师交付的产出，创建类图 构建用户界面 开发和测试源代码 这种传统方法的问题在于它是一种单向的\"移交\"模式。架构师在设计完成后将产物移交给开发团队，但架构师与开发人员之间存在物理和虚拟的障碍。这种方法不再适用的原因主要有以下几点： 信息丢失和脱节：架构师的决策有时无法有效传达给开发团队，而开发团队在实现过程中对架构产生的改变也鲜有反馈给架构师。 缺乏协作与同步：唯一不变的就是变，业务是不断发展的，系统架构也是不断演进和迭代的。这种单向模式导致双方缺乏紧密、双向的合作，无法及时响应业务和技术变化，导致架构变得脆弱且难以维护。 更适合现代软件开发的方法应如下图所示： FOSA Figure 2.2. Making architecture work through collaboration 在这种双向交互的方式中，架构师不再仅仅是交付设计产出，而是在整个软件生命周期中，不断对开发团队进行领导和指导，开发团队在实现过程也，也不断将遇到的问题和改变返回给架构师时，双方紧密合作、互通有无、共同前进。 2. 知识三角 FOSA Figure 2-3. The pyramid representing all knowledge 第一层 (底层)：你知道你知道的 (Stuff you know you know) 这是你知识体系的基石，是你明确掌握、能够熟练运用的技能和知识。它们是你的\"舒适区\"和核心竞争力。比如掌握编程语言（Go/Rust）的基本语法和常见框架。 第二层 (中层)：你知道你不知道的 (Stuff you know you don't know) 这是你已经意识到但尚未掌握的领域。你可能听说过某个技术、某个概念，知道它的存在和价值，但还没有系统学习或实践过。这是你明确的学习目标和成长方向。比如编程语言（Go/Rust）的编译原理和编译期优化技术。 第三层 (顶层)：你不知道你不知道的 (Stuff you don't know you don't know) 这是你的认知盲区。你甚至不知道这些知识、技术或方法论的存在。它们往往是突破瓶颈、实现认知跃迁的关键，也是最大的风险和机遇所在。一个人的成长，很大程度上就是不断将第三层的 \"未知未知\"转化为第二层的\"已知未知\"的过程。比如 AI 领域中的各种新兴技术。 3. 广度还是深度 对于架构师而言，广度比深度要更重要，原因如下： 架构师的职责不是\"做\"，而是\"选择怎么做\"。只有当架构师拥有广泛的知识宽度时，能在不同的业务场景中，通过权衡对比，选择出最适合当前业务的架构模式。 每个人的时间和精力是有限的，我们不可能同时精通所有的技术，所以架构师必须在广度和深度之间有所侧重。为了避免\"过时专业知识\"，架构师需要保持学习最新前沿技术，试图在多个领域保持深度会导致精力耗尽，只能牺牲部分深度以换取更大的广度。 软件架构中一切都是\"权衡\"。只知道一种解决方案的架构师是做不出权衡的，只有当知道多种解决方案，并清楚其中的优劣，才能在充满各种限制的现实场景中做出\"最不坏\"的架构设计。 在笔者看来，为了后期可以更快、更扎实的扩展我们的广度，前期的\"深度探索\"尤为重要。拥有一个深度的知识领域，就像在浩瀚的知识海洋中打下了一个坚实的\"锚\"。当你学习新知识时，可以把它关联到你的“锚点”上，而不是让它漂浮在空中。正所谓：一通百通。 如果你深度研究过 Go 的 GMP 调度模型、goroutine 的实现、channel 的底层结构，你不仅仅是学会了 Go 的并发。你真正理解了用户态线程、M:N 调度、CSP (Communicating Sequential Processes) 模型 等核心概念。当你再去学习 Rust 的 async/await 和 tokio 时，你会发现虽然语法和所有权规则完全不同，但其背后的异步运行时、任务调度、Future/Executor 模型 等思想，都与你已有的知识体系遥相呼应。你的学习过程不再是死记硬背，而是比较、关联和迁移，效率极高。 所以，一个更完整、更理想的技术人员成长路径应该是： 职业前期 (深耕期)： 深度优先，广度为辅。 选择一个你感兴趣且有前景的主航道（如 Go/Rust 后端开发），投入 80% 的精力向下猛扎，直到成为该领域的专家。用 20% 的精力保持对周边领域的关注。这个阶段的目标是 \"立足\"。 职业中期 (拓展期)： 深度与广度并重。 在你的根据地已经非常扎实之后，开始有意识地、系统性地拓展你的知识广度，将深耕期遇到的问题和知识点串联起来，形成体系。从 \"I 型人才\" 向 \"T 型人才\" 转变。这个阶段的目标是 \"连接\"。 职业后期 (整合期)： 广度优先，深度为基。 当你需要承担架构师、技术负责人等角色时，你的主要价值来自于广阔的视野和权衡决策能力。你过往的深度积累，则为你的决策提供了坚实的支撑和深刻的洞察力。这个阶段的目标是 \"引领\"。 4. 维持动手能力 很多架构师与开发团队的协议日益疏远，很大程度源于架构师脱离一线开发环境太久了，以下是一些简单且有效保持技术深度和动手能力的方法： 避免\"瓶颈陷阱（bottleneck trap）\"并委派核心代码：架构师不应独自承担关键路径或框架代码的开发，因为这会使其成为团队的瓶颈。应将这些核心代码委派给开发团队。架构师可以专注于编码一到三个迭代后的业务功能（例如，一个服务或一个屏幕），从而既能获得实践经验，又能让开发团队拥有核心代码的所有权，并更好地理解他们在开发过程中可能遇到的问题。 持续做概念验证（POCs）：POC 不仅要求架构师编写代码，还能通过实践验证架构决策。在这个过程中，架构师需要严格要求自己产出高质量的代码，因为开发团队后续的工作，很大程度会在 POC 的基础上进行扩展。 处理技术债务：通常这些任务优先级较低，即使未能在一个迭代内完成，也不会对项目造成严重影响。这能让架构师获得实践编码经验，同时解放开发团队去处理关键功能用户故事。 参与修复非紧急漏洞：处理 BUG 能够帮助架构师识别代码库乃至架构中的问题和弱点。 构建效率提升工具：开发一些提升效率的小工具，一方面可以帮助开发团队更高效率地开展工作，另一方面也可以有效维持架构师的动手能力。 编写适应性函数（Fitness Functions）：辅助编写单元测试、集成测试等自动化测试，一来能帮助架构师维持动手能力，二来能帮助架构师更深入理解项目细节，三来可以提高项目架构的健壮性。 参与代码评审（Code Review）：代码评审能让架构师保持对代码库的参与度，同时确保架构合规性，并发现团队中的指导和辅导机会。","tags":["读书笔记","软件架构","fosa"],"categories":["架构设计"]},{"title":"Go 底层原理丨深度剖析 Gin 框架核心机制：从 HTTP 请求生命周期到高性能设计哲学","path":"/2025/06/30/go-gin/","content":"本篇笔者将尝试基于 Gin v1.10.1 来进行一趟 Gin 源码之旅，为了使我们的学习更有方向，在开始之前，我们来思考一个问题： 一个 HTTP 请求从抵达 Gin 到返回响应，其完整的旅程是怎样的？ 启动服务： r.Run() 究竟做了什么？（提示：它内部调用了 Go 标准库的 http.ListenAndServe） 请求入口： 当一个请求到来，Go 的 http.Server 是如何将请求交给 Gin 的核心 Engine 处理的？（提示：Engine 本身就是一个 http.Handler） 上下文创建： gin.Context 是在何时被创建的？它封装了什么？ 路由匹配： Gin 如何根据请求的 URL 快速找到对应的处理函数？ 中间件执行： r.Use() 添加的中间件是如何形成一个“调用链”的？c.Next() 的作用机制是什么？ 业务处理： 你的业务逻辑处理函数（Handler）是如何被调用的？ 响应返回： c.JSON() 或 c.String() 这样的函数，最终是如何将数据写入到 http.ResponseWriter 的？ 资源回收： gin.Context 对象在请求结束后是如何被回收的？（提示：sync.Pool） 优雅关闭： 服务在关闭过程中，如何保证当前请求被正确完整处理？ 从本篇中你可以学到什么 阅读本文，你将不仅仅是学会如何使用 Gin 框架，更是能深入到底层，理解其高效运作背后的原理。这趟旅程将为你揭示一个完整的 HTTP 请求在 Gin 中的生命周期，让你在未来的开发与面试中都更具深度和信心。 具体来说，你将收获以下核心知识点： Go Web 服务核心原理 理解 Gin 的 r.Run() 如何封装并启动标准库的 http.Server。 掌握 Go net/http 服务如何通过 net.Listener 的 Accept() 循环来接收 TCP 连接，并为每个连接开启独立 Goroutine 进行处理的并发模型。 Gin 的高性能设计哲学 剖析 Gin 如何通过 sync.Pool 对象池技术来复用 gin.Context，从而大幅减少内存分配和 GC 压力，这是 Gin 高性能的关键之一。 学习 Go http.Server 中优雅关闭（Shutdown）的完整实现，包括： 如何通过 Context 控制超时。 如何区分并分别处理监听器（Listener） 和连接（Connection）。 高效轮询等待中的指数退避（Exponential Backoff） 与抖动（Jitter） 策略。 精巧的 Radix Tree 路由实现 深入理解 Gin 高性能路由的基数树（Radix Tree）实现原理，包括 methodTrees 的整体结构。 彻底搞懂路由 node 节点的每个字段的精确含义，特别是 indices（快速索引）和 wildChild（通配符标志）这两个性能优化的法宝。 掌握路由的查找（getValue）过程：包括前缀匹配、静态路由匹配、以及如何通过 skippedNodes 实现回溯（Backtracking） 机制来保证静态路由的优先级。 掌握路由的注册（addRoute）过程：包括最核心的节点分裂（Split Edge） 逻辑，以及如何通过 panic 来避免通配符冲突，从而在构建时就保证路由树的逻辑正确性。 中间件的洋葱模型 揭秘 Gin 中间件的核心 c.Next() 的工作机制，理解 HandlersChain 和 index 索引是如何协同工作，实现了优雅的“洋葱模型”调用链。 框架的扩展性与接口设计 了解 Gin 如何通过 RouterGroup 组合的方式，巧妙地为 Engine 和 RouterGroup 自身都实现 IRouter 接口，从而支持灵活的路由分组与嵌套。 学习 c.JSON() 背后的 render.Render 接口设计，理解其如何将不同格式（JSON、XML、HTML 等）的响应渲染逻辑解耦。 启动服务 r.Run() func (engine *Engine) Run(addr ...string) (err error) address := resolveAddress(addr)\terr = http.ListenAndServe(address, engine.Handler())\treturn 地址解析 其中 resolveAddress 就是解析监听地址，默认为 :8080： func resolveAddress(addr []string) string switch len(addr) case 0: // 如果没传地址 // 先尝试从环境变量 PORT 中获取监听端口 if port := os.Getenv(PORT); port != return : + port // 默认 8080 端口 return :8080\tcase 1: // 传了地址，则使用传递的地址参数 return addr[0]\tdefault: // 只允许传递一个地址，否则 panic panic(too many parameters) 启动监听 r.Run() 底层使用的其实还是标准库 http 的 ListenAndServe()： func (s *Server) ListenAndServe() error // 如果服务正在关闭中，直接返回报错 if s.shuttingDown() return ErrServerClosed // ... 省略非核心代码\tln, _ := net.Listen(tcp, addr)\treturn s.Serve(ln) 核心看 s.Serve(ln)： // Serve 方法会针对监听器 l 接收到来的连接建立新的服务协程。// 每个服务协程会读取请求，并随后调用 s.Handler 来作出回应。// 只有当监听器返回的连接是 [*tls.Conn] 类型，并且这些连接在 TLS 配置的 NextProtos 中设置了“h2”选项时，才会启用 HTTP/2 支持。// 服务调用函数 always 会返回一个非空的错误，并关闭 l。// 在 Server.Shutdown 或 Server.Close 之后，返回的错误为 ErrServerClosed。func (s *Server) Serve(l net.Listener) error // ... // 创建 context\tbaseCtx := context.Background()\tif s.BaseContext != nil // 如果有自定义的 context 构造器，则使用自定义的来初始化 baseCtx = s.BaseContext(origListener) if baseCtx == nil panic(BaseContext returned a nil context) ctx := context.WithValue(baseCtx, ServerContextKey, s)\tfor // 监听客户端连接 rw, err := l.Accept() // ... connCtx := ctx // 初始化一个连接对象 c := s.newConn(rw) c.setState(c.rwc, StateNew, runHooks) // before Serve can return // 服务这个连接对象 go c.serve(connCtx) 服务基础 context 的初始化和相关状态处理，作为后面在每个连接上的请求响应的 gin.Context 的基础。 for 循环监听客户端连接。 为每个客户端建立 conn 连接对象。 c.serve(connCtx) 服务每一个连接。 请求入口 c.serve(connCtx) 连接握手 重点来看 c.serve(connCtx)： func (c *conn) serve(ctx context.Context) if tlsConn, ok := c.rwc.(*tls.Conn); ok // tls 握手 // 超时控制\tctx, cancelCtx := context.WithCancel(ctx)\tc.cancelCtx = cancelCtx\tdefer cancelCtx() // 跟踪当前 listener，里面其实是一个 waitGroup，用于优雅重启确保监听器完全关闭 if !s.trackListener(l, true) // 可以理解为 wg.Add(1) return ErrServerClosed defer s.trackListener(l, false) // 可以理解为 wg.Done()\tfor // 读取请求数据，返回的 w 是一个 response，用于响应数据 w, err := c.readRequest(ctx) if err != nil // 异常处理 return // ----- 核心具体的 HTTP 处理函数 serverHandlerc.server.ServeHTTP(w, w.req) // 结束请求，响应数据 w.finishRequest() // 非长连接则直接返回，否则继续复用当前连接 if !w.conn.server.doKeepAlives() return 如果配置了 TLS，则进行 TLS 加密握手； 创建超时控制 context，对于超时连接强制关闭； for 循环 c.readRequest(ctx) 读取请求数据； 执行 ServeHTTP(w, req) 执行具体的 HTTP 处理业务； 结束请求，w.finishRequest() 响应数据； 非长连接则直接返回，释放连接，否则复用当前连接处理后续请求。 读取请求 其中 c.readRequest() 核心逻辑是： func (c *conn) readRequest(ctx context.Context) (w *response, err error) req, err := readRequest(c.bufr)\tif err != nil return nil, err // ... 一些请求检验和请求头的检查设置\tw = response conn: c, cancelCtx: cancelCtx, req: req, // 请求元数据 reqBody: req.Body, // 请求体 handlerHeader: make(Header), contentLength: -1, closeNotifyCh: make(chan bool, 1), wants10KeepAlive: req.wantsHttp10KeepAlive(), wantsClose: req.wantsClose(), w.cw.res = w\tw.w = newBufioWriterSize(w.cw, bufferBeforeChunkingSize)\treturn w, nil 写回响应 其中 c.finishRequest() 核心逻辑是： func (w *response) finishRequest() // 标记处理完毕\tw.handlerDone.Store(true) // 写 HTTP Status OK\tif !w.wroteHeader w.WriteHeader(StatusOK) // 将响应数据全部写入缓冲区，并回收缓冲区，后续复用\tw.w.Flush()\tputBufioWriter(w.w)\tw.cw.close()\tw.conn.bufw.Flush() // 清空请求体相关数据，用于后续请求复用\tw.conn.r.abortPendingRead()\tw.reqBody.Close()\tif w.req.MultipartForm != nil w.req.MultipartForm.RemoveAll() 上下文创建 回收 sync.Pool 现在我们进入 ServeHTTP(w, req) 执行具体的 HTTP 处理业务，这里会先为每一个请求创建上下文，然后再进行请求处理。 func (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request) handler := sh.srv.Handler\thandler.ServeHTTP(rw, req)func (engine *Engine) ServeHTTP(w http.ResponseWriter, req *http.Request) // 从 sync.Pool 对象池中获取一个 gin.Context c := engine.pool.Get().(*Context) // 重置 gin.Context，使其与当前 request 绑定\tc.writermem.reset(w)\tc.Request = req\tc.reset() // 处理请求\tengine.handleHTTPRequest(c) // 将 gin.Context 返回 sync.Pool\tengine.pool.Put(c) 可以看到这里使用了 sync.Pool 对象池来管理 gin.Context 对象，通过复用对象来避免重复创建和销毁带来的额外开销。 路由匹配 sh.ServeHTTP 核心处理逻辑是 engine.handleHTTPRequest(c)： func (engine *Engine) handleHTTPRequest(c *Context) httpMethod := c.Request.Method\trPath := c.Request.URL.Path // 消除请求路径中的重复斜杠，比如 /hello//user 会处理为 /hello/user // 这里在不同的版本默认策略是不一样的，在 1.5.0 版本是默认开启的，在 1.10.0 版本是关闭的！\tif engine.RemoveExtraSlash rPath = cleanPath(rPath) // 寻找请求路由对应的处理器，并执行。\tt := engine.trees\tfor i, tl := 0, len(t); i tl; i++ // 将下文详细分析 break // 如果设置了 HandleMethodNotAllowed，则会在找不到对应路由的情况下， // 尝试在 Allow Header 中返回相同路由，但是不同 HTTP Method 的处理器。\tif engine.HandleMethodNotAllowed allowed := make([]string, 0, len(t)-1) for _, tree := range engine.trees if tree.method == httpMethod continue if value := tree.root.getValue(rPath, nil, c.skippedNodes, unescape); value.handlers != nil allowed = append(allowed, tree.method) if len(allowed) 0 c.handlers = engine.allNoMethod c.writermem.Header().Set(Allow, strings.Join(allowed, , )) serveError(c, http.StatusMethodNotAllowed, default405Body) return // 找不到对应的路由，返回 404\tc.handlers = engine.allNoRoute\tserveError(c, http.StatusNotFound, default404Body) 路由树结构 methodTrees 这里我们重点来看一下 Gin 的路由树是怎样的，先看一下数据结构： // Gin Enginetype Engine struct // ...\ttrees methodTrees// 方法路由树type methodTree struct method string\troot *node// 路由树节点type node struct path string\tindices string\twildChild bool\tnType nodeType\tpriority uint32\tchildren []*node // child nodes, at most 1 :param style node at the end of the array\thandlers HandlersChain\tfullPath string// 方法路由树列表type methodTrees []methodTree// 请求处理函数func (engine *Engine) handleHTTPRequest(c *Context) // ... t := engine.trees for i, tl := 0, len(t); i tl; i++ if t[i].method != httpMethod continue root := t[i].root value := root.getValue(rPath, c.params, c.skippedNodes, unescape) // ... break // ... 如下图所示： Gin 路由树结构示意图 路由树节点 node 这里我们重点解释下 node 结果中的字段含义，这对后续的路由查找分析非常重要： type node struct path string // 当前节点所代表的 URL 路径片段\tindices string // 子节点的索引，用于快速查找\twildChild bool // 标志位，表示是否存在通配符子节点（:param 或 *catchall）\tnType nodeType // 节点的类型（静态、参数、通配符等）\tpriority uint32 // 节点的优先级，用于路由注册时的排序\tchildren []*node // 子节点列表\thandlers HandlersChain // 匹配该节点路径时，需要执行的处理函数链（包含中间件和主 handler）\tfullPath string // 完整的路由注册路径 1. path string 含义：这个字段存储了当前节点所代表的 URL 路径片段 或 公共前缀。它不是完整的 URL 路径，而是树中一个分支的字符串。基数树会尽可能地将多个路由的公共前缀合并到一个 path 中以节省空间。 举例：假设你注册了两个路由：/user/profile 和 /user/settings。那么可能会有一个父节点的 path 是 /user/，然后它有两个子节点，一个 path 是 profile，另一个是 settings。 2. indices string 含义：这是一个非常巧妙的性能优化字段。它是一个字符串，其中每个字符都是对应 children 切片中子节点的 path 的 第一个字符。它的作用是作为 children 的一个快速查找索引。当需要寻找下一个节点时，程序只需用请求路径的下一个字符来和 indices 进行匹配，就能立刻知道应该访问 children 中的哪个元素，而无需遍历整个 children 切片。 举例：一个父节点 n 的 path 是 /。它有三个子节点，path 分别是 articles、blog 和 contact。 n.children[0].path = \"articles\" n.children[1].path = \"blog\" n.children[2].path = \"contact\" 那么，n.indices 的值就会是 \"abc\"。 当一个请求 /blog/test 到来时，程序匹配完父节点的 / 后，看到下一个字符是 b，它直接在 indices (\"abc\") 中找到 b 是第二个字符，于是就直接去访问 children[1]，非常高效。 3. wildChild bool 含义：一个布尔标志位。如果为 true，表示这个节点的子节点中 存在一个通配符节点（即 :param 或 *catchall 类型的节点）。 作用：这同样是一个性能优化。在路由查找时，如果静态子节点（通过 indices）没有匹配上，程序只需检查 wildChild 这一个布尔值，就能快速知道是否需要进一步尝试匹配通配符子节点，避免了额外的条件判断。根据约定，通配符子节点永远是 children 数组的最后一个元素。 4. nType nodeType 含义：表示当前节点的类型。nodeType 是一个整数类型，通常有以下几种值： static (静态)：节点的 path 是一个固定的字符串，例如 /about。 root (根)：整棵树的根节点。 param (参数)：表示一个命名参数，例如 :id。路径 /users/:id 中的 :id 部分就是一个 param 类型的节点。 catchAll (通配符)：表示一个“全匹配”参数，例如 *filepath。路径 /static/*filepath 中的 *filepath 就是一个 catchAll 类型的节点。 作用：在路由查找时，getValue 函数通过 switch n.nType 来决定如何处理当前节点和剩余的请求路径。例如，遇到 param 类型就要提取参数值，遇到 catchAll 就要捕获所有剩余路径。 5. priority uint32 含义：节点的优先级。这个值在 构建路由树 的时候使用，而不是在请求时查找时使用。 作用：它的值是根据注册到这个节点的路由数量以及其子孙节点的路由数量计算出来的。当插入新路由可能导致树结构冲突时，priority 可以帮助算法决定如何拆分和重组节点，以保持树的正确性和高效性。简单来说，它代表了一个节点的\"权重\"或\"繁忙程度\"。 6. children []*node 含义：一个 *node 指针的切片，存储了所有直接的子节点。这是构成树状结构的核心字段。 规则：这个切片有一个重要规则：如果存在通配符子节点（:param 或 *catchall），它 必须并且只能是切片中的最后一个元素。静态子节点（static）则排在前面，它们的顺序与 indices 字符串中字符的顺序一一对应。 7. handlers HandlersChain 含义：HandlersChain 本质上是一个 []HandlerFunc，也就是一个处理函数的切片。 作用：这是路由查找的最终目标。当一个请求的 URL 完整匹配到某个节点时，这个节点的 handlers 字段就包含了需要被执行的所有函数，这其中可能包括多个中间件（Middleware）和最终处理业务逻辑的那个主函数（Handler）。如果一个节点的 handlers 为 nil，说明它只是一个中间路径节点，不能直接处理请求。 8. fullPath 含义：存储了用户在代码中定义的 完整的、原始的路由注册字符串。 作用： 调试与日志：在中间件或日志系统中，你可以通过 c.FullPath() （它读取的就是这个值）获知当前请求匹配到的是哪条原始路由规则，这对于监控和问题排查非常有用。 模板渲染或 URL 生成：在某些场景下，你可能需要根据路由名称或模式来生成 URL，fullPath 提供了这个原始模式。 举例： 你注册了 router.GET(\"/users/:id/profile\", ...). 这会被拆分成多个 node。假设最终匹配到 profile 那个 node： 它的 path 可能是 \"profile\"。 但它的 fullPath 会是 \"/users/:id/profile\"。 总结 这 8 个字段协同工作，共同构建了一个既节省内存又查找飞快的路由树。 path, children, nType 定义了树的 基本结构和逻辑。 indices 和 wildChild 是为了 极致性能 而设计的巧妙索引。 handlers 和 fullPath 存储了路由的 最终目标和元数据。 priority 则在幕后默默地保证了这棵树在动态构建过程中的 稳定性和合理性。 路由定位 root.GetValue 接下来我们来看 root.GetValue() 具体是如何定位路由处理器的，这个方法非常长，我们逐一分解。 好的，我们来一起深入解析一下 Gin 框架中这个核心的路由查找函数 (n *node) getValue。 这是一个非常精妙的函数，它的背后是高性能路由技术的典型实现。为了真正理解它，我们需要遵循“由表及里、由浅入深”的原则，从它的目标、使用的数据结构，再到具体的代码执行逻辑，一步步进行剖析。 第 1 步：前缀匹配 prefix := n.pathif len(path) len(prefix) if path[:len(prefix)] == prefix path = path[len(prefix):] // ... 继续寻找子节点 这是基数树最基本的操作。代码首先检查当前请求路径 path 是否以当前节点 n 的路径 n.path 为前缀。 如果匹配：说明路径的前半部分对了，然后从 path 中“砍掉”已经匹配上的前缀，准备在子节点中继续匹配剩余的 path。 如果不匹配：说明走错路了，需要回溯或直接返回未找到。 第 2 步：在子节点中选择\"道路\" (静态路由) idxc := path[0]for i, c := range []byte(n.indices) if c == idxc n = n.children[i] continue walk 在砍掉前缀后，path 是剩余的待匹配路径。idxc := path[0] 取出剩余路径的第一个字符。然后，代码遍历 n.indices 这个“索引目录”。 n.indices 存储了所有子节点的路径的第一个字符。 如果 idxc 在 n.indices 中找到了匹配项 c，就意味着存在一个正确的子节点可以继续走下去。 n = n.children[i] 将当前节点 n 更新为找到的子节点。 continue walk 跳回到 walk 循环的开始，在新节点上重复 第 1 步 的前缀匹配。 这个设计非常高效，因为它避免了对 children 切片的完整遍历，而是通过一个字符的比较就快速定位了下一个节点。 第 3 步：处理\"岔路口\" - 通配符 (Wildcard) 如果静态路由没找到（for 循环结束），程序会检查是否存在通配符子节点。 if !n.wildChild // ... 没有通配符子节点，处理找不到的情况 return value// Handle wildcard child, which is always at the end of the arrayn = n.children[len(n.children)-1]switch n.nType case param:\tcase catchAll: n.wildChild 是一个布尔值，表示当前节点是否有一个通配符子节点（:param 或 *catchall）。按照约定，通配符子节点永远是 children 数组的最后一个元素。如果存在，就直接跳到这个通配符节点继续匹配。 接着，switch n.nType 根据通配符节点的类型进行处理： case param (例如 /users/:id): 它会从剩余的 path 中\"截取\"出参数值。截取的规则是到下一个 / 或者路径末尾。 例如，如果 path 是 123/profile，它会截取出 123 作为参数值。 然后将参数的键（如 id）和值（如 123）存入 params。 如果 / 后面还有路径（如 profile），则继续在当前参数节点的子节点中进行 walk。 case catchAll (例如 /static/*filepath): 这就更简单了，它会把 所有 剩余的 path 都作为参数值。 例如，如果 path 是 css/main.css，整个字符串都会被捕获。 catchAll 节点一定是路径的终点，找到后直接返回结果。 第 4 步：到达终点与\"没路了\"的处理 if path == prefix // ... return value 这里说明已经找到了\"终点\"，进行最后的一系列检查。 第一种情况：到达真终点。 // We should have reached the node containing the handle.// Check if this node has a handle registered.if value.handlers = n.handlers; value.handlers != nil value.fullPath = n.fullPath return value 这是最完美的情况！我们找到了一个与请求路径完全匹配的节点，并且这个节点上确实注册了至少一个处理函数，这里我们设置好 fullPath 属性然后就可以直接返回了。 第二种情况：到达假终点。 // If the current path does not equal / and the node does not have a registered handle and the most recently matched node has a child node// the current node needs to roll back to last valid skippedNodeif n.handlers == nil path != / // skippedNodes 记录了所有我们路过的、存在岔路口（即有其他路径可选）的节点。 for length := len(*skippedNodes); length 0; length-- // 从后往前遍历 skippedNode := (*skippedNodes)[length-1] // 取出最近的一个岔路口 *skippedNodes = (*skippedNodes)[:length-1] // 将其从待办列表中移除。 if strings.HasSuffix(skippedNode.path, path) // 判断这个岔路口的完整路径是否以我们当前这个死胡同路径结尾。 path = skippedNode.path // 如果检查通过，就意味着我们找到了一个可以复活的存档点 n = skippedNode.node // 滚到当时路过那个岔路口的状态 if value.params != nil *value.params = (*value.params)[:skippedNode.paramsCount] globalParamsCount = skippedNode.paramsCount continue walk // 读档后，选择另一条路重新开始走 // 检查不通过，说明没有后悔药可以吃了，查找失败。 我们到达了节点 n，但这个节点的 handlers 是 nil！并且，为了避免对根路径 / 的误判，加了 path != \"/\" 的条件。 这意味着我们走到了一个\"死胡同\"或者说一个\"假终点\"。路径虽然匹配了，但这只是一个中间节点（例如 /users），它本身不能处理请求，真正的终点在它的子节点上（例如 /users/list 或 /users/:id）。但我们的请求路径已经用完了，无法再往下走了。 为什么会发生这种情况？ 这通常发生在有路由冲突或歧义时，路由器“贪婪地”选择了一条看似正确但实际上是死胡同的路。 试想一下情况： 注册路由 A: /users/new (静态) 注册路由 B: /users/:id (动态) 用户请求: GET /users/new 路由器在匹配完 /users/ 后，剩下 new。此时它面临一个选择：是匹配静态的 new 节点，还是匹配动态的 :id 节点？虽然 Gin 会优先匹配静态节点，但我们可以设想一个场景：如果 /users/new 这个路由没有注册 handler（开发者忘了写），而 /users/:id 注册了。 当请求 /users/new 时，它会先走到 new 节点。发现 handlers 是 nil，于是就进入了这个回溯逻辑。 第 5 步：智能建议 - TSR (Trailing Slash Redirect) // We can recommend to redirect to the same URL without a// trailing slash if a leaf exists for that path.value.tsr = path == / n.handlers != nil tsr 是 Gin 的一个非常人性化的功能。 场景 1: 你注册了 /users，但用户请求了 /users/。 场景 2: 你注册了 /users/，但用户请求了 /users。 在这两种情况下，Gin 不会直接返回 404 Not Found。getValue 函数在发现“几乎”匹配（就差一个尾部斜杠）时，会将 value.tsr 设置为 true。上层逻辑接收到这个 true 信号后，就会向客户端返回一个 301 或 307 重定向建议，告诉浏览器应该访问另一个带或不带斜杠的 URL。这提升了用户体验。 第 6 步：回溯 (Backtracking) 这是函数中最复杂，但也最能体现其强大的部分。这里的逻辑跟第 4 步中到达\"假终点\"大致是相同的。 // the current node needs to roll back to last valid skippedNodefor length := len(*skippedNodes); length 0; length-- skippedNode := (*skippedNodes)[length-1] *skippedNodes = (*skippedNodes)[:length-1] if strings.HasSuffix(skippedNode.path, path) // ... 回滚状态，重新 walk continue walk 同样是考虑以下路由： /users/:id /users/new 我们假设另外一种情况，当一个请求 GET /users/new 到来时： 它首先匹配到 /users/ 前缀。 剩下的路径是 new。此时，它既可能匹配静态的 new，也可能匹配参数 :id。 大多数路由器的实现会优先匹配静态路径。但如果 getValue 先进入了 :id 的分支，它会把 new 当作 :id 的值。如果 :id 节点下没有更多子路径，查找就会失败。 这时，就需要回溯。skippedNodes 记录了\"上一个有其他选择的路口\"（例如，那个同时存在静态子节点和通配符子节点的 /users/ 节点）。 代码会回退到那个路口，并尝试另一条路（即匹配 new 静态路径），最终找到正确的 handlers。 这个机制确保了 静态路由的优先级总是高于通配符路由，即使它们的路径结构很相似。 总结 Gin 的 (n *node) getValue 函数是一个基于基数树 (Radix Tree) 的、高度优化的路由查找实现。它的执行过程可以概括为： 循路前进：沿着基数树，通过前缀匹配 (n.path) 和索引查找 (n.indices)，快速匹配 URL 的静态部分。 灵活应变：当遇到通配符节点 (:param 或 *catchall) 时，能正确解析路径参数。 终点判断：当路径完全匹配时，检查当前节点是否有 handlers，有则成功返回。 智能容错：当精确匹配失败，但存在仅差一个尾部斜杠的路由时，会给出重定向建议 (TSR)。 迷途知返：通过 skippedNodes 机制实现回溯，确保在有多种可能匹配路径（静态 vs 通配符）时，能够做出正确的选择，保证路由匹配的准确性。 通过这些精巧的设计，Gin 在保证强大功能的同时，实现了极高的路由性能。 我画了个流程图，供你参考： graph TD subgraph MainProcess [主流程] direction TB A[\"getValue(path, ...)\"]:::startend %% Stage 1: Traversal Loop subgraph TraversalPhase [\"第一阶段：遍历深入 (WALK 循环)\"] direction TB W[\"循环开始在当前节点 n\"]:::process C1{\"路径前缀匹配 且 路径有剩余?\"}:::decision P1[\"削减已匹配路径\"]:::process C2{\"匹配静态子节点?\"}:::decision P2[\"记录回溯点(skippedNodes)n = 进入静态子节点\"]:::process C3{\"有通配符子节点?\"}:::decision P3[\"n = 进入通配符子节点\"]:::process C4{\"节点类型是 :param?\"}:::decision P4[\"处理 :param, 截取并保存参数\"]:::process C5{\"参数后还有剩余路径?\"}:::decision end %% Stage 2: Final Adjudication Junction[\"无法继续深入转到最终裁决\"]:::decision subgraph FinalAdjudication [第二阶段：最终裁决] direction TB C_FINAL_BACKTRACK{\"需要回溯?(当前节点无 handler)\"}:::decision P_FINAL_BACKTRACK[\"执行回溯遍历 skippedNodes 查找备用路径若找到, 则恢复现场\"]:::process C_FINAL_HANDLER{\"找到 handler?(n.handlers != nil)\"}:::decision SUCCESS[\"成功返回 value (含 handlers)\"]:::startend P_FINAL_TSR[\"TSR 检查检查是否存在 +/- 斜杠的“近亲”路由\"]:::process FINAL_RETURN[\"返回 value(可能含 TSR, 或为空)\"]:::startend end B4_CatchAll[\"处理 *catchAll截取所有剩余路径保存参数, 赋值 handlers直接成功返回\"]:::startend %% --- Connections (Corrected Syntax) --- A --> W W --> C1 %% Traversal Logic C1 -- \"是(Yes)\" --> P1 P1 --> C2 C2 -- \"是(Yes)\" --> P2 P2 -- \"进入下一轮\" --> W C2 -- \"否(No)\" --> C3 C3 -- \"是(Yes)\" --> P3 P3 --> C4 C4 -- \"是(Yes)\" --> P4 P4 --> C5 C5 -- \"是(Yes)\" --> W C4 -- \"否(No), 是 *catchAll\" --> B4_CatchAll %% Exits from Traversal to Adjudication C1 -- \"否(No)\" --> Junction C3 -- \"否(No): 无路可走\" --> Junction C5 -- \"否(No): 路径耗尽\" --> Junction %% Adjudication Logic Junction --> C_FINAL_BACKTRACK C_FINAL_BACKTRACK -- \"是(Yes)\" --> P_FINAL_BACKTRACK P_FINAL_BACKTRACK -- \"回到循环\" --> W C_FINAL_BACKTRACK -- \"否(No): 无需回溯\" --> C_FINAL_HANDLER C_FINAL_HANDLER -- \"是(Yes)\" --> SUCCESS C_FINAL_HANDLER -- \"否(No)\" --> P_FINAL_TSR P_FINAL_TSR --> FINAL_RETURN end %% Styling classDef startend fill:#9f9,stroke:#333,stroke-width:2px,color:#000 classDef panic fill:#f99,stroke:#333,stroke-width:2px,color:#000 classDef decision fill:#ffc,stroke:#333,stroke-width:2px,color:#000 classDef process fill:#9cf,stroke:#333,stroke-width:2px,color:#000 中间件执行 c.Next 执行机制 经过路由匹配，我们找到了处理当前请求的节点，返回的 value 结构如下： type nodeValue struct handlers HandlersChain\tparams *Params\ttsr bool\tfullPath string 其中处理函数就是 handlers，它是类型的 HandlersChain，其实就是 []HandleFunc： // HandlersChain defines a HandlerFunc slice.type HandlersChain []HandlerFunc// HandlerFunc defines the handler used by gin middleware as return value.type HandlerFunc func(*Context) 在 engine.handleHTTPRequest() 中，找到了处理节点后，执行了下面 4 行代码，其中核心就是 c.Next()： c.handlers = value.handlersc.fullPath = value.fullPathc.Next()c.writermem.WriteHeaderNow() 我们来看一下 c.Next()： type Context struct // ...\thandlers HandlersChain // 请求链路\tindex int8 // 当前处理的 HandleFunc 在 handlers 中的索引 // ...func (c *Context) Next() c.index++\tfor c.index int8(len(c.handlers)) c.handlers[c.index](c) c.index++\tfunc (c *Context) reset() // ...\tc.handlers = nil c.index = -1 // 这里初始值是 -1，因为第一次执行 Next() 的时候，会 c.index++\t// ... 它的逻辑其实简单，就是通过递增 index 依次执行 HandlersChain。 我们顺带看一下 c.Abort()，真是聪明！将 index 设置为 abortIndex，这样后面的 handler 就执行不到了！ func (c *Context) Abort() c.index = abortIndex 接口实现 // IRouter defines all router handle interface includes single and group router.type IRouter interface IRoutes\tGroup(string, ...HandlerFunc) *RouterGroup// IRoutes defines all router handle interface.type IRoutes interface Use(...HandlerFunc) IRoutes\tHandle(string, string, ...HandlerFunc) IRoutes\tAny(string, ...HandlerFunc) IRoutes\tGET(string, ...HandlerFunc) IRoutes\tPOST(string, ...HandlerFunc) IRoutes\tDELETE(string, ...HandlerFunc) IRoutes\tPATCH(string, ...HandlerFunc) IRoutes\tPUT(string, ...HandlerFunc) IRoutes\tOPTIONS(string, ...HandlerFunc) IRoutes\tHEAD(string, ...HandlerFunc) IRoutes\tMatch([]string, string, ...HandlerFunc) IRoutes\tStaticFile(string, string) IRoutes\tStaticFileFS(string, string, http.FileSystem) IRoutes\tStatic(string, string) IRoutes\tStaticFS(string, http.FileSystem) IRoutes 注册路由的核心接口是 IRouter，并且为 Engine 和 RouterGroup 实现了 IRouter 接口： Gin IRouter 接口及其实现 type Engine struct RouterGroup ...// RouterGroup is used internally to configure router, a RouterGroup is associated with// a prefix and an array of handlers (middleware).type RouterGroup struct Handlers HandlersChain\tbasePath string\tengine *Engine\troot bool 查看源码后我们发现，其实真正实现 IRouter 接口的，只有 RouterGroup！然后在 Engine 中组合 RouterGroup，这样我们既可以直接在 Engine 上（根路径）注册路由，需要注意的是，IRouter 中有一个方法： Group(string, ...HandlerFunc) *RouterGroup 这样就巧妙地实现了分组路由和递归分组路由的功能！ 我们先来看看 Group() 的实现： func (group *RouterGroup) Group(relativePath string, handlers ...HandlerFunc) *RouterGroup return RouterGroup Handlers: group.combineHandlers(handlers), // 组合当前 group 和 handlers（深拷贝），并返回新的 handlers 列表 basePath: group.calculateAbsolutePath(relativePath), // 合并路径 engine: group.engine,\t// 一个接口最多支持 127-1 个处理器const abortIndex int8 = math.MaxInt8 1func (group *RouterGroup) combineHandlers(handlers HandlersChain) HandlersChain finalSize := len(group.Handlers) + len(handlers)\tassert1(finalSize int(abortIndex), too many handlers)\tmergedHandlers := make(HandlersChain, finalSize)\tcopy(mergedHandlers, group.Handlers) // 深拷贝当前 group 拥有的 handler\tcopy(mergedHandlers[len(group.Handlers):], handlers) // 深拷贝新注册的 handler\treturn mergedHandlersfunc (group *RouterGroup) calculateAbsolutePath(relativePath string) string return joinPaths(group.basePath, relativePath) 再来看看注册路由的具体实现，以 GET() 为例： func (group *RouterGroup) GET(relativePath string, handlers ...HandlerFunc) IRoutes return group.handle(http.MethodGet, relativePath, handlers)func (group *RouterGroup) handle(httpMethod, relativePath string, handlers HandlersChain) IRoutes absolutePath := group.calculateAbsolutePath(relativePath) // 合并路径\thandlers = group.combineHandlers(handlers) // 组合当前 group 和 handlers（深拷贝），并返回新的 handlers 列表\tgroup.engine.addRoute(httpMethod, absolutePath, handlers) // 添加路由\treturn group.returnObj() 注册逻辑在 engine.addRoute() 中： func (engine *Engine) addRoute(method, path string, handlers HandlersChain) // 获取 method 对应的路由树，不存在则创建 root := engine.trees.get(method)\tif root == nil root = new(node) root.fullPath = / engine.trees = append(engine.trees, methodTreemethod: method, root: root) // 添加路由\troot.addRoute(path, handlers) 路由注册 核心逻辑在 root.addRoute() 中，这个函数的目标是在路由树中为给定的 path 和 handlers 找到一个安身之处，必要时会重塑树的结构。 整个函数的核心是一个 walk 循环，它模拟了从树的根节点开始，一步步向下走，直到找到或创造出新路由位置的过程。接下来我们细细拆解这个过程。 初始化与空树处理 fullPath := pathn.priority++// Empty treeif len(n.path) == 0 len(n.children) == 0 n.insertChild(path, fullPath, handlers) n.nType = root return n.priority++: 每当有一个路由需要经过或终止于当前节点 n，这个节点的优先级（权重）就会增加。这反映了该节点在路由树中的\"繁忙\"程度。 空树判断: 这是最简单的基础情况。如果当前节点 n 的 path 和 children 都为空，说明这是一棵空树（或者说我们正处于一个未初始化的根节点）。 操作: 直接调用 n.insertChild(path, fullPath, handlers)，将整条 path 作为第一个孩子插入，并把自己的类型设置为 root。整个添加过程结束。 如果不是空树，程序就进入 walk 循环，开始真正的\"建树之旅\"。 i := longestCommonPrefix(path, n.path) 这是循环体内的第一步，也是最关键的一步。它计算了 要插入的新路径 path 和 当前节点路径 n.path 之间的最长公共前缀 (Longest Common Prefix) 的长度 i。这个 i 的值，决定了接下来所有的操作。 要不，刷个 leetcode 放松一下？🤡🤡🤡 longestCommonPrefix 场景一：节点分裂 (Split Edge) // Split edgeif i len(n.path) // 分裂当前 n，继承之前的 indices, children, handlers 和 wildChild child := node path: n.path[i:], wildChild: n.wildChild, nType: static, indices: n.indices, children: n.children, handlers: n.handlers, priority: n.priority - 1, fullPath: n.fullPath, n.children = []*nodechild // 把分裂出来的节点作为当前节点的 child n.indices = bytesconv.BytesToString([]byten.path[i]) // 重置 indices，目前只有一个元素，就是分裂出来的节点的第一个字符 n.path = path[:i] // 缩短前缀 n.handlers = nil // 清空 handlers，因为当前节点已经是中间节点了 n.wildChild = false // 清空通配符标识 n.fullPath = fullPath[:parentFullPathIndex+i] // 重置 fullPath 触发条件: i len(n.path)。这意味着公共前缀的长度 i 小于当前节点 n 的路径长度。换句话说，新路径和当前节点路径在中间某个位置出现了\"分叉\"。 经典例子: 当前节点 n.path 是 \"/hello\"，要插入的新路径 path 是 \"/help\"。 LCP 是 \"/hel\"，长度 i 为 4。 i len(\"/hello\") (4 6) 条件成立。 操作 (这是最精妙的部分): 创建新子节点 child: 这个 child 节点继承了当前节点 n \"后半段\"的路径，即 n.path[i:] (例子中是 \"lo\")。 它也完全继承了 n 之前的所有子节点 (n.children)、handlers、wildChild 状态等。它的 priority 会减 1，因为父节点 n 的 priority 已经加过了。 改造当前节点 n: 当前节点 n 被\"改造\"成一个新的、更短的 父节点/分支节点。 n.path 被截断为公共前缀 path[:i] (例子中是 \"/hel\")。 n.handlers 被设为 nil，因为它现在只是一个中间节点。 n.children 被重置，现在只包含刚刚创建的那个 child 节点。 n.indices 也被更新，只包含指向新 child 节点的索引字符 (例子中是 'l')。 结果: 执行完分裂后，树的结构从 (node path=\"/hello\") 变成了 (node path=\"/hel\") - (child path=\"lo\")。此时，我们还没有处理新路径 \"/help\" 剩下的部分 (\"p\")。代码会自然地流转到下面的 if i len(path) 逻辑，将 \"p\" 作为 /hel 节点的第二个孩子插入。 场景二：继续向下走或创建新分支 // Make new node a child of this nodeif i len(path) path = path[i:] c := path[0] // 处理 /?a=1b=2 这种情况 if n.nType == param c == / len(n.children) == 1 parentFullPathIndex += len(n.path) n = n.children[0] n.priority++ continue walk // 看看能不能找到匹配的静态子节点 for i, max := 0, len(n.indices); i max; i++ if c == n.indices[i] parentFullPathIndex += len(n.path) i = n.incrementChildPrio(i) n = n.children[i] continue walk // 找不到匹配的静态子节点，且当前节点非通配符节点，则插入一个新的子节点 if c != : c != * n.nType != catchAll // []byte for proper unicode char conversion, see #65 n.indices += bytesconv.BytesToString([]bytec) child := node fullPath: fullPath, n.addChild(child) n.incrementChildPrio(len(n.indices) - 1) n = child else if n.wildChild // 如果新的 path 是通配符路径，当前节点 n 也是通配符节点，那需要检查是否有冲突 // 按照约定，通配符子节点永远是 children 切片的最后一个元素，取出最后一个元素，成为当前的 n n = n.children[len(n.children)-1] n.priority++ // 判断通配符是否不冲突，需要满足 3 个条件 // 1. len(path) = len(n.path) n.path == path[:len(n.path)] if len(path) = len(n.path) n.path == path[:len(n.path)] // Adding a child to a catchAll is not possible n.nType != catchAll // Check for longer wildcard, e.g. :name and :names (len(n.path) = len(path) || path[len(n.path)] == /) continue walk // Wildcard conflict pathSeg := path if n.nType != catchAll pathSeg = strings.SplitN(pathSeg, /, 2)[0] prefix := fullPath[:strings.Index(fullPath, pathSeg)] + n.path panic( + pathSeg + in new path + fullPath + conflicts with existing wildcard + n.path + in existing prefix + prefix + ) // 将 fullPath，handlers 赋予处理后的最终的当前节点 n.insertChild(path, fullPath, handlers) return 触发条件: i len(path)。这意味着在匹配完公共前缀后（或者说，完整匹配了当前节点的 path 后），要插入的新路径 path 还有剩余部分。 例子: 当前节点 n.path 是 \"/users\"，要插入的新路径是 \"/users/new\"。 LCP 是 \"/users\"，长度 i 为 6。i == len(n.path)。 i len(\"/users/new\") (6 10) 条件成立。 具体过程如下： path = path[i:]: 更新 path 为剩余未处理的部分 (例子中是 \"/new\")。 c := path[0]: 取出剩余路径的第一个字符 (例子中是 /)。 检查现有子节点: for i, max := 0, len(n.indices); ... 这是最常见的\"向下走\"逻辑。程序用字符 c 去匹配 n.indices，如果找到了匹配的静态子节点，就增加那个子节点的 priority，然后 n = n.children[i]，将 n 更新为那个子节点，continue walk，从新的 n 开始下一轮循环。 插入新子节点: 如果 for 循环没找到匹配的子节点，并且 c 不是通配符 (: 或 *)，程序就会创建一个新的静态子节点，更新 n.indices，并将 n 指向这个新创建的子节点。 如果 c 是通配符，会进入 else if n.wildChild 逻辑。这个逻辑主要是用来处理通配符冲突的。例如，你不能在 /:id 之后再添加 /:user。如果存在冲突，程序会 panic 并给出非常清晰的错误信息。如果没有冲突（例如，在 /:id 节点下添加子节点 /profile），则会继续向下 walk。这里检查了 3 个条件： 条件 ① len(path) = len(n.path) n.path == path[:len(n.path)]：检查新路径 path 是否以已存在的通配符路径 n.path 开头。 可能兼容：已存在 :id，新来 :id/profile。\":id/profile\" 以 \":id\" 开头，通过。 绝对冲突：已存在 :id，新来 :user。\":user\" 并不以 \":id\" 开头，检查失败，将直接跳到 panic。这正是我们之前讨论的，/:id 和 /:user 无法共存的逻辑实现。 条件 ② n.nType != catchAll：检查已存在的通配符节点类型不是 catchAll (* 类型)。 catchAll 类型的通配符（例如 /static/*filepath）是终极的，它会匹配所有后续路径。因此，在它后面再添加任何子节点（例如 /static/*filepath/more）都是没有意义的，也是不被允许的。如果已存在的是 catchAll，此条件不满足，将 panic。 条件 ③ (len(n.path) = len(path) || path[len(n.path)] == '/')：这是一个非常精妙的检查，用于确保通配符的\"边界清晰\"，防止部分重叠的歧义命名。它分为两种允许的情况： len(n.path) = len(path): 新路径和旧通配符路径完全一样（或更短，但由于条件 A，只能是完全一样）。例如，已存在 :id，新来的也是 :id（后面可能要加子节点）。 path[len(n.path)] == '/': 新路径比旧通配符路径更长，并且紧跟着的第一个字符必须是 /。例如，已存在 :id，新来的是 :id/profile，profile 前面必须有 / 分隔。 如果所有这三个条件都奇迹般地满足了，说明新路径是现有通配符的一个完全合法的\"子路径\"或\"扩展\"。程序就会执行 continue walk，继续愉快地向下走，处理路径剩下的部分。 n.insertChild(path, fullPath, handlers): 这是创建新节点的最终调用，并将 path、fullPath 和 handlers赋予这个新的叶子节点。然后 return，添加过程结束。 场景三：终点命中，添加 Handlers // Otherwise add handle to current nodeif n.handlers != nil panic(handlers are already registered for path + fullPath + )n.handlers = handlersn.fullPath = fullPathreturn 触发条件: 循环走到了一个节点 n，并且 i == len(n.path) 且 i == len(path)。这意味着要插入的路径和当前节点的路径完全一样。 含义: 找到了一个已经存在的、路径完全匹配的节点。 操作: 检查 n.handlers 是否已经存在。如果存在，说明重复注册路由，这是不允许的，程序会 panic。 如果不存在，就将 handlers 和 fullPath 赋予当前节点 n。 return，添加过程结束。 总结 addRoute 函数通过一个 walk 循环，非常精妙地处理了向基数树中插入新路由的所有可能情况： 从计算 LCP 开始，判断新路由与当前节点的关系。 如果新路由与现有节点路径部分重叠，则分裂现有节点，创造出一个新的分支节点。 如果新路由是现有节点路径的超集，则深入到子节点中，或创建新的子节点。 如果新路由与现有节点 路径完全重合，则为其附加处理函数，或因重复注册而报错。 我画了个流程图，供你参考： graph TD subgraph MainProcess [主流程] direction TB %% Node Definitions A[\"addRoute(path, handlers)\"]:::startend A1[\"n.priority++\"]:::process C0{\"树为空?\"}:::decision %% Empty Tree Path B0[\"insertChild(path, ...)n.nType = rootreturn\"]:::startend %% Main Walk Loop W[\"WALK 循环开始\"]:::process W1[\"i := longestCommonPrefix(path, n.path)\"]:::process C1_SplitEdge{\"i < len(n.path)?(需要分裂节点?)\"}:::decision %% Split Edge Logic B1[\"节点分裂 (Split Edge)1. 创建 child 继承 n 的后半段路径和属性2. 改造 n 为父节点, path 截为公共前缀3. n 的 children 重置为 [child]4. n.handlers = nil\"]:::process %% Continue After Split or No Split C2_MorePath{\"i < len(path)?(新路径还有剩余?)\"}:::decision %% Path A: No More Path (Exact Match) C3_HasHandlers{\"n.handlers != nil?\"}:::decision P1[\"panic('路由重复注册')\"]:::panic B2[\"n.handlers = handlersn.fullPath = fullPathreturn\"]:::startend %% Path B: More Path Left W2[\"path = path[i:]c := path[0]\"]:::process C4{\"静态子节点匹配成功?(for c in n.indices)\"}:::decision B3[\"n = 匹配的子节点n.priority++continue walk\"]:::process %% Wildcard Logic C5{\"c 是通配符 : 或 * ?\"}:::decision C5_1{\"n 已有通配符子节点?(n.wildChild)\"}:::decision C6{\"兼容性检查通过?\"}:::decision B4[\"n = 已存在的通配符子节点n.priority++\"]:::process B5[\"continue walk\"]:::process P2[\"panic('通配符冲突')\"]:::panic %% Insert New Child B6[\"创建新子节点1. 创建 child node2. 更新 n.indices3. n.addChild(child)4. n = child\"]:::process B7[\"n.insertChild(path, ...)return\"]:::startend %% Connections A --> A1 --> C0 C0 -- \"是(Yes)\" --> B0 C0 -- \"否(No)\" --> W W --> W1 --> C1_SplitEdge C1_SplitEdge -- \"是(Yes)\" --> B1 --> C2_MorePath C1_SplitEdge -- \"否(No)\" --> C2_MorePath C2_MorePath -- \"否(No) - 路径完全匹配\" --> C3_HasHandlers C3_HasHandlers -- \"是(Yes)\" --> P1 C3_HasHandlers -- \"否(No)\" --> B2 C2_MorePath -- \"是(Yes) - 路径有剩余\" --> W2 W2 --> C4 C4 -- \"是(Yes)\" --> B3 --> W C4 -- \"否(No)\" --> C5 C5 -- \"是(Yes)\" --> C5_1 C5_1 -- \"否(No)\" --> B6 C5_1 -- \"是(Yes)\" --> B4 --> C6 C6 -- \"是(Yes)\" --> B5 --> W C6 -- \"否(No)\" --> P2 C5 -- \"否(No) - 静态\" --> B6 B6 --> B7 end %% Styling classDef startend fill:#9f9,stroke:#333,stroke-width:2px,color:#000 classDef panic fill:#f99,stroke:#333,stroke-width:2px,color:#000 classDef decision fill:#ffc,stroke:#333,stroke-width:2px,color:#000 classDef process fill:#9cf,stroke:#333,stroke-width:2px,color:#000 响应返回 ctx.Json 我们的业务逻辑，会在上一步的中间件执行就顺带被执行了。现在我们来看一下请求结果是如何被返还回去的，这里以 ctx.Json() 为例。 // JSON serializes the given struct as JSON into the response body.// It also sets the Content-Type as application/json.func (c *Context) JSON(code int, obj any) c.Render(code, render.JSONData: obj)// Render writes the response headers and calls render.Render to render data.func (c *Context) Render(code int, r render.Render) // 设置 HTTP 响应状态码\tc.Status(code) // 检查是否有必要写 response body，没必要则直接返回\tif !bodyAllowedForStatus(code) r.WriteContentType(c.Writer) c.Writer.WriteHeaderNow() return // 写 response body\tif err := r.Render(c.Writer); err != nil _ = c.Error(err) c.Abort() 这里的核心是 r.Render，它是一个接口： // Render interface is to be implemented by JSON, XML, HTML, YAML and so on.type Render interface // Render writes data with custom ContentType.\tRender(http.ResponseWriter) error\t// WriteContentType writes custom ContentType.\tWriteContentType(w http.ResponseWriter) Render 接口实现类 这里我们看一下 JSON 的实现： // Render (JSON) writes data with custom ContentType.func (r JSON) Render(w http.ResponseWriter) error return WriteJSON(w, r.Data)// WriteJSON marshals the given interface object and writes it with custom ContentType.func WriteJSON(w http.ResponseWriter, obj any) error writeContentType(w, jsonContentType)\tjsonBytes, err := json.Marshal(obj)\tif err != nil return err _, err = w.Write(jsonBytes)\treturn err// github.com/gin-gonic/gin/response_writer.gofunc (w *responseWriter) Write(data []byte) (n int, err error) w.WriteHeaderNow()\tn, err = w.ResponseWriter.Write(data)\tw.size += n\treturn 其实逻辑就 2 步： 写 content-type: application/json。 调用标准库的 http.responseWriter 将响应结果写回缓冲区。 在调用链处理完毕后，最后就回到了我们在请求入口 c.serve(connCtx) 中分析到的 finishRequest，然后通过建立好的 TCP 连接传递给客户端。 优雅关闭 httpServer.Shutdown Gin 框架本身不提供优雅关闭的功能，需要使用标准库的 http.Server 来实现优雅关闭（最好结合 tableflip 实现无缝切换）。 基础实现如下： // 创建一个通道来接收系统信号quit := make(chan os.Signal, 1)// 监听 SIGINT (Ctrl+C) 和 SIGTERM 信号signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)r := gin.Default()// ... 注册路由httpServer := http.Server Addr: :9010, Handler: r,go func() if err := httpServer.ListenAndServe(); err != nil err != http.ErrServerClosed log.Fatal(服务器启动失败: , err) ()// 阻塞等待信号-quit// 使用 Shutdown 进行优雅关闭if err := httpServer.Shutdown(context.Background()); err != nil log.Fatal(err)log.Println(服务器已关闭) 我们看一下 Shutdown() 的具体逻辑： func (s *Server) Shutdown(ctx context.Context) error // 将服务置为关闭中的状态，拒绝新的连接\ts.inShutdown.Store(true) // 执行钩子函数\ts.mu.Lock()\tlnerr := s.closeListenersLocked()\tfor _, f := range s.onShutdown go f() s.mu.Unlock() // 等待现有连接执行完毕 关闭\ts.listenerGroup.Wait() // 指数退避与抖动\tpollIntervalBase := time.Millisecond\tnextPollInterval := func() time.Duration interval := pollIntervalBase + time.Duration(rand.Intn(int(pollIntervalBase/10))) pollIntervalBase *= 2 if pollIntervalBase shutdownPollIntervalMax pollIntervalBase = shutdownPollIntervalMax return interval timer := time.NewTimer(nextPollInterval())\tdefer timer.Stop()\tfor if s.closeIdleConns() // 直接关闭空闲的连接，如果全部已经关闭了，就返回 true return lnerr select case -ctx.Done(): // 超时强制关闭 return ctx.Err() case -timer.C: // 重新设置下次轮询的时间 timer.Reset(nextPollInterval()) 核心逻辑如下： 将服务置为关闭中的状态，这样在 Accept 的时候，就会拒绝新的连接了。 执行用户自定义的钩子函数，Gin 框架扩展性的体现。 s.listenerGroup.Wait() 等待监听器彻底关闭，这里对应了前面 Serve 中的这段代码： func (s *Server) Serve(l net.Listener) error // ...\tif !s.trackListener(l, true) return ErrServerClosed defer s.trackListener(l, false)\t// ...func (s *Server) trackListener(ln *net.Listener, add bool) bool s.mu.Lock()\tdefer s.mu.Unlock()\tif s.listeners == nil s.listeners = make(map[*net.Listener]struct) if add if s.shuttingDown() return false s.listeners[ln] = struct s.listenerGroup.Add(1) // -------- 开启监听 else delete(s.listeners, ln) s.listenerGroup.Done() // -------- 结束监听 return true 不断轮询关闭空闲的连接，直到整个服务处于静止状态。这里使用的指数退避和抖动： const shutdownPollIntervalMax = 500 * time.Millisecond // 最长轮询等待时间为 500mspollIntervalBase := time.Millisecond // 从 1ms 开始nextPollInterval := func() time.Duration interval := pollIntervalBase + time.Duration(rand.Intn(int(pollIntervalBase/10))) // 加 10% 随机抖动时间 pollIntervalBase *= 2 // 翻倍 if pollIntervalBase shutdownPollIntervalMax // 限制最长轮询等待时间 pollIntervalBase = shutdownPollIntervalMax return interval 指数退避：轮询的间隔时间 pollIntervalBase 从 1 毫秒开始，每次轮询后都翻倍，直到一个最大值 （shutdownPollIntervalMax）。这非常高效：开始时频繁检查，以便服务能快速关闭；如果耗时较长，就降低检查频率，避免空转浪费 CPU。 抖动：在基础间隔上增加一个 10% 的随机时间。这在分布式系统中是一个好习惯，可以避免多个服务在同一时刻执行相同操作，造成\"惊群效应\"。 返回，完成服务的安全关闭。 流程总结 最后我们做一个汇总。一个 HTTP 请求在 Gin 框架中的完整旅程可以总结为以下几个核心阶段： 1. 启动与监听： Gin 服务的启动入口是 r.Run()，它首先会解析监听地址，默认使用 :8080 端口，也可以通过 PORT 环境变量或直接传参来指定。 其底层核心是调用了 Go 标准库的 http.ListenAndServe，进一步通过 net.Listen 监听 TCP 端口，并最终在 Serve 方法中进入一个 for 循环，通过 l.Accept() 来接收新的客户端连接。 每当接收到一个新连接，就会创建一个 conn 对象，并为其开启一个独立的 goroutine（go c.serve(connCtx)）来处理后续的请求。 2. 请求处理入口： 在 c.serve 的 for 循环中，服务器通过 c.readRequest(ctx) 读取和解析原始的 HTTP 请求数据。 最关键的一步是调用 serverHandlerc.server.ServeHTTP(w, w.req)，这将请求的 ResponseWriter 和 Request 对象传递给了 Gin 引擎的核心处理逻辑。 请求处理完毕后，会调用 w.finishRequest() 来写入响应数据、刷新缓冲区并清理资源，为下一次请求复用做准备。 3. 上下文创建与回收： Gin 引擎的 ServeHTTP 方法是处理所有请求的入口。为了提高性能，Gin 使用 sync.Pool 对象池来复用 gin.Context 对象。 每次处理新请求时，会从池中 Get() 一个 Context，重置其内部状态并与当前请求的 ResponseWriter 和 Request 绑定。 请求处理完毕后，Context 对象会被 Put() 回对象池，从而避免了频繁创建和销毁对象带来的垃圾回收压力。 4. 路由匹配： Gin 的核心路由机制是基于一个 methodTrees 结构，它为每种 HTTP 方法（GET、POST 等）维护一棵独立的基数树（Radix Tree）。 这棵树由 node 节点构成，node 结构的 path、indices、wildChild、children 等字段协同工作，构建了一个既节省内存又查找飞快的路由树。indices 字段通过存储子节点首字母作为快速索引，是其高性能的关键之一。 路由查找由 root.getValue() 方法执行，它通过一系列步骤（前缀匹配、静态路由查找、通配符匹配）来定位处理器。 getValue 的设计非常精巧，它利用 skippedNodes 实现了回溯（Backtracking） 机制，以确保在面对静态路由（/users/new）和动态路由（/users/:id）的选择时，能够优先匹配静态路由，保证了路由的准确性。同时，它还支持 TSR（Trailing Slash Redirect） 建议，提升了用户体验。 5. 中间件与业务逻辑执行： 路由匹配成功后，找到的 HandlersChain（一个 []HandlerFunc 切片）会被赋值给 gin.Context。 c.Next() 方法通过一个 for 循环和递增的 index 索引，依次调用 HandlersChain 中的所有处理函数（包括全局中间件、分组中间件和最终的业务 Handler）。 c.Abort() 方法通过将 index 设置为一个极大值来巧妙地中断调用链的执行。 6. 响应返回： 当业务逻辑中调用 c.JSON() 等方法时，实际上是调用了 c.Render()。 Render 方法会设置 HTTP 状态码，并通过 render.Render 接口来执行具体的渲染逻辑。例如，render.JSON 会使用标准库的 json.Marshal 将对象序列化，然后通过 http.ResponseWriter 将数据写入响应体。 7. 优雅关闭： Gin 自身不提供优雅关闭，但可以与标准库的 http.Server 结合实现。 httpServer.Shutdown() 的核心逻辑是： 首先，通过原子操作 s.inShutdown.Store(true) 设置关闭状态，并调用 s.closeListenersLocked()关闭监听器，从源头阻止新连接的建立。 并发执行通过 onShutdown 注册的钩子函数。 进入一个轮询循环，不断调用 s.closeIdleConns() 来关闭已处理完请求的空闲连接。 这个轮询采用了指数退避 (Exponential Backoff) 和抖动 (Jitter) 策略，在保证能快速关闭的同时，避免了在等待期间空转浪费 CPU。 整个关闭过程受传入的 context.Context 控制，可以实现超时强制关闭，避免无限期等待。","tags":["go"],"categories":["go"]},{"title":"FOSA丨01丨软件架构概述","path":"/2025/06/26/fosa-ch1/","content":"本系列文章通过逐章回答《Fundamentals of Software Architecture》（下文简称 FOSA）一书中的课后思考题，来深入理解书中的核心概念和理论，从而提升我们的软件架构设计能力。本篇为第一章内容。 本章的课后题是： What are the four dimensions that define software architecture? 软件架构定义的四个维度是什么？ What is the difference between an architecture decision and a design principle? 架构决策和设计原则有什么区别？ List the eight core expectations of a software architect. 软件架构师的核心期望有哪些？ What is the First Law of Software Architecture? 软件架构的第一定律是什么？ 1. 软件架构定义的四个维度 structure of the system: refres to the type of architecture style (or styles) the system is implemented in (such as microservices, layered, or microkernel). architecture characteristics: define the success criteria of a system, which is generally orthogonal to the functionality of the system. architecture decisions: define the rules for how a system should be constructed. design principles: differ from architecture decisions in that a design principle is a guideline rather than a hard-and-fast rule. FOSA 中强调，软件架构不单单是\"架构\"本身，因为它无法揭示系统为何如此构建（why is more important than how），所以书中用了 4 个维度来定义软件架构的方方面面。 1.1 系统结构 系统结构可能就是我们最常谈到的\"架构\"，指的的整个软件的架构风格（Architecture Style），例如微服务（Microservices）、分层架构（Layered）或微内核（Microkernel）。 书中后续篇章详细介绍了以下 8 种架构风格： 分层架构（Layered Architecture） 流水线架构（Pipeline Architecture） 微内核架构（Microkernel Architecture） 基于服务的架构（Service-Based Architecture） 事件驱动架构（Event-Driven Architecture） 基于空间的架构（Space-Based Architecture） 编排驱动的服务导向架构（Orchestration-Driven Service-Oriented Architecture） 微服务架构（Microservices Architecture） FOSA 强调，只用结构来描述架构是不够的，因为它无法揭示系统为何如此构建。因此我们在设计之初，明确选择和识别现有系统的架构风格是基础，但必须超越这一层去理解其背后的驱动因素。 1.2 架构特性 架构特性就是一堆的 -ilities，书中偏爱\"架构特性\"这个描述，而非\"非功能性需求\"或\"质量属性\"，因为这二者带有负面或事后评估的含义，而架构特性，应当是在架构设计之初，就被纳入深入思考。 架构特性有 3 个标准： 指定非领域设计考虑：更关注\"如何\"实现需求和\"为什么\"做出某些选择，而不是应用程序\"应该做什么\"的功能需求。 影响设计的某种结构方面：架构特性要求在设计中进行特殊的结构考虑。 对应用程序成功其重要作用：每个架构特性都会带来架构的复杂度，所以要尽可能选择最少的、但对成功至关重要的架构特性予以实施。 架构特性可以分为 3 大类： 操作型架构特性 结构型架构特性 交叉切面架构特性 架构特性分类 将任意一个架构特性实现到极致的代价都是极大的，相应也会带来更复杂的架构设计，所以在项目早期，我们需要与领域专家和业务干系人紧密合作，识别和明确最重要的架构特性，用最少的努力，达到最大的产品效果。 1.3 架构决策 架构决策定义了系统如何构建的规则。它们构成了系统的约束，并指导开发团队哪些是被允许的，哪些是不允许的。比如在分层架构中，架构师可能会规定只有业务层额和服务层可以访问数据库，从而限制了表示层直接调用数据库。 架构决策应当被文档化，例如使用架构决策记录（Architecture Decision Records，ADRs），这有助于解释决策的背景、理由和后果，避免重复讨论。 架构决策记录 ADR 1.4 设计原则 设计原则是指导方针（Guideline）而非硬性规则（hard-and-fast rule），用于提供指引和建议，帮助开发团队在面对特定情景时做出最佳选择。 2. 架构决策和设计原则的区别 架构决策：硬性规则 设计原则：指导方针 3. 软件架构师的核心期望 3.1 八大期望 Make Architecture Decisions 架构师应定义指导团队技术决策的架构决策和设计原则。核心是\"指导（guide）\"而非\"指定（specify\"。 Continually Analyze the Architecture 架构师必须持续、全面地分析技术和问题域的变化，以确保架构的健壮性和业务相关性。 Keep Current with Latest Trends 架构师需要不断跟踪和保持对最新技术、框架、平台和环境的了解。 Ensure Compliance with Decisions 架构师需要持续验证开发团队是否遵循已定义、文档化和沟通的架构决策和设计原则。 Diverse Exposure and Experience 架构师应接触并体验多种多样的技术、框架、平台和环境，从而帮助架构师在面对新问题时，可以从更广阔的技术栈中选择最佳解决方案。 Have Business Domain Knowledge 优秀的软件架构师不仅理解技术，还理解问题空间的业务领域，从而设计出贴合业务发展的有效架构。 Possess Interpersonal Skills 成为一名有效架构师，人际交往能力至少占一半。架构师需要积极与团队协作、进行双向沟通，提供指导和辅导，避免成为象牙塔架构师（Ivory Tower Architecture）。 Understand and Navigate Politics 架构师需要理解企业内部的政治环境，并能驾驭其复杂性。在架构决策受到挑战时，沟通过程中需要始终提供技术和业务上的双重理由，学习将强硬要求转化为请求，并利用同理心和影响力而非头衔来推动决策。 3.2 成长指南 成长为一名合格的软件架构师是一个持续学习和实践的过程，涵盖技术、沟通和领导力等多个方面，这里笔者结合 FOSA 书中的 19~24 章内容进行梳理总结： 持续学习和扩展技术广度 20 分钟法则：每天至少投入 20 分钟学习新知识或深入研究特定主题，这里笔者推荐可以每天（或每周）跟 LLM 深入探讨一个技术话题或前沿技术概念。 技术雷达：利用如 ThoughtWorks 技术雷达的方法来组织和评估新技术，识别值得投入时间深入研究的\"试用\"技术，并根据趋势更新知识库。 社交媒体：积极利用社交媒体发现新趋势和技术，将其放入个人技术雷达的\"评估\"环中。 实践权衡分析 失败乃成功之母，实践是检验真理的唯一标准。只有在实践中不断的进行架构设计、权衡抉择、推翻重建，才能不断深化对理论知识、业务领域和现实世界的理解。 培养批判性思维，避免过度设计和\"黄金镀层\"现象，专注于解决实际问题，而不是为了技术而技术。软件架构中没有错误的答案，只有昂贵的答案。 培养沟通和协作能力 通用语言：参考 DDD（领域驱动设计）思想，在所有项目相关沟通中（包括代码和文档）都使用通用语言，避免沟通歧义，减少沟通成本。 4C 原则：在沟通中始终关注沟通（Communication）、协作（Collaboration）、清晰（Clarity）和简洁（Concisensess）这 4 个要素。 主动倾听：认真倾听利益相关者的声音，理解他们的业务需求和痛点，并寻求澄清。 通过榜样领导团队 赢得尊重 辅导和引导 化请求为帮助 使用清单 深入业务领域 分析业务领域和子域，识别公司的主要活动领域、竞争策略。 学习领域驱动设计，始终让业务驱动软件设计决策，而不是为了应用最新的技术而技术。 4. 软件架构的第一定律 Everything in software architecture is a trade-off. If an architecture thinks they have discovered something that isn't a trade-off, more likely they just haven't identified the trade-off yet. Why is more important than how. 软件架构中的一切都是权衡。一个解决方案是否是\"最佳\"的，取决于部署环境、业务驱动因素、公司文化、预算、时间限制、开发人员技能集以及其他数多种因素。架构师应追求\"最不差架构（least worst architecture）\"，而非\"最佳架构\"。试图支持过多的架构特性往往会导致过于通用且笨重的设计，使其难以成功。","tags":["读书笔记","软件架构","fosa"],"categories":["架构设计"]},{"title":"Q&A丨在 AI 时代，如何应对技术焦虑？","path":"/2025/06/21/qa-how-to-deal-with-tech-anxiety-in-ai-era/","content":"长期以来我一直陷于技术海洋之中无法自拔，各种追逐技术，有过偶尔的专注，也获得了很大的提升，但更多时候，还是处于多头乱撞、内耗焦虑当中。 在 AI 快速发展的这段时间，我的焦虑更甚，减法做得越来越差，所以最近专门跟 Google Gemini 聊了这个话题，我觉得它的回答非常好，解答了我很多的疑惑，也让我对未来有了更多的信心，特此整理此篇，以图内心获得更多的宁静。 在 AI 时代，还有必要学习底层技术吗？ 第一个问题我是跟 ChatGPT 讨论的：在 AI 时代，程序员还有必要学习底层技术吗？ 它给了我 5 个直击内心的理由： 抽象层终会\"泄露\"，底层知识是你的救生筏； 问题的根源，往往深藏于你看不到的地方； 真正的工程判断力，建立在对权衡的理解之上； 你的职业天花板，由底层知识决定； 创新，源于对第一性原理的掌握。 我觉得非常有道理： 当 AI 失灵或表现不佳时，能拯救你的不是另一个 AI，而是你对底层的掌控力。 只懂“驾驶”的人在车坏了时只能打电话求助，而懂“机械”的人能自己打开发动机盖解决问题。我要做后者。 AI 可以成为我的顾问，但最终做出决策、并为之负责的人是我。我的决策质量，直接取决于我的底层知识深度。 AI 会拉平初级和中级工程师的差距，但底层技术功底是区分高级/资深工程师与普通工程师的护城河。 学习底层技术，不仅是为了解决今天的问题，更是为了获得解决明天未知问题、甚至定义未来的能力。 所以在保持积极学习前沿技术的同时，我应当将更多的时间，有计划地、持续地加深我的纵向底层知识，从\"为什么会这样？\"开始，一路向下挖掘，直到触及问题的本质。 如何应对技术焦虑 第二个问题我是跟 Google Gemini 讨论的：后端工程师如何应对技术焦虑。 原则重于朝生暮死的工具 后端开发的核心能力，强调那些超越特定语言或框架的底层原则。尽管工具日新月异，但其试图解决的根本问题和所遵循的基本模式却保持着相当的一致性。 后端工程技能三阶模型： 是什么：掌握基本工具和概念，能够完成独立任务。 怎么做：熟练运用框架和模式，构建稳定、可维护的系统。 为什么：进行系统级思考，设计高可用、高扩展性的复杂系统。 技术焦虑的一个重要来源，是将职业发展视为一个永无止境的\"技能清单\"勾选过程。今天学会了 Django，明天又要追逐 FastAPI；刚掌握了 Docker，Kubernetes 的生态又让人望而生畏。这种思维模式必然导致疲于奔命。 真正的“不变之核”，不是技能本身，而是该技能所体现的原则。 技术焦虑的逻辑拆解： 开发者面临的困境是新技术层出不穷，感到焦虑 [用户问题]。 行业分析罗列了大量\"必备技能\"，如 Python、Django、Docker、Kubernetes 等 。 如果只是简单地告诉开发者去学习这个清单，只会加剧焦虑，因为清单永远在变长。 与此同时，另一些分析强调了基础概念的重要性，如面向对象、结构化思维、解决问题的能力 。 将这两类信息结合起来，我们能看到更深层的联系： Django 和 Ruby on Rails 是 MVC（模型-视图-控制器）架构模式的实现。 Spring Boot 框架深度应用了依赖注入（DI）和面向切面编程（AOP）的思想。 Docker 和 Kubernetes 是为了解决“环境一致性”和“服务编排与生命周期管理”这两个根本问题而诞生的解决方案。 因此，真正持久的技能，是理解这些问题和模式（即\"为什么\"），而不仅仅是掌握某个工具（即\"是什么\"）。一个理解了 MVC 模式的工程师，可以快速上手任何一个采用类似模式的新框架。而一个只\"会用 Django\"的工程师，当行业风向转变时，可能会陷入困境。 数据结构、算法、计算机网络、操作系统和计算理论等 CS 基础知识为工程师提供了一个统一的、抽象的框架，用以推理和分析所有计算系统，无论其外在形态如何变化 。这种力量体现在它能够培养一种至关重要的能力：结构化思维和问题分解。 CS 基础知识的另一个巨大价值在于，它能帮助工程师\"快速理解不熟悉的系统\"，比如： 当你看到一个 HTML 或 XML 文档时，你不会只看到一堆标签，你会看到一棵树（Tree）。这个认知让你立刻能够运用所有关于树的知识来思考它：DOM 遍历算法（深度优先、广度优先）、节点操作的效率、以及如何优化渲染性能。 当你接触到一个新的键值存储（Key-Value Store）系统时，你脑海中浮现的应该是哈希表（Hash Table）。这个抽象模型让你能够立即开始推理其核心特性和潜在问题：哈希冲突如何解决？负载因子过高时性能会如何衰减？它的时间复杂度在理想和最坏情况下分别是多少？ 当你研究像 Apache Kafka 这样的消息系统时，你会认识到从单个消费者的角度看，一个 Topic 本质上就是一个队列（Queue）。这个模型帮助你理解消费者组（Consumer Group）的行为、偏移量（Offset）的管理机制，以及消息的顺序性保证等核心概念。 这种通过高层抽象和模式匹配来快速定位和理解新技术核心本质的能力，是在日新月异的技术环境中保持方向感和学习效率的关键。它让你在面对任何一个新框架、新平台或新工具时，都能迅速地抓住其要害，而不是迷失在纷繁复杂的 API 和配置细节之中。 在人工智能时代，我们越来越多地与一些极其复杂的系统打交道，尤其是大型语言模型（LLM），它们在很多开发者眼中就像一个\"黑箱\"。我们知道如何向它提问并获得惊艳的答案，但对其内部工作原理却知之甚少。这种未知感，正是技术焦虑的重要来源之一——我们称之为\"黑箱焦虑\"。 一个拥有扎实 CS 基础的工程师面对一个新 AI 系统的场景： 场景一：向量数据库。 当他听说一个应用使用了向量数据库（Vector Database）来实现语义搜索时 ，他不会仅仅惊叹于其\"神奇\"的效果。他的大脑会立即启动基于 CS 基础的推理： 数据结构层面： 为了实现高效的近邻搜索，这个向量数据库内部很可能使用了某种空间分割数据结构，比如 k-d 树（k-d tree），或者更现代的、基于图的 HNSW（Hierarchical Navigable Small World）算法。这两种结构在查询速度、内存占用和索引构建时间上有什么不同的权衡？ 算法层面： 它使用的距离度量是欧氏距离还是余弦相似度？这对于不同类型的嵌入向量（Embeddings）意味着什么？ 系统层面： 这是一个单体数据库还是分布式系统？如果是分布式的，它是如何处理数据分片和查询路由的？ 场景二：AI Agent 系统。 当他了解到 AI Agent 能够自主规划并执行一系列复杂任务时 ，他不会感到无所适从。他会联想到： 算法层面： 这种任务规划本质上是一个在巨大的状态空间中进行搜索的问题。它可能在内部使用了某种图搜索算法，比如 A* 算法，或者蒙特卡洛树搜索（MCTS）。这些算法的潜在缺陷是什么？比如，是否可能陷入局部最优解，或者面临组合爆炸的问题？ 系统层面： 这个 Agent 系统是如何与外部工具（Tools）进行交互的？是通过结构化的 API 调用吗？那么 API 的可靠性和延迟将成为整个系统的瓶颈。它如何处理工具调用失败的情况？有重试机制或错误处理逻辑吗？ 工程师心智模型 一个工程师最持久、最宝贵的资产，并非某项具体的技术，而是一种特定的思维方式。这种\"工程师心智模式\"（Engineering Mindset）是运行所有其他技能的底层操作系统，是应对一切变化的最终依仗。 综合多方研究，我们可以将工程师心智模式的核心特质归纳为以下几点： 系统性的问题解决方法 数据驱动与逻辑推理 对持续改进的执着 韧性与适应性 主动的好奇心 软技能： 沟通 协作 时间管理 AI 革命在历史进程中的位置 软件开发的历史并非一条平滑的直线，而是由一系列深刻的范式转移（Paradigm Shift）所驱动的。这些变革往往是为了应对上一代范式所暴露出的危机或局限性而生 。 个体创作时代（Individual Creation）：在软件开发的早期，程序被视为天才程序员在\"作坊\"中创作的精妙艺术品。 工程范式时代（Engineering Paradigm）：随着软件系统规模和复杂度的急剧增长，个体创作模式难以为继，导致了\"软件危机\"。为了应对危机，业界引入了工业化生产的管理思想，提出了\"软件工程\"的概念，强调需求分析、流程分解、文档规范和质量控制，将软件开发从个体创作推向了大规模、有组织的群体生产 。 开源范式时代（Open Source Paradigm）：工程范式在应对互联网时代的需求不确定性和快速变化时显得力不从心。此时，以\"代码开源、过程开放、大众参与\"为特征的开源运动蓬勃发展，形成了一种新的范式。它不强调预先确定的需求，而是通过\"自下而上、演化涌现\"的方式，激发大规模群体的创作灵感和智慧 。 群智范式时代（Crowd Intelligence Paradigm）：为了平衡工程范式的确定性和开源范式的不可控性，群智范式应运而生。它试图在规范生产和自由创作之间找到平衡，通过\"宏观演化，微观求精\"的理念，结合核心团队的引导和外围群体的贡献，实现软件的持续迭代和演化 。 范式 核心理念 对\"需求\"的看法 对\"质量\"的看法 对\"效率\"的看法 主要瓶颈 工程范式 自上而下，逐步求精 开发的起点和依据，需预先明确和规范化 满足需求规格的程度，通过验证和测试保障 投入产出比，通过过程控制和自动化提升 协同效率瓶颈（人月神话），无法适应网络时代的需求不确定性 开源范式 自下而上，演化涌现 不必预先明确，可由开发者自身构思驱动 体现为社区规模和口碑 体现为项目迭代效率（如缺陷修复速率） 结果不可控，将创意作品收敛为产品的成本极高 群智范式 宏观演化，微观求精 持续获取与凝练的过程，以原型版本和疑修(Issue)集合呈现 宏观上是生态适应能力，微观上是版本满足里程碑的程度 包含激发效率和汇聚效率，体现为迭代演化的成本与时间 如何设计高效的协作机制与智能化工具以保障激发与汇聚效能 当前企业和社会对 AI 的采纳过程，与十多年的云计算转型惊人地相似，我们可以从中汲取宝贵的经验教训： 始于业务问题，而非技术本身（Start with the Business Problem) 清晰评估现状（Assess the Current State） 切勿忽视人的因素（Don't forget the Humans） 警惕技术蔓延与技术债务（Avoid Sprawl and Techinal Debt） AI 不是威胁而是工具 生成式 AI 正以前所未有的深度和广度渗透到软件开发生命周期（Software Development Lifecycle, SDLC）的每一个环节，从根本上改变着开发者的工作方式 。 构思与规划（Ideation Planning）： 需求识别与优先级排序: 分析用户反馈、市场趋势数据，辅助产品负责人识别关键需求并进行优先级排序。 可行性与资源预测: 基于历史项目数据，预测项目成本、时间和资源需求，做出更精准的规划。 数据驱动决策: 使早期规划更具客观依据，减少主观臆断。 减少需求冲突: 自动识别不完整或相互矛盾的需求描述，降低后期返工风险。 设计（Design） 原型加速创建: 快速生成用户界面原型、线框图和流程图。 架构模式推荐: 基于项目约束和最佳实践，推荐最优的系统架构或设计模式。 缩短设计周期: 大幅减少手动绘制原型和设计文档的时间。 避免早期架构失误: 借助 AI 的知识库，帮助团队在项目初期做出更稳健的架构选择 。 开发（Development） 智能代码补全与生成: 基于上下文，自动补全代码行、函数甚至整个逻辑块 (如 GitHub Copilot) 。 从自然语言生成代码: 根据高层级的功能描述，直接生成多种语言的代码片段 。 代码重构与翻译: 自动将老旧代码（如 COBOL）重构为更易读的现代语言（如 Java），或在不同语言间进行转换。 测试（Testing） 自动化测试用例生成: 从用户故事或需求文档直接生成功能测试用例，包括人类测试者可能忽略的边缘情况。 AI 辅助代码审查: 在代码提交前，实时扫描潜在的安全漏洞、逻辑错误或性能瓶颈。 提升测试覆盖率: 自动生成全面的测试套件，确保软件质量。 缺陷左移 (Shift-Left): 在开发早期发现并修复缺陷，显著降低修复成本和后期风险 。 部署（Deployment） 自动化部署脚本生成: 自动生成部署脚本或基础设施即代码（IaC）的配置文件（如 Terraform, Ansible）。 CI/CD 流水线优化: 辅助编排代码、基础设施和配置管理，实现更快的持续交付。 减少手动部署错误: 自动化配置过程，降低人为失误的概率。 加速产品上市时间: 实现更快速、更频繁的生产环境更新，快速响应市场变化。 运维与维护（Maintainice Operations） 智能监控与异常检测: 学习系统正常行为模式，主动识别异常，预测潜在故障 。 自动化事件响应与修复: 自动执行事件分类、根本原因分析，甚至触发自动化修复脚本 。 文档与知识库维护: 自动生成或更新技术文档、API 文档和知识库文章 。 从被动响应到主动预防: 在问题影响用户之前进行干预。 降低平均解决时间 (MTTR): 快速定位并解决生产问题。 提升知识管理效率: 确保文档与代码同步，降低团队沟通成本。 开发者角色的转变 经验丰富的工程师的角色，就从亲自编写每一行代码，转变为更高层次的审查者、整合者和架构师 。他们的核心工作变成了： 定义问题与设定目标：清晰地向 AI 描述要实现的功能和约束条件。 批判性审查 AI 的输出：评估 AI 生成的代码是否符合架构设计、是否遵循编码规范、是否存在潜在的性能和安全问题。 整合与调试：将 AI 生成的代码片段无缝地整合到现有系统中，并调试其中可能存在的错误。 做出架构权衡：决定何时使用 AI、使用哪个 AI 工具，并对 AI 无法处理的、需要深刻理解业务和系统长期演进的复杂架构问题做出决策。 智能后端的崛起：AIOps 与系统架构 AI 不再仅仅是用于构建后端的工具，而是成为了后端系统本身的核心组成部分。这种融合催生了名为 AIOps 的新领域，正在彻底改变我们对系统运维和架构的认知。 AIOps，即人工智能运维（Artificial Intelligence for IT Operations），是由 Gartner 提出的概念，指的是将大数据和机器学习技术应用于 IT 运维流程，以实现自动化和增强 。在传统的运维模式中，工程师们常常扮演着\"救火队员\"的角色，在系统发生故障后被动地响应告警、排查问题。而 AIOps 的目标，是利用 AI 的预测和模式识别能力，将运维从被动响应转变为主动预防，甚至实现预测性维护 。 AIOps 通过分析海量的系统遥测数据（包括日志、指标和追踪），为后端系统的稳定性、性能和安全性带来了革命性的提升。 主动的事件侦测与预防（Proactive Incident Detection Prevention）：传统监控系统依赖于预设的静态阈值（例如，CPU 使用率超过 90% 则告警）。而 AIOps 平台通过机器学习算法，能够学习系统在不同负载和时间下的\"正常行为\"基线。当系统行为偏离这个动态基线时，即使没有触及任何静态阈值，AIOps 也能识别出异常，从而在问题升级为严重故障、影响到终端用户之前，就向工程师发出预警 。 告警降噪与智能关联（Noise Reduction Intelligent Alerting）：在复杂的微服务架构中，一个底层的故障（如数据库慢查询）可能会引发连锁反应，导致成百上千个相关服务的告警同时爆发，形成\"告警风暴\"，让待命工程师（On-call Engineer）不堪重负。AIOps 能够自动将这些相关的告警进行关联和分组，并识别出最初的根源事件，将数百条告警压缩为一条包含丰富上下文的、可操作的事件通知，极大地减少了告警噪音，降低了工程师的认知负担。 自动化的根本原因分析 (Automated Root Cause Analysis)：当故障发生时，最耗时的工作往往是定位根本原因（Root Cause）。AIOps 通过分析跨越整个技术栈（应用、中间件、数据库、网络、基础设施）的数据，能够自动识别不同组件之间的因果关系和依赖关系，快速推断出问题的根源，将工程师从繁琐的手动排查中解放出来 。 自愈与自动化修复 (Self-Healing and Automated Remediation)：这是 AIOps 最前沿的演进方向，有时也被称为\"Agentic AIOps\"。在这种模式下，系统不仅能检测和分析问题，还能自主地采取行动进行修复。例如： 检测到某个服务实例无响应时，自动重启该实例。 预测到流量高峰即将来临时，自动扩展相关服务的计算资源。 发现数据库连接池耗尽时，自动调整连接池大小。 识别到安全威胁时，自动执行隔离或封禁 IP 等安全策略 。 AIOps 的崛起，意味着后端工程师在运维领域的角色正在发生根本性的转变。他们的工作重心将从被动的\"救火\"，转向主动的\"防火\"和\"消防系统设计\"。具体来说： 成为 AIOps 平台的架构师和维护者； 理解并应用机器机器学习模型的基本原理、适用场景和局限性； 为可观测性而设计（Design for Observability），为 AIOps 提供高质量的\"燃料\"。 AI 赋能工程师的基础技能栈 数学与统计学基础（Mach Stats Foundation）：这是理解 AI 模型\"如何工作\"的基石，而不仅仅是\"如何使用\"。一个扎实的数学基础能让你在面对模型调优、性能瓶颈分析和结果解读时，具备更深刻的洞察力。 线性代数 (Linear Algebra)：理解向量、矩阵、张量及其运算。这是理解数据表示、神经网络结构和各种转换的基础 。 微积分 (Calculus)：理解导数、偏导数和链式法则。这是理解梯度下降等优化算法如何工作的关键 。 概率论与统计学 (Probability and Statistics)：掌握概率分布、贝叶斯定理、假设检验等概念。这是理解和评估模型性能、处理不确定性的基础 。 核心机器学习概念（Core ML Concepts）：这是 AI 应用领域的通用语言，构成了解决大多数商业问题的基础。 学习范式：清晰地区分监督学习（Supervised Learning）、无监督学习（Unsupervised Learning）和强化学习（Reinforcement Learning）的适用场景 。 关键算法：了解一些经典的算法，如线性回归、逻辑回归、支持向量机（SVM）、K-均值聚类（K-Means）以及集成方法（如随机森林、梯度提升树）。 核心流程：掌握特征工程（Feature Engineering）、模型评估（Model Evaluation）和超参数调优（Hyperparameter Tuning）等关键环节 。 深度学习与生成式 AI（Deep Learning Generative AI）：这是当前 AI 浪潮的核心驱动力，也是后端工程师需要重点关注的新兴领域。 神经网络基础：理解神经网络的基本构成，如卷积神经网络（CNNs）和循环神经网络（RNNs）的原理和应用场景 。 Transformer 架构：深入理解作为现代大语言模型（LLM）基石的 Transformer 架构 。 生成式 AI 核心技术：熟悉检索增强生成（Retrieval-Augmented Generation, RAG）的原理和实现方式，这是将私有数据与 LLM 结合的关键技术 。 AI 框架与平台（AI Framework and Platforms）：将理论知识转化为实践能力，离不开对主流工具的掌握。 开发框架：具备使用 PyTorch 进行模型构建和训练的实践经验 。 模型生态：熟悉 Hugging Face 等平台，能够利用其丰富的预训练模型生态系统来加速开发 。 云 AI 服务：了解并能够使用主流云服务商（如 AWS Bedrock, Azure AI, Google Vertex AI）提供的 AI 平台和 API 服务 。 应对焦虑的小建议 高效地休息 (Take Effective Breaks)：长时间不间断地工作，尤其是在编程这种高强度脑力劳动中，会导致效率下降和精神紧张。采用番茄工作法（Pomodoro Technique），即工作 25-30 分钟后，强制自己休息 5 分钟，或者每工作 1.5-2 小时，进行 10-20 分钟的休息，能够有效恢复精力 。关键在于，休息时要真正地\"脱离\"，即离开电脑屏幕，站起来走动，或者看看远方，而不是切换到手机上继续浏览信息 。 关注身体健康 (Physical Well-being)：身心健康密不可分。规律的体育锻炼和健康的营养摄入，是维持长期成功的两个最重要因素 。运动能释放内啡肽，缓解压力；均衡的饮食能为大脑提供稳定的能量。这就像维护一台高性能的服务器，必须为其提供稳定的电力和良好的散热。 正念与减负荷 (Mindfulness and De-stimulation)： 正念练习：每天进行几分钟的冥想练习，可以帮助训练大脑的专注力，减少杂念。可以使用 Headspace、Waking Up 等应用，或者 YouTube 上 的引导式冥想视频 。 减少咖啡因/酒精摄入：过量的咖啡因会加剧焦虑感。可以尝试用绿茶等含有 L-茶氨酸（L-Theanine）的饮品来替代，它具有镇静作用 。 避免过度刺激：下班后，尽量避免进行高刺激性的活动，如玩竞技类游戏、看动作大片或无休止地刷社交媒体。可以选择散步、听有声书、阅读、做瑜伽等舒缓的活动，让大脑真正地放松下来 。 设立清晰的边界 (Set Boundaries)：在远程办公和弹性工作日益普遍的今天，工作与生活的边界变得模糊，这极易导致职业倦怠。必须有意识地设立并捍卫自己的边界。 明确工作时间：设定固定的上下班时间，并严格遵守。 管理通知：使用手机的\"专注模式\"或类似功能，在非工作时间屏蔽工作相关的通知，在工作时间屏蔽不必要的干扰 。 优先排序：要清醒地认识到，家庭、健康和个人生活，远比修复一个明天才到截止日期的 BUG 更重要。学会对不合理的要求说\"不\"。 关注过程，而非终点 (Focus on Process, Not Outcome)：拥抱成长型思维，将你的目标从\"完全掌握 AI\"这个不切实际的终点，转变为\"保持持续学习的状态\"这个可控的过程。技术的演进没有终点，因此你的学习也不应有终点。接受这一点，能让你从对\"完成\"的焦虑中解脱出来，转而享受学习和进步本身带来的乐趣。 如何高效学习底层与新技术 应对当前挑战的最优策略，是向\" AI 增强的 T 型工程师（AI-Augmented T-Shaped Engineer）\"转型： 深度的垂直支柱：投入时间系统性学习那些具有长期价值的计算机科学基础原理，如算法、系统设计、编译原理等。这是职业生涯的\"压舱石\"，构成了 T 型的垂直笔画。 广阔的水平横梁：建立一个高效的机制，以保持对新兴技术，特别是 AI 领域的广泛、自适应的认知。 精通技艺：用费曼学习法实现深度理解 需要掌握： 计算机理论与编程范式：《计算机程序的构造和解释》。 编译器与语言原理：《用 go 语言自制解释器》、《用 go 语言自制编译器》。 算法与数据结构：《业务开发算法 50 讲》、《数据结构与算法之美》。 操作系统：《手写 OS》、《Writing an OS in Rust》。 软件工艺：《程序员的修炼：从优秀到卓越》。 费曼学习法： 选择一个概念； 尝试教会一个 12 岁的孩子。 识别理解的缺口。 回顾与简化。 水平横梁：构建情报引擎 微实践 优质信息源： 信息源名称 关注领域 频率 为何具有高信噪比 Benedict's Newsletter 宏观科技战略与趋势 每周 由顶尖分析师提供深刻的行业洞察，帮助理解技术背后的商业逻辑 The Pragmatic Engineer 软件工程实践、文化、职业发展 每周 由前 Uber 工程领导者撰写，提供来自一线的、深入的工程管理和技术决策分析 TLDR Newsletter 科技、编程、网络安全新闻摘要 每日 极其简明扼要，用几句话总结当日最重要的技术新闻，适合快速扫描 Import AI AI 研究、政策与安全 每周 由 Anthropic 联合创始人策划，提供对 AI 领域重大进展和伦理影响的专业解读 ByteByteGo Newsletter 系统设计与架构 每周 深入浅出地讲解复杂的系统设计概念，对后端工程师极具价值 学习新技术的最佳方式是动手实践。然而，为了不影响核心的深度学习，这种实践必须是目标明确且时间受限的。这里引入\"即时学习\"（Just-in-Time Learning）和\"微问题解决\"（Micro-Problem Solving）两个概念。 即时学习（JIT Learning）：这是一种按需学习的模式，即在需要应用某项知识或技能时才去学习它，而不是进行大规模的预先学习 。这种方式可以减轻认知负担，并将学习与实际应用紧密结合，从而提高知识留存率。 玩具项目（Toy Projects）：当一个新技术（例如一个新的 AI 框架如 LangGraph 或一个向量数据库）出现并显得重要时，为其分配一个严格限定时间的\"玩具项目\"，比如一个周末或几个晚上的时间 。项目的目标不是构建一个生产级应用，而是通过动手实践，获得对该技术核心概念、API 设计和工作流程的直观理解。 微问题解决（Micro-Problem Solving）：将玩具项目分解为一系列最小的可执行任务 。例如，学习构建一个 RAG（检索增强生成）应用，可以分解为以下微问题： 加载一篇 PDF 文档； 将文本分割成块（chunking）； 为文本块生成向量嵌入（embeddings）； 将嵌入存储到向量数据库； 根据用户查询检索最相关的文本块； 将检索到的内容与原始查询结合，生成最终答案 。 这种分解使得学习过程 manageable，并能提供持续的、小步快跑式的成就感。 拓展横梁：认知 AI 产品生态系统 在 AI 时代，一名高级工程师的\"广度\"已不再局限于技术本身。由于 AI 技术的选择（如模型、框架）直接影响到产品的成本、延迟、用户信任乃至商业模式，工程决策与商业战略的联系变得前所未有地紧密 。 因此，现代工程师的 T 型横梁需要延伸至对 AI 产品生态的理解。这包括： AI 产品管理框架：了解如何识别和验证 AI 用例，评估其商业价值、技术可行性和用户可用性 。 AI 产品上市策略（GTM）：理解 AI 产品，尤其是具有不确定性输出的概率性产品的市场定位、定价模型和营销渠道策略 。 构建可防御的\"护城河\"：明白在 AI 技术本身易于复制的背景下，产品的长期竞争力更多地来自于专有数据、独特的工作流集成、强大的用户体验和生态系统 。 时间管理：深度工作与扫描协议 深度工作模块 (The Vertical Bar)：这是为 T 型模型的\"垂直支柱\"——即基础原理学习——专门预留的时间。 方法：每周规划 2 到 3 个不可协商的、时长为 2 小时的“深度工作”时间块 。将这些时间块像对待最重要的会议一样标记在日历上，并告知团队成员在此期间除非紧急情况，否则不要打扰。 认知科学研究表明，一次中断后，人需要平均 23 分钟才能重新进入专注状态，对于复杂的编程任务，这个时间更长 。因此，长时间、不受干扰的模块对于攻克 SICP 或 DDIA 中的复杂概念至关重要。 扫描与即时学习模块 (The Horizontal Bar)：这是为 T 型模型的“水平横梁”——即技术雷达扫描和即时探索——设计的时间。 方法：安排更短、更频繁的时间块，例如每日 30-45 分钟，用于\"扫描\"活动。利用这段时间阅读筛选过的新闻通讯、浏览新技术发布、或推进你的\"玩具项目\"。这个协议的目的是防止对新技术的追逐侵占宝贵的深度工作时间。 任务批处理 (Task Batching)： 方法：将性质类似的\"浅层工作\"（shallow work）归集在一起，在指定的时间块内一次性处理完毕 。例如，将所有非紧急的邮件回复、代码审查（Code Review）或行政事务集中在下午的某个固定时段处理。这可以有效减少任务切换带来的认知损耗，为深度工作保留宝贵的精力。","tags":["思考"],"categories":["ai问答"]},{"title":"Q&A丨在 AI 时代，还有必要学习底层技术吗？","path":"/2025/06/17/qa-should-learn-underlying-principles-in-ai-era/","content":"毫无疑问，绝对有必要，甚至比以往任何时候都更加重要。 AI 时代，应用层的技术和工具变得越来越“智能”和“易用”，这反而造成了一种危险的错觉：似乎我们不再需要理解那些复杂的底层原理了。但真相恰恰相反。 把自己想象成一位 F1 赛车手。AI 工具就像一个极其先进的驾驶辅助系统，它能帮你自动换挡、优化过弯路线，甚至在某些直道上自动驾驶。这能让你开得更快，但如果你想成为冠军，你必须理解： 引擎的工作原理（操作系统、CPU）：为什么在某个转速下引擎会抖动？如何压榨出极限动力？ 空气动力学（网络、I/O）：为什么今天赛车感觉有点“飘”？是风向变了还是下压力调校出了问题？ 轮胎与悬挂（数据结构、数据库）：如何根据赛道温度和磨损情况选择轮胎？悬挂的软硬如何影响抓地力？ 只依赖驾驶辅助系统的车手，在赛车一切正常时表现尚可。一旦出现任何异常——轮胎异常磨损、引擎过热、通信系统延迟——他将束手无策，只能无奈退赛。而理解底层的车手，能通过细微的反馈感知到问题，与工程师沟通，调整策略，最终赢得比赛。 五个无法回避的理由 当你再对学习底层技术产生动摇时，请用以下几点来说服自己： 1. 抽象层终会“泄漏”，底层知识是你的救生筏 这是软件工程颠扑不破的“抽象泄漏定律”（The Law of Leaky Abstractions）。任何为了简化而存在的上层工具（包括 AI），都无法完美隐藏其底层的复杂性。当问题发生时，这个“泄漏”就会出现。 场景 A：AI 帮你生成了一段代码，用于从数据库查询数据，但在生产环境压力下响应极慢。AI 无法告诉你原因是索引失效、发生了锁竞争还是因为 N+1 查询。这时，你需要数据库底层知识来分析执行计划、优化索引。 场景 B：一个由 AI 调度的微服务出现随机性高延迟。AI 无法告诉你这是因为容器的 CPU 被限制、发生了网络丢包，还是因为 JVM 的垃圾回收（GC）暂停。这时，你需要操作系统和网络的底层知识来定位根源。 场景 C：你的 RAG 应用召回结果不理想。AI 无法告诉你是因为向量嵌入模型选择不当，还是向量数据库的索引策略（如 HNSW）参数需要调优。这需要你理解数据结构和算法。 说服自己：当 AI 失灵或表现不佳时，能拯救你的不是另一个 AI，而是你对底层的掌控力。 2. 问题的根源，往往深藏于你看不到的地方 高级的故障排查（Troubleshooting）是后端工程师的核心价值之一。问题的表象（Symptom）和根源（Root Cause）往往不在同一个层面。 一个 API 超时，表象是应用层错误。根源可能是 TCP 连接池耗尽、是 DNS 解析缓慢、是磁盘 I/O 达到瓶颈。 AI 可以帮你分析日志，找到那个超时的 API。但它很难跨越多个技术栈，将零散的线索串联起来，形成一个完整的证据链，最终定位到那个深藏的根源。这种系统性的诊断能力，源于你脑中那张完整的技术底层地图。 说服自己：只懂“驾驶”的人在车坏了时只能打电话求助，而懂“机械”的人能自己打开发动机盖解决问题。我要做后者。 3. 真正的工程判断力，建立在对权衡（Trade-off）的理解之上 AI 可以提供“方案”，但无法为你做出最佳的“决策”。工程的核心是权衡。 用关系型数据库还是 NoSQL？用 Redis 还是本地缓存？服务间通信用 gRPC 还是 RESTful API？ 每一个选择背后，都是对性能、成本、一致性、可用性、开发效率等一系列因素的综合考量。而这些考量的依据，正是来自你对各种技术底层实现机制的理解。不了解 TCP 和 HTTP/2 的底层差异，你如何能在 gRPC 和 REST 之间做出最合理的选择？不了解 B+树和 LSM 树，你如何为特定场景选择最合适的数据库？ 说服自己：AI 可以成为我的顾问，但最终做出决策、并为之负责的人是我。我的决策质量，直接取决于我的底层知识深度。 4. 你的职业天花板，由底层知识决定 随着 AI 自动化掉越来越多重复性的、模式化的编码工作，未来后端工程师的价值会更加向两个方向集中： 向上：深入理解业务，进行业务建模和架构设计。 向下：解决硬核的技术难题，进行极致的性能优化和系统稳定性保障。 这两个方向，都极度依赖底层技术。没有底层知识，你的架构设计就是空中楼阁；没有底层知识，你永远无法成为解决最棘手问题的那个关键人物。 说服自己：AI 会拉平初级和中级工程师的差距，但底层技术功底是区分高级/资深工程师与普通工程师的护城河。 5. 创新，源于对第一性原理的掌握 我们使用的所有 AI 工具、框架、平台，本身就是由那些深刻理解底层技术的人创造出来的。他们不是简单地“使用”技术，而是基于对计算、存储、网络等第一性原理的理解，去“创造”新的技术。 如果你满足于只做一个技术的使用者，或许可以忽略底层。但如果你心中还有一丝火花，想在技术的世界里留下自己的印记，想创造出真正有价值的东西，那么掌握底层就是你唯一的路径。 说服自己：学习底层技术，不仅是为了解决今天的问题，更是为了获得解决明天未知问题、甚至定义未来的能力。 结论：如何平衡学习 这并非要求你立刻放下所有 AI 学习，去埋头啃《计算机程序的构造和解释》。正确的做法是： 保持“T 型”结构：在积极拥抱和学习 AI 这个横向技能的同时，有计划地、持续地加深你的纵向底层知识。 问题驱动学习：在应用 AI 或开发业务时，遇到任何性能、稳定性、或“匪夷所思”的问题，都不要轻易放过。把它当作一个深入学习底层技术的绝佳契机。从“为什么会这样？”开始，一路向下挖掘，直到你触及问题的本质。 AI 时代，底层技术不是你的“备选项”，而是你的“压舱石”。它能让你的技术大船在 AI 的巨浪中，行得更稳、更快、更远。","tags":["思考"],"categories":["ai问答"]},{"title":"读书笔记丨《Rust Atomics and Locks》","path":"/2025/06/12/note-rust-atomics-and-locks/","content":"系列文章： Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序 Rust 原理丨从汇编角度看原子操作 Rust 实战丨手写一个 SpinLock Rust 实战丨手写一个 oneshot channel Rust 实战丨手写一个 Arc Rust 原理丨操作系统并发原语 Rust 实战丨手写一个 Mutex Rust 实战丨手写一个 Condvar Rust 实战丨手写一个 RwLock 读书笔记丨《Rust Atomics and Locks》👈 本篇 本文整理总结 Mara Bos 所著《Rust Atomics and Locks》一书的核心内容，系统归纳 Rust 并发编程中原子操作与锁机制的关键概念和技术细节。内容涵盖原子操作与内存顺序、Rust 并发的基础工具（内部可变性、线程安全保证、操作系统并发原语），以及多种常用并发原语（自旋锁、一次性通道、原子引用计数、互斥锁、条件变量、读写锁）的实现要点。通过这些内容的串联，展示概念之间的联系、底层原理、Rust 标准库实现方式与实际工程实践的关系，方便读者快速回顾并用作知识索引。 原子操作基础 Rust 提供了一系列原子类型（如 AtomicBool、AtomicUsize 等）来实现线程之间的无锁并发数据共享。对原子类型的操作分为三大类别： Load（加载） 与 Store（存储）：分别用于原子地读取和写入值。 Fetch-and-Modify（获取并修改）：在返回旧值的同时，原子地对值进行修改，例如 fetch_add、fetch_sub、fetch_or、fetch_and、fetch_xor、swap 等。 Compare-and-Exchange（比较并交换）：即原子地执行“如果当前值等于预期值则交换”的操作，包括 compare_exchange 和 compare_exchange_weak。 不同计算机体系结构对原子操作的支持方式有所差异，主要体现在指令集类别和内存模型上。现代CPU通常分为两种指令集架构： CISC（复杂指令集）：典型代表是 x86 架构（包括 x86-64）。CISC 指令集功能丰富、单条指令可执行复杂操作，但硬件实现复杂。x86-64 是基于 CISC 的 64 位扩展架构，由 AMD 设计主导，追求通过硬件复杂性换取高性能和广泛兼容性，主导了桌面与服务器领域。 RISC（精简指令集）：典型代表是 ARM 架构（如 ARM64）。RISC 指令集精简、每条指令功能单一，硬件实现相对简单且能效更高。ARM64 基于 RISC 的 64 位架构，由 ARM 设计，指令简洁、低功耗，在移动设备占主导地位并逐步进入服务器和 PC 领域。 原子性的实现机制： 在 x86-64 上，保证原子性的关键是使用 lock 前缀锁定总线或缓存行来原子执行指令；而在 ARM64 上，则依赖 Load-Linked/Store-Conditional（LL/SC）指令对实现原子操作。例如，x86 上原子的 compare-and-swap 通常通过 LOCK CMPXCHG 指令完成，而 ARM 上通过一对原子链式的加载/存储指令完成。如果处理器不支持所需的原子指令，Rust 会在编译期报错以防止不安全的并发操作。 内存序模型差异： 不同架构的内存序保证也不同。x86-64 属于强顺序（Total Store Order, TSO）模型，对内存操作的重排序有限制，而 ARM64 属于弱顺序模型，需要显式的内存屏障来保证顺序。具体来说，x86-64 默认禁止以下重排序： Load → 后续操作 不允许乱序：例如在同一线程中，先执行的读取不能被后执行的操作越过。 Store → 前序操作 不允许乱序：一个存储不能提前到先前未执行完的操作之前。 Store → 随后的 Load 则可能乱序：即一个存储操作后紧跟的加载操作在实际执行中可能被处理器提升到存储之前执行（典型的 Store-Load 重排）。 相比之下，ARM 等弱序模型中，大多数内存操作在缺乏同步指令时都可能被重排序，因此所有原子操作默认可能乱序，需要利用内存屏障指令（ARM 上如 dmb ish、dmb ishld）来提供顺序保证。此外，在 compare-and-exchange 操作上，x86-64 并未提供真正的“弱”版本（即不会出现无故失败的情况），因而 Rust 中的 compare_exchange_weak 在 x86 上实际上与强语义等价；而 ARM64 的 LL/SC 实现存在可能的自发失败，因此区分了 weak 版本以便需要时重试。 综上，硬件架构对原子操作的支持直接影响Rust并发库的实现策略：在强序的 x86 上，一些内存序保证可由硬件天然提供，而在弱序的 ARM 上则必须借助显式屏障指令来达成。Rust 的原子类型实现会针对不同架构插入相应的汇编指令，以确保提供声明的原子性和内存序语义。 内存顺序与内存模型 多线程环境下面临两大核心问题： 指令乱序执行（Out-of-Order Execution）：现代CPU会对指令进行乱序执行和优化，这可能导致程序实际执行顺序与源码顺序不一致。 跨线程内存可见性：不同线程对内存修改的可见性无法保证——一个线程写入的数据，何时及如何对其他线程可见，需要通过同步手段来控制。 为解决上述问题，需要了解内存模型中的两类关键顺序关系： Sequenced-Before（先行顺序）：描述单个线程内操作的先后顺序。按照程序中的先后关系确定，在同一线程内如果操作 A 在源码中位于操作 B 之前，那么 A sequenced-before B。先行顺序遵循以下规则：若操作 B 数据依赖于 A，则 A 必定在 B 之前执行；针对同一原子变量的操作按程序顺序执行；若两个独立操作无数据依赖且访问不同变量，那么处理器可能对它们重排。总之，先行顺序限定单线程的执行次序，为编译器和 CPU 提供本线程内优化的依据。 Happens-Before（先发生）：描述跨线程的操作顺序和可见性。如果操作 A happens-before 操作 B，意味着 A 的所有内存效果对于 B 是可见的。Happens-Before 建立在线程间的同步关系上，例如：在同一线程中，函数 f() 调用在前而 g() 在后，则 f() happens-before g()；一个线程调用 thread::spawn 创建新线程发生在对该新线程的 join 之前；再如互斥锁的加锁先于解锁操作（unlock 发生在 lock 之前释放锁的线程完成临界区之后）。这些 Happens-Before 规则确保了特定事件的跨线程可见性和执行顺序。 Rust 的原子操作支持五种内存顺序（Ordering 枚举），从最松弛到最严格依次为 Relaxed、Release、Acquire、AcqRel、SeqCst。不同的内存顺序决定了原子操作在乱序和可见性方面的保证强度。下表总结了它们的语义、保证、使用场景和示例： 内存顺序 说明 保证 适用场景 示例 Relaxed 最宽松的内存顺序 - 仅保证操作的原子性- 不提供任何同步保证- 不建立 happens-before 关系 - 简单计数器- 极高性能要求且确定不需要跨线程同步- 已通过其他方式确保数据可见性同步 counter.fetch_add(1, Ordering::Relaxed) Release 用于存储操作 - 此操作之前的所有内存访问不会被重排到它之后- 与后续线程的 Acquire 操作配对可建立 happens-before 关系 - 典型“生产者-消费者”模型- 发布共享数据给其他线程- 写入一个“初始化完成”标志 data.store(val, Ordering::Release) Acquire 用于加载操作 - 此操作之后的所有内存访问不会被重排到它之前- 与另一个线程先前的 Release 操作配对可建立 happens-before 关系 - “生产者-消费者”模型中获取数据- 读取共享数据（需确保数据已由其他线程准备好）- 检查某个初始化完成的标志 let val = data.load(Ordering::Acquire) AcqRel 读改写操作的组合语义 - 同时具有 Acquire 和 Release 的所有内存顺序保证- 只能用于原子读-改-写操作（RMW），对读取部分提供 Acquire 保证，对写入部分提供 Release 保证 - 需要双向内存同步的原子操作v- 实现锁等同步原语（例如原子自增既读取又写入）- 较复杂的原子同步场景 value.fetch_add(1, Ordering::AcqRel) SeqCst 全局顺序一致的最强顺序 - 包含 AcqRel 的所有保证- 所有线程对所有 SeqCst 原子操作的观察顺序一致（总排序）- 提供跨线程全局的内存顺序一致性 - 需要严格的全局一致性场景- 不确定使用哪种顺序时采用（保守策略）- 对性能要求不敏感的代码 flag.store(true, Ordering::SeqCst) 需要注意的是，Release 通常与 Acquire 搭配使用，共同建立线程间的同步关系。当线程 A 使用 Release 语义写入某个共享变量，线程 B 之后使用 Acquire 语义读取到了该值，那么可以保证：线程 A 中那次 Release 写入之前的所有内存写操作，对线程 B 在 Acquire 读取之后的所有操作都是可见的。换言之，通过 Release-Acquire 的配对建立了跨线程的 happens-before：生产者线程写入的数据对消费者线程可见。这就是典型的生产者-消费者模式同步的原理。例如，一个线程完成初始化后将标志位设为 true（Release），另一个线程反复以 Acquire 读取该标志，当读到 true 时即可安全地读取之前初始化的数据。 另一个重要概念是 释放序列（Release Sequence）。释放序列指的是：“以一次 Release 操作为开头，紧跟其后的、在同一原子变量上的所有同一线程的写操作或读改写操作（RMW），共同形成一个连续序列”。如果某线程在 Acquire 读取时读到了这个序列中的任意一个写，那么该 Acquire 将和序列开头的那次 Release 建立同步关系。简单理解，Release 序列涵盖了 Release 写入线程接下来对同一原子变量的后续修改，以及可能由其他线程执行的 RMW 操作，从而确保 Acquire 端读取到序列中任何结果时，都能看到序列开头 Release 之前的所有内存效果。 底层实现上，编译器和CPU通过内存屏障（Memory Barrier）指令来实现上述内存顺序保证。主要有三类内存屏障： 读屏障（Load Barrier）：确保屏障之前的所有读操作都已完成，并阻止后续读操作提前执行（即后面的读不会跑到屏障之前）。这相当于 Acquire 语义的效果，在很多架构上，Acquire Load 会在汇编层插入读屏障指令或使用带Acquire语义的特殊读指令。 写屏障（Store Barrier）：确保屏障之前的所有写操作都已对内存可见，并阻止后续写操作提前执行（不让后面的写越到屏障之前）。这对应 Release 语义，Release Store 常通过写屏障指令或带Release语义的原子写指令实现。 全屏障（Full Barrier）：同时具有读屏障和写屏障效果，禁止任何读或写的重排序。这通常用于 SeqCst 场景，确保全局一致的内存顺序。在 x86 上 MFENCE 指令就是一个全屏障，而 ARM 上则需要 DMB ISH 等全内存屏障指令来达到 SeqCst 效果。 通过合理选择内存顺序和屏障，程序员可以在性能和内存可见性保证之间取得平衡：在确保数据跨线程可见性和操作顺序正确的前提下，尽量减少不必要的开销。 Rust 并发的基础工具 UnsafeCell：内部可变性的支柱 Rust 的所有权和借用规则在编译期就保证了单线程情况下的数据安全，例如通过不可变引用 T 阻止对数据的修改，通过可变引用 mut T 确保独占访问。然而，在实现并发数据结构时，经常需要突破这一限制：比如在只有不可变引用的情况下也能修改内部数据（典型场景是通过锁对象的不可变引用来获取内部数据的可变引用）。为此，Rust 提供了一个底层机制 UnsafeCell 来支持内部可变性。 UnsafeCell 是 Rust 标准库中的一个关键类型，它包装了一个数据，使得即使只有对该容器的不可变引用，也可以通过 UnsafeCell 提供的方法来修改内部的数据。当然，这种内部修改必须在 unsafe 块中进行，因为它突破了编译器的借用检查。很多线程同步原语（例如 Mutex、RwLock，甚至原子类型如 AtomicBool 本身）内部都使用了 UnsafeCell 来绕过编译期的限制，从而允许在并发场景下安全地修改受保护的数据。 基于 UnsafeCell，Rust 标准库封装了几种提供不同程度内部可变性的类型： Cell：提供最小程度的内部可变性，只能用于复制语义的类型（实现 Copy 的类型）。通过 get 和 set 方法在不违反借用规则的情况下读取或修改内部的值。 RefCell：提供运行时的借用检查，实现更灵活的内部可变性，可以在运行时确保不可变借用和可变借用的不混淆。但 RefCell 不是线程安全的（不实现 Sync），只能用于单线程场景。 Mutex：它利用锁机制保证线程安全的内部可变性。Mutex 本质上也是通过 UnsafeCell 允许内部数据可变访问，但同时提供了线程间互斥保护。相应地，Mutex的使用成本较高，需要在多线程间进行加锁和解锁的开销。 通过 UnsafeCell 提供的内部可变性支撑，Rust 才能编写出安全的并发数据结构。在这些结构中，我们将看到 UnsafeCell 的身影，它保证了在编译器视角下“不可能”的行为（即在不可变引用下修改数据）在运行时被合理地使用。 Send 与 Sync：无数据竞争的基石 Rust 类型系统通过两个重要的标记 trait（Marker Trait）来实现线程安全的静态检查，这两个 trait 就是 Send 和 Sync。它们没有具体的方法，实现这两个 trait 的类型会自动享有编译器的特殊处理，用于标记跨线程的使用安全性： Send 表示一个类型的所有权可以在线程间安全地传递。如果类型 T 实现了 Send，则 T 或 mut T 可以从一个线程转移到另一个线程。例如，大部分基础类型和完全由 Send 组成的复合类型都是 Send。如果某个类型内部包含不安全的共享状态且未保护，那它可能不会实现 Send（编译器会自动推导决定，开发者也可手动禁止）。 Sync 表示一个类型的不可变引用可以在线程间安全共享。也就是说，如果 T 实现了 Sync，则允许多个线程同时拥有对该类型的不可变引用。这要求类型内部的所有可能被并发访问的可变状态都已经被保护（比如用了原子或锁）。大部分基本类型的不可变引用都是 Sync，像 i32 等是可以多线程共享的。如果某类型不是 Sync，则即使它本身是只读的，也无法同时被多个线程引用（例如 RcT 因为不是线程安全的引用计数，因此 RcT 也不是 Sync）。 Send 和 Sync 这两个标记 trait 是 Rust 保证无数据竞争（Data Race Free）的基石。编译器通过这两个trait禁止不安全的跨线程操作：任何在多个线程间共享或转移的值必须是 Send/Sync 的，否则将无法编译。这种机制在编译期就杜绝了绝大部分数据竞争情况，开发者只有在显式使用 unsafe 绕过时才可能违背这个保证。因此，在实现并发原语时，确保正确实现 Send/Sync（或适当地禁止它们）是非常重要的。Rust 标准库多数类型默认实现 Send/Sync（如果其组成部分实现的话），但像 RcT（非线程安全引用计数）或 RefCellT（仅供单线程内部可变性）则没有实现，以防误用。 操作系统并发原语：原子等待与唤醒 高效的阻塞式并发离不开操作系统提供的底层同步原语。目前主流的桌面/服务器操作系统（如 Linux、macOS、Windows）都提供了类似的原子等待/唤醒机制，以构建更高级的锁和并发结构。这些机制允许线程在等待某个条件时挂起睡眠，避免忙轮询浪费CPU，并在条件达成时高效地被唤醒。概括来说，有三个最重要的底层操作： wait( AtomicU32, expected_value )：当指定的原子变量值等于给定期待值时，让当前线程陷入休眠等待；如果原子值不匹配则立即返回。这个操作通常是原子级的检查并挂起，如 Linux 上的 futex 系统调用 (futex_wait) 或 Windows 上的 WaitOnAddress 等，它们允许用户态线程在内核中高效睡眠，等待变量改变。 wake_one( AtomicU32 )：唤醒一个在指定原子变量上等待的线程。对应地，Linux futex 提供 futex_wake 可以唤醒等待在同一个地址上的一个线程；Windows 提供 WakeByAddressSingle 等。 wake_all( AtomicU32 )：唤醒所有在该原子变量上等待的线程。对应 futex 的 wake 可以指定唤醒所有等待的线程，Windows 有 WakeByAddressAll 等。 这些原语本质上将原子变量的变化和线程调度结合起来，实现了类似 “当原子变量处于某值时让线程睡眠，直到变量改变再唤醒” 的机制。这为构建更高级的同步工具奠定了基础：例如 Mutex 在获取不到锁时，可以调用 wait 挂起线程等待锁的原子标志变为可用；当释放锁时，调用 wake_one 唤醒一个等待线程继续争夺锁。相比纯用户态的自旋，这种机制大幅提高了效率，因为等待线程无需占用 CPU。Rust 标准库的实现以及第三方并发库（如 parking_lot）都会利用操作系统提供的此类原语（在 Linux 上通常就是 futex，在其他平台可能用条件变量等模拟）来实现高性能的阻塞同步。 本书的示例实现中多次使用了 atomic-wait crate 来模拟 futex 等行为，以便在稳定版上构建自定义锁。 自旋锁（SpinLock） 自旋锁是一种最简单的锁实现，其特性是当一个线程尝试获取锁而锁已被占用时，该线程不会阻塞睡眠，而是在循环中反复检查锁是否可用（“忙等待”）。自旋锁避免了线程切换的开销，适用于临界区极短、锁持有时间非常小的场景，因为忙等待会浪费CPU时间。如果临界区较长，使用自旋锁会导致CPU空转浪费资源，此时应采用会阻塞线程的锁（如 Mutex）。Rust 标准库并未提供显式的自旋锁类型，但可以使用原子操作很容易地实现一个。书中通过实现一个简化的 SpinLock 展示了原子操作和 RAII 等在并发中的应用： v0：最小化原子标记的实现 – 采用一个原子布尔标志表示锁状态（如 AtomicBool），最初版本只实现了基本的 lock() 和 unlock()。当标志从 false 变为 true 表示获取锁成功。线程获取锁时使用原子交换 (swap) 将标志设为 true，并在获取失败时不断重试（忙等）。该版本功能最小，不负责保护任何数据。 v1：绑定受保护数据 – 将锁和需要保护的数据封装在一起，定义为如 SpinLockT，内部持有一个 UnsafeCellT 来存储数据。这保证了数据只能通过获取锁后才能访问，提升了数据安全性。接口上提供 lock() 返回一个包裹了 mut T 的锁守卫类型，以确保在持有锁期间才能访问内部数据。 v2：引入 RAII 实现自动解锁 – 利用 Rust 的 RAII（Resource Acquisition Is Initialization）惯用法，返回一个锁守卫（如 SpinLockGuard），其 Drop 实现中自动调用解锁操作。这样，当锁守卫离开作用域时（不论是正常离开还是因为 panic 等非常路径），都能确保锁被正确释放，不必依赖用户手动调用 unlock()。这一版本显著提高了易用性和安全性，防止了因忘记解锁导致的死锁。 通过这三个版本的迭代，我们看到了从低级原子操作构建锁的基本过程，以及RAII 模式在确保资源释放中的威力。自旋锁虽然简单，但在Rust并发中并不作为公开接口广泛使用，更多是用于底层实现。例如标准库 Mutex 在短暂自旋优化时内部也会自旋尝试锁，以减少进入内核的机会。一般来说，高层代码应尽量使用更高级的 Mutex 而非自旋锁，除非在非常性能敏感且确定临界区极短的场景下才考虑自旋锁。 一次性通道（One-shot Channel） 一次性通道指的是只发送一次消息并被接收一次的通信通道。它可以理解为只能容纳单个消息的生产者-消费者队列，在某些场景下（例如线程初始化后将结果发送给主线程）非常有用。Rust 标准库提供的 std::sync::mpsc 通道是多次发送的通用通道，而一次性通道可以做特别的优化。书中通过多个版本构建了一个一次性通道，以展示锁与无锁编程、内存管理以及线程同步的技巧： v0：基于互斥锁的简单通道 – 首先实现一个通用版本，内部用 Mutex + Condvar 等构造一个能发送任意条消息的通道，然后限制只发送一次。这是最直接的实现，但完全依赖锁，性能较低。 v1：引入 UnsafeCell 与 AtomicBool 去锁化 – 利用内部可变性和原子操作，实现一个无锁的一次性通道。使用 AtomicBool 标志消息是否已被接收，UnsafeCell 存放实际消息数据，实现 send 时设置数据和标志位，recv 时读取数据。通过原子操作保证发送和接收的同步，不再使用锁，从而减少开销。 v2：使用 MaybeUninitT 代替 OptionT – 优化内存布局。由于一次性通道中的消息在发送前不存在，发送后存在，而 Option 构造会在内存中额外存储一个枚举标志，占用空间且可能导致不必要的初始化检查。改用 MaybeUninit 后，只在内存中保留消息所需空间，避免了初始化与未初始化状态的双重判断，减少内存开销。 v3：增加动态检查以提高安全性 – 在发送和接收操作中增加运行时检查，以确保不会出现重复发送、重复接收等误用情况。例如，如果重复调用 send 就立即 panic，防止破坏内部状态。这些检查提升了组件的健壮性。 v4：实现 Drop Trait 以自动清理 – 为通道的发送端和接收端实现 Drop，在对象被销毁时自动清理未被接收的值或通知另一端。这样可以避免内存泄漏，并在接收端丢弃前通知发送端等，提高使用时的正确性。 v5：拆分 Sender 和 Receiver 类型 – 将通道的发送者和接收者角色分开成不同的类型，确保编译层面对各自操作的权限限制。例如 Sender 只能发送不能接收，Receiver 只能接收不能发送。这一版本还通过类型系统防范了一些误用（如接收端克隆等）。 v6：用生命周期替代 Arc，消除引用计数开销 – 之前版本为了让 Sender 和 Receiver 在不同线程中存在，可能使用了 Arc 来共享通道内部状态。然而在一次性通道中，发送和接收两者的作用域其实可以静态地确定（如在线程函数中先发送后接收）。通过引入 Rust 的生命周期参数，将通道限定在创建它的作用域内传递引用，而非通过 Arc，在编译期保证了内存安全的同时，免去了原子引用计数的运行时开销。 v7：使用线程停放（park/unpark）机制，实现阻塞等待 – 先前版本的接收操作可能需要不停查询标志（自旋等待消息就绪），v7 引入了 Rust 标准库的 park()/unpark() 机制：当接收端发现消息尚未准备好时，调用 thread::park 挂起当前线程；发送端在放入消息后调用接收线程的句柄的 unpark 来唤醒它。这种做法避免了忙等占用 CPU，转而让线程阻塞等待，提升效率。同时通过去掉先前用于轮询的如 is_ready 标志，使误用的可能性进一步降低。 v8：使用 PhantomData 防止跨线程误用 – 最终版本利用 PhantomData 标记保证类型安全。通过在 Sender/Receiver 中加入带有生命周期或非 Send/Sync 标记的 PhantomData，编译器可以禁止用户将 Sender/Receiver 在不安全的方式跨线程使用。例如，可以防止 Sender/Receiver 被不恰当地发送到其他线程导致未定义行为。PhantomData 不占用空间，但可以在类型系统中充当标记，确保一次性通道的用法受到静态约束，彻底封堵错误用法。 经过上述多次迭代，一次性通道从最初的锁实现逐步演进为无锁且高效的实现，并且在类型系统层面保证了安全使用。这个过程展示了 Rust 并发编程从简单正确到性能优化再到类型安全的完整思路。在实际工程中，类似的思想被用于实现各种高性能通道和同步数据结构，如 Rust 标准库和 crossbeam 库中的通道实现等。 Arc 原子引用计数智能指针 Arc（Atomically Reference Counted）是 Rust 标准库提供的多线程引用计数智能指针类型，允许在多个线程间共享所有权。通过原子增减引用计数，Arc 能确保即使多个线程同时持有指针，底层数据也只会在最后一个指针被释放后销毁。书中通过实现一个简化版的 Arc 及其改进版本，解释了 Arc 的工作原理和演进： v0：基础版本，单原子计数的多所有权 – 使用一个原子计数器（如 AtomicUsize）记录引用数，每次克隆 Arc 增加计数，删除时减少计数，减到零时销毁数据。这个版本实现了跨线程的多所有权共享。然而，此时 Arc 依然缺乏两方面能力：内部可变性（无法获得可变引用去修改内部数据），以及防止循环引用（如果两个 Arc 相互引用会导致计数永不为零，内存泄漏）。 v1：引入独占借用 mut ArcT 及内部锁，实现可变访问 – 标准库的 Arc 并不支持在存在多个引用时直接获得内部数据的可变引用，但本书尝试了一个改进思路：当我们持有 Arc 的独占引用 mut ArcT 时（意味着当前线程拥有 Arc 对象的独占管理权，没有其他 Arc 克隆存在），可以安全地修改内部数据。为此，v1 在 Arc 内部增加了一把原子锁或标志，用于在独占修改内部数据时临时阻止其他线程的访问。从而提供一个安全的 get_mut() 方法：当检测到当前只有一个强引用且锁定成功时，返回内部数据的可变引用。这实现了条件下的内部可变性：在不破坏线程安全的前提下，允许独占 Arc 进行内部修改。 v2：加入弱引用（Weak）以解决循环引用问题 – 弱引用是 Arc 非常重要的补充。Weak 指针不计入强引用计数，因此不会阻止数据被回收。v2 引入 WeakT 类型，当需要引用可能形成环的对象时，用 Weak 替代 Arc 中的一部分引用关系。这样即使对象之间形成环，相互的弱引用不会使强计数增加到无法归零，一旦没有强引用，数据可以正确释放。Weak 指针需要通过 upgrade() 尝试提升为 Arc 使用，在数据已被释放时会得到 None，从而避免了悬垂引用。 v3：区分强引用计数和弱引用计数，优化无循环场景性能 – 在 v2 基础上，v3 将引用计数拆分为两个独立的计数：强引用计数和弱引用计数。强计数为 0 时表示数据可销毁，但实际销毁要等弱计数也为 0（表示没有悬挂的 Weak 指针）。通过分离计数，Arc 在没有 Weak 场景下也不需要去增减弱计数，减少了不必要的开销。只有在存在 Weak 的情况下才维护弱计数。这一优化与实际 Rust 标准库 Arc 的实现一致：标准库 Arc 在内部使用两个计数（一个原子usize记录强引用数，一个记录弱引用数），并据此管理内存回收。 Arc 智能指针的实现展示了原子操作在内存管理中的应用。由于使用了原子引用计数，Arc 的克隆、释放可以由多线程安全地执行。此外，引入 Weak 指针解决循环引用、双计数优化性能等设计，都是实际工业级智能指针需要考虑的问题。Rust 标准库的 ArcT 正是采用类似机制，保证了线程安全又兼顾性能。在并发编程实践中，Arc 被广泛用来共享只读数据或通过内部加锁来共享可变数据（比如 ArcMutexT 组合），理解其实现对深入掌握 Rust 的内存管理和线程安全非常有帮助。 互斥锁（Mutex） Mutex（互斥锁）是一种常用的线程同步原语，用于保证同一时刻只有一个线程可以访问某份数据。Rust 提供了 std::sync::MutexT 实现安全且高效的互斥锁，在内部结合了自旋和操作系统锁机制。书中构建了一个简化的 Mutex 实现，通过多个版本逐步逼近实际库中的优化： v1：基本可用的互斥锁 – 首先利用 atomic-wait 等底层原语实现一个基本的 Mutex。内部使用一个原子标志表示锁状态，采用类似自旋锁的方式尝试获取锁，如果失败则调用底层 wait() 将线程阻塞，等待锁可用时被唤醒。同时运用 RAII 思想，实现一个锁守卫，在 Drop 中释放锁。这一版本已经具备 Mutex 的核心功能：能阻塞等待，避免忙等；用 RAII 确保解锁。 v2：避免不必要的系统调用 – 优化 Mutex 在无竞争情况下的性能。设想如果锁空闲，线程应尽量避免调用内核提供的 wait 等系统调用，因为进入内核态有开销。v2 改进在于：获取锁时先用原子操作快速检查和设置锁标志，如果成功则直接进入临界区，不需要任何系统调用；只有当锁已被占用且需要等待时，才调用 wait 进入睡眠。同样，释放锁时如果发现没有任何线程在等待（可通过一个等待计数或标志判断），则不调用 wake_one，避免无谓的系统调用开销。这个版本通过“先检查再睡眠”的策略，提高了低争用情况下的性能。 v3：加入短暂自旋进一步减少系统调用 – 在锁竞争发生时，立即阻塞线程进入内核可能不是最优选择。如果锁很快就会被释放，先忙等一小段时间可避免不必要的上下文切换。v3 引入了自旋等待的优化：当发现锁被占用时，不马上调用 wait 阻塞线程，而是让线程自旋循环尝试获取锁若干次（或等待若干纳秒）。如果在短暂自旋期间锁被释放获取成功，就无需进入内核；只有超过自旋时限仍未成功，才执行阻塞等待。这个优化利用了临界区很短这种常见情况，大幅减少了内核调度开销。在实际实现中，Rust 标准库和 parking_lot 库的 Mutex 都采用了类似的自旋-休眠结合策略，以平衡延迟和吞吐量。 经过这些改进，Mutex 实现已经非常接近真实场景：在无竞争时开销极低（原子操作和用户态逻辑），在有轻微竞争时通过自旋避免陷入内核，在竞争激烈或锁长时间被持有时再进入内核等待，从而综合优化不同场景下的性能。值得一提的是，Rust 标准库的 Mutex 实际上在底层是调用操作系统的原生实现（例如 Windows 临界区，pthread mutex等），而社区提供的 parking_lot crate 则使用了自定义的高性能实现（正是类似书中描述的策略）。理解这些机制有助于选择和使用锁，以及调优并发性能。 条件变量（Condvar） 条件变量是一种配合互斥锁使用的同步原语，用于线程间等待和通知机制。一个典型的 Condvar 场景是：线程 A 获取锁并检查某个条件，如果条件不满足则在条件变量上等待（这会释放锁并挂起线程A）；另一个线程B稍后满足条件后获取同一把锁，改变条件并通知唤醒条件变量，线程A被唤醒后重新获取锁继续执行。Rust 提供了 std::sync::Condvar 来实现这一机制。书中通过两步实现了 Condvar 的核心原理并进行了优化： v1：利用 atomic-wait 实现等待/唤醒 – 条件变量需要将线程挂起和唤醒与某个共享条件相结合。v1 实现中，每个 Condvar 内部维护一个原子计数或标志，当线程调用 wait() 时，先解锁关联的 Mutex，然后调用 atomic_wait 在该原子上休眠等待。另一线程调用 notify_one 或 notify_all 时，对同一个原子值执行修改并调用 wake_one 或 wake_all 来唤醒等待线程。这样利用前述操作系统原语，就实现了条件变量的等待和唤醒机制。需要注意唤醒后线程会重新尝试获取最初的Mutex锁，以恢复对共享状态的保护（这一过程通常由Condvar实现封装好）。 v2：增加等待者计数避免无效唤醒 – 为了优化唤醒通知的性能，v2 在 Condvar 内部引入了一个原子计数 num_waiters，记录当前有多少线程在此 Condvar 上等待。这样，当调用 notify_one/all 时，可以先检查如果没有等待者就直接返回，避免进行系统调用唤醒。同理，在等待时也增加和减少这个计数。这个简单的计数避免了无谓的唤醒操作调用（例如没有线程在等待却调用了唤醒系统调用）。实际中，大部分条件变量实现都会维护类似的状态来提升效率。Rust 标准库的 Condvar 在具体实现上依赖于系统提供的条件变量（如 pthread_cond_t），但原理一致。 Condvar 的实现难点在于要安全地结合互斥锁使用，以及防止经典的“丢失信号”问题（即信号发送与等待错过时机导致永久沉睡）。通过精心设计的顺序（例如在 Mutex 解锁和 wait 挂起之间避免竞争）以及必要的重试循环（被唤醒后通常需要再次检查条件），可以确保 Condvar 的正确性。这部分内容体现了操作系统原语与高级并发抽象之间的结合：Rust 的实现隐藏了许多细节，但理解它有助于我们正确使用 Condvar（例如明白必须在循环中等待条件、防止虚假唤醒等）。 读写锁（RwLock） 读写锁允许多个读者并行地读取数据，但在有写者持有锁时所有读者都被阻止。这样在读多写少的场景下能提高并发性能。Rust 提供了 std::sync::RwLockT 来实现读写锁。与 Mutex 类似，RwLock 在实现上也可以结合自旋和操作系统原语，并需考虑读者与写者的公平性。书中实现的简易 RwLock 经过了三步演进，着重解决写者可能的饥饿问题： v1：核心读写锁语义 – 初始版本使用 atomic-wait 等机制和 RAII，支持多个读者或单个写者的互斥。具体来说，可用一个原子计数来表示锁状态：例如计数为非负时表示当前有该数目的读者持锁，计数为 -1 表示有写者持锁。实现 read_lock 时尝试将计数递增（若当前不是写模式），write_lock 则尝试将计数从0变为 -1 来独占。如果操作失败则调用 wait 挂起等待。当持锁线程释放锁时，更新计数并根据情况调用 wake_one/all 唤醒等待的线程。这一版本实现了基本的 RwLock 功能。 v2：增加独立的写者唤醒计数 – 为了避免写线程在竞争中空转，v2 引入了一个单独的 writer_wake_counter 或标志。因为读锁可能同时唤醒多个等待的读者，而写者通常只需单独唤醒。当一个写线程在等待时，如果持续有读锁进来，它可能长时间得不到机会。通过维护一个专门针对写者的等待计数或标志，释放锁时如果发现有写者在等待，可以更有针对性地唤醒写线程。这避免了写线程无谓地循环等待所有读者离开锁。 v3：使用奇偶数编码巧妙解决写者饥饿 – 最终版本采取了一个巧妙的方案：利用原子计数的最低位或奇偶性来标记写者意图，从而平衡读/写公平性。一种常见做法是，将计数的某一位作为“有写者等待”的标记。当有写线程等待时，设置该标记使新的读锁请求被延迟（即不再允许新的读者获取锁），从而逐步清空现有读者并最终让写者上锁。一旦写者获取锁并释放后，再清除标记放行读者。这种通过奇偶位（或其他位）区分读写状态的编码方式，可以防止写者一直被源源不断的读者饿死，又不会过度牺牲读性能。实际工程中，不少读写锁实现（包括 Rust 标准库和 parking_lot 的 RwLock）都采用类似思路：既避免写者饥饿，又保持尽可能高的读并发。 读写锁相对于互斥锁有更复杂的状态管理，需要处理多读者、单写者之间的切换，以及公平性策略。通过上述改进，我们既实现了基本功能，又确保在竞争激烈时系统不会偏科（比如始终偏向读者或写者）。在 Rust 标准库中，RwLock 使用系统的 pthread_rwlock 或相似机制实现，而更优化的方案如 parking_lot::RwLock 则采用自己的算法，和书中描述的策略不谋而合。对于开发者来说，了解这些实现细节能够帮助理解 RwLock 的特性（例如为什么有时写锁可能拿不到是因为读锁频繁进来等），从而做出更好的并发设计决策。 总结 通过对《Rust Atomics and Locks》全书内容的梳理，我们系统了解了 Rust 并发编程从底层原理到高级抽象的一系列知识点： 原子操作及其内存序保证构成了无锁并发的基础，不同架构对其支持有所区别，理解硬件内存模型有助于写出正确高效的原子操作代码。 内存模型中的 happens-before 等概念和五种内存顺序提供了分析并发行为的工具，Rust 的类型系统和内存屏障一起确保了跨线程操作的可见性和有序性。 UnsafeCell 和 Send/Sync 等机制是 Rust 提供的编译期保障，既允许必要的“不安全”修改又保证线程安全边界分明，使我们能放心地构建并使用并发原语。 操作系统提供的 futex 等原语连接了用户态原子操作和内核调度，Rust 并发库巧妙地加以利用，实现既省CPU又高吞吐的锁和阻塞结构。 通过几个典型并发原语（自旋锁、通道、Arc、Mutex、Condvar、RwLock）的实现迭代，我们看到如何将上述原理应用于实际：从简单正确的初始版本，不断优化以提高性能和安全性，最终达到与工业级实现类似的效果。这些案例也强调了概念之间的联系——如自旋锁和 Mutex 体现了忙等与阻塞两种等待策略，Arc 的内部可变性依赖 UnsafeCell 而其计数安全依赖原子操作，条件变量依赖 Mutex 配合以及操作系统原语等等。 Rust 并发的设计追求“无数据竞争”的同时，通过类型系统和底层优化取得了很好平衡。《Rust Atomics and Locks》深入浅出地展示了这一领域的方方面面。本笔记希望帮助读者快速回顾书中内容，在需要时可将各章节要点作为知识索引参考。相信将来面对具体并发编程挑战时，这些基础理论和实现经验将成为宝贵的指导，帮助我们写出健壮高效安全的 Rust 并发代码。 Happy Coding! Peace~","tags":["读书笔记","rust"],"categories":["读书笔记"]},{"title":"Rust 实战丨手写一个 RwLock","path":"/2025/06/11/rust-action-rwlock/","content":"系列文章： Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序 Rust 原理丨从汇编角度看原子操作 Rust 实战丨手写一个 SpinLock Rust 实战丨手写一个 oneshot channel Rust 实战丨手写一个 Arc Rust 原理丨操作系统并发原语 Rust 实战丨手写一个 Mutex Rust 实战丨手写一个 Condvar Rust 实战丨手写一个 RwLock 👈 本篇 本篇我们继续参考 Rust Atomics and Locks 一书来手写一个读写锁：RwLock。这也是这本书中的最后一个实战案例了，很幸运我们能坚持到现在，为自己鼓掌 👏🏻 ！ 在本章开始之前，我们假设你已经： 熟悉并理解 Rust 的各种原子操作。 阅读过 Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序 和 Rust 原理丨从汇编角度看原子操作，并理解内存顺序和内存屏障的原理和使用方法。 理解 Rust UnsafeCellT 提供的内部可变性允许我们在持有共享引用 的时候可以对数据进行修改。 阅读过 Rust 原理丨操作系统并发原语 并了解 atomic-wait crate 中 wait/wake_one/wake_all 的适用场景和使用方法。 读完本篇你能学到什么 掌握读写锁的核心语义和基本原理 理解读锁可共享、写锁需独占的设计哲学 学会使用原子操作和 Guard 模式实现线程安全 掌握 UnsafeCell 实现内部可变性的技巧 解决写线程无用循环问题 识别并发程序中的性能瓶颈 学会通过独立原子变量避免竞争 理解 atomic-wait 的高效使用方式 巧妙解决写饥饿难题 掌握奇偶数编码表达复杂状态的艺术 理解并发系统中性能与公平性的权衡 学会设计状态机解决复杂同步问题 v1：基础实现 按照惯例，我们先来思考数据结构该如何定义。所谓读写锁，即读锁与读锁之间是可以共存，而读锁与写锁、写锁与写锁之间是互斥的。 我们需要一个变量 state 来表示当前是否有读线程、有多少读线程、是否有写线程，这里可以用 state 的值来表示有多少读线程，当其为最大值时，表示有写线程。因为 atomic-wait 仅支持 AtomicU32，所以这里我们的数据类型也定义为 AtomicU32. 抢到写锁的时候，是可以对数据 value 进行修改的，但因我们只持有 RwLock 共享引用，为了修改数据，我们依旧需要依赖 UnsafeCell 提供内部可变性。 我们需要 2 个守卫类型，分别作为读守卫和写守卫，它们都持有 RwLock 的引用。 读守卫只能获取共享引用，所以我们为其实现 Deref trait。 写守卫可以获取独占引用，所以我们为其实现 Deref 和 DerefMut trait。 另外，因为读锁是共享，写锁是独占，所以我们在为 RwLockT 实现 Sync trait 的时候，要求 T 必须满足 Sync+Send。 综上，我们可以定出以下的数据结构： pub struct RwLockT /// The number of read locks, u32::MAX if write locked. state: AtomicU32, value: UnsafeCellT,pub struct ReadGuarda, T rwlock: a RwLockT,pub struct WriteGuarda, T rwlock: a RwLockT,unsafe implT Sync for RwLockT where T: Sync + Send implT Deref for WriteGuard_, T type Target = T; fn deref(self) - Self::Target unsafe *self.rwlock.value.get() implT DerefMut for WriteGuard_, T fn deref_mut(mut self) - mut Self::Target unsafe mut *self.rwlock.value.get() implT Deref for ReadGuard_, T type Target = T; fn deref(self) - Self::Target unsafe *self.rwlock.value.get() implT RwLockT pub fn new(value: T) - Self Self state: AtomicU32::new(0), value: UnsafeCell::new(value), OK，现在我们需要来实现最重要的功能：上锁和解锁。 我们先来看上读锁和解锁：当 state != u32::MAX 的时候，我们就可以尝试对 state 进行加 1 抢占读锁，在销毁 ReadGuard 的时候，我们只需要对 state 进行减 1，当最后一个读锁释放的时候，还需要唤醒一个潜在的阻塞中的写线程。且这里，加 1 和减 1 需要用一对 Acquire 和 Release 建立 happens-before 关系。 代码如下： implT RwLockT // ... pub fn read(self) - ReadGuard_, T let mut s = self.state.load(Relaxed); loop if s u32::MAX assert!(s != u32::MAX - 1, too many readers); // 尝试上读锁 match self.state.compare_exchange_weak(s, s + 1, Acquire, Relaxed) Ok(_) = return ReadGuard rwlock: self , Err(e) = s = e, if s == u32::MAX // 如果已经上了写锁，就陷入等待 wait(self.state, u32::MAX); s = self.state.load(Relaxed) implT Drop for ReadGuard_, T fn drop(mut self) if self.rwlock.state.fetch_sub(1, Release) == 1 // 最后一个读锁释放 wake_one(self.rwlock.state); // 唤醒一个潜在的写线程 现在来看上写锁和解锁：我们只需要尝试将 state 置为 u32::MAX，如果成功了，则说明上写锁成功，否则需要陷入等待。在解锁的时候，只需要将 state 置为 0，并唤醒所有潜在的阻塞的读线程和写线程。同样，这里也需要一对 Acquire 和 Release 建立 happens-before 关系。 代码如下： implT RwLockT // ... pub fn write(self) - WriteGuard_, T // 尝试置为 u32::MAX while let Err(s) = self.state.compare_exchange(0, u32::MAX, Acquire, Relaxed) wait(self.state, s); // 陷入阻塞 WriteGuard rwlock: self implT Drop for WriteGuard_, T fn drop(mut self) self.rwlock.state.store(0, Release); wake_all(self.rwlock.state); // 唤醒所有阻塞的写线程和读线程 到这里，我们一个基本可用的 RwLock 就实现完毕了，你可以参考下图进行辅助理解： sequenceDiagram participant A as 线程A (读者) participant B as 线程B (读者) participant C as 线程C (写者) participant State as RwLock State participant Wait as Wait/Wake 机制 Note over State: state = 0 (未锁定) A->>State: read() - compare_exchange_weak(0, 1) Note over State: state: 0 → 1 State-->>A: 成功，返回 ReadGuard B->>State: read() - compare_exchange_weak(1, 2) Note over State: state: 1 → 2 (两个读锁) State-->>B: 成功，返回 ReadGuard Note over A, B: 💡 读锁可以并发持有 C->>State: write() - compare_exchange(0, u32::MAX) State-->>C: 失败 (state=2, 不等于0) C->>Wait: wait(&state, 2) Note over C: 🔒 写线程进入等待状态 Note over A: 线程A完成读操作 A->>State: drop(ReadGuard) - fetch_sub(1) Note over State: state: 2 → 1 Note over A: 不是最后一个读锁，不唤醒 Note over B: 线程B完成读操作 B->>State: drop(ReadGuard) - fetch_sub(1) Note over State: state: 1 → 0 Note over B: ✨ 最后一个读锁释放！ B->>Wait: wake_one(&state) Note over C: 🎯 写线程被唤醒 C->>State: write() - compare_exchange(0, u32::MAX) Note over State: state: 0 → u32::MAX State-->>C: 成功，返回 WriteGuard Note over C: 线程C执行写操作... C->>State: drop(WriteGuard) - store(0) Note over State: state: u32::MAX → 0 C->>Wait: wake_all(&state) Note over Wait: 📢 唤醒所有等待的线程 我们来写下单元测试验证功能是否正确： #[cfg(test)]mod tests use std:: thread::self, sleep, time::Duration, ; use super::*; #[test] fn one_thread_should_work() let rwl = RwLock::new(vec![1, 2, 3]); let r1 = rwl.read(); assert_eq!(r1.len(), 3); let r2 = rwl.read(); assert_eq!(r2.len(), 3); drop(r1); drop(r2); let mut w = rwl.write(); w.push(4); drop(w); let r3 = rwl.read(); assert_eq!(r3.len(), 4); #[test] fn cross_thread_should_work() let rwl = RwLock::new(vec![]); thread::scope(|s| s.spawn(|| let mut w = rwl.write(); w.push(1); w.push(2); ); s.spawn(|| sleep(Duration::from_millis(100)); let r1 = rwl.read(); println!(:?, *r1); let r2 = rwl.read(); println!(:?, *r2); ); ) 执行结果是通过的： running 2 teststest rwlock::tests::one_thread_should_work ... oktest rwlock::tests::cross_thread_should_work ... ok v2：避免写线程无用循环 implT RwLockT // ... pub fn write(self) - WriteGuard_, T while let Err(s) = self.state.compare_exchange(0, u32::MAX, Acquire, Relaxed) wait(self.state, s); WriteGuard rwlock: self 观察一下我们上个版本的 write() 实现，这里可能会出现一种情况：当上读锁非常频繁的时候，state 的值是一直在变化的，这个时候，wait(self.state, s) 就有可能因为 state 的值发生了变化，不等于 s 了，就不会陷入等待，而是继续循环尝试将 state 从 0 置为 u32::MAX。这时如果上读锁非常频繁的话，那这里会一直尝试获取，且是无意义的尝试。 sequenceDiagram participant W as 写线程 participant R1 as 读线程1 participant R2 as 读线程2 participant State as RwLock State participant Wait as Wait 机制 Note over State: state = 1 (有一个读锁) W->>State: write() - compare_exchange(0, u32::MAX) State-->>W: 失败，返回 s=1 Note over W: 准备调用 wait(&state, 1) R1->>State: drop(ReadGuard) - fetch_sub(1) Note over State: state: 1 → 0 ⚡️ (在 wait 调用前状态就变了！) W->>Wait: wait(&state, 1) Note over Wait: ❌ 当前 state=0，不等于期望值1，立即返回！ Wait-->>W: 立即返回，没有真正等待 Note over R2: 💨 新读线程抢在 compare_exchange 前获取锁！ R2->>State: read() - compare_exchange_weak(0, 1) Note over State: state: 0 → 1 Note over W: 🔄 进入下一轮循环 W->>State: compare_exchange(0, u32::MAX) State-->>W: 失败，返回 s=1 (锁又被抢了！) Note over W: 又准备调用 wait(&state, 1) R2->>State: drop(ReadGuard) - fetch_sub(1) Note over State: state: 1 → 0 ⚡️ (又在 wait 前变化了！) W->>Wait: wait(&state, 1) Wait-->>W: 又是立即返回！ Note over W: 🔄 无用循环开始... Note over W: ⚠️ 写线程永远无法真正进入等待状态！ Note over W: 💀 CPU 被白白消耗在无效的重试上 Note over State: 🎯 问题核心：wait 调用前 state 总是被读锁改变 Note over State: 💡 解决：独立的 writer_wake_counter 避免这种竞争 这里问题的根源在于 read() 和 write() 监听的都是同一个原子变量 state。 解决这个问题的方案之一，就是可以另外加一个原子变量，专门给 write()，从而与 read() 区分开来，避免互相影响，从而造成 write() 时产生无用循环。 我们在 RwLock 中新增一个属性：writer_wake_counter，用于唤醒抢占写锁的线程。 pub struct RwLockT /// The number of read locks, u32::MAX if write locked. state: AtomicU32, /// Incremented to wake up waiters. writer_wake_counter: AtomicU32, value: UnsafeCellT,implT RwLockT pub fn new(value: T) - Self Self state: AtomicU32::new(0), writer_wake_counter: AtomicU32::new(0), value: UnsafeCell::new(value), // ... 这个时候： 我们在 write() 时没抢到锁，就不去 wait(state) 了，而是 wait(writer_wake_counter)。 最后一个 ReadGuard 在 drop 的时候，我们修改 writer_wake_counter 的值，并唤醒一个潜在的阻塞的写线程。 WriteGuard 的时候，不仅要唤醒所有阻塞的读线程，还要唤醒一个潜在的阻塞的写线程。 implT RwLockT // ... pub fn write(self) - WriteGuard_, T while self .state .compare_exchange(0, u32::MAX, Acquire, Relaxed) // 尝试抢锁 .is_err() // 失败的话取出 writer_wake_counter，供待会 `wait` 使用。 let w = self.writer_wake_counter.load(Acquire); if self.state.load(Relaxed) != 0 // 再次检查 state，如果为 0，就不阻塞了，再次尝试抢锁 wait(self.writer_wake_counter, w); // 否则陷入等待 WriteGuard rwlock: self // 抢锁成功，返回守卫对象 implT Drop for ReadGuard_, T fn drop(mut self) if self.rwlock.state.fetch_sub(1, Release) == 1 self.rwlock.writer_wake_counter.fetch_add(1, Release); wake_one(self.rwlock.writer_wake_counter); implT Drop for WriteGuard_, T fn drop(mut self) self.rwlock.state.store(0, Release); self.rwlock.writer_wake_counter.fetch_sub(1, Release); wake_one(self.rwlock.writer_wake_counter); // 唤醒一个写线程 wake_all(self.rwlock.state);\t// 唤醒所有的读线程 通过这个简单的优化，我们就可以避免写线程因为 state 的频繁变化而产生无用循环了。再次运行之前的测试用例，顺利通过的话就没有问题啦！ v3：避免写饥饿 这个版本我们来做最后一个可尝试优化：避免写饥饿。 在之前的实现中，只要没上锁，或者上的是读锁，那么就可以一直不断地上读锁。而只有当没有任何读线程和写线程存在的时候，才可以上写锁。这就非常容易造成写饥饿了，因为那些后来的读线程依旧可以成功上读锁，而写线程抢到锁的条件太苛刻了。 为了解决这个问题，我们可以修改一下 state 的定义： 当 state 为 0 的时候，说明没上锁。 当 state 为 u32::MAX 的时候，说明上了写锁。 当 state 为非 0 的偶数的时候，说明上了读锁，且没有阻塞中的写锁，这个时候可以继续上读锁。 当 state 为其他奇数的时候，说明这个时候有阻塞中的写锁，后面来的读锁也需要阻塞。 pub struct RwLockT /// 读线程的数量 *2，当有阻塞中的写线程的时候 +1 /// 如果 state=u32::MAX，说上了写锁。 /// /// 这意味着，当 state 是偶数的时候，可以继续上读锁， /// 但是，如果 state 是奇数的话，上读锁会被阻塞。 state: AtomicU32, /// Incremented to wake up waiters. writer_wake_counter: AtomicU32, value: UnsafeCellT, 通过这样的调整，我们就可以相对公平地去避免写饥饿的问题了。为此，我们需要调整读写锁的上锁和解锁的逻辑。 read(): 当 state 为偶数的时候，我们可以尝试对其进行 +2 继续抢占读锁； 当 state 为奇数的时候，我们需要调用 wait 陷入等待。 ReadGuard.drop(): 解锁的时候对 state 进行 -2； 如果之前的值是 3 的话，说明这是最后一个释放的读锁，且存在被阻塞的写线程，这个时候需要调用 wake_one 进行唤醒。 write(): 如果 state 为 0，则说明此时没有上锁：可以尝试将其置为 u32::MAX 抢占锁： 如果成功，直接返回。 如果抢锁失败，则重新读取 state 值进行重新判断。 如果 state 为非 0 偶数，则说明此时已经上了读锁了，我们需要将 state +1 设置为奇数，表示有写线程被阻塞着，阻止后面新来的读线程抢占读锁。 如果 state 为非 0 奇数： 如果 state 等于 1，那说明既没有上写锁，也没有上读锁，但是有一个阻塞中的写线程，那有可能就是当前线程自己了！这个时候，我们依旧是可以尝试将其置为 u32::MAX 抢占锁，所以 state 为 1 的情况，可以跟 state 为 0 的情况进行合并处理。 如果 state 大于 1，则说明已经上了读锁，这个时候，需要陷入等待，等前面的读锁都释放了，才能进入下一轮循环，重新尝试抢占写锁。 WriteGuard.drop(): 将 state 置为 0； 唤醒潜在的所有阻塞的读线程和一个写线程。 我画了个流程图，供你辅助理解： sequenceDiagram participant R1 as 读线程1 (已持有) participant R2 as 读线程2 (已持有) participant W as 写线程 (等待) participant R3 as 读线程3 (新来) participant State as RwLock State participant Wait as Wait/Wake 机制 Note over State: state = 4 (两个读锁，4 = 2×2) Note over R1, R2: 💡 两个读线程正在工作... W->>State: write() - 尝试获取写锁 Note over W: 发现有读锁，无法获取 W->>State: compare_exchange(4, 5) - 设置写等待标志 Note over State: state: 4 → 5 (奇数！有等待的写线程) W->>Wait: wait(&writer_wake_counter, w) Note over W: 🔒 写线程进入等待，但已设置奇数标志 Note over R3: 💨 新的读线程想要获取锁 R3->>State: read() - 检查 state % 2 Note over R3: state=5 是奇数，发现有等待的写线程！ R3->>Wait: wait(&state, 5) Note over R3: 🚫 读线程被阻塞，无法抢占！ Note over R1: 读线程1完成工作 R1->>State: drop(ReadGuard) - fetch_sub(2) Note over State: state: 5 → 3 (仍然是奇数) Note over R1: 不是最后一个读锁，不唤醒 Note over R2: 读线程2完成工作 R2->>State: drop(ReadGuard) - fetch_sub(2) Note over State: state: 3 → 1 Note over R2: ✨ 检测到 state=3，是最后一个读锁！ R2->>Wait: wake_one(&writer_wake_counter) Note over W: 🎯 写线程被唤醒 W->>State: write() - compare_exchange(1, u32::MAX) Note over State: state: 1 → u32::MAX State-->>W: ✅ 成功获取写锁！ Note over W: 写线程执行关键操作... Note over R3: 读线程3仍在等待，公平性得到保证！ W->>State: drop(WriteGuard) - store(0) Note over State: state: u32::MAX → 0 W->>Wait: wake_all(&state) - 唤醒所有等待的读线程 Note over R3: 🎯 读线程3被唤醒，现在可以获取锁了 R3->>State: read() - compare_exchange_weak(0, 2) Note over State: state: 0 → 2 (偶数，正常读锁) 综上分析，我们最新的代码如下： implT RwLockT // ... pub fn read(self) - ReadGuard_, T let mut s = self.state.load(Relaxed); loop // 如果是 0，则说明现在没有上锁， // 如果是偶数，则说明现在上的是读锁，写没有阻塞中的写线程， // 这两种情况，都可以尝试将 state+2 进行抢占读锁。 if s % 2 == 0 assert!(s != u32::MAX - 2, too many readers); match self.state.compare_exchange_weak(s, s + 2, Acquire, Relaxed) Ok(_) = return ReadGuard rwlock: self ,\t// 抢占成功，直接返回。 Err(e) = s = e, // 如果 state 为奇数，说明有阻塞中的写线程。 if s % 2 == 1 // 这时候，后面来的读线程都需要阻塞，等待写锁的获取和释放。 wait(self.state, s); s = self.state.load(Relaxed) pub fn write(self) - WriteGuard_, T let mut s = self.state.load(Relaxed); loop // 0: 没有上锁，可以尝试抢锁。 // 1: 没有上锁，但有阻塞的写线程，可能就是自己，依旧可以尝试抢锁。 if s = 1 // 尝试将 state 置为 u32::MAX，成功则直接返回 match self.state.compare_exchange(s, u32::MAX, Acquire, Relaxed) Ok(_) = return WriteGuard rwlock: self , Err(e) = s = e; continue; // 非 0 的偶数，说明现在上的是读锁，且之前没有阻塞的写线程。 if s % 2 == 0 // 将 state+1 置为奇数，表示已经有阻塞的写线程了，阻止后来的读线程抢占读锁。 match self.state.compare_exchange(s, s + 1, Relaxed, Relaxed) Ok(_) = Err(e) = s = e; continue; let w = self.writer_wake_counter.load(Acquire); s = self.state.load(Relaxed); if s = 2 // 如果已经上了读锁，则陷入等待，等待前面所有读锁的释放。 wait(self.writer_wake_counter, w); s = self.state.load(Relaxed); // 释放读锁implT Drop for ReadGuard_, T fn drop(mut self) // 对 state -=2，如果之前的值是 3，则说明当前是最后一个释放的读锁，且存在阻塞中的写线程。 if self.rwlock.state.fetch_sub(2, Release) == 3 // 唤醒一个阻塞中的写线程。 self.rwlock.writer_wake_counter.fetch_add(1, Release); wake_one(self.rwlock.writer_wake_counter); // 释放写锁implT Drop for WriteGuard_, T fn drop(mut self) // 将 state 重置为 0。 self.rwlock.state.store(0, Release); // 唤醒一个潜在的阻塞的写线程。 self.rwlock.writer_wake_counter.fetch_sub(1, Release); wake_one(self.rwlock.writer_wake_counter); // 唤醒所有潜在的阻塞的读线程。 wake_all(self.rwlock.state); 这个时候，我们重新运行之前的测试用例，可以发现的顺利通过的！不过我们还需要加一个测试用例，来测试写饥饿是否被解决了！ #[test]fn writer_starvation_should_resolved() for _ in 0..100 // 跑 100 次，避免偶然 let rwl = RwLock::new(vec![]); thread::scope(|s| s.spawn(|| // 线程 1 let mut w = rwl.write(); w.push(1); w.push(2); ); s.spawn(|| // 线程 2 sleep(Duration::from_millis(10)); let r1 = rwl.read(); println!(:?, *r1); let r2 = rwl.read(); println!(:?, *r2); sleep(Duration::from_millis(50)); // stay locked to block after writers and readers ); s.spawn(|| // 线程 3 sleep(Duration::from_millis(20)); let mut w2 = rwl.write(); w2.push(3); ); s.spawn(|| // 线程 4 sleep(Duration::from_millis(30)); let r = rwl.read(); assert_eq!(r.len(), 3); // must get lock after w2 ); ) 在上面这个测试用例中，线程 2/3/4 在开始之前分别会睡眠 10/20/30ms，所以抢占锁的时间顺序是 w-r1-r2-w2-r。并且线程 2 在退出之前，还睡眠了 50ms，所以线程 3 和线程 4 在抢占锁的时候，r1/r2 都还没释放。 如果我们成功解决了写饥饿问题的话，那这个时候，线程 3 应该会将 state 置为奇数，防止线程 4 抢占读锁。所以当线程 4 抢到读锁的时候，线程 3 一定已经释放写锁了，即 vec 里面一定有 3 条数据！通过运行 writer_starvation_should_resolved 测试用例，很幸运，我们已经成功解决了写饥饿的问题了！ 完整代码可以参考：conutils/rwlock。 总结 本篇文章通过三个渐进式版本完整展示了如何从零开始手写一个功能完备的读写锁（RwLock）。 我们的实现历程体现了系统编程中常见的优化思路： v1 基础实现：实现了核心的读写锁语义，确保功能正确性 v2 性能优化：通过独立的 writer_wake_counter 避免写线程无用循环，提升了性能 v3 公平性保证：巧妙使用奇偶数编码解决写饥饿问题，在性能与公平性间找到平衡 通过这次实战，我们不仅掌握了 RwLock 的实现细节，更重要的是学会了并发编程的系统性思维方式。这些经验将在后续的系统编程实践中发挥重要作用。 这也是我们学习 Rust Atomics and Locks 这本优秀书籍的收官之战，笔者由衷地佩服 Mara Bos 能在短短 200 多页的篇幅将 Rust 的并发编程阐述得如何透彻、清晰且足够深入细节，笔者在整理本系列笔记的过程中，也真的是获益颇丰，希望也能给你带来一些收获，那我们下篇文章见！ Happy Coding! Peace~","tags":["rust","并发编程"],"categories":["rust","rust 实战"]},{"title":"Rust 实战丨手写一个 Condvar","path":"/2025/06/09/rust-action-condvar/","content":"系列文章： Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序 Rust 原理丨从汇编角度看原子操作 Rust 实战丨手写一个 SpinLock Rust 实战丨手写一个 oneshot channel Rust 实战丨手写一个 Arc Rust 原理丨操作系统并发原语 Rust 实战丨手写一个 Mutex Rust 实战丨手写一个 Condvar 👈 本篇 Rust 实战丨手写一个 RwLock 本篇我们继续参考 Rust Atomics and Locks 一书来手写一个条件变量：Condition Variable，简称 Condvar。 在本章开始之前，我们假设你已经： 熟悉并理解 Rust 的各种原子操作。 阅读过 Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序 和 Rust 原理丨从汇编角度看原子操作，并理解内存顺序和内存屏障的原理和使用方法。 理解 Rust UnsafeCellT 提供的内部可变性允许我们在持有共享引用 的时候可以对数据进行修改。 阅读过 Rust 原理丨操作系统并发原语 并了解 atomic-wait crate 中 wait/wake_one/wake_all 的适用场景和使用方法。 在 Rust 中，Condvar 是一个配合 Mutex 使用的线程同步原语，主要作用是让线程在满足某些“条件”之前主动睡眠（阻塞），待条件达成时再被唤醒。 典型的就是生产者和消费者模式，我们先来看一下标准库的 Condvar 如何使用： #[cfg(test)]mod tests use std:: collections::VecDeque, sync::Condvar, Mutex, thread, time::Duration, ; #[test] fn condvar_usage() let queue = Mutex::new(VecDeque::new()); let not_empty = Condvar::new(); thread::scope(|s| // 消费者 s.spawn(|| loop let mut q = queue.lock().unwrap(); let item = loop if let Some(item) = q.pop_front() // 从队列中获取数据 break item; // 获取到则返回 else q = not_empty.wait(q).unwrap(); // 获取不到数据则阻塞等待 ; drop(q); dbg!(item); ); // 生产者 for i in 0..10 queue.lock().unwrap().push_back(i); // 往队列里面投放数据 not_empty.notify_one(); // 唤醒潜在的阻塞线程 thread::sleep(Duration::from_secs(1)); ); 在上面这个例子中，我们实现了一对生产者和消费者： 消费尝试获取锁，并从队列中获取数据： 如果有，则释放锁并返回。 如果没有，则调用条件变量的 wait 陷入阻塞并释放锁。 生产者尝试获取锁，并往队列中投放数据，并调用条件变量的 notify_one 唤醒潜在的阻塞线程。 flowchart LR A[生产者] -->|添加数据| B[队列] B -->|取数据| C[消费者] A -->|notify_one| C C -->|wait| A 读完本篇你能学到什么 你是否在多线程编程中遇到过这些令人头疼的问题： 🤔 性能浪费问题：消费者线程需要不断轮询检查数据是否就绪，即使没有数据也要持续占用 CPU，这种\"忙等待\"让程序效率低下？ 🤔 复杂的同步逻辑：在生产者-消费者模式中，如何让消费者在没有数据时优雅地进入休眠，而不是无休止地检查？ 🤔 竞态条件的困扰：如何确保在多线程环境下，唤醒操作不会丢失，线程不会因丢失唤醒而永远沉睡？ 🤔 性能优化的疑惑：系统调用开销很大，如何避免在没有等待线程时进行无意义的唤醒操作？ 🤔 内存顺序的选择：在实现同步原语时，到底该用 Acquire、Release 还是 Relaxed？如何分析 happens-before 关系？ 如果这些问题曾经让你困惑，那么本文正是为你准备的。下面我们就正式开始从零开始构建一个条件变量（Condvar），用最直观的方式解答这些并发编程中的经典难题。 v1：基础实现 先来思考一下如何定义 Condvar 这个数据结构，参考标准库，它会有 3 个方法： wait(MutexGuard): 释放 MutexGuard 并陷入等待。 notify_one(): 唤醒一个 wait 的线程。 notify_all(): 唤醒所有 wait 的线程。 看过本系列前面几篇的读者应该可以敏锐觉察到，这里就是对应了 atomic-wait 中的 wait/wake_one/wake_all。 那局势就比较明朗了，我们可以在 condvar.wait(guard) 的时候调用 atomic_wait::wait(atomic) ，然后在 condvar.notify_one() 的时候修改 atomic 然后调用 atomic_wait::wake_one() 唤醒线程，condvar.notify_all() 也同理。 因此 Condvar 需要有一个 AtomicU32 类型的属性，这里我们称为 counter。故 Condvar 结构暂且定义如下： pub struct Condvar counter: AtomicU32,impl Condvar pub fn new() - Self Self counter: AtomicU32::new(0), notify_one 和 notify_all 所上所述，就非常简单了： impl Condvar // ... pub fn notify_one(self) self.counter.fetch_add(1, Relaxed); wake_one(self.counter); pub fn notify_all(self) self.counter.fetch_add(1, Relaxed); wake_all(self.counter); 现在就剩下 wait 了，它的基本原理： 接收一个 MutexGuard； 释放 MutexGuard； 陷入等待，等待唤醒； 被唤醒后，再次抢占锁。 综上，我们可以有以下实现： pub struct MutexGuarda, T pub(crate) mutex: a MutexT, // ---- 需要公开 mutex 字段，这里使用 pub(crate) 限制 crate 外部访问impl Condvar //... pub fn waita, T(self, guard: MutexGuarda, T) - MutexGuarda, T let counter_value = self.counter.load(Relaxed); // Unlock the mutex by dropping the guard, // but remember the mutex so we can lock it again. let mutex = guard.mutex; drop(guard); // Wait, but only if the counter hasnt changed since unlocking. wait(self.counter, counter_value); mutex.lock() 注意，为了访问 guard.mutex，这里我们使用的是之前自己手写的 MutexGuard，并将 mutex 字段的私有程度修改为 pub(crate)。代码比较简单，这里就不赘述了，完整流程可参考下图理解。 我们修改一下测试用例，运行后发现也是可以通过的！ #[cfg(test)]mod tests use std::collections::VecDeque, thread, time::Duration; use crate::condvar::Condvar, mutex::Mutex; #[test] fn condvar_usage() let queue = Mutex::new(VecDeque::new()); let not_empty = Condvar::new(); thread::scope(|s| s.spawn(|| loop let mut q = queue.lock(); let item = loop if let Some(item) = q.pop_front() break item; else q = not_empty.wait(q); ; drop(q); dbg!(item); ); for i in 0..10 queue.lock().push_back(i); not_empty.notify_one(); thread::sleep(Duration::from_secs(1)); ); v2：减少不必要的系统调用 第 1 个版本中，我们在 notify_one 和 notify_all 分别都无条件调用了 wake_one 和 wake_all 尝试唤醒潜在的线程，但是这个时候可能并没有线程被阻塞着，那这个系统调用就白白浪费了。 所以在这个版本中，我们尝试来优化这一点。为此，我们需要记录当前阻塞中的线程的数量，所以需要给 Condvar 加一个属性 num_waiters： pub struct Condvar counter: AtomicU32, num_waiters: AtomicUsize,impl Condvar pub fn new() - Self Self counter: AtomicU32::new(0), num_waiters: AtomicUsize::new(0), 在 notify_one 和 notify_all 的时候，我们仅当 num_waiters0 的时候，才进行系统调用： impl Condvar // ... pub fn notify_one(self) if self.num_waiters.load(Relaxed) 0 self.counter.fetch_add(1, Relaxed); // TODO: memory order wake_one(self.counter); pub fn notify_all(self) if self.num_waiters.load(Relaxed) 0 self.counter.fetch_add(1, Relaxed); // TODO: memory order wake_all(self.counter); 在 wait 的时候，我们先标记自己是等待的，即 num_waiters++，然后在被唤醒后，解除这个标记，即 num_waiters--： impl Condvar // ... pub fn waita, T(self, guard: MutexGuarda, T) - MutexGuarda, T self.num_waiters.fetch_add(1, Relaxed); // TODO: memory order ---- New!!! let counter_value = self.counter.load(Relaxed); // Unlock the mutex by dropping the guard, // but remember the mutex so we can lock it again. let mutex = guard.mutex; drop(guard); // Wait, but only if the counter hasnt changed since unlocking. wait(self.counter, counter_value); self.num_waiters.fetch_sub(1, Relaxed); //TODO: memory order ---- New!!! mutex.lock() OK，这里又到了最关键的问题了：操作 num_waiters 时该用什么内存顺序？ 这个关键的问题的关键是什么呢？是要确定哪些地方需要建立 happens-before 关系！ 很明显，我们这里的关键就是要防止 wake_one 的丢失，即确保如果一个线程即将进入等待状态，那么后续的通知操作能够看到这个等待者的存在。所以这里我们需要 notify_one() 中的 load 和 condvar.wait() 中的 fetch_add 建立 happens-before 关系。至于 fetch_sub 就无所谓了，因为这个时候已经被唤醒了，丢失或者重复唤醒都无所谓了。 不过这里其实可以省掉这对 Release 和 Acquire，直接用 Relaxed！为什么呢？ 在 condvar.wait 的 fetch_add 之前，我们必须先拿到 MutexGuard，即通过 lock() 抢占到锁，lock() 里面是啥操作？是一个 Acquire! implT MutexT // ... pub fn lock(self) - MutexGuardT lock_contended(self.state); // Swap successfully, means locked. MutexGuard mutex: self fn lock_contended(state: AtomicU32) let mut spin_count = 0; while let Err(s) = state.compare_exchange(0, 1, Ordering::Acquire, Ordering::Relaxed) if s == 1 if spin_count 100 spin_count += 1; std::hint::spin_loop(); continue; _ = state.compare_exchange(1, 2, Ordering::Acquire, Ordering::Relaxed); wait(state, 2) 我们在调用 atomic-wait::wait 陷入等待之前，要先 drop(guard)，别忘了，drop(guard) 里面是啥操作？是一个 Release！ implT Drop for MutexGuard_, T fn drop(mut self) // If there are threads waiting for the lock, wait one of them. if self.mutex.state.swap(0, Ordering::Release) == 2 wake_one(self.mutex.state); 所以呀，这里其实天然就已经有一对 Release 和 Acquire 了！happens-before 关系是成立的！所以我们之前的代码就已经满足要求了，再次运行前面的测试用例，依旧是顺利通过的！ 完整代码可参考：conutils/condvar。具体流程你可以参考下图辅助理解。 sequenceDiagram participant Consumer as 消费者线程 participant Producer as 生产者线程 participant Mutex as Mutex状态 participant NumWaiters as num_waiters participant Counter as counter Consumer->>Mutex: lock() [Acquire] Consumer->>Consumer: 检查条件，发现需要等待 Consumer->>NumWaiters: fetch_add(1) [Relaxed] Consumer->>Counter: load() -> counter_value Consumer->>Mutex: drop(guard) [Release] Consumer->>Counter: wait(counter_value) Note over Producer: 生产者在另一个线程 Producer->>Mutex: lock() [Acquire] ✅ 与Consumer的Release同步 Producer->>Producer: 修改共享数据 Producer->>Mutex: drop(guard) [Release] Producer->>NumWaiters: load() > 0? ✅ 看到Consumer的increment Producer->>Counter: fetch_add(1) [Relaxed] Producer->>Counter: wake_one() Consumer->>Consumer: 被唤醒 Consumer->>Mutex: lock() [Acquire] Consumer->>NumWaiters: fetch_sub(1) [Relaxed] 另外，即使 notify_one() 在 wait() 之前调用，atomic_wait::wait() 的语义也能保证正确性。因为 wait(counter, expected_value) 只有在 counter 的值等于 expected_value 时才会阻塞，如果 counter 已经被修改，wait 会立即返回。 总结 通过本文的学习，我们从零开始实现了一个功能完整的条件变量（Condvar），并在这个过程中解决了多个重要问题： 理解条件变量的本质：Condvar 本质上是一个配合 Mutex 使用的线程同步工具，它解决了\"如何让线程在条件不满足时休眠，条件满足时被唤醒\"这一经典并发编程问题。 掌握两种实现策略： v1 基础版本：直接使用 atomic-wait 实现等待与唤醒机制 v2 优化版本：通过 num_waiters 计数器避免不必要的系统调用 深入理解内存顺序：通过分析 happens-before 关系，我们发现可以使用 Relaxed 内存顺序，因为 Mutex 的 Release/Acquire 操作已经提供了必要的同步保障。 掌握了这些知识后，你可以： 在生产者-消费者场景中高效地同步线程 理解标准库 std::sync::Condvar 的实现原理 在设计自己的同步原语时做出正确的内存顺序选择 识别并避免并发编程中的常见陷阱 条件变量虽然概念简单，但其背后涉及的原子操作、内存顺序、操作系统原语等知识却相当深入。通过亲手实现，我们不仅掌握了工具的使用，更重要的是理解了其背后的设计思想，这为我们后续学习更复杂的并发编程技巧打下了坚实基础。 下篇，我们将完成 Rust Atomics and Locks 的最后一个实战案例：手写一个读写锁（RwLock）！ Happy Coding! Peace~","tags":["rust","并发编程"],"categories":["rust","rust 实战"]},{"title":"Rust 实战丨手写一个 Mutex","path":"/2025/06/09/rust-action-mutex/","content":"系列文章： Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序 Rust 原理丨从汇编角度看原子操作 Rust 实战丨手写一个 SpinLock Rust 实战丨手写一个 oneshot channel Rust 实战丨手写一个 Arc Rust 原理丨操作系统并发原语 Rust 实战丨手写一个 Mutex 👈 本篇 Rust 实战丨手写一个 Condvar Rust 实战丨手写一个 RwLock 继上篇 Rust 原理丨操作系统并发原语，我们学习了不同操作系统下的并发原语实现，理解了它们最重要的贡献就是提供了一套 wait/wake_one/wake_all 的机制。本篇，我们将借助 Rust Atomics and Locks 的作者 Mara Bos 封装的 atomic-wait crate，来手写一个自己的 Mutex！ v1：基本实现 首先我们来思考一下如何定义数据结构： 我们需要 1 个原子变量 state 来记录锁的状态（0: unlocked, 1: locked），因为 atomic-wait 只支持 AtomicU32，所以这里我们的类型也定义为 AtomicU32。 另外我们需要一个 value 字段来保存数据，当抢到锁的时候，是可以对 value 进行修改的，但是这个时候只有共享引用，所以我们需要 UnsafeCell 来提供内部可变性。 同时，贯彻 RAII（Resource Acquisition Is Initialization，资源获取即初始化）原则： 我们在 lock(self) 成功时返回一个 MutexGuard，它包含 Mutex。 在 MutexGuard drop 的时候，我们将 state 重置为 0，表示释放锁，并唤醒一个潜在的阻塞线程。 为了让 Mutex 可以在线程之间共享，我们需要为其实现 Sync trait，而又因为 Mutex 实现的是独占访问，上锁成功的线程是拥有 T 的所有权的，即要求 T 可以在线程中转移，即要求 T 需要实现 Send trait。 综上，我们定义的 Mutex 和 MutexGuard 结构如下： pub struct MutexT /// 0: unlocked /// 1: locked state: AtomicU32, value: UnsafeCellT,pub struct MutexGuarda, T mutex: a MutexT,unsafe implT Sync for MutexT where T: Send implT MutexT pub fn new(value: T) - Self Self state: AtomicU32::new(0), value: UnsafeCell::new(value), 为了方面访问内部数据，我们为 MutexGuard 实现 Deref 和 DerefMut 这 2 个 trait: implT Deref for MutexGuard_, T type Target = T; fn deref(self) - Self::Target unsafe *self.mutex.value.get() implT DerefMut for MutexGuard_, T fn deref_mut(mut self) - mut Self::Target unsafe mut *self.mutex.value.get() 当 MutexGuard 离开作用域的时候，即被 drop 的时候，我们需要释放锁，并调用 wake_one 去唤醒一个潜在的阻塞线程： implT Drop for MutexGuard_, T fn drop(mut self) self.mutex.state.store(0, Release); wake_one(self.mutex.state); 这里将 state 设置为 0，使用的内存顺序是 Release，是为了跟 lock 的时候使用 Acquire 建立 happens-before 原则，确保 state 的真实值在各个线程中都是可见的。 在 lock 的时候，我们需要将 state 从 0 替换为 1，如果成功，则说明上锁成功，直接返回 MutexGuard，如果失败，则说明锁已经被抢占了，这个时候我们使用 atomic-wait 的 wait() 陷入休眠，等待 wake_one 信号唤醒，再尝试抢锁。 implT MutexT // ... pub fn lock(self) - MutexGuardT while self.state.swap(1, Acquire) == 1 wait(self.state, 1); MutexGuard mutex: self 至此，我们第一个版本的 Mutex 就完工了！是不是很简单！我们来写 2 个单元测试验证一下基本逻辑是否正确： #[test]fn one_thread_should_work() let l = Mutex::new(vec![]); let mut guard = l.lock(); guard.push(1); drop(guard); let guard = l.lock(); assert_eq!(guard[0], 1);#[test]fn cross_thread_should_work() let l = Mutex::new(vec![]); thread::scope(|s| s.spawn(|| let mut guard = l.lock(); guard.push(1); sleep(Duration::from_millis(100)); // sleep for makeing the second thread to be blcoked. ); sleep(Duration::from_millis(10)); // make sure the first thread get the lock s.spawn(|| let mut guard = l.lock(); guard.push(2); ); ); let guard = l.lock(); assert_eq!(guard.len(), 2); 运行成功： running 2 teststest mutex::tests::one_thread_should_work ... oktest mutex::tests::cross_thread_should_work ... ok v2：减少系统调用 当 MutexGuard 的时候，我们将 state 置为 0，并调用 wake_one 唤醒一个潜在的线程，这个时候如果没有阻塞中的线程的话，那这个系统调用就比较浪费了。 所以在 v2 版本我们尝试来优化这一点。为此，我们需要扩展我们的 state 字段，新增表示是否有阻塞线程的能力。 pub struct MutexT /// 0: unlocked /// 1: locked, but no blocked thread /// 2: locked, but has blocked threads state: AtomicU32, value: UnsafeCellT, 修改了 state 的定义后我们需要修改上锁和解锁的逻辑，在上锁的时候，我们先尝试将 state 从 0 置为 1，如果成功了，说明抢到了锁，否则，我们将 state 置为 2，表示有线程被阻塞了。 这里书中的实现是这样的： implT MutexT // ... pub fn lock(self) - MutexGuardT lock_contended(self.state); MutexGuard mutex: self fn lock_contended(state: AtomicU32) if state.compare_exchange(0, 1, Acquire, Relaxed).is_err() while state.swap(2, Acquire) != 0 wait(state, 2) implT Drop for MutexGuard_, T fn drop(mut self) if self.mutex.state.swap(0, Release) == 2 println!(wake_one); wake_one(self.mutex.state); lock(): 如果成功将 state 从 0 变换为 1，则说明当前线程抢锁成功，直接返回 MutexGuard。 如果失败了，就将 state 置为 2，然后调用 wait 进入休眠。 unlock(): 将 state 置为 0，如果之前是 2 的话，那就说明有线程被阻塞着，这个时候才调用 wake_one 唤醒一个阻塞的线程。 这个地方，笔者觉得有一些问题， state.swap(2, Acquire) 这一行代码会无条件将 state 置为 2，也就是说，当这个线程抢到锁后，它在 unlock() 的时候，无论有没有在阻塞的线程，这个时候 state 都是 2，所以都会调用 wake_one。 sequenceDiagram participant A as 线程A (持有锁) participant B as 线程B (尝试获锁) participant State as Mutex State participant System as 系统调用 Note over State: state = 1 (线程A持有锁) B->>State: compare_exchange(0, 1) State-->>B: 失败 (返回 1) Note over B: 进入 while 循环 B->>State: swap(2) Note over State: state: 1 → 2 State-->>B: 返回 1 (≠ 0) B->>System: wait(&state, 2) Note over B: 线程B进入等待状态 Note over A: 线程A释放锁 A->>State: swap(0) Note over State: state: 2 → 0 State-->>A: 返回 2 A->>System: wake_one() Note over System: 正确的唤醒！线程B确实在等待 Note over B: 线程B被唤醒，继续循环 B->>State: swap(2) Note over State: state: 0 → 2 State-->>B: 返回 0，退出循环 Note over B: 获得锁，但state=2 (问题所在) Note over B: 使用锁... B->>State: unlock() - swap(0) Note over State: state: 2 → 0 State-->>B: 返回 2 B->>System: wake_one() Note over System: 不必要的调用！此时没有等待者 我们可以运行上面的测试用例 cross_thread_should_work，可以看到输出了 2 个 wake_one，但是通过分析，应该只需要调用一次 wake_one 就足够了。 test mutex::tests::cross_thread_should_work ... oksuccesses:---- mutex::tests::cross_thread_should_work stdout ----wake_onewake_one 笔者的实现如下： fn lock_contended(state: AtomicU32) while let Err(s) = state.compare_exchange(0, 1, Acquire, Relaxed) if s == 1 _ = state.compare_exchange(1, 2, Acquire, Relaxed); wait(state, 2) 我们获取 state.compare_exchange(0,1) 的返回值，如果成功，说明抢到锁，直接返回。 如果失败了： 原始值是 1，那我们就尝试将 state 从 1 交换为 2，然后调用 wait 陷入休眠。 原始值是 2，说明已经有别的线程也被阻塞了，这个时候直接调用 wait 陷入休眠。 当被 wake_one 唤醒时，重新执行 state.compare_exchange(0,1) 抢占锁。 这个新的流程中，我们抢到锁的时候，state 会被正确的设置为 1 而不是 2，这个时候，在 drop 的时候就不会有不必要的 wake_one 的调用了。 sequenceDiagram participant A as 线程A (持有锁) participant B as 线程B (尝试获锁) participant State as Mutex State participant System as 系统调用 Note over State: state = 1 (线程A持有锁) B->>State: compare_exchange(0, 1) State-->>B: 失败，返回 s=1 Note over B: s == 1，尝试设置等待者标志 B->>State: compare_exchange(1, 2) Note over State: state: 1 → 2 State-->>B: 成功 B->>System: wait(&state, 2) Note over B: 线程B进入等待状态 Note over A: 线程A完成工作，释放锁 A->>State: unlock() - swap(0) Note over State: state: 2 → 0 State-->>A: 返回 2 A->>System: wake_one() Note over System: 正确唤醒线程B Note over B: 线程B被唤醒，重新尝试获取锁 B->>State: compare_exchange(0, 1) Note over State: state: 0 → 1 (关键！) State-->>B: 成功！退出循环 Note over B: 🎯 获得锁，state=1 (正确状态) Note over B: 使用锁进行工作... B->>State: unlock() - swap(0) Note over State: state: 1 → 0 State-->>B: 返回 1 (不是2！) Note over B: ✅ 返回值是1，不调用wake_one Note over System: 🎯 避免了不必要的系统调用 我们重新运行上面的测试用例 cross_thread_should_work，可以看到只输出了 1 个 wake_one： test mutex::tests::cross_thread_should_work ... oksuccesses:---- mutex::tests::cross_thread_should_work stdout ----wake_one 不过笔者在做 benchmark 后发现书中的实现性能其实更高，在 macbook m2max 机器上，书中的版本要比我的版本快 5~10% 左右，猜测大概率是 swap 的性能要比 compare_exchange 高。 v3：短暂自旋进一步避免系统调用 还有一种潜在的优化是，我们可以在抢锁失败且返回 state 为 1 的时候，进行短暂的自旋，如果实际场景中占用锁的时间非常短，那我们就可以再省略一次 wake 的系统调用了。 不过值得注意的是，这种优化未必是正向的，一方面，如果锁占用时间比较长，那前面的自旋就白白浪费了，另一方面，自旋的次数带来的性能消耗，未必就比系统调用要小（不同的平台表现可能很不一样）。 书中给出的经验值是自选 100 次。 优化后的 lock_contended 如下： fn lock_contended(state: AtomicU32) let mut spin_count = 0; while let Err(s) = state.compare_exchange(0, 1, Acquire, Relaxed) if s == 1 if spin_count 100 spin_count += 1; std::hint::spin_loop(); continue; _ = state.compare_exchange(1, 2, Acquire, Relaxed); wait(state, 2) 感兴趣的读者可以使用 criterion 做一个 benchmark 看看自旋与之前的版本的性能差异有多少。 完整代码可参考：conutils/mutex。 总结 在本篇中，我们从零开始，结合 Rust 原子操作和内存顺序的核心知识，实现一个 Rust 中的 Mutex 锁，逐步揭示 Mutex 背后的等待与唤醒机制，为更好理解标准库中的 Mutex 奠定了良好的基础。下篇，我们将尝试手写一个条件变量 Condition Variable！ Happy Coding! Peace~","tags":["rust","并发编程"],"categories":["rust","rust 实战"]},{"title":"Rust 原理丨操作系统并发原语","path":"/2025/06/08/rust-os-primitives/","content":"系列文章： Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序 Rust 原理丨从汇编角度看原子操作 Rust 实战丨手写一个 SpinLock Rust 实战丨手写一个 oneshot channel Rust 实战丨手写一个 Arc Rust 原理丨操作系统并发原语 👈 本篇 Rust 实战丨手写一个 Mutex Rust 实战丨手写一个 Condvar Rust 实战丨手写一个 RwLock 在本系列的前面所有篇章中，我们对非阻塞类的并发操作进行了详细的阐述和实践（除了 SpinLock，不过自旋锁是通过自旋来实现阻塞作用，本质上线程并没有陷入阻塞等待的状态）。 后面我们将继续参考 Rust Atomics and Locks 书中的后续篇章，继续手写几个阻塞类的并发工具，有 Mutex（互斥锁）、RwLock（读写锁）和 CondVar（条件变量）。它们都有一个共同的特点：线程会陷入阻塞，让出 CPU，在等待某个条件满足要求后，会被唤醒并重新调度执行。这就需要借助内核的能力了，我们需要内核支持： 记住那些陷入阻塞的线程； 在满足条件后，能够唤醒对应的正确的线程。 熟悉操作系统原理的读者应该清楚，我们编写的应用程序，一般是处于用户态，而想要跟内核进行交互，需要陷入内核态，而这种切换，很大程度需要依赖于操作系统提供的系统调用能力，即 syscall。 所以在进入手写 Mutex、RwLock 和 CondVar 篇章之前，我们需要先来学习一下，不同的操作系统，都为我们在并发操作中提供了什么样的能力和限制。 在 Rust Atomics and Locks 第八章（Operating System Primitives）中，作者介绍并比较了各平台提供的操作系统级并发原语，包括 POSIX 的 pthread 系列、Linux 的 futex、macOS 的 os_unfair_lock，以及 Windows 的重量级内核对象、轻量级对象和基于地址的等待机制。 在本篇，笔者将基于自己的理解，尝试对这章进行梳理和总结，以便为后面的手写实践篇章奠定一个良好的理论基础，这里还是建议读者去阅读原文，以便获得更多的细节，加深理解。 POSIX 线程原语 pthread 在 Unix 类操作系统中，比如 Linux，libc 就承担了跟内核进行交互的标准接口。在 libc 的基础之前，诞生了一个标准：Portable Operationg System Interface，即熟知的 POSIX。在 Rust 中，对应了 libc crate。 Windows 系统并不遵循 POSIX 标准，而是一系列的系统库来提供内核交互能力，比如 kernel32.dll。 针对线程操作，POSIX 定义了一系列的数据类型和函数，即所谓的 pthreads。它提供了以下几个比较重要的并发原语，我将其归纳为一个表格，供你参考。 Linux：Futex 用户态等待与唤醒 在 Linux 中，所有 pthread 原语的实现，都是通过 futex 这个系统调用。它是全程是 fast user-space mutex。它的实现核心是：通过操作一个 32 位的原子变量来实现等待和唤醒。等待操作会将一个线程陷入睡眠，而唤醒操作会唤醒那些操作同一个原子变量的睡眠中的线程。 这里我们简单进行一下展开，思考一下这个 futex 这个名字的含义，fast user-space mutex 翻译成中文就是快速用户空间互斥锁。我们知道，系统调用的代价是比较昂贵的，需要频繁地在用户态和内核态之间进行切换，对性能是很不友好的。 在 Linux 系统中，futex 机制并非独立存在，而是与互斥锁、条件变量等同步原语协同工作，形成 “用户态自旋 + 内核态等待” 的分层设计，以兼顾性能与功能。 比如在 Mutex 互斥锁场景下，采用 “两级等待” 策略： 用户态自旋阶段：尝试获取锁时先通过原子操作（如atomic_compare_exchange）自旋尝试，避免内核调用。 内核态等待阶段：若自旋失败，通过 Futex 的 FUTEX_WAIT 陷入内核，将线程挂起，直到其他线程通过 FUTEX_WAKE 唤醒。 这样多数短时间持锁场景可在用户态完成，仅在长时间竞争时陷入内核，相比纯内核互斥锁（如 spinlock）大幅降低系统调用开销。 这里有个很重要的点：判断和陷入等待，是原子的。也就是说，线程 A 在确定陷入等待时，如果关联的原子变量已经发生了变化，这个时候，不会陷入等待，而是会直接返回。这也就避免了唤醒信号的丢失。 这里我整理了 futex 的核心操作，供你参考： macOS：公平的 pthread 与非公平的 os_unfair_lock 在 macOS 上，线程/锁的内核 syscalls（__psynch_* 等）不是公开稳定 ABI，官方要求开发者只通过 LibSystem（libc + libpthread + Objective-C/Swift runtime 等）来访问，它们都完全实现了 pthread。 不过值得注意的是，在 macOS 10.12 版本之前，macOS 的 pthread lock 默认都是公平锁（fair locks），不过在 macOS 10.12 (Sierra, 2016) 起新增了 os_unfair_lock，它是一个不公平、阻塞型、低开销的锁，取代了已弃用的 OSSpinLock。 需要注意，os_unfair_lock 没有提供对应的条件变量或读写锁功能 。也就是说，如果需要使用条件等待或读写锁语义，仍需使用 pthread_cond_t 或 pthread_rwlock_t 等 POSIX 原语，或者使用更高层的 GCD（Grand Central Dispatch）并发模型。Apple 将os_unfair_lock 定位为替代早期的 OSSpinLock 的低级锁，以解决 OSSpinLock 存在的优先级反转问题，同时提供比 pthread_mutex 更快的性能。os_unfair_lock 内部会在必要时让出 CPU 而非自旋等待，从而避免高优先级线程饥饿，但调度上又不像 pthread_mutex 那样严格 FIFO。 Windows Windows 提供了一系列独特的并发原语，可分为重量级内核对象、轻量级对象（如 Critical Section、SRW 锁、Condition Variable 条件变量等）和基于地址的等待机制三大类。它们在 API 设计、用法和实现上各不相同，体现了 Windows 从早期到现代的演进。 重量级内核对象：基于 HANDLE 的 wait 与 notify Windows 的重量级同步原语是由内核完全管理的对象，典型代表包括：Mutex（互斥量）、Event（事件）、Semaphore（信号量）、WaitableTimer（可等待计时器）等 。这些对象通过 Windows API 创建，相当于创建了一个内核对象句柄（HANDLE），类似打开文件会得到文件句柄一样 。每个对象在内核有对应的数据结构，操作系统维护其状态和等待队列。具体可以参考： 重量级内核对象。 我整理了它们的基本使用方式，供你参考： 轻量级对象：CriticalSection、SRWLock 与 ConditionVariable \"轻量级\"同步原语是指不以独立内核对象形式存在、主要在用户态运作、仅在必要时调用内核的机制。 是不是已经开始有点 futex 的感觉了？🤭 CRITICAL_SECTION 它并非通过 Create 函数得到句柄，而是定义为结构体 CRITICAL_SECTION，需调用 InitializeCriticalSection() 初始化，之后直接用地址操作。本质上是一个递归互斥锁，同一个线程可以多次 Enter，内部有一个递归计数，必须对应次数的 Leave 才能完全释放。 Critical Section 在未争用情况下尝试通过用户态 Atomic 操作获取，比如 CAS 交换为当前线程，成功则进入，失败则可能先自旋尝试，依旧失败再进入内核等待。 SRW Locks SRW Locks 不支持递归获取，同一线程如果持有写锁，再请求写锁会死锁。SRW 之所以被称为 \"slim\" 锁，是因为其实现相当高效，无锁时获取和释放都是用户态的 Atomic 操作，发生争用时，内核用一个优化的等待机制管理等待队列。 Condition Variable 是 Vista 时代引入的新原语，它必须搭配 Critical Section 或 SWR Lock 使用。 基于地址的等待机制：WaitOnAddress Windows 在 8 版（2012）引入了全新的底层同步机制，与 Linux futex 非常相似，主要函数有： WaitOnAddress(address, compare_address, _,_): 让当前线程在 address 指向的内存值满足特定条件前进入睡眠，函数会将 address 处提供的值和 compare_address 提高的值逐字节比较，如果全等，则线程睡眠，等待后续唤醒，如果不等，函数立即返回。与 futex_wait 相同，比较与睡眠是一个原子操作：在检查内存值与期望值决定休眠的过程中，若有其他线程改变了 address 或发起唤醒，系统会保证不漏掉信号。 WakeByAddressSingle(address): 唤醒在指定地址上等待的一个线程。 WakeByAddressAll(address): 唤醒在指定地址上等待的所有线程。 在实现上，WaitOnAddress 非常轻量，没有显式的内存对象或句柄。当线程等待时，内核只是将线程放入与那块内存地址相关联的等待队列中，唤醒时根据地址找到等待线程列表进行唤醒。 总结 通过对 3 个不同的操作系统的分析，从大的角度来讲，我们会发现它们的并发原语最重要的就是要利用原子变量，在用户态实现 3 个操作，以减少系统调用的出现，进一步提升性能。这 3 个操作可以归纳为： wait(AtomicU32): 在原子变量等于期望值的时候陷入等待，否则直接返回。 wake_one(AtomicU32): 唤醒某个 wait() 在当前变量的线程。 wake_all(AtomicU32): 唤醒所有 wait() 在当前变量的线程。 所以下一步如果我们想在编程语言的层面上（Rust）实现自己的 Mutex、REMutex 和 CondVar，第一步就是需要针对不同的操作系统实现一套 wait/wake_one/wake_all 以屏蔽不同操作系统的实现差异，幸运的是 Rust Atomics and Locks 的作者 Mara Bos 已经帮我们实现好了：atomic-wait。下篇，我们就利用这个 crate，来一步步手写一个自己的 Mutex！ Happy Coding! Peace~","tags":["rust","并发编程","操作系统"],"categories":["rust","rust 原理"]},{"title":"Rust 原理丨从汇编角度看原子操作","path":"/2025/06/05/rust-atomic-in-processor/","content":"系列文章： Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序 Rust 原理丨从汇编角度看原子操作 👈 本篇 Rust 实战丨手写一个 SpinLock Rust 实战丨手写一个 oneshot channel Rust 实战丨手写一个 Arc Rust 原理丨操作系统并发原语 Rust 实战丨手写一个 Mutex Rust 实战丨手写一个 Condvar Rust 实战丨手写一个 RwLock 继上篇 Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序，我们详细介绍了 Rust 中的原子操作及内存顺序和内存屏障的诸多概念。我们知道，之所以要在硬件层面之上的编程语言中，抽象出这些顶层概念，是为屏蔽底层硬件的差异。那么本篇，我们就尝试从汇编代码和硬件层面来分析在不同的计算机架构下这些概念是如何被实现的，它们之间就有哪些具体的差异。 在展开之前，我们先来复习一下 Rust 中的内存顺序和内存屏障。 Rust 支持五种内存顺序（Ordering），从最松散到最严格依次为： 内存顺序 说明 保证 适用场景 示例 Relaxed 最宽松的内存顺序 - 仅保证操作的原子性- 不提供任何同步保证- 不建立 happens-before 关系 - 简单计数器- 性能要求极高且确定不需要同步- 已通过其他方式确保同步 counter.fetch_add(1, Ordering::Relaxed) Release 用于存储操作 - 之前的内存访问不会被重排到此操作之后- 与 Acquire 配对使用可建立 happens-before 关系 - 生产者-消费者模式- 发布共享数据- 初始化完成标志 data.store(42, Ordering::Release) Acquire 用于加载操作 - 之后的内存访问不会被重排到此操作之前- 与 Release 配对使用可建立 happens-before 关系 - 生产者-消费者模式- 获取共享数据- 检查初始化标志 data.load(Ordering::Acquire) AcqRel 同时包含 Acquire 和 Release 语义 - 结合了 Acquire 和 Release 的所有保证- 用于读改写操作 - 需要双向同步的原子操作- 锁的实现- 复杂的同步原语 value.fetch_add(1, Ordering::AcqRel) SeqCst 最严格的内存顺序 - 包含 AcqRel 的所有保证- 所有线程看到的所有 SeqCst 操作顺序一致- 提供全局的顺序一致性 - 需要严格的全局顺序- 不确定使用哪种顺序时- 对性能要求不高的场景 flag.store(true, Ordering::SeqCst) 内存屏障主要分为以下几种类型： Load Barrier（读屏障） 确保在屏障之前的所有读操作都执行完成 防止后续读操作被重排到屏障之前 对应 Acquire 语义 Store Barrier（写屏障） 确保在屏障之前的所有写操作都执行完成 防止后续写操作被重排到屏障之前 对应 Release 语义 Full Barrier（全屏障） 同时包含读屏障和写屏障的功能 防止任何内存操作的重排序 对应 SeqCst 语义 读完本篇你能学到什么 汇编分析能力：掌握从 Rust 代码到汇编指令的完整分析链路，能够使用 cargo-show-asm 或 Compiler Explorer 等工具深入理解代码的底层实现。 跨平台差异洞察：深刻理解 x86-64（CISC）与 ARM64（RISC）两大主流架构在原子操作实现上的本质差异，为性能优化和平台适配提供理论基础。 内存顺序选择策略：不再需要死记硬背五种内存顺序，而是基于硬件特性和性能考量做出明智选择 —— 知道何时用 Relaxed 追求极致性能，何时必须上 SeqCst 保证正确性。 原子性保证机制：理解为什么同样的汇编代码，普通操作与原子操作在编译器层面有本质区别，以及对齐访问与跨缓存行访问的不同行为。 硬件协议原理：掌握 MESI 缓存一致性协议、x86 的 lock 机制、ARM 的 LL/SC 机制等底层实现原理，能够解释多核环境下的数据同步过程。 性能优化洞察：理解不同架构下内存屏障的开销差异，为高性能并发代码提供优化方向（如 ARM64 上 compare_exchange_weak 的真实优势）。 并发问题调试：当遇到并发 bug 时，能够从汇编层面分析问题根因，判断是内存顺序问题还是原子性问题。 架构适配能力：在跨平台开发中，能够针对不同架构的特性（如 x86-64 的强顺序 vs ARM64 的弱顺序）做出相应的代码调整。 锁与无锁数据结构设计：基于硬件原理设计高效的同步原语，理解何时选择基于 CAS 的无锁算法，何时选择传统锁机制。 在进入汇编代码的世界之前，我们先简单补充 2 个重要概念，分别是指令集和 CPU 缓存一致性协议 MESI。 指令集 两种指令集： CISC（Complex Instruction Set Computing，复杂指令集） RISC（Reduced Instruction Set Computing，精简指令集） 二者对比： 特征 RISC CISC 指令集 精简，指令数目少 复杂，指令数目多 指令复杂性 指令简单，每条指令执行单一功能 指令复杂，可以执行多个功能 寻址方式 简单寻址方式 复杂寻址方式 硬件实现 易于实现 实现复杂 编译器 高效编译器 编译器效率相对较低 运算速度 快速 相对慢 具体可参考：risc vs. cisc。 两种指令集分别对应两种最典型的计算机架构： x86-64：基于 CISC（复杂指令集）的 64 位扩展架构，由 AMD 设计并主导，兼容 x86 32 位生态，通过硬件复杂性换取高性能与广泛兼容性，主导桌面与服务器领域。 arm64：基于 RISC（精简指令集）的 64 位架构，由 ARM 设计，以精简指令、高能效为核心，原生支持低功耗场景，主导移动设备并逐步扩展至服务器与 PC 领域。 在本篇中，我们只涉及 2 个平台： x86_64-unknown-linux-musl（以下简称 x86-64） aarch64-unknown-linux-musl（以下简称 ARM64） 要将 Rust 代码编译为指定平台的可执行文件： 安装对应的目标平台 rustup target add x86_64-unknown-linux-musl # x86-64rustup target add aarch64-unknown-linux-musl # ARM64 编译时使用 --target 标志 cargo build --release --target x86_64-unknown-linux-muslcargo build --release --target aarch64-unknown-linux-musl 缓存一致性协议 MESI 在多核系统中，每个核心都有自己的缓存（L1/L2 Cache），而内存中的数据可能被多个核心同时读取或修改。如果不加控制，会导致以下问题： 缓存不一致（Cache Coherence Problem）：不同核心的缓存可能持有同一内存地址的不同副本。 脏数据（Dirty Data）：某个核心修改了数据，但其他核心仍使用旧值。 MESI（Modified, Exclusive, Shared, Invalid）是一种广泛使用的 缓存一致性协议（Cache Coherence Protocol），用于确保多核处理器系统中各个核心的缓存数据保持一致。它定义了缓存行的 4 种状态，并通过状态转换和消息传递机制来协调多核间的数据访问。 状态 含义 特点 M (Modified) 当前核心独占此数据，且已修改（与内存不一致） 只有本核心有最新数据，必须写回内存后才能被其他核心读取。 E (Exclusive) 当前核心独占此数据，但未修改（与内存一致） 可以安全读取或修改，无需通知其他核心。 S (Shared) 多个核心共享此数据（与内存一致） 所有核心只能读取，不能直接修改（需先升级为 M 或 E）。 I (Invalid) 缓存行无效（数据已过期或未加载） 必须从内存或其他核心重新加载最新数据。 更多细节可参考：维基百科 MESI。 查看 Rust 汇编代码 查看 Rust 汇编代码的常用方式有以下几种： cargo rustc --lib --release --target x86_64-unknown-linux-musl -- --emit asm cargo-show-asm（推荐 ✅） cargo asm --release --target=x86_64-unknown-linux-musl --lib module::func_name Compiler Explorer （推荐 ✅） 接下来我们来看下各种 Atomic 操作的汇编代码是什么样的。 Store x86-64： 普通类型的赋值操作跟原子操作在 Relaxed 顺序下生成的汇编的一模一样的！ 在强顺序一致性要求的 SeqCst 下，使用了带有 Lock 语义的 xchg 指令保证内存顺序。 ARM64： 普通类型的赋值操作跟原子操作在 Relaxed 顺序下生成的汇编的也是一模一样的！ 在强顺序一致性要求的 SeqCst 下，使用了原子存储指令 stlr 保证内存顺序。 那么问题就来了：普通类型的赋值操作与 Relaxed 的原子操作生成的汇编一样，那凭什么后者就有原子性的保证呢？ 在上述 2 个架构中，这仅能说明 mov 和 str 在（当前选择的）硬件层面是原子的，无论是否使用 Atomic 类型。这因为 CPU 的缓存一致性协议（MESI）和总线锁定机制确保对齐操作不会撕裂（tearing）。 但是对于未对齐或跨缓存行访问，普通操作不保证原子性，可能被拆分为多次访问（如未对齐的 i64 可能拆为 2 个 32 位写入）。 所以在 Rust 编译器上： 普通操作（*x=0）：Rust 不将其视为原子操作，即使生成的汇编与 Relaxed 原子操作相同。编译器可能优化或重排普通操作，破坏原子性假设。 如：循环中的多次普通写入可能被合并为一次（优化后仅保留最后一次写入）。 原子操作（x.store(0, Relaxed)）：Rust 强制保证原子性，无论硬件是否隐式支持： 对齐访问：直接生成 mov（利用硬件原子性）。 未对齐访问：插入额外指令（如 lock cmpxchg）确保原子性。 禁止编译器优化重排或消除操作。 Load x86-64： 三段代码生成的汇编代码一模一样！这是因为 x86-64 的强顺序策略默认保证 mov 具有顺序一致性（类似 SeqCst），因此无需显示内存屏障。 ARM64： 对于普通类型的加载操作和 load Relaxed 生成的汇编代码是一样的。 对于 load SeqCst，使用了专门的原子加载指令 ldar，它会隐式插入内存屏障，保证该操作之前的所有内存访问对其他线程可见。 虽然 x86-64 对于上面的 3 段代码生成的汇编是一样的，但这只是 x86-64 硬件层面上的保证，且跟之前一样，仅在对齐时是原子的，如果未对齐或跨缓存行访问，是可能被撕裂成 2 个操作的。 在 ARM64 中，不依靠硬件层面的复杂性，而通过 ldar 原子加载指令来保证原子性。 Read-Modify-Write x86-64: 使用 lock 指令来锁定总线或缓存行，从而实现原子性。 ARM64: 使用 LL/SC 机制来实现原子操作（有点类似与乐观锁的味道）。 Compare-and-Exchange x86-64: 二者没有任何区别，或者可以理解为，x86-64 就没有专门实现 compare_exchange_weak。 ARM64: 二者实现是不同的，在 ARM64 上，compare_exchange_weak 是真的具备 weak 的特性。所以如果在特定场景下想用 compare_exchange_weak 来进一步提升性能，在上层也一定要用循环来主动重试，避免虚假失败。 Fence x86-64: Release 和 Acquire 并没有额外使用的指令。只有使用 SeqCst 内存屏障的时候，会插入一条 mfence (memory fence) 指令，这条指令会保证在越过它之前，前面所有的内存操作都已经完成。 ARM64: Release、AcqRel 和 SeqCst 都插入了一条 dmb ish(data memory barrier, inner shared domain)。而 Acquire 则插入了一条 dmb ishld，它只会等待 load 操作的完成，但是允许 store 操作重排序到它后面。 总结对比 到这里我们可以得到以下结论： x86-64 保证原子性的关键是 lock 机制，ARM64 保证原子性的关键是 LL/SC 机制。 x86-64 保证内存顺序的关键是 mfence 指令，ARM64 保证内存顺序的关键是 dmb ish 和 dmb ishld 指令。 x86-64 没有实现真实的 compare_exchange_weak，ARM64 实现了 compare_exchange_weak。 x86-64 使用的是强顺序策略，具体来说： Load→ 后续操作：禁止重排序（如 Load A → Store B 必须保持顺序）。 Store→ 前序操作：禁止重排序（如 Load A → Store B 中 Store B 不能提前到 Load A 前）。 Store→ 后续 Load：允许重排序（如 Store A → Load B 可能实际执行为 Load B → Store A ARM 使用的是弱顺序策略，即所有的原子操作都可能被重排序。 x86-64 中，Relaxed、Acquire、Release 和 AcqRel 的内存顺序效果是一致的。ARM64 中，Relaxed 没有任何内存顺序的保证，而 Release、AcqRel 和 SeqCst 是一样昂贵的，Acquire 稍微轻量一点，只保证了前面的 load 不会重排到后面。 Rust Atomics and Locks 书中给出了一张更细节的图，感兴趣的读者可以研究一下。 Rust Atomics and Locks: An overview of the instructions that the various atomic operations compile down to on ARM64 and x86-64 for each memory ordering 硬件原理 最后我们尝试从硬件层面来进一步理解原子操作的底层实现。这块笔者并不专业，更多的是尝试通过 ChatGPT 等 LLM 查阅资料，进行梳理总结。 原子操作的底层实现（如 x86 的 lock 前缀或 ARM 的 LL/SC）依赖于硬件级别的协同机制，其核心是通过 缓存一致性协议、总线仲裁 和 指令集层面的特殊支持 来保证多核环境下的原子性和内存顺序。 x86 的 lock 前缀：总线锁定与缓存一致性 总线锁定（Bus Locking） 当 CPU 执行 lock cmpxchg 时，lock 前缀会向总线（或缓存一致性协议）发送信号，临时独占内存地址的访问权，阻止其他核心的干扰。 锁定范围：现代 CPU 通常锁定缓存行（通常 64 字节），而非整个总线。 硬件支持：通过处理器的 原子操作单元 和 缓存控制器 协同实现。 MESI 缓存一致性协议 缓存一致性协议（如 MESI）会在硬件层面上确保所有核心对内存修改的观察一致：任何核心的修改会立即（或按协议约定）传播到其他核心的缓存。 lock 操作会强制目标缓存行进入 Modified（独占修改） 状态，并通知其他核心的缓存行失效（Invalid）。 如： 核心 A 执行 lock inc [x]，缓存行 x 变为 Modified。 核心 B 尝试读取 x，触发缓存一致性协议： 核心 A 将修改后的值写回主存或核心 B 的缓存（取决于协议变种如 MESIF/MOESI）。 核心 B 的缓存行 x 变为 Shared 或 Exclusive。 内存屏障的隐含保证 即使代码使用 Relaxed 内存序，lock 会隐式插入 StoreLoad 屏障，确保： 该指令前的所有写操作对其他核心可见。 该指令后的读操作不会重排到指令前。 现代优化：缓存锁定（Cache Locking） 新式 CPU（如 Intel Skylake+）优先在缓存层面实现原子性，仅当跨缓存行或未对齐时才降级为总线锁定，减少性能损耗。 ARM 的 LL/SC（Load-Linked/Store-Conditional）：轻量级独占标记 独占访问标记（Exclusive Monitor） 硬件状态机：每个 CPU 核心维护一个 独占访问标记，记录最近通过 ldxr 加载的内存地址。 标记触发：ldxr [x] 会标记地址 x 为当前核心的独占访问区域。 标记清除条件： 其他核心修改了 x 的缓存行（通过缓存一致性协议）。 当前核心执行 clrex 或上下文切换。 条件存储（**stxr**）的原子性校验 校验独占标记：stxr 执行时，硬件会检查目标地址的独占标记是否仍属于当前核心： 若标记有效：存储成功，返回 0。 若标记失效：存储失败，返回 1（需重试）。 与缓存一致性协议的交互 ARM 的 ACE 协议：LL/SC 依赖缓存一致性协议（如 CHI 或 ACE）监听其他核心的修改： 核心 A 执行 ldxr [x]，缓存行 x 进入 Exclusive 状态。 若核心 B 写入 x，缓存行在核心 A 中变为 Invalid，独占标记被清除。 核心 A 的后续 stxr 会因标记失效而失败。 内存顺序的灵活控制 ARM 的内存序（如 Relaxed/SeqCst）通过显式屏障指令实现： ldapr（Load-Acquire）：确保后续操作不重排到加载前。 stlr（Store-Release）：确保前序操作不重排到存储后。 总结 本篇文章通过查看 x86_64-unknown-linux-musl 和 aarch64-unknown-linux-musl 两大平台下的汇编代码 ，深入剖析了 Rust 原子操作的底层实现机制，揭示了同一行 Rust 代码在不同平台上截然不同的机器级行为。 到目前为止，我们学习的都是无锁（non-blocking）操作，下篇，我们将继续学习 Rust Atomics and Locks 中的第八章《Operating System Primitives》，为手写阻塞类组件（Mutex、RwLock、CondVar）做理论准备，咱们下篇见！ Happy Coding! Peace~","tags":["rust","rust 原理","并发原理","原子操作","汇编"],"categories":["rust","rust 底层原理"]},{"title":"Rust 实战丨手写一个 Arc","path":"/2025/06/03/rust-action-arc/","content":"系列文章： Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序 Rust 原理丨从汇编角度看原子操作 Rust 实战丨手写一个 SpinLock Rust 实战丨手写一个 oneshot channel Rust 实战丨手写一个 Arc 👈 本篇 Rust 原理丨操作系统并发原语 Rust 实战丨手写一个 Mutex Rust 实战丨手写一个 Condvar Rust 实战丨手写一个 RwLock 继上篇 Rust 实战丨手写一个 oneshot channel，本篇我们继续参考 Rust Atomics and Locks 一书，来实现一个 Arc。 在本章开始之前，我们假设你已经： 熟悉并理解 Rust 的各种原子操作。 阅读过 Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序，并理解内存顺序和内存屏障的原理和使用方法。 理解 Rust UnsafeCellT 提供的内部可变性允许我们在持有共享引用 的时候可以对数据进行修改。 Arc 简介 Arc（Atomic Reference Counted）是 Rust 标准库里位于 std::sync 模块中的智能指针，用于 在多个线程之间安全地共享只读数据。和只适用于单线程场景的 RcT 不同，ArcT 的引用计数增减操作使用原子指令，从而保证跨线程的内存安全。 为什么需要 Arc？ let p = Person age: 18, name: hedon.to_string(), address: China.to_string() ;thread::scope(|s| s.spawn(|| println!(:?, p)); // ✅ scope 内的线程可以借用);thread::spawn(move || println!(:?, p)); // ❌ 需要 static 生命周期 问题：thread::spawn 要求 'static 生命周期，但 p 只是栈上变量的借用。 解决：使用 Arc::new(p) 把数据移到堆上，通过原子引用计数实现多所有权： let p = Arc::new(Person /* ... */ );let p_clone = p.clone(); // 原子计数 +1thread::spawn(move || println!(:?, p_clone)); // ✅ 编译通过 读完本篇你能学到什么 原子操作的内存顺序选择：通过引用计数这个具体例子，理解 Relaxed / Acquire / Release / fence 的使用场景。 弱引用解决循环引用：WeakT 的设计原理和在图结构中的应用。 Arc::get_mut 的安全性保证：理解「非原子两步校验」的巧妙设计。 零成本抽象的实现细节：UnsafeCell + ManuallyDrop 相比 OptionT 的优势。 v0: 基础引用计数 数据结构 我们先来分析一下基本的数据结构该如何定义，在之前的 Rust 实战丨手写一个 SpinLock，我们讲到了 C++/Rust 常用的并发编程方式 RAII（Resource Acquisition Is Initialization，资源获取即初始化），其核心思想是：在对象构造函数中获取资源，在析构函数中释放资源。上面的案例中，我们在初始化 Person 的时候其实也是应用了这种思路。 let p = Arc::new(Person age: 18, name: hedon.to_string(), address: China.to_string(),); 所以我们的自定义 Arc 需要泛型 T，使其可以承载不同的数据类型的数据 data，同时为了做引用计数，它需要一个数值类型 ref_count 来计数，为了并发安全，我们可以选择原子类型 AtomicUsize。 在 Arc 中，我们需要自己管理 data 的生命周期，除了使用裸指针 *mut T 或 *const T 之外，我们可以使用 std::ptr::NonNullT ，它是一个零成本、保证非空、支持协变（可安全向子类型转换）的裸指针包装器，具体可参考附录 1. NonNullT。 综上，我们可以定义以下数据结构： struct ArcDataT ref_count: AtomicUsize, data: T,pub struct ArcT ptr: NonNullArcDataT,implT ArcT pub fn new(data: T) - Self Arc ptr: NonNull::from(Box::leak(Box::new(ArcData ref_count: AtomicUsize::new(1), data, ))), 我们定义了 ArcData 和 Arc 两个结构，其中 ArcData 包含引用计数 ref_count 和实际数据 data。 在 Arc 中，我们使用 NonNull 来管理 ArcData 的生命周期。初始化时，先用 Box::new() 在堆上分配内存，再通过 Box::leak() 放弃 BoxT 的所有权，交由 Arc 自行管理。 graph TB subgraph Stack [\"栈内存 Stack\"] ArcStruct[\"ArcTptr: NonNullArcDataT\"] end subgraph Heap [\"堆内存 Heap\"] ArcDataStruct[\"ArcDataTref_count: AtomicUsize(1)data: T\"] end ArcStruct -->|指向| ArcDataStruct subgraph Process [\"内存分配过程\"] Step1[\"Box::new(ArcData)\"] Step2[\"Box::leak()\"] Step3[\"NonNull::from()\"] Step1 --> Step2 Step2 --> Step3 end classDef stackStyle fill:#e1f5fe,stroke:#01579b,stroke-width:2px classDef heapStyle fill:#f3e5f5,stroke:#4a148c,stroke-width:2px classDef processStyle fill:#fff3e0,stroke:#e65100,stroke-width:2px class ArcStruct stackStyle class ArcDataStruct heapStyle class Step1,Step2,Step3 processStyle 另外，跨线程发送 ArcT 会导致 T 对象被共享，即 T 需要满足 Sync trait，而跨线程发送 ArcT 也会导致需要由另外一个线程来释放 T，所以需要 T 满足 Send trait。所以只有当 T 满足 Send+Sync 的时候，ArcT 才是 Send 的，对于 Sync 也是同理，因为我们可以为 ArcT 分别实现 Sync 和 Send trait： unsafe implT: Send + Sync Send for ArcT unsafe implT: Send + Sync Sync for ArcT 为了能够便捷的获取 data，我们先为 ArcT 实现一个 data() 用于获取 ArcDataT，同时为其实现 Deref trait，用于像指针一样无感操作 data: T： implT ArcT fn data(self) - ArcDataT unsafe self.ptr.as_ref() implT Deref for ArcT type Target = T; fn deref(self) - Self::Target self.data().data 维护引用计数 基础部分我们已经铺垫完了，现在我们需要来实现 2 个最重要的 trait 了： Clone: Arc 引用计数的关键，在每次 clone() 的时候，我们不拷贝 data，而是让 ref_count 自增，进行引用计数。 Drop: 在 ArcT 实例离开作用域的时候，我们需要让 ref_count 自减，同时在最后一个 ArcT 被销毁时，我们需要主动释放 ArcDataT 的内存资源。 我们先来实现 Clone： implT Clone for ArcT fn clone(self) - Self // TODO: 处理整型溢出的情况 self.data().ref_count.fetch_add(1, Ordering::Relaxed) Arc ptr: self.ptr 因为这里没有其他原子操作需要跟当前操作建立严格的 happens-before 关系，所以这里我们可以使用最松的 Relaxed 内存顺序。 接下来看下 Drop： impl T Drop for ArcT fn drop(mut self) if self.data().ref_count.fetch_sub(1, ???) == 1 // ----- 这里需要什么使用内存顺序约束？ unsafe drop(Box::from_raw(self.ptr.as_ptr())); 对于 Drop，我们就需要好好想一想需要什么内存顺序约束了。在最后一个 drop 的时候，我们有 2 个目标： 不能过早释放：确保在引用计数减到 0 并销毁对象之前，没有别的线程仍在使用这份数据； 要看得见别人写的东西：如果别的线程在它们各自的 drop 里面对共享对象做了写入，最后一个线程做析构时必须\"看到\"这些写入，否则就可能出现数据竞争或次序错误。 换言之，我们需要最后一个 fetch_sub 跟前面其他每一个 fetch_sub 都建立起 happens before 关系，也即我们需要一对 Release 和 Acquire 来保证 happens-before。 这里简单回顾一下 Release 和 Acquire，不熟悉的读者可以参阅：Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序。 Release: 作用于写操作（store），确保该操作之前的所有内存访问不会被重排到这个 Release 操作之后。 Acquire: 作用于读操作（load），确保该操作之后的所有内存访问不会被重排到这个 Acquire 操作之前。 当一个线程通过 Acquire 读取到另一个线程通过 Release 写入的值时，会建立一个 happens-before 关系：线程 A 中 Release 写入之前的所有内存写操作，对于线程 B 中 Acquire 读取之后的所有内存读操作都是可见的。 Release-Sequence 概念补充：当多个线程对同一个原子变量执行 Release 操作时，这些操作会形成一个 \"release-sequence\"。后续任何一个 Acquire 操作读取到这个序列中的任意值，都能与整个序列建立 happens-before 关系。这正是为什么我们的 fence(Acquire) 能够与之前所有线程的 fetch_sub(Release) 形成同步的关键。 我们当然可以使用 Release 和 Acquire 的结合体 AcqRel 来一步到位解决这个问题。不过考虑到只有最后一个 drop 需要满足这个关系，我们可以尝试做得更优雅一些。 在这种仅需在临界值保证 happens-before 的场景下，我们都可以单独在临界情况下使用一个 fence 来建立起 happens-before。 具体来说： 对于非最终 drop：我们只需要使用 Release，即 fetch_sub(1, Ordering::Release)，它保证了别的线程如果最终做\"最后一次 drop\"，只要它对相同原子执行一条 Acquire 操作，它就能同步到前面所有线程对数据做过的改动。 对于最终 drop：在调用 fetch_sub 的时候我们仍需要 Release 语义，但是调用时，我们并不知道自己是不是\"最后那个线程\"。当 fetch_sub 返回 1 的时候，说明我们是\"最后那个线程\"。这个时候，我们需要建立一个 fetch(Acquire)，与之前的所有 Release 形成配对，确保看到之前所有的历史写入，这个时候我们才能确定已经没有别的线程在使用数据了，我们才可以安全地销毁对象。 如下图所示； sequenceDiagram participant A as Thread A participant B as Thread B participant C as Thread C(最后 drop) %% 普通写入 A->>A: write shared data … B->>B: write shared data … %% 非最终 drop：Release 写 A->>A: fetch_sub (Release) ⬅ 计数 n→n-1 B->>B: fetch_sub (Release) ⬅ 计数 n-1→n-2 %% 形成 release-sequence Note over A,B: 这两次 Release 写组成同一条 release-sequence %% 最终 drop：Release 写 & 返回 1 C->>C: fetch_sub (Release) ⬅ 返回 1 → 计数 0 %% Acquire fence 同步 C-->>C: fence (Acquire)【接收 release-sequence】 %% 安全析构 C->>C: drop(Box::from_raw) %% 结果说明 Note over C: fence (Acquire) 使 A/B对共享数据的写必定在析构前可见 经过这么一顿分析后，我们最终的 Drop 实现如下： implT Drop for ArcT fn drop(mut self) // ❶ 所有 `drop` 都执行：Release if self.data().ref_count.fetch_sub(1, Ordering::Release) == 1 // ❷ 只有最后一个引用才会进来 std::sync::atomic::fence(Ordering::Acquire); // ❸ 补上 Acquire unsafe // ❹ 现在可以安全地回收并析构 drop(Box::from_raw(self.ptr.as_ptr())); 实现逻辑： 所有 drop 都用 Release 语义减引用计数，为可能的\"最后一次 drop\"做准备 只有最后一次 drop（返回值为 1）才需要额外的 fence(Acquire) 与之前所有的 Release 建立 happens-before 安全析构：现在可以确保看到所有历史写入，没人再持有引用 完整代码 自此，我们第一个版本的 Arc 就实现完毕了，我们来看一下最终完成的代码： use std::sync::atomic::AtomicUsize, Ordering, fence;use std::ptr::NonNull;use std::ops::Deref;struct ArcDataT ref_count: AtomicUsize, data: T,pub struct ArcT ptr: NonNullArcDataT,implT ArcT pub fn new(data: T) - Self Arc ptr: NonNull::from(Box::leak(Box::new(ArcData ref_count: AtomicUsize::new(1), data, ))), fn data(self) - ArcDataT unsafe self.ptr.as_ref() unsafe implT: Send + Sync Send for ArcT unsafe implT: Send + Sync Sync for ArcT implT Deref for ArcT type Target = T; fn deref(self) - Self::Target self.data().data implT Clone for ArcT fn clone(self) - Self self.data().ref_count.fetch_add(1, Ordering::Relaxed); Arc ptr: self.ptr implT Drop for ArcT fn drop(mut self) if self.data().ref_count.fetch_sub(1, Ordering::Release) == 1 fence(Ordering::Acquire); unsafe drop(Box::from_raw(self.ptr.as_ptr())); 单元测试 写个单元测试验证一下： #[test]fn arc_should_work() static NUM_DROPS: AtomicUsize = AtomicUsize::new(0); struct DetectDrop; impl Drop for DetectDrop fn drop(mut self) NUM_DROPS.fetch_add(1, Ordering::Relaxed); // 创建 2 个 Arc 共享一个元组，包含一个字符串和 DetectDrop 对象 let x = Arc::new((hedon, DetectDrop)); let y = x.clone(); // 将 x 转移到另外一个线程并使用它 let t = thread::spawn(move || assert_eq!(x.0, hedon); // 可以正常使用 ); // 这里，y 应该也是可以正常使用的 assert_eq!(y.0, hedon); // 等待 t 线程执行完毕 t.join().unwrap(); // 这个时候 `x` 已经被释放了，但是`y` 还没有被释放， // 所以 `DetectDrop` 应该还没被释放。 assert_eq!(NUM_DROPS.load(Ordering::Relaxed), 0); // 手动释放掉 `y`，这是最后一个 `Arc`，所以 `DetectDrop` 应该会被释放 drop(y); assert_eq!(NUM_DROPS.load(Ordering::Relaxed), 1); 执行结果是 ok 的： running 1 testtest arc::tests::arc_should_work ... oksuccesses:successes: arc::tests::arc_should_worktest result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s v1: 实现 get_mut 获取可变引用 按照前面版本的实现，我们是不能为 ArcT 实现 DerefMut trait 的，因为我们不能无条件地提供 mut T 可变引用，因为它可能同时被其他的 ArcT 所访问中。 不过，当满足一定条件的时候，我们还是可以提供 mut T 可变引用的。具体来说，需要满足 2 个条件： 使用 mut ArcT 保证当前 ArcT 只有一个地方在使用； 需要确保全局只有一个 ArcT。 为了避免跟 DerefMut 混淆，我们将其声明为一个静态方法，并将 ArcT 作为参数，同时因为只有满足特定条件的情况下，才能返回 mut T，所以我们使用 Option 作为返回值。如下： pub fn get_mut(arc: mut ArcT) - Optionmut T if arc.data().ref_count.load(Ordering::Relaxed) == 1 fence(Ordering::Acquire); // Safety: 没有其他地方可以访问数据， // 因为我们使用了 mut 独占引用，而且此时只有一个 `Arc`。 unsafe Some(mut arc.ptr.as_mut().data) else None 这里我们在确定只有一个 Arc 实例的时候，使用 fetch(Acquire) 来跟所有其他 Arc 执行 drop() 的 fetch_sub(Release) 建立 happens-before 关系。 因为此时 ref_count=1，说明仅有当前 Arc 这一个实例，而 get_mut 需要的又是独占引用，所以当前 Arc 不可能再被拿去做 clone() 操作，所以在这个情况下，是可以保证有且仅有一个 Arc 实例，所以我们是可以安全返回 mut T 的。 返回的 mut T 会隐式地借用参数 mut ArcT 的生命周期，而 Rust 不允许可变引用和只读引用交叉存在，所以当前仅剩的这个 Arc，直到 mut T 作用域结束之前，都是不可用的，所以在那之前，不会再被拿去执行 clone() 和 Deref 等操作，所以是安全的。 #[test]fn get_mut_should_be_safe() let mut x = Arc::new(vec![]); let v = Arc::get_mut(mut x).unwrap(); v.push(1); let y = x.clone(); // ---- cannot borrow `x` as immutable v.push(2); 如上面这个例子，就会报错： error[E0502]: cannot borrow `x` as immutable because it is also borrowed as mutable -- src/arc.rs:117:17 |114 | let v = Arc::get_mut(mut x).unwrap(); | ------ mutable borrow occurs here...117 | let y = x.clone(); | ^ immutable borrow occurs here118 | v.push(2); | - mutable borrow later used here v2: 弱指针 WeakT 解决循环引用 循环引用问题分析 graph TD A --> B; A --> C; 引用计数在各种数据结构的表示中非常有用，如上图所示的树形结构，当释放 A 的时候，因为 B 和 C 不再被引用，所以也可以顺带释放了。 但是，如果 B 和 C 也持有对 A 的引用，即形成了循环引用，那按照我们之前的实现，A、B、C 都将永远不会被释放了，因为它们的引用计数永不为 0，哪怕它们三者均不再被使用。如下图所示： graph TD A B; A C; B -.-> A; C -.-> A; 为了应对这种场景，Rust 的标准库中提出的解决方案是：WeakT，也称弱指针。T 可以在 ArcT 和 WeakT 之间共享，当所有的 ArcT 都失效的时候，T 被释放，无论此时是否有 WeakT 的存在。 如下图所示，父节点对子节点使用的是强引用，确保了父节点存活的时候，子节点都存在。而子节点对父节点使用的是弱引用，只即保留了子节点回溯父节点的能力，也不会阻止父节点的释放。 当父节点被释放时，所有强引用计数归零，节点可以依次被释放。 graph TD A -- Arc --> B; A -- Arc --> C; B -.->|Weak| A; C -.->|Weak| A; 数据结构 OK，做了这么多铺垫后，我们来思考一下现在的 ArcData 该如何调整。 之前我们用 ref_count 做引用计数，它代表的都是强引用，现在我们需要记录弱引用数量的相关字段。 当只有弱引用的时候，data 就已经被释放了，我们需要使用 None 来表示这种情况，所以 data 应该是一个 OptionT 类型。 当 ArcDataT 被一个 Arc 和多个 Weak 共享时，释放最后一个 Arc 时，我们仅拥有 ArcDataT 不可变引用，这个时候我们需要将其从 Some(T) 置为 None，即要在不可变引用上实现修改，这就涉及到了前几篇提到的：内部可变性。UnsafeCell 是 Rust 提供的一个内部可变性工具类型，它包装一个数据，使得即使在只有不可变引用的情况下也可以进行修改（当然需要在 unsafe 块中操作）。所以我们需要将 data 再用 UnsafeCell 包一层，以满足此场景的需求。 综上，最新的 ArcData 定义如下： struct ArcDataT // `Arc` 的数量，也即数据 T 的强引用计数。 data_ref_count: AtomicUsize, // `Arc` + `Weak` 的数量。 alloc_ref_count: AtomicUsize, // 数据，当只有 `Weak` 的时候，为 None。 data: UnsafeCellOptionT, 然后我们需要定义一个新结构 WeakT ，用来表示弱引用，假定这个时候，我们将维护 ArcData 存活的职责交给 WeakT，那就可以将 ArcData 转给 WeakT 持有，然后在 ArcT 中持有一个 WeakT： struct ArcDataT // `Arc` 的数量，也即数据 T 的强引用计数。 data_ref_count: AtomicUsize, // `Arc` + `Weak` 的数量。 alloc_ref_count: AtomicUsize, // 数据，当只有 `Weak` 的时候，为 None。 data: UnsafeCellOptionT,pub struct ArcT weak: WeakT,pub struct WeakT ptr: NonNullArcDataT,unsafe implT: Send + Sync Send for WeakT unsafe implT: Send + Sync Sync for WeakT implT ArcT pub fn new(data: T) - Self Arc weak: Weak ptr: NonNull::from(Box::leak(Box::new(ArcData alloc_ref_count: AtomicUsize::new(1), data_ref_count: AtomicUsize::new(1), data: UnsafeCell::new(Some(data)), ))), , implT WeakT fn data(self) - ArcDataT unsafe self.ptr.as_ref() implT Deref for ArcT type Target = T; fn deref(self) - Self::Target let ptr = self.weak.data().data.get(); // Safety: 这个时候还有 Arc 存在，所以 data 肯定是生效的。 unsafe (*ptr).as_ref().unwrap() 在上述代码中，除了数据结构之外，我们做了 3 点调整： 只需要为 WeakT 实现 Sync 和 Send trait，这个时候 ArcT 就会被自动实现这 2 个 triat。 data(self) 辅助函数，移到了 WeakT 身上。 ArcT 要从 Deref 获取 T，需要先经过一道 WeakT。 graph LR subgraph Stack [\"栈内存 Stack\"] ArcStruct[\"ArcTweak: WeakT\"] WeakStruct[\"WeakTptr: NonNullArcDataT\"] end subgraph Heap [\"堆内存 Heap\"] ArcDataStruct[\"ArcDataTdata_ref_count: AtomicUsize(1)alloc_ref_count: AtomicUsize(1)data: UnsafeCellOptionT\"] end ArcStruct -->|包含| WeakStruct WeakStruct -->|指向| ArcDataStruct classDef stackStyle fill:#e1f5fe,stroke:#01579b,stroke-width:2px classDef heapStyle fill:#f3e5f5,stroke:#4a148c,stroke-width:2px classDef processStyle fill:#fff3e0,stroke:#e65100,stroke-width:2px classDef refStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px class ArcStruct,WeakStruct stackStyle class ArcDataStruct heapStyle class Step1,Step2,Step3,Step4,Step5 processStyle class DataRef,AllocRef,DataContent refStyle 维护引用计数 现在重点来了，我们需要来思考 WeakT 和 ArcT 的 Clone 和 Drop 该如何实现。其实重点就是该如何管理 alloc_ref_count 和 data_ref_count 的计数。 再次明确下我们的定义： data_ref_count: 代表的是强引用 ArcT 的数量。 alloc_ref_count: 代表的是 ArcT + WeakT 的数量。 因此： Clone: 当 WeakT 拷贝时，仅增加了弱引用的数量，所以我们只需对 alloc_ref_count 进行自增。 当 ArcT 拷贝时，不仅需要拷贝内部的 WeakT，还增加了强引用的数量，所以我们还需要对 data_ref_count 进行自增。 Drop: 当 ArcT 被释放时，不仅释放了其内部的 WeakT，还减少了一个强引用，所以需要对 data_ref_count 进行自减。另外，如果是最后一个 ArcT 被释放，我们需要将 ArcDataT.data 置为 None。 当 WeakT 被释放时，我们仅需减少弱引用的数量，即对 alloc_ref_count 进行自减。另外，当最后一个 WeakT 被释放时（此时肯定也没有 ArcT 了），我们还需要负责释放 ArcDataT 。 综上，我们的实现如下： implT Clone for WeakT fn clone(self) - Self if self.data().alloc_ref_count.fetch_add(1, Ordering::Relaxed) usize::MAX / 2 std::process::abort(); ; Self ptr: self.ptr implT Clone for ArcT fn clone(self) - Self let weak = self.weak.clone(); if weak.data().data_ref_count.fetch_add(1, Ordering::Relaxed) usize::MAX / 2 std::process::abort(); Arc weak implT Drop for WeakT fn drop(mut self) if self.data().alloc_ref_count.fetch_sub(1, Ordering::Release) == 1 fence(Ordering::Acquire); // Safety: 最后一个 WeakT 已经被释放了。 unsafe drop(Box::from_raw(self.ptr.as_ptr())); implT Drop for ArcT fn drop(mut self) if self .weak .data() .data_ref_count .fetch_sub(1, Ordering::Release) == 1 fence(Ordering::Acquire); let ptr = self.weak.data().data.get(); // Safety: data_ref_count 已经为 0 了，没有地方在使用 data 了。 unsafe (*ptr) = None 这里考虑到实际场景中的内存限制，我们对引用计数进行了简单的限制，当其超过 usize::Max/2 的时候，就执行 std::process:abort() 让整个进程崩溃。 get_mut() 接下来我们来考虑下 get_mut(arc: mut ArcT) 该如何修改。什么时候可以返回 mut T，很明显，当且仅当只有一个 ArcT 的时候才可以。 故 get_mut 的实现修改如下： pub fn get_mut(arc: mut ArcT) - Optionmut T if arc.weak.data().alloc_ref_count.load(Ordering::Relaxed) == 1 fence(Ordering::Acquire); // Safety: 这个时候没有其他地方能使用数据，因为只有一个 `Arc`， // 同时我们拥有仅存的这个 `Arc` 的不可变引用。 let arcdata = unsafe arc.weak.ptr.as_mut() ; let option = arcdata.data.get_mut(); let data = option.as_mut().unwrap(); Some(data) else None 到这里，我们执行之前的 arc_should_work 测试用例，可以发现是顺利通过的。 dowgrade() 回顾下面这张图，为了提供 A 的 WeakT 指针，我们需要给 ArcT 提供一个 downgrade() 方法，用于将强引用降为弱引用，这是必然可以成功的。同时，当我们也可以为 WeakT 提供一个 upgrade() 方法，用于持有弱引用的情况下可以尝试访问数据，当然这未必能成功。 graph TD A -- Arc --> B; A -- Arc --> C; B -.->|Weak| A; C -.->|Weak| A; downgrade() 比较简单，我们直接 clone() 一个 WeakT 就可以了： implT ArcT pub fn downgrade(self) - WeakT self.weak.clone() upgrade() upgrade() 就比较复杂了，只有当存在 Arc 的时候，data 才没被释放，这个时候，才能返回升级后的 ArcT，即要求 data_ref_count0。 implT WeakT pub fn upgrade(self) - OptionArcT // 获取 data_ref_count 的值 let mut n = self.data().data_ref_count.load(Ordering::Relaxed); loop // 如果等于 0，则说明 data 已经被释放了，直接返回 None if n == 0 return None; assert!(n usize::MAX); // 不为 0 的话，data_ref_count 尝试进行 +1，失败了就重试 if let Err(e) = self.data().data_ref_count.compare_exchange_weak( n, n + 1, Ordering::Relaxed, Ordering::Relaxed, ) n = e; continue; // 成功了，则 clone weak 即可（执行 alloc_ref_count++） return Some(Arc weak: self.clone() ); 我们来写一下测试用例，验证使用了 WeakT 后，我们的资源能否正确释放。在这之前，我们先写一个非 WeakT 版本的，看看资源是否真的没有被释放： #[test]fn no_weak_should_not_free_resource() static NUM_DROPS: AtomicUsize = AtomicUsize::new(0); struct Node // 使用 RefCell，利用其内部可变性，方便我们建立父子关系 child: RefCellVecArcNode, parent: RefCellOptionArcNode, // ---- 这里持有的是强引用 impl Drop for Node fn drop(mut self) NUM_DROPS.fetch_add(1, Ordering::Relaxed); impl Node pub fn new() - Self Self child: RefCell::new(vec![]), parent: RefCell::new(None), // 建立父子关系 pub fn add_child(parent: ArcNode, child: ArcNode) *child.parent.borrow_mut() = Some(parent.clone()); parent.child.borrow_mut().push(child); // 限定作用域，离开作用域后，root/child1/child2 正常情况应该被释放。 let root = Arc::new(Node::new()); let child1 = Arc::new(Node::new()); let child2 = Arc::new(Node::new()); Node::add_child(root, child1); Node::add_child(root, child2); assert_ne!(NUM_DROPS.load(Ordering::Relaxed), 3); // 不为 3 执行上述用例后，我们可以发现 NUM_DROPS 还是 0，即 root/child1/child2 均没有被释放资源。 你可以将 parent 的引用修改为弱引用，然后再重新执行测试用例，就会发现 root/child1/child2 的资源都被正确释放了。 struct Node child: RefCellVecArcNode, parent: RefCellOptionWeakNode, // ---- 这里持有的是弱引用impl Node pub fn new() - Self Self child: RefCell::new(vec![]), parent: RefCell::new(None), // 建立父子关系 pub fn add_child(parent: ArcNode, child: ArcNode) *child.parent.borrow_mut() = Some(parent.downgrade()); // ---- 使用 downgrade 转为弱引用 parent.child.borrow_mut().push(child); v3: 分离强弱引用，避免无用消耗 上个版本我们通过引入了弱引用 WeakT，成功解决了循环引用的问题，这是个非常大的进步！ 不过我们仍然有进一步优化的空间，可以观察到，ArcT 的每次拷贝，都会伴随一次拷贝 WeakT，但是，很多时候，我们其实没有循环引用关系的，也即我们并不是每一次都需要 WeakT，所以上个版本的实现，其实是有很多的浪费的。 数据结构 所以我们可以考虑将 WeakT 从 ArcT 中分离出来： pub struct ArcT ptr: NonNullArcDataT,unsafe implT: Send + Sync Send for ArcT unsafe implT: Send + Sync Sync for ArcT pub struct WeakT ptr: NonNullArcDataT,unsafe implT: Send + Sync Send for WeakT unsafe implT: Send + Sync Sync for WeakT implT ArcT // 调整 Arc 的构造函数 pub fn new(data: T) - Self Arc ptr: NonNull::from(Box::leak(Box::new(ArcData data_ref_count: AtomicUsize::new(1), alloc_ref_count: AtomicUsize::new(1), data: UnsafeCell::new(ManuallyDrop::new(data)), ))), // Arc 已经没有 Weak 了，这个时候，时候给它补一个 data() 辅助函数 fn data(self) - ArcDataT unsafe self.ptr.as_ref() // 相应地调整 DerefimplT Deref for ArcT type Target = T; fn deref(self) - Self::Target unsafe *self.data().data.get() 同时我们需要对 ArcDataT 结构中的引用技术进行重新定义： struct ArcDataT // `Arc` 的数量 data_ref_count: AtomicUsize, // `Weak` 的数量，当存在 `Arc` 时，额外加 1 alloc_ref_count: AtomicUsize, // 数据，当只有 `Weak` 的时候，释放它。 data: UnsafeCellManuallyDropT, 我们总共做了 3 个重要的调整： ArcT 不再包含 WeakT，而是各自独立，不过它们之前会共享底层的 ArcDataT。 alloc_ref_count 的语义，从\"Arc + Weak 的数量\"，变成\"Weak 的数量，当存在 Arc 时，额外加 1\"。 换言之，对于所有的 ArcT，都共有一个隐式的 WeakT ，当释放最后一个 ArcT 的时候，这个隐式的 WeakT 也需要被释放（本质是执行 alloc_ref_count 减一，同时如果没有其他的 WeakT，需要顺带释放 ArcDataT。 data 字段，我们使用 ManuallyDrop 来替代 Option，我们之前使用 None 来表示数据已经被释放，但其实 data_ref_count 已经能表达这层意思了，所以我们这里使用 ManuallyDrop 来进一步节省内存资源。 std::mem::ManuallyDropT 是一个零成本（zero-cost）包装器。它不改变 T 的布局，也不会产生额外的字节，在一些情况下，可以比 OptionT 少一些标记位和字节对齐所占据的额外空间。具体可参考：附录 2. ManuallyDropT。 graph TB subgraph Stack [\"栈内存 Stack\"] ArcStruct[\"ArcTptr: NonNullArcDataT\"] WeakStruct[\"WeakTptr: NonNullArcDataT\"] end subgraph Heap [\"堆内存 Heap\"] ArcDataStruct[\"ArcDataTdata_ref_countalloc_ref_countdata\"] end ArcStruct -->|直接指向| ArcDataStruct WeakStruct -->|直接指向| ArcDataStruct classDef stackStyle fill:#e1f5fe,stroke:#01579b,stroke-width:2px classDef heapStyle fill:#f3e5f5,stroke:#4a148c,stroke-width:2px class ArcStruct,WeakStruct stackStyle class ArcDataStruct heapStyle 维护引用计数 修改了引用计数的语义后，我们需要重新思考如何管理引用计数。 Clone: 当 WeakT 拷贝时，仅增加了弱引用的数量，所以我们依旧只需对 alloc_ref_count 进行自增。 当 ArcT 拷贝时，这个时候，我们就只需要对 data_ref_count 进行自增即可。 Drop: 当 ArcT 被释放时，我们需要对 data_ref_count 进行自减。另外，如果是最后一个 ArcT 被释放，我们需要释放 data ，同时，我们还需要释放那个代表所有 Arc 的隐式 Weak。 当 WeakT 被释放时，我们仅需减少弱引用的数量，即对 alloc_ref_count 进行自减。另外，当最后一个 WeakT 被释放时，我们还需要负责释放 ArcDataT 。 具体的实现如下： implT Clone for WeakT fn clone(self) - Self if self.data().alloc_ref_count.fetch_add(1, Ordering::Relaxed) usize::MAX / 2 std::process::abort(); ; Self ptr: self.ptr implT Clone for ArcT fn clone(self) - Self if self.data().data_ref_count.fetch_add(1, Ordering::Relaxed) usize::MAX / 2 std::process::abort(); Self ptr: self.ptr implT Drop for WeakT fn drop(mut self) // 思考下这里为什么要用 Release？后面我们会揭晓！ if self.data().alloc_ref_count.fetch_sub(1, Ordering::Release) == 1 fence(Ordering::Acquire); // Safety: 最后一个 WeakT 已经被释放了。 unsafe // 释放 ArcDataT drop(Box::from_raw(self.ptr.as_ptr())); implT Drop for ArcT fn drop(mut self) if self.data().data_ref_count.fetch_sub(1, Ordering::Release) == 1 fence(Ordering::Acquire); // 最后一个 `Arc` 被释放，需要做 2 件事情： // 1. 释放 ArcData.data // 2. 释放那个代表所有 `Arc` 的隐式 `Weak` unsafe ManuallyDrop::drop(mut *self.data().data.get()); // 在 `Weak` 那边 `drop()` 的时候： // 1. 会执行 alloc_ref_count--; // 2. 如果刚好是最后一个 `Weak`，那会顺带销毁 `AraData`。 drop(Weak ptr: self.ptr ); 对于这个隐式 WeakT，如果你一时无法很好地 Get 到那个点，可以尝试手动写写整个实现的过程，相信多走几遍流程，你会有那种茅塞顿开的感觉！ flowchart TD subgraph Clone[\"Clone 操作\"] WeakClone[\"WeakT::clone()\"] ArcClone[\"ArcT::clone()\"] WeakClone --> WeakInc[\"alloc_ref_count.fetch_add(1)\"] ArcClone --> ArcInc[\"data_ref_count.fetch_add(1)\"] WeakInc --> WeakCheck{\"计数 > usize::MAX/2?\"} ArcInc --> ArcCheck{\"计数 > usize::MAX/2?\"} WeakCheck -->|是| Abort1[\"std::process::abort()\"] ArcCheck -->|是| Abort2[\"std::process::abort()\"] WeakCheck -->|否| WeakNew[\"返回新的 WeakT\"] ArcCheck -->|否| ArcNew[\"返回新的 ArcT\"] end subgraph Drop[\"Drop 操作\"] WeakDrop[\"WeakT::drop()\"] ArcDrop[\"ArcT::drop()\"] WeakDrop --> WeakDec[\"alloc_ref_count.fetch_sub(1, Release)\"] ArcDrop --> ArcDec[\"data_ref_count.fetch_sub(1, Release)\"] WeakDec --> WeakDropCheck{\"返回值 == 1?最后一个 Weak?\"} ArcDec --> ArcDropCheck{\"返回值 == 1?最后一个 Arc?\"} WeakDropCheck -->|是| WeakFence[\"fence(Acquire)\"] WeakDropCheck -->|否| WeakEnd[\"结束\"] ArcDropCheck -->|是| ArcFence[\"fence(Acquire)\"] ArcDropCheck -->|否| ArcEnd[\"结束\"] WeakFence --> WeakFree[\"释放 ArcDataTBox::from_raw(ptr)\"] ArcFence --> ArcDataFree[\"释放 dataManuallyDrop::drop()\"] ArcDataFree --> ImplicitWeak[\"释放隐式 Weakdrop(Weak { ptr })\"] ImplicitWeak --> WeakDrop end subgraph Memory[\"内存释放顺序\"] Step1[\"① Arc 释放 data 内容\"] Step2[\"② Arc 释放隐式 Weak\"] Step3[\"③ Weak 释放 ArcData 结构\"] Step1 --> Step2 Step2 --> Step3 end classDef cloneStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px classDef dropStyle fill:#ffebee,stroke:#c62828,stroke-width:2px classDef memStyle fill:#fff3e0,stroke:#e65100,stroke-width:2px classDef criticalStyle fill:#fce4ec,stroke:#880e4f,stroke-width:3px class WeakClone,ArcClone,WeakInc,ArcInc,WeakNew,ArcNew cloneStyle class WeakDrop,ArcDrop,WeakDec,ArcDec,WeakEnd,ArcEnd dropStyle class WeakFree,ArcDataFree,ImplicitWeak criticalStyle class Step1,Step2,Step3 memStyle 至此，我们重新运行之前的测试用例 arc_should_work()，可以发现的成功通过的。 下面我们继续来调整 get_mut、downgrade() 和 upgrade()。 upgrade() upgrade() 比较简单，它的规则还是不变，只有当存在 ArcT 时，即 data_ref_count0 时，才能升级成功： implT WeakT //... pub fn upgrade(self) - OptionArcT // 获取 data_ref_count 的值。 let mut n = self.data().data_ref_count.load(Ordering::Relaxed); loop // 如果为 0，表示已经没有 Arc 了，升级失败。 if n == 0 return None; assert!(n usize::MAX); // 存在 Arc，则对 data_ref_count 尝试进行 CAS 加 1。 if let Err(e) = self.data().data_ref_count.compare_exchange_weak( n, n + 1, Ordering::Relaxed, Ordering::Relaxed, ) // CAS 失败，则重试。 n = e; continue; // CAS 成功，则返回升级后的 Arc。 return Some(Arc ptr: self.ptr ); get_mut() 接一下我们看 get_mut，规则也是不变：当前仅当只有一个 ArcT，且不存在 WeakT 时，才可以返回可变引用。 // 两个条件// 1. 没有 WeakT// 2. 没有其他的 ArcTpub fn get_mut(arc: mut ArcT) - Optionmut T // 将 alloc_ref_count 从 1 置为 usize::Max。 // 如果失败：说明之前不是 1，即存在其他的 WeakT，无法获取 mut T， None // 如果成功：说明当前没有 WeakT，第一步校验通过。 if arc .data() .alloc_ref_count .compare_exchange(1, usize::MAX, Ordering::Acquire, Ordering::Relaxed) .is_err() return None; let is_unique = arc.data().data_ref_count.load(Ordering::Relaxed) == 1; // 释放 alloc_ref_count arc.data().alloc_ref_count.store(1, Ordering::Release); // 如果存在其他的 Arc，则无法获取 mut T，返回 None if !is_unique return None; // 不存在其他的 Arc，返回 mut T fence(Ordering::Acquire); unsafe Some(mut *arc.data().data.get()) 代码的逻辑就不赘述了，但是这里有 3 个原子操作的内存顺序我们需要讨论一下！ 分别是： // 获取 WeakT 的数量并暂时锁定arc .data() .alloc_ref_count .compare_exchange(1, usize::MAX, Ordering::Acquire, Ordering::Relaxed) // 获取 ArcT 的数量let is_unique = arc.data().data_ref_count.load(Ordering::Relaxed) == 1;...fence(Ordering::Acquire); // 释放 alloc_ref_countarc.data().alloc_ref_count.store(1, Ordering::Release); 首先看第 1 个，我们要判断当前是否已经没有 WeakT 了，所以我们需要保证看到之前所有 drop(WeakT) 的写入操作，所以这里需要建立起一个 happens-before，因此我们之前为 WeakT 实现 Drop trait 的时候，使用的是 Release，而这里，需要使用 Acquire 来进行配对，建立 happens-before。 然后看第 2 个，这里我们要判断是否仅有一个 ArcT，所以我们需要保证看到之前所有 dropArcT 的写入操作，因此我们之前为 ArcT 实现 Drop trait 的时候，使用的是 Release。但是在这里，我们仅需在仅剩 1 个 Arc 的时候，才有必要建立 happens-before，所以当 is_unique=true 时，我们补一个 fence(Acquire) 屏障，来建立跟 drop(ArcT) 的 happens-before。 在分析第 3 个之前，我们需要来尝试挖一下当前 get_mut 的漏洞！相信有部分读者在看到上述实现后，跟笔者一样，会有一个疑惑：上述 2 个条件的检查，并不是原子的，这样的检查还安全可靠吗？ 比如说： pub fn get_mut(arc: mut ArcT) - Optionmut T if arc .data() .alloc_ref_count .compare_exchange(1, usize::MAX, Ordering::Acquire, Ordering::Relaxed) .is_err() return None; // ------- 在这个间隙：另外一个 Arc downgrade() - Weak，然后 drop(Arc) // ------- 那它也是检查通过的。 let is_unique = arc.data().data_ref_count.load(Ordering::Relaxed) == 1; arc.data().alloc_ref_count.store(1, Ordering::Release); if !is_unique return None; fence(Ordering::Acquire); unsafe Some(mut *arc.data().data.get()) 在上面这个例子中： 假设我们已经通过了第一个检查； 在进行第二个检查之前的这个间隙中，存在另外一个 ArcT； 然后它通过 downgrade() 生成了一个 WeakT； 然后再 drop(ArcT)； 这个时候我们再检查 ArcT 的时候，会发现只有 1 个，检查就通过了。但是其实这个时候，存在了其他的 WeakT，所以是有问题的！ sequenceDiagram participant A as T1 :get_mut() participant B as T2 :downgrade()+drop(Arc) participant C as ArcData A->>C: CAS alloc_ref 1→MAX ✔ Note over A,B: 非原子窗口 B->>C: clone Weak alloc_ref++ (Relaxed) B->>C: drop(Arc) fetch_sub data_ref (Release) A->>C: load data_ref ==1 ✔ A->>C: store alloc_ref MAX→1 (Release) A-->>A: fence(Acquire) → 返回 &mut T（已失效） 所以：我们不能让 downgrade() 在这个间隙中成功执行！这也是我们在前面将 alloc_ref_count 置为 usize::Max（上锁）的原因，在后面的 downgrade() 中，我们肯定要检查这个值，如果 alloc_ref_count=usize::Max，就不能 downgrade() 成功，直到 alloc_ref_store(1, _) 的时候，才能 downgrade() 成功。不过这个时候已经晚了！如果是这样，那 is_unique 肯定就不为 true，所以会返回 None，这个时候，我们不会返回 mut T，所以，危机就解除了！ downgrade() 我们先来看一下 downgrade() 的实现： implT ArcT // ... pub fn downgrade(self) - WeakT // 获取 Weak 的引用计数 let mut n = self.data().alloc_ref_count.load(Ordering::Relaxed); loop // 如果为 usize::MAX，说明已经被锁住了，这个时候自旋重试！ if n == usize::MAX std::hint::spin_loop(); n = self.data().alloc_ref_count.load(Ordering::Relaxed); assert!(n usize::MAX - 1); // alloc_ref_count 没被锁住，尝试进行 +1 操作。 if let Err(e) = self.data().alloc_ref_count.compare_exchange_weak( n, n + 1, Ordering::Acquire, Ordering::Relaxed, ) // +1 失败，重试。 n = e; continue; // +1 成功，表示降级成功，返回 Weak。 return Weak ptr: self.ptr ; 在 downgrade() 的实现中，我们跟 get_mut 遥相呼应： 如果 alloc_ref_count=usize::MAX，说明被锁住，这个时候需要自旋等待并重试。 std::hint::spin_loop()会向 CPU 发送特定指令（如 x86 的 pause 或 ARM 的 yield），提示当前处于忙等待状态，有利于优化 CPU 行为。 如果 alloc_ref_count 没被锁住，我们尝试进行 CAS，这里成功的时候使用的 Acquire ，为什么呢？这是为了跟前面还未讨论的第 3 个原子操作建立 happens-before！ 现在我们终于可以来解开这第 3 个原子操作的原子顺序谜团了： // 释放 alloc_ref_countarc.data().alloc_ref_count.store(1, Ordering::Release); 这里我们必须保证跟 download() 的 compare_exchange(_,_,Acquire,_) 建立起 happens-before 关系，即将 alloc_ref_count 置为 1 的结果必须被 downgrade() 所在的线程看到。不然的话，这里 compare_exchange 成功了，将 alloc_ref_count 置为 2 了，但是 get_mut 又将其置为 1 了，就乱套了！ 完整代码 到这里我们终于是完成了 v3 版本的优化工作了，真棒！介于篇幅已经够长了，这里就不再贴出完整的代码了，感兴趣的读者可以参阅：hedon-rust-road/conutils/arc。 浅探标准库的 ArcT 在最后，我们来看一下标准库的 ArcT 是如何实现的，看文章开头说的实现一个可以媲美标准库的 ArcT 是不是在吹牛！ 标准库（rustc 1.87.0）的 ArcT 位于 sync.rs 文件中，我们来看下它的数据结构定义： pub struct Arc T: ?Sized, #[unstable(feature = allocator_api, issue = 32838)] A: Allocator = Global, ptr: NonNullArcInnerT, phantom: PhantomDataArcInnerT, alloc: A,pub struct Weak T: ?Sized, #[unstable(feature = allocator_api, issue = 32838)] A: Allocator = Global, ptr: NonNullArcInnerT, alloc: A, 其他一些莫名其妙的标记咱就不管了，映入眼帘可以看到一个 ArcInner，这不就是咱的 ArcData 吗！点进去看下： struct ArcInnerT: ?Sized strong: atomic::AtomicUsize, // the value usize::MAX acts as a sentinel for temporarily locking the // ability to upgrade weak pointers or downgrade strong ones; this is used // to avoid races in `make_mut` and `get_mut`. weak: atomic::AtomicUsize, data: T, 好家伙！这不就是咱的 ArcData ! strong: 对应我们的 data_ref_count weak: 对应我们的 alloc_ref_count weak 字段上面的注释也揭示了其核心逻辑跟咱是高度一致的！ 我们简单看下最重要的 Drop 和 Clone 的实现，因为这涉及到引用计数的维护： const MAX_REFCOUNT: usize = (isize::MAX) as usize;// Arc: CloneimplT: ?Sized, A: Allocator + Clone Clone for ArcT, A fn clone(self) - ArcT, A let old_size = self.inner().strong.fetch_add(1, Relaxed); // fetch_add if old_size MAX_REFCOUNT // 溢出保护 abort(); // 返回 unsafe Self::from_inner_in(self.ptr, self.alloc.clone()) // Arc: Dropunsafe impl#[may_dangle] T: ?Sized, A: Allocator Drop for ArcT, A #[inline] fn drop(mut self) // fetch_sub release 强引用 if self.inner().strong.fetch_sub(1, Release) != 1 return; // 最后一个 Arc 释放的时候，补一个 fence(Acquire)， // 与前面所有的 release 建立 happens-before。 acquire!(self.inner().strong); // 最后一个 Arc 释放的时候，释放 ArcInner 里面的 data unsafe self.drop_slow(); // Weak: CloneimplT: ?Sized, A: Allocator + Clone Clone for WeakT, A #[inline] fn clone(self) - WeakT, A if let Some(inner) = self.inner() let old_size = inner.weak.fetch_add(1, Relaxed); // fetch_add 弱引用数量 if old_size MAX_REFCOUNT // 溢出保护 abort(); // 返回 Weak Weak ptr: self.ptr, alloc: self.alloc.clone() // Weak: Dropunsafe impl#[may_dangle] T: ?Sized, A: Allocator Drop for WeakT, A fn drop(mut self) let inner = if let Some(inner) = self.inner() inner else return ; if inner.weak.fetch_sub(1, Release) == 1 // fetch_sub release 弱引用 acquire!(inner.weak); // 最后一个补一个 fence(acquire) unsafe // 释放 ArcInner self.alloc.deallocate(self.ptr.cast(), Layout::for_value_raw(self.ptr.as_ptr())) 可以看到跟咱前面的实现是一样一样的！为啥？因为 Rust Atomics and Locks 一书的作者Mara Bos 就是 Rust 标准库团队的领导之一！牛逼！ 总结 通过三轮迭代，我们不仅实现了一个媲美了标准库的 ArcT ，还更进一步体验了 Rust 并发抽象的设计哲学： v0 —— 能用：单计数 ArcDataT 解决了跨线程多所有权，但仍缺乏可变视图与断环能力。 v1 —— 能改：借助独占 mut ArcT + 原子 锁，在 不破坏并发安全 的前提下开放了 get_mut。 v2 —— 能回收：双计数 + WeakT 消除了父子互指造成的泄漏，展示了 弱引用 在所有权图中的价值。 v3 —— 更轻巧：将 Weak 独立、用 ManuallyDrop 替换 OptionT，让没有循环引用需求的场景不再为弱引用买单。 下篇我们将尝试实现一个 MutexT，敬请期待！ Happy Coding! Peace~ 附录 1. NonNullT std::ptr::NonNullT 是一个 零成本、非空、协变 的裸指针包装器。它本质上只是把一个原始指针塞进 #[repr(transparent)] 的新类型中，但强制保证 绝不为空，因此可以拿到「空值当作枚举判别位」这份额外信息 —— OptionNonNullT 与一个普通指针占用同样大小。 重要特性： 永远非空：创建时若传入空指针即触发 未定义行为。编译器可依赖这一点做优化，例如把 OptionNonNullT 合并为一个指针宽度。 协变：与 *mut T 不同，NonNullT 可以在 U: DerefTarget = T 的场景下安全向子类型转换（因为禁止空值带来了额外保证），这使它非常适合构建自定义智能指针。 无自动 Drop：NonNull 只存地址，不持有所有权；销毁与释放内存仍由外部逻辑决定（如 Box::from_raw、Vec::dealloc 等）。 我们将其与裸指针、 BoxT 智能指针和引用 T 做一个简单的比较： 特性 *mut T / *const T NonNullT BoxT / T 是否允许为空 ✅ ❌ 必须非空 不适用 协变性 *mut 不协变 协变（可安全向子类型转换） T 协变 Option 优化 ❌ 多 1 B 判别字节 ✅ 与裸指针同尺寸 已自带优化 所有权 / drop 责任 没有 没有（纯指针） 有 Send / Sync 与 T 无关 默认 !Send !Sync 取决于 T 常见用途 FFI、底层算法 智能指针内部、侵入式容器、裁掉空判 高层所有权模型 关键的 API： 分组 代表方法 说明 构造 NonNull::new(ptr) → OptionNonNullT unsafe NonNull::new_unchecked(ptr) NonNull::dangling() 安全 / 不安全 / 占位 引用视图 unsafe fn as_ref / as_mut 把裸指针临时借用成 T / mut T 裸指针互转 fn as_ptr 取回 *mut T，解引用仍需 unsafe 类型转换 fn castU fn cast_mut / cast_const 保留地址，换类型或可变性 地址运算 unsafe fn add / byte_add / sub / offset 指针算术，与 ptr::add 族一致 切片助手 NonNull::slice_from_raw_parts(data, len) fn len() 1.70+ 稳定，为 [T] 提供非空裸切片构造 (rustwiki.org) ⚠️ 任何把 NonNull 重新解释为引用或解引用的操作，都必须在 unsafe 块里手动保证内存有效性与别名规则。 典型使用场景： 场景 作用 智能指针内部实现 (Box/Rc/Arc) 需要 非空 原始指针存放被管对象，且要在 Option 等场景下节省空间 自研侵入式链表 / 红黑树 节点自身持有前后指针字段 NonNullNodeT，天然避免空判分支 惰性初始化 / Vec::new 先用 NonNull::dangling() 占位，等真正分配后再写入正确地址 FFI C API 明确保证参数永不为空时，用 NonNull 在类型层面表达前置条件 自引用结构（Pin 工作区） 在完成真正初始化前使用 NonNull::dangling() 保存指向自身字段的指针 2. ManuallyDropT std::mem::ManuallyDropT 是一个零成本（zero-cost）包装器。把值包在 ManuallyDrop 里会告诉编译器：请不要在作用域结束时自动调用它的 Drop 实现，什么时候释放（或是否释放）由我手动决定。 ManuallyDropT 只是把 \"是否自动 drop\" 这一语义从编译器搬到了程序员身上； 它不改变 T 的布局，也不会产生额外的字节，因此是零开销； unsafe 责任：你必须保证一份值恰好析构一次（不能漏掉，也不能多调）。 它的核心 API 如表所示： API 作用 重要注意点 ManuallyDrop::new(value) 把 T 包装成 ManuallyDropT，关闭自动析构 零开销；之后需显式 drop 或移动走 ManuallyDrop::drop(mut self) (unsafe) 手动触发内部值的析构 必须保证之后不会再访问 / 再 drop ManuallyDrop::take(mut self) (unsafe) 从包装里\"搬走\"值（等价于 ptr::read） v 里留下未初始化内存，不能再用或再 drop ManuallyDrop::into_inner(self) (unsafe) 消费 ManuallyDrop 返回内部值 取得所有权后，原包装已被移走（不会 double-drop）","tags":["rust","并发编程"],"categories":["rust","rust 实战"]},{"title":"Rust 实战丨手写一个 oneshot channel","path":"/2025/05/29/rust-action-oneshot-channel/","content":"系列文章： Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序 Rust 原理丨从汇编角度看原子操作 Rust 实战丨手写一个 SpinLock Rust 实战丨手写一个 oneshot channel 👈 本篇 Rust 实战丨手写一个 Arc Rust 原理丨操作系统并发原语 Rust 实战丨手写一个 Mutex Rust 实战丨手写一个 Condvar Rust 实战丨手写一个 RwLock 继上篇 Rust 实战丨手写一个 SpinLock，本篇我们继续参考 Rust Atomics and Locks 一书，来实现一个 oneshot channel。 在 Go 语言中，有一句名言： Don't communicate by sharing memory, share memory by communicating. 不要通过共享内存来通信，而要通过通信来共享内存。 讲的就是通道 channel。使用 channel 来通信，一方面可以避免共享状态的并发竞争问题，另一方面可以解耦生产者和消费者。 channel 根据生产者和消费者的数量，可以分为以下几种： 单生产者单消费者 (SPSC) 单生产者多消费者 (SPMC) 多生产者单消费者 (MPSC) 多生产者多消费者 (MPMC) 在单生产者单消费者这个分类中，有一种特殊且常用的场景，叫一次性通道（oneshot channel）。它在 SPSC 的基础上增加了额外约束：整个生命周期内只传递一次数据，传递完成后通道就失效了。 熟悉 Go 语言的读者应该对以下使用场景很熟悉，这些都是典型的 oneshot channel 应用： func demo1() done := make(chan struct)\tgo func() // ... do something close(done)\t()\t-done func demo2() oneShot := make(chan string, 1)\tgo func() oneShot - generateText()\t()\ttext := -oneShot\tdoSthWithText(text)func generateText() string // ...func doSthWithText(text string) // ... 在 Rust 社区里面，就有一个非常优秀的 oneshot 实现，在详细深入它的实现之前，我们先参考 Rust Atomics and Locks 一书，来尝试实现一个 oneshot channel! 读完本篇你能学到什么 一次性 (oneshot) 通道的场景与优势 了解它与多生产者/多消费者通道的区别 掌握常见使用模式（如线程同步、单次结果返回） Rust 并发核心原语的渐进式实践 UnsafeCell：内部可变性基石 AtomicBool + Ordering::Release,Acquire：最小化同步原语 MaybeUninitT：零成本延迟初始化 用所有权与生命周期设计零误用 API 将 Sender / Receiver 拆分并一次性消费 生命周期引用 vs Arc 的权衡与替换技巧 线程挂起/唤醒机制 std::thread::park / unpark 的阻塞式等待模型 类型系统层面的“防呆”手段 利用 PhantomData 禁止跨线程误用 Drop 手动回收，避免内存泄漏 一步步优化的思考路径 如何发现问题 → 提出假设 → 实现 → 验证 → 再迭代 带着这些目标，跟随本文一路迭代到 v8，你将拥有一个高性能、零误用的 oneshot channel，以及一整套可迁移到其它并发场景的设计思维。 热身版 v0：基于锁的通道 我们先来实现一个万能版 channel 热热身。顾名思义，channel 分为 2 个功能，send 和 receive，其中： send 往 channel 一头放数据。 receive 从 channel 另外一头取数据，如果没有数据，则阻塞住，直到有数据时返回取出数据并返回。 在 Rust 中，我们可以用队列 VecQueue 来作为数据的承载，同时为了对队列访问的并发安全，我们需要使用锁 Mutex 来保护它，另外，在消费者取数据时，如果没有数据，则需要阻塞并等待唤醒（使用循环等待就太耗 CPU 了），所以我们可以使用条件变量 Condvar 来实现挂起和唤醒。 经过以上分析，我们可以定义如下的结构： use std:: collections::VecDeque, sync::Condvar, Mutex,;pub struct ChannelT queue: MutexVecDequeT, item_ready: Condvar, send 和 receive 方法也比较简单，如下所示： implT ChannelT pub fn new() - Self Self queue: Mutex::new(VecDeque::new()), item_ready: Condvar::new(), pub fn send(self, message: T) // 上锁并从队列后面插入数据 self.queue.lock().unwrap().push_back(message); // 唤醒一个等待数据的线程 self.item_ready.notify_one(); pub fn receive(self) - T // 抢占队列 let mut b = self.queue.lock().unwrap(); loop // 尝试从队列中获取数据，如果获取到，则直接返回（并释放锁） if let Some(message) = b.pop_front() return message; // 没有数据，则挂起当前线程（同时释放锁） b = self.item_ready.wait(b).unwrap(); 这里需要注意的是，self.queue.lock().unwrap() 返回的 b 是一个 MutextGuard，所以当执行 self.item_ready.wait(b) 的时候，在挂起当前线程的时候，会释放 b，所以这里不会一直占用锁，而导致其他线程抢不到锁。 这个版本的实现在功能上当然没有问题，但是在性能上还有非常多可以优化的地方，尤其是在锁的使用上，在高并发的情况下，锁的竞争会非常激烈。 OK，热完身后，我们开始基于这个实现，来一步步实现一个高性能的 oneshot channel！ 基础版 v1：unsafe 提醒使用者 pub struct ChannelT queue: MutexVecDequeT, item_ready: Condvar, 我们先来分析一下，一个 oneshot channel 的结构，需要包含哪些字段。 首先它可能会有 0 条数据或 1 条数据，所以很当然，数据可以用一个 Option 来承载。 另外，send 和 receive 可以在不同的线程中被调用，所以我们只能用共享只读引用，而不是用 mut 独享可变引用，但是 send 和 receive 都需要对数据进行修改，所以我们这里就需要一个支持内部可变性的数据结构，这个时候，就用到了上篇 Rust 实战丨手写一个 SpinLock 介绍的 UnsafeCellT，它允许在共享引用下进行内部可变性修改，是 Rust 并发原语的基石，这里不再赘述。 最后，我们需要一个变量来表明是否有数据，为了并发安全，这里可以用 AtomicBool，为此，我们也增加了一个 is_ready 的方法，用于判断数据是否已准备好。对于原子变量，我们使用一对 Release 和 Acquire（Release 确保之前的写入对其他线程可见，Acquire 确保能看到之前的 Release 写入）来确保原子变量的跨线程可见性。 基于以上分析，我们定出了新的 Channel 结构： pub struct ChannelT message: UnsafeCellOptionT, ready: AtomicBool, 对应的 send、receive 和 is_ready 实现如下： implT ChannelT pub fn new() - Self Self message: UnsafeCell::new(None), ready: AtomicBool::new(false), /// Safety: Only call this once! pub unsafe fn send(self, message: T) unsafe self.message.get().write(Some(message)); self.ready.store(true, Ordering::Release); pub fn is_ready(self) - bool self.ready.load(Ordering::Acquire) /// Safety: Only call this once, /// and only after is_ready() returns true! pub unsafe fn receive(self) - T unsafe self.message.get().read().unwrap() 在这个版本中： 我们暂且使用 unsafe 加注释的方式，来 提醒 使用者，send 和 receive 只能被调用一次，同时，在调用 receive 之前，必须先使用 is_ready 进行数据检查。 对于原子变量，我们使用一对 Release 和 Acquire 来确保原子变量的跨线程可见性，具体可参考 Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序。 另外别忘了，UnsafeCellT 是不支持 Sync 的，所以为了我们的 Channel 可以跨线程使用，我们需要为其实现 Sync trait： // 1. ChannelT 可以在不同的线程中被分别执行 send 和 receive，所以它的引用可以在线程中共享，所以需要实现 Sync；// 2. T 由线程 1 生成并放入 Channel，然后由线程 2 从 Channel 中获取，所以它需要从一个线程转移到另外一个线程，所以需要实现 Send。unsafe implT Sync for ChannelT where T: Send 使用方法如下： #[test]fn one_thread_should_work() let channel = Channel::new(); unsafe channel.send(1); ; if channel.is_ready() let msg = unsafe channel.receive() ; assert_eq!(msg, 1); #[test]fn cross_thread_should_work() let channel = Channel::new(); thread::scope(|s| s.spawn(|| sleep(Duration::from_millis(10)); unsafe channel.send(1); ; ); loop if channel.is_ready() let res = unsafe channel.receive() ; assert_eq!(res, 1); break; ); 基础版 v2：使用 MaybeUninit 替代 Option 减少内存开销 我们先来思考一个问题：OptionT 的内存占用是多少？ 结论是：：OptionT 相比于 T，可能需要额外消耗标记位和填充位的空间。具体可参考附录：1. OptionT 的内存占用是多少。 另外一点是，OptionT 其实已经包含了是否存在值的信息了，它跟 ready 这个标志的作用其实重复了，有一些浪费。 在当下场景，我们可以使用另外一个数据结构来替代 OptionT —— MaybeUninitT，相比于 OptionT，它有以下优势： 内存占用优化：在 OptionT 中，对于非空指针优化（Niche Optimization）的类型，None 会占用额外的空间（一个字节的标签+可能的对齐填充）。而 MaybeUninitT 本身就是一个大小与 T 相同的未初始化内存，它没有标签，因此不会引入额外的内存开销。 避免初始化开销：使用 OptionT 时，在初始化时设置为 None，实际上会写入一个表示 None 的值（即进行初始化）。而 MaybeUninitT 的 uninit() 不会对内存进行任何初始化，这在性能敏感的场景下可以避免不必要的初始化开销（特别是当 T 很大时）。 更灵活地控制初始化：在通道的实现中，消息可能由生产者写入，然后通过设置 ready 标志来通知消费者。使用 MaybeUninit 允许我们延迟初始化，直到实际需要写入消息的时候。这样，在通道创建时，我们不需要为 T 类型的值进行任何初始化（即使是 None），而是留出一块未初始化的内存，在后续由生产者写入实际的值。 与原子标志配合更高效：在上个版本的视线中，ready 是一个 AtomicBool，用于指示消息是否就绪。在 OptionT 版本中，我们需要检查 Option 是否为 Some，同时还要检查 ready 标志。而使用 MaybeUninit 后，我们完全依赖 ready 标志来判断消息是否可用，避免了双重检查（因为 MaybeUninit 本身不携带状态，所以状态完全由 ready 控制）。这样，结构体的内存布局更紧凑，且访问模式更直接。 潜在的性能提升：由于避免了额外的标签和初始化，以及更紧凑的内存布局，可能会提高缓存利用率，从而提升性能。 MaybeUninitT 有以下常用方法： 常用方法 作用 安全级别 MaybeUninit::uninit() 创建一块完全未初始化的内存 const fn、safe as_mut_ptr() / as_ptr() 取出裸指针，供外部写入或读取 safe assume_init() / assume_init_read() 告诉编译器“这里已经是一个合法的 T 了”，并返回它 unsafe（因为你得保证真初始化过） write(val) 按位把 val 复制/移动 到这块未初始化内存；此后视为已初始化 unsafe 经过上面一顿分析，我们来使用 MaybeUninitT 来替代 OptionT，进一步减少内存占用和提升性能，新的 Channel 结构如下： pub struct ChannelT message: UnsafeCellMaybeUninitT, ready: AtomicBool,implT ChannelT pub fn new() - Self Self // 创建一块完全未初始化的内存，先占位 message: UnsafeCell::new(MaybeUninit::uninit()), ready: AtomicBool::new(false), 对应的 send 和 receive 实现更新如下： implT ChannelT /// Safety: Only call this once! pub unsafe fn send(self, message: T) unsafe // 从 UnsafeCellMaybeUninitT 中取出 MaybeUninitT 并写入数据。 (*self.message.get()).write(message); self.ready.store(true, Ordering::Release); /// Safety: Only call this once, /// and only after is_ready() returns true! pub unsafe fn receive(self) - T // 从 UnsafeCellMaybeUninitT 中取出 MaybeUninitT 并读出数据。 unsafe (*self.message.get()).assume_init_read() 其中 send 中，(*self.message.get()) 从 UnsafeCellMaybeUninitT 中取出 MaybeUninitT，然后调用 write 方法把 message 写入这块未初始化的内存中，此后视为已初始化，并可以使用使用 assume_init_read 进行读取。 修改后，测试代码没有发生变化，我们执行之前的测试代码，发现还是可以通过的！ 基础版 v3：增加动态检查提高安全性 上述版本中，我们通过 unsafe 和注释去“要求”调用者严格遵循以下约束： send 和 receive 最多只调用一次。 receive 调用之前，必须先经过 is_ready 的检查。 在这个版本中，我们加一下动态检查，如果调用者不按要求做事，那就直接 panic 给出告警。 在 receive 中，我们需要做 2 点保证：① 已经有数据了，② 数据只被消耗了一次。 pub unsafe fn receive(self) - T if !self.ready.swap(false, Ordering::Acquire) panic!(no message available!) unsafe (*self.message.get()).assume_init_read() 这里我们使用 swap，将 ready 从 false 转为 true，达到了 2 个目的： 如果返回了 false，则说明之前的 ready 为 false，即数据没准备好。 如果返回了 true，则说明数据已经准备好了，这个时候，也已经将 ready 置为 false，这样后面调用的 receive 也将失败。 对于 send，我们需要保证只写入一次，所以这里我们需要引入一个新的变量 in_user，表示 send 是否已经使用了： pub struct ChannelT message: UnsafeCellMaybeUninitT, in_use: AtomicBool, // 新变量，表示 send 是否已经使用了。 ready: AtomicBool,implT ChannelT pub fn new() - Self Self message: UnsafeCell::new(MaybeUninit::uninit()), in_use: AtomicBool::new(false), ready: AtomicBool::new(false), 在 send 中，我们依旧使用 swap，来将 in_use 从转为 true，如果返回 true，则说明之前已经执行过 send 了，这个时候将执行 panic 进行告警。 /// Panics when trying to send more than one message.pub unsafe fn send(self, message: T) if self.in_use.swap(true, Ordering::Relaxed) panic!(cant send more than one message) unsafe (*self.message.get()).write(message); self.ready.store(true, Ordering::Release); 通过上述的 2 个优化，我们的 Channel 又“安全”了一丢丢！ 基础版 v4：实现 Drop 自动清理无用内存 因为我们使用了 MaybeUninitT，所以我们需要自己管理 T 的内存管理，但在上述的实现中，可能存在一种情况，导致内存得不到释放：我们只执行了 send，但直到 Channel 超过作用域的时候，都没有被 receive。 为此，我们可以为 Channel 实现 Drop trait，当有数据的时候，对MaybeUninitT 进行内存释放： implT Drop for ChannelT fn drop(mut self) if *self.ready.get_mut() unsafe self.message.get_mut().assume_init_drop(); 安全非阻塞版 v5：提供安全方法，减少使用者误用 在这个版本中，我们来解决前面实现的最大问题：方法是不安全的，严重依赖调用者的自觉性，没有充分发挥 Rust 强大编译器的检查能力。 回顾我们的需求：我们要实现的是一个 oneshot channel，即只能调用一次 send 和 receive。 第一个问题是：如何利用 Rust 天然的编译器检查能力来约束这一点呢？很明显，就是所有权机制！什么东西只能执行一次呢？消耗所有权的东西！ // 对于第一个参数为 self 的方法，执行时，会转移所有权，执行后，原变量就不能再用了，因为所有权已经转移了。fn do(self) 好，那第二个问题就来了：self 方法只能调用一次，但很明显我们总共需要 2 次的调用（send 和 receive），所以这里我们可以将 Channel 进行拆开，分成 Sender 和 Receiver。 那第三个问题也就随之而来了，Sender 和 Receiver 都需要持有 Channel，并且可能处于不同的线程，这里我们可以先用 Arc 来对 Channel 进行引用。 解决了上述 3 个问题，我们可以梳理新的数据结构： pub struct SenderT channel: ArcChannelT,pub struct ReceiverT channel: ArcChannelT,struct ChannelT message: UnsafeCellMaybeUninitT, ready: AtomicBool,pub fn channelT() - (SenderT, ReceiverT) let a = Arc::new(Channel message: UnsafeCell::new(MaybeUninit::uninit()), ready: AtomicBool::new(false), ); (Sender channel: a.clone() , Receiver channel: a ) Channel 中移除了 in_use 属性，因为我们已经有 self 做所有权检查了，不再需要 in_use 来避免重复调用 send 了。 新增了 SenderT 和 ReceiverT 两个结构，它们都各自持有了一个 ArcChannel。 对应的 send 和 receive 方法当然也就转移到 SenderT 和 ReceiverT 身上了，实现也和之前基本一致： implT SenderT pub fn send(self, messgae: T) unsafe (*self.channel.message.get()).write(messgae) ; self.channel.ready.store(true, Ordering::Release); implT ReceiverT pub fn is_ready(self) - bool self.channel.ready.load(Ordering::Relaxed) /// Safety: only after is_ready() returns true! pub fn receive(self) - T if !self.channel.ready.swap(false, Ordering::Acquire) panic!(no message available!); unsafe (*self.channel.message.get()).assume_init_read() // Drop 没变 修改了结构了，我们需要修改对应的测试代码： #[test]fn one_thread_should_work() let (sender, receiver) = channel(); sender.send(1); if receiver.is_ready() let msg = receiver.receive(); assert_eq!(msg, 1); #[test]fn cross_thread_should_work() let (sender, receiver) = channel(); thread::scope(|s| s.spawn(|| sleep(Duration::from_millis(10)); sender.send(1); // 没有 unsafe 了！ ); loop if receiver.is_ready() let res = receiver.receive(); // 没有 unsafe 了！ assert_eq!(res, 1); break; ); 在最新的测试代码中，我们已经不再需要 unsafe 代码了！这对于使用者来说，就非常友好了！ 而且这个时候，你如果尝试执行多次 send 和 receive 的时候，编译器就会报错了！ 不过它还是有 2 个缺点： Arc 的复制还是有一些开销的。 我们依旧依赖使用者提前用 is_ready 来检查，否则直接调用 receive 就有可能会 panic。 我们先来解决第 1 个问题。 安全非阻塞版 v6：使用生命周期加引用，避免 Arc 的复制开销 为了避免 Arc 的开销，我们需要在 SenderT 和 ReceiverT 中持有 ChannelT 的引用，而引用的对象的生命周期是不确定的，所以我们需要加入生命周期标注，来告诉编译器我们的引用是逻辑自洽的。 新的结构和构造函数如下： pub struct Sendera, T channel: a ChannelT, // 使用引用替代 Arc，并加入生命周期标注pub struct Receivera, T channel: a ChannelT, // 使用引用替代 Arc，并加入生命周期标注pub struct ChannelT message: UnsafeCellMaybeUninitT, ready: AtomicBool,implT ChannelT pub const fn new() - Self Self message: UnsafeCell::new(MaybeUninit::uninit()), ready: AtomicBool::new(false), pub fn splita(a mut self) - (Sendera, T, Receivera, T) *self = Self::new(); (Sender channel: self , Receiver channel: self ) // Drop 没变 在这个版本的实现中，我们新增了 split 方法： 其中参数 'a mut self 表明它是一个独占引用，即 channel.split() 不会有并发问题。 第一行代码 *self = Self::new() 我们对原有的 Channel 进行重置，保证拆分之前通道里绝对没有残留数据，避免旧消息被下一对 Sender/Receiver 误使用。 'a 直接来自于 'a mut self，保证两端把手绝不会比原始 Channel 活得更久。 新的测试代码如下： #[test]fn one_thread_should_work() let mut channel = Channel::new(); let (sender, receiver) = channel.split(); sender.send(1); if receiver.is_ready() let msg = receiver.receive(); assert_eq!(msg, 1); #[test]fn cross_thread_should_work() let mut channel = Channel::new(); let (sender, receiver) = channel.split(); thread::scope(|s| s.spawn(|| sleep(Duration::from_millis(100)); sender.send(1); ); while !receiver.is_ready() assert_eq!(receiver.receive(), 1); ); 安全阻塞版 v7：去掉 is_ready 完全避免使用者误调用 截止目前的实现版本中，我们还是依赖使用者在执行 receive 之前先执行 is_ready 进行数据检查，还是存在一定的误操作性。现在我们来实现一个完全阻塞的版本，来完全避免这个情况。 我们需要做几件事情： 去掉 is_ready 方法； 在 receive 中，根据 ready 判断是否存在数据： 如果存在，则直接取出数据并返回； 如果不存在，则需要先挂起当前线程，等待唤醒（直接 CPU 循环检查肯定可以，但不够优雅！咱不干！）； 在 send 中，放入数据后，尝试唤醒可能处于挂起中的线程。 那现在最重要的一个问题是：如何唤醒处于挂起中的线程？更进一步，唤醒哪个线程？ 这里其实是说不定的，因为 Sender 和 Receiver 都可能被放入任何一个线程中，不过在 Rust Atomics and Locks 书中，作者假定了 Receiver 会固定在调用 split 的那个线程。 笔者认为这个假设是简单且有效的，回顾一下我们前面举的 Go 语言的 2 个例子： func demo1() done := make(chan struct) // 类似于 split\tgo func() // ... do something close(done)\t()\t-done // receiver func demo2() oneShot := make(chan string, 1) // 类似于 split\tgo func() oneShot - generateText()\t()\ttext := -oneShot // receiver\tdoSthWithText(text) 在这两个最常见的 oneshot channel 的例子中，Receiver 就是处于调用 split 的线程中。所以我们可以基于这个假设来实现这个版本。 先回顾下 Thread 2 个最核心的方法： Thread.park(thread): 挂起线程，等待唤醒。 thread.unpark(): 唤醒线程。 首先我们需要在 Sender 中保存待唤醒的线程： pub struct Sendera, T channel: a ChannelT, receiving_thread: Thread, 在 split() 的时候，我们需要获取当前线程并保存在 Sender 中： pub fn splita(a mut self) - (Sendera, T, Receivera, T) *self = Self::new(); ( Sender channel: self, // 获取当前线程。记住！这里我们假设了 receiver 会固定在 split 的线程中！ receiving_thread: thread::current(), , Receiver channel: self , ) 在 recevie 的时候，如果没有数据，我们就可以挂起当前线程，等待唤醒： implT Receiver_, T pub fn receive(self) - T while !self.channel.ready.swap(false, Ordering::Acquire) thread::park(); // 挂起当前线程，即 thread::current() 线程 unsafe (*self.channel.message.get()).assume_init_read() 在 send 完数据后，唤醒可能挂起的线程： implT Sender_, T pub fn send(self, messgae: T) unsafe (*self.channel.message.get()).write(messgae) ; self.channel.ready.store(true, Ordering::Release); Thread::unpark(self.receiving_thread); // 唤醒 receiver 线程 最后删除之前的 is_ready 方法，然后更新我们的测试代码： #[test]fn one_thread_should_work() let mut channel = Channel::new(); let (sender, receiver) = channel.split(); sender.send(1); let msg = receiver.receive(); assert_eq!(msg, 1);#[test]fn cross_thread_should_work() let mut channel = Channel::new(); let (sender, receiver) = channel.split(); thread::scope(|s| s.spawn(|| sleep(Duration::from_millis(100)); sender.send(1); ); assert_eq!(receiver.receive(), 1); // 不再需要检查 is_ready，这里会阻塞一直直到有数据到来 ); 最终版 v8：使用 PhantomData 来保证 Receiver 处于 split() 线程 是不是觉得，上述实现已经完美无瑕了！其实不然，我们虽然假设了 Receiver 处于调用 split() 的线程中，但是还是无法阻止使用者将 Receiver 转移到其他线程。 再次回顾下我们现在的 Channel 和 Receiver： pub struct Receivera, T channel: a ChannelT,pub struct ChannelT message: UnsafeCellMaybeUninitT, ready: AtomicBool,unsafe implT Sync for ChannelT where T: Send 我们为 ChannelT 实现了 Sync trait，而标准库中有这 2 行代码： implT: ?Sized + Sync Sync for T implT: ?Sized + Sync Send for T 所以 ChannelT 实现了 Sync/Send trait，而 Receiver 只持有了一个 'a ChannelT，所以它也是 Send 的！ 所以 Receiver 是可以被转移到其他线程的，即下述的测试代码在编译上也是通过的： #[test]fn cross_thread_should_work() let mut channel = Channel::new(); let (sender, receiver) = channel.split(); // 执行 split() 的线程 thread::scope(|s| s.spawn(|| sleep(Duration::from_millis(100)); sender.send(1); ); // 这里在另外一个线程中，执行了 `receiver.receive()` s.spawn(|| assert_eq!(receiver.receive(), 1); ); ); 但是我们执行后会发现，receiver.receive() 会被永久阻塞住，这是因为 sender.send(1) 只会唤醒执行 split() 的线程。 为了避免这种情况的发生，我们需要强行防止 Receiver 实现 Send trait！ 怎么办呢？我们需要做到 2 件事情： 让 Receiver 持有一个非 Sync 的属性； 这个属性除了标记没有其他作用，最好不要占用任何的资源。 这里我们介绍一位新朋友：PhantomData： PhantomData 是一个零大小类型（Zero-Sized Type, ZST），用于在编译期向类型系统传递额外信息，而不占用运行时内存。 我们可以用它在包一个 !Send 的类型，这样 Receiver 就是 !Send 的了。关于 PhantomData 的更多介绍，可以参考附录 2：PhantomData。 在介绍完 PhantomData 后，我们就可以使用它来防止 Receiver 实现 Send trait 了： pub struct Receivera, T channel: a ChannelT, _no_send: PhantomData*const (), 其中 *const() 是 !Send 的，所以我们的 Receiver 再也不会被转移到其他线程了，而 send 是要求 self，所以即便是 Sync 的也无所谓了，因为无法通过引用来执行 receive() 方法。 现在我们可以再次执行上面的测试代码（强行将 Receiver 移动到其他线程中），将会得到以下的报错： error[E0277]: `*const ()` cannot be sent between threads safely -- src/oneshotchannel.rs:103:21 |103 | s.spawn(|| | ----- ^- | | | | _______________|_____within this `closure@oneshotchannel.rs:103:21` | | | | | required by a bound introduced by this call104 | | assert_eq!(receiver.receive(), 1);105 | | ); | |_____________^ `*const ()` cannot be sent between threads safely 到这里，通过 8 个版本，我们一步步实现了一个高性能、低内存占用且安全可用的 oneshot channel 了！ 总结 至此，通过 8 个小版本，我们不仅手写了一个 高性能、安全友好的 oneshot channel，还更进一步体验了 Rust 在并发领域 “以类型系统驱动正确性” 的威力。我们来做一个简单的小结。 关键收获： 版本 新增能力 解决了什么问题 v0 Mutex + Condvar 通用通道 打开话题、对比后续无锁方案 v1 UnsafeCellOptionT + AtomicBool 去锁化、最小可行一次性通道 v2 替换为 MaybeUninitT 节省内存避免双状态检查 v3 运行时检查 (swap) 阻止未准备/二次调用导致 UB v4 Drop 清理 防止“只 send 不 recv”泄漏 v5 Sender / Receiver 所有权 API 编译期保证“仅调用一次” v6 生命周期引用替换 Arc 消除引用计数开销 v7 park / unpark 阻塞模型 使用者不再需要轮询 is_ready v8 PhantomData 防跨线程误用 类型系统彻底封死错误用法 核心记忆点： 内部可变性：UnsafeCell 是所有并发原语的基石。 延迟初始化：MaybeUninitT + “就绪标志” 是零成本组合。 Release / Acquire：最轻量的跨线程可见性保障。 所有权设计 API：让编译器替你兜底逻辑约束。 PhantomData：零大小但能影响 Send/Sync 的类型级标记。 迭代思路：先跑通，再收口安全性与性能，最后用类型系统“防呆”。 完整的代码可以参考：conutils-oneshot。 oneshot crate 浅探 附录 1. OptionT 的内存占用是多少？ 在 Rust 中，OptionT 类型占用的内存，取决于泛型参数 T 的类型特性。具体可分为两种情况： 当 T 是非指针类型（如基本类型、结构体等）时，OptionT 需要额外的空间存储 Some 或 None 的标签，此时： 内存布局：包含一个 1 字节的标签（标识 Some 或 None）和 T 类型的数据空间（可能包含对齐填充）。 即使为 None，仍需保留 T 所需的内存空间（含填充），以保障枚举值大小统一。如 Optioni32 占用 8 字节（1 字节标签 + 4 字节 i32 + 3 字节填充）。 当 T 是不可为空的指针类型（如 BoxT、T、mut T）时，Rust 编译器会启用空指针优化（Niche Optimization）。即利用指针不能为 0 的特性，将 None 标识为全零位模式（0x00），而 Some(ptr) 存储实际指针地址，此时无需额外标签。这个时候，None 和 Some(T) 不占用任何的额外空间，大小与 T 相同。 我们可以写个程序来简单验证一下： fn test_option() // 1. 基本数据类型 let i: i32 = 1; let i_none: Optioni32 = None; let i_some: Optioni32 = Some(1); println!(基本类型：); println!(i32: bytes, ptr: :p, mem::size_of_val(i), i); // 4 bytes println!(Nonei32: bytes, ptr: :p, mem::size_of_val(i_none), i_none); // 8 bytes println!(Somei32: bytes, ptr: :p, mem::size_of_val(i_some), i_some); // 8 bytes // 2. 自定义类型 #[repr(C)] struct Data a: u64, b: u32, let data = Data a: 1, b: 1 ; let data_none: OptionData = None; let data_some: OptionData = Some(Data a: 1, b: 1 ); println!( 自定义结构体：); println!(Data: bytes, ptr: :p, mem::size_of_val(data), data); // 16 bytes println!(NoneData: bytes, ptr: :p, mem::size_of_val(data_none), data_none); // 24 bytes println!(SomeData: bytes, ptr: :p, mem::size_of_val(data_some), data_some); // 24 bytes // 3. 指针类型 let b = Box::new(1); let b_none: OptionBoxi32 = None; let b_some: OptionBoxi32 = Some(Box::new(1)); println!( 指针类型：); println!(Boxi32: bytes, ptr: :p, mem::size_of_val(b), b); // 8 bytes println!(NoneBoxi32: bytes, ptr: :p, mem::size_of_val(b_none), b_none); // 8 bytes println!(SomeBoxi32: bytes, ptr: :p, mem::size_of_val(b_some), b_some); // 8 bytes let none_value = unsafe *(b_none as *const _ as *const i64) ; println!(NoneBoxi32 bit pattern: :#x, none_value); // 0x0 let some_value = unsafe *(b_some as *const _ as *const i64) ; println!(NoneBoxi32 bit pattern: :#x, some_value); // 0x15d0043c0 在笔者的电脑下，输出如下： 基本类型：i32: 4 bytes, ptr: 0x16bef203cNonei32: 8 bytes, ptr: 0x16bef2040Somei32: 8 bytes, ptr: 0x16bef2048自定义结构体：Data: 16 bytes, ptr: 0x16bef2200NoneData: 24 bytes, ptr: 0x16bef2210SomeData: 24 bytes, ptr: 0x16bef2228指针类型：Boxi32: 8 bytes, ptr: 0x16bef23f8NoneBoxi32: 8 bytes, ptr: 0x16bef2400SomeBoxi32: 8 bytes, ptr: 0x16bef2408NoneBoxi32 bit pattern: 0x0 通过输出我们可以观察到经过空指针优化，OptionBoxi32 的 None 和 Some 都只占 8 字节（与 Boxi32 相同），同时通过为 None 复用类型的无效位模式（如 0x0）消除枚举标签，实现零成本抽象。 2. PhantomData Rust 中的 PhantomData 是一个零大小类型（Zero-Sized Type, ZST），用于在编译期向类型系统传递额外信息，而不占用运行时内存。它在泛型编程、生命周期管理和所有权标记中扮演关键角色。 它有以下的核心特性和作用： 零内存开销：PhantomDataT 本身不存储任何数据，编译后会被优化掉，因此不会增加结构体的实际内存占用。 use std::marker::PhantomData;struct WrapperT data: u32, _marker: PhantomDataT, // 不占空间 标记未使用的泛型参数：Rust 要求泛型参数必须在结构体中被显式使用。若泛型参数未直接出现在字段中，可通过 PhantomData 标记其存在性，避免编译错误。 struct ResourceT handle: *mut (), _phantom: PhantomDataT, // 标记类型 T 声明生命周期依赖：当结构体包含原始指针（如 *const T）时，PhantomData 可绑定生命周期，确保引用的数据有效性。 struct Slicea, T start: *const T, end: *const T, _phantom: PhantomDataa T, // 绑定生命周期 a 协变与逆变控制：通过 PhantomData'a T 或 PhantomData*mut T 等不同形式，调整类型的协变/逆变行为。 use std::marker::PhantomData;use std::rc::Rc;// 标记类型为 !Send 且 !Syncstruct NotThreadSafe _marker: PhantomDataRc(), // Rc() 本身是 !Send + !Sync","tags":["rust","并发编程"],"categories":["rust","rust 实战"]},{"title":"Rust 实战丨手写一个 SpinLock","path":"/2025/05/13/rust-action-spinlock/","content":"系列文章： Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序 Rust 原理丨从汇编角度看原子操作 Rust 实战丨手写一个 SpinLock 👈 本篇 Rust 实战丨手写一个 oneshot channel Rust 实战丨手写一个 Arc Rust 原理丨操作系统并发原语 Rust 实战丨手写一个 Mutex Rust 实战丨手写一个 Condvar Rust 实战丨手写一个 RwLock 在并发编程中，锁（lock）是一种常用的同步机制，用于保护共享数据避免竞态条件。然而，在许多编程语言中，锁的使用往往需要手动“加锁”和“解锁”。手动解锁的时机很难控制——如果程序在临界区出现错误而跳出了正常流程，开发者可能会忘记解锁锁，从而导致其他线程永远无法取得该锁，发生死锁。另外，还可能发生重复解锁的问题：比如线程 A 解锁后，线程 B 很快加锁，这时如果线程 A 的异常处理代码再次执行了解锁操作，就会把线程 B 的锁过早释放，造成数据竞态。另外，大部分语言中锁和它所保护的数据缺乏关联：编译器并不知道某个数据必须在特定锁保护下访问，这样一来，程序员很容易犯“未加锁就访问数据”的错误。这些问题对于新手来说尤其常见，而且编译器无法帮助检查并发使用上的这些 Bug。 为了解决上述问题，理想情况是让锁的管理和资源的生命周期绑定，由语言帮我们自动管理解锁。Rust 正是通过所有权和生命周期机制，实现了资源与作用域生命周期的绑定，即典型的 RAII 技术（Resource Acquisition Is Initialization，资源获取即初始化）。 在 Rust 标准库中，像 Mutex（互斥锁）就利用了 RAII：获取锁会返回一个守卫对象（例如 MutexGuard），当守卫对象被丢弃（析构）时自动解锁，从而避免显式解锁的麻烦。Rust 标准库的 Mutex 底层利用了操作系统的锁机制，在线程争用时会使线程休眠挂起，以避免浪费 CPU。然而，在一些场景下，比如无操作系统环境（no_std）的内核开发、中断处理、或者临界区极短的场合，我们可能希望使用自旋锁（SpinLock）来忙等待锁，而不进入休眠。自旋锁在锁竞争短暂时能省去线程切换的开销，但如果锁被占用时间过长，会浪费大量 CPU 时间，因此需要慎重使用。 接下来，我们将参考 Rust Atomics and Locks 一书，从零开始实现一个 Rust 版本的 SpinLock。我们会按三个版本逐步引入功能和概念： 首先实现基本的 v0 版本（不保护具体数据，仅提供加锁/解锁机制）； 然后扩展为能够保护数据的 v1 版本； 最后加入 RAII 机制实现自动解锁的v2版本。 过程中，我们会讨论相关的 Rust 并发概念，包括 Atomic 原子类型、Ordering 内存序、UnsafeCell、Send/Sync 并发安全标记、以及 RAII 中的 Deref/Drop trait 等。 让我们一步步实现这个自旋锁吧！ 读完本篇你能学到什么 自旋锁与互斥锁的权衡：了解自旋锁适合的场景（极短临界区、内核/中断上下文、无 OS 环境），以及为何在锁竞争时间较长时应优先选择休眠式互斥锁。 原子操作 + 内存顺序的实战用法：学会使用 AtomicBool，并理解 Acquire / Release 在加锁、解锁时建立的 happens-before 关系；掌握 swap 与 spin_loop 的配合细节。 内部可变性（UnsafeCell）：掌握如何在持有不可变引用的情况下对数据进行安全修改。 RAII + Drop 机制消除“忘记解锁”Bug：通过 SpinLockGuard + Drop，体验如何把“资源释放”交给作用域管理，彻底根除忘记/重复解锁的风险。 Deref / DerefMut 的零成本抽象：掌握为守卫对象实现 Deref/DerefMut，让使用者像操作普通引用一样操作受保护数据，而不引入额外运行时开销。 基础版 v0：自旋锁的基本实现 我们先从最基础的版本开始，我们在 lock 的时候，如果失败了，就一直循环尝试，直到成功获取锁： pub struct SpinLock // 原子布尔标志，表示锁是否被占用 locked: AtomicBool,impl SpinLock pub const fn new() - Self Self locked: AtomicBool::new(false), pub fn lock(self) // 使用原子操作尝试将 flag 变为 true，并返回之前的值： // 如果返回的是 true，则说明锁已经被其他线程抢走了。 // 如果返回的是 false，则说明当前线程抢占锁成功。 // 获取锁使用 Acquire 语义以确保后续对受保护数据的内存访问不会被重排到锁获取之前。 while self.locked.swap(true, Ordering::Acquire) // 向处理器发出一个提示，表示当前线程正忙等待。 // 这在某些架构上可以减少功耗或让处理器优化性能（比如 x86 上的 PAUSE 指令），避免无效地占用总线。 std::hint::spin_loop(); pub fn unlock(self) // 将标志置回 false，释放锁。使用 Release 语义以确保之前临界区的修改对后续获取锁的线程可见。 self.locked.store(false, Ordering::Release); 在上述实现中，我们定义了结构 SpinLock，它包含一个原子变量 locked。 在 lock 方法中，我们尝试对 locked 原子变量进行 swap 为 true 的操作，swap 会返回交换之前的值，如果是 false，那就说明抢锁成功了，这个时候 lock 就成功返回，否则，则调用 std::hint::spin_loop() 进行自旋，在下一次 while 循环中再尝试获取锁。 在 unlock 方法中，我们只需要将 locked 设置为 false 即可。 示例图如下： 这里有几个需要关注的点： std::hint::spin_loop() 会向 CPU 发送特定指令（如 x86 的 pause 或 ARM 的 yield），提示当前处于忙等待状态。这允许 CPU 优化执行行为： 降低功耗：减少自旋期间的计算资源消耗。 提升多线程效率：在超线程架构中，避免单个核心的忙等待阻塞其他线程的执行。 locked 是一个原子变量，对其的操作称为原子操作（Atomic Operation）。原子操作是指在多线程情况下不可被中断的操作，能保证对变量的读/写要么完整完成要么不发生，因此不存在数据竞争。在 Rust 中，每个原子操作都需要指定内存顺序（Memory Ordering）参数，用于约束编译器和 CPU 对指令重排的规则。更详细的规则可参阅：Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序。 这里我们内存顺序使用了一对 Acquire 和 Release。其中： 获取锁的时候使用 Acquire 确保后续对受保护数据的内存访问不会被重排到锁获取之前。 释放锁的时候使用 Release 确保之前临界区内的所有修改都完成发布（对其他线程可见），再让其他线程获取锁。 我们来撰写单元测试： #[cfg(test)]mod tests #[test] fn one_thread_should_work() let lock = SpinLock::new(); let mut data = vec![]; // 临界区资源 lock.lock(); data.push(1); // 临界区代码 lock.unlock(); lock.lock(); print!(:?, data); // 临界区代码 lock.unlock(); #[test] fn cross_thread_should_work() let data = vec![]; // 临界区资源 let lock = SpinLock::new(); thread::scope(|s| s.spawn(|| lock.lock(); unsafe let data_ptr = data as *const Veci32 as *mut Veci32; (*data_ptr).push(1); // 临界区代码 lock.unlock(); ); sleep(Duration::from_millis(100)); lock.lock(); unsafe let data_ptr = data as *const Veci32 as *mut Veci32; (*data_ptr).push(2); // 临界区代码 lock.unlock(); ); lock.lock(); print!(:?, data); // 临界区代码 lock.unlock(); 在跨线程的测试用例 cross_thread_should_work 中，为了对 data 进行修改，我们只能在 unsafe 里面强行使用裸指针来进行操作，否则编译就会失败。 升级版 v1：将锁与数据关联 在上一个基础版本中，虽然这么做能起到互斥的作用，但是存在 2 个问题： 我们会发现操作临界资源非常麻烦，因为临界资源的类型，可能是不满足 Sync 和 Send 的，所以它们无法在跨线程中进行传递或转移，所以即便我们能从逻辑上断定它们是并发安全的，但是编译器可没那么聪明，所以我们只能通过 unsafe 强行绕过编译期的检查。 锁和被保护的数据是分离的。程序员必须小心确保每次访问共享数据都正确地调用了 lock() 和 unlock()。一旦忘记调用 unlock()，或者搞错了加锁解锁的配对关系，编译器都不会报错，但程序的并发行为就可能出问题。 显然，我们希望让锁与数据关联起来，从语法层面降低误用的可能，同时便于我们为临界资源的数据类型限定相关的 trait，提高资源访问的便捷性。这正是下一步要做的改进。 我们看看标准库的 Mutex 是怎么实现的： pub fn lock(self) - LockResultMutexGuard_, T unsafe self.inner.lock(); MutexGuard::new(self) pub struct MutexGuarda, T: ?Sized + a lock: a MutexT, poison: poison::Guard, 可以发现，标准的锁是将要保护的临界资源放在了锁里，在获取锁的时候，就返回这个临界资源的 Guard。 OK，我们先不着急引入这个 Guard，我们就直接在获取锁的时候返回临界资源的可变引用即可。 更新后的版本如下所示； pub struct SpinLockT locked: AtomicBool, value: UnsafeCellT,implT SpinLockT pub fn new(value: T) - Self Self locked: AtomicBool::new(false), value: UnsafeCell::new(value), pub fn lock(self) - mut T while self.locked.swap(true, Ordering::Acquire) std::hint::spin_loop(); // Safety: 我们知道这个时候同时只可能有一个线程能获取到 value， // 也知道这个 value 一定存在，所以可以直接 unwrap()。 unsafe self.value.get().as_mut().unwrap() pub fn unlock(self) self.locked.store(false, Ordering::Release); unsafe implT Send for SpinLockT where T: Send unsafe implT Sync for SpinLockT where T: Send 可以看到，我们在 SpinLock 中加入了类型为 UnsafeCellT 的字段 value，然后在 lock() 抢到锁的时候，通过 self.value.get().as_mut().unwrap() 获取 value 的可变引用，我们知道这里是安全的，所以 unsafe 是安全的。 在这个版本中，我们见到了一个新朋友 UnsafeCell，事实上它是 Rust 标准库中所有的并发工具的基石，它涉及到了一个概念：内部可变性。 在 Rust 的类型系统中，如果我们只有一个对锁的不可变引用（SpinLockT），按正常规则是无法直接获得对内部数据的可变引用（mut T）的——毕竟 Rust 不允许在仅持有不可变引用的情况下修改数据。但对于实现锁这种特殊结构，我们清楚只有获取锁后才会独占数据的访问权，此时产生一个可变引用是安全的。为了突破编译器的限制，我们需要借助 std::cell::UnsafeCell。 UnsafeCell 是 Rust 提供的一个内部可变性工具类型，它包装一个数据，使得即使在只有不可变引用的情况下也可以进行修改（当然需要在 unsafe 块中操作）。很多线程同步原语（比如 Mutex、AtomicBool 自身等）内部都用 UnsafeCell 来允许内部数据的可变访问。 标准库中，基于 UnsafeCellT，封装了一些满足内部可变性的类型： CellT: 只允许 Copy 类型，通过 get()/set() 操作。 RefCellT: 运行时借用检查，但不是 Sync。 MutexT: 线程安全，但性能开销大。 同时也因为 UnsafeCellT 并不满足 Send 和 Sync trait，所以我们需要手动为其实现： unsafe implT Send for SpinLockT where T: Send unsafe implT Sync for SpinLockT where T: Send 我们修改我们的测试用例： #[cfg(test)]mod tests #[test] fn one_thread_should_work() let lock = SpinLock::new(vec![]); // 临界资源包在锁里面了 let data = lock.lock(); data.push(1); // 临界区代码 lock.unlock(); let data = lock.lock(); print!(:?, data); // 临界区代码 lock.unlock(); #[test] fn cross_thread_should_work() let lock = SpinLock::new(vec![]); // 临界资源包在锁里面了 thread::scope(|s| s.spawn(|| let data1 = lock.lock(); data1.push(1); // 临界区代码 lock.unlock(); ); sleep(Duration::from_millis(100)); let data2 = lock.lock(); data2.push(2); // 临界区代码 lock.unlock(); ); let data = lock.lock(); print!(:?, data); // 临界区代码 lock.unlock(); 这个版本的测试用例中，对于使用者来说，很明显就简洁很多了，再也不需要使用 unsafe 这种危险工具了。 最终版 v2：引入 RAII 的自旋锁守卫 v1 的实现仍然存在隐患，它要求调用者严格按照正确的顺序使用。我们可以想象一些误用场景： 忘记解锁： 如果线程获得了锁却没有调用 unlock() 就结束了，那么锁将一直保持锁定状态，导致其他线程永远自旋等待，无法前进。 重复解锁： 如果调用者不小心对同一个锁调用了两次 unlock()，第二次解锁会将另一个线程持有的锁误释放，造成数据同时被两个线程访问的风险。 未加锁访问： 由于我们提供了 lock() 返回 mut T 的接口，调用者理论上可以持有这个引用不放，然后调用 unlock() 解锁。这样一来，就出现了一个悬空引用——锁已经释放但仍持有先前的 mut T，如果此时另一线程加锁并修改数据，两个线程将同时持有对同一数据的可变引用，发生数据竞争！换言之，v1 的接口并不能防止调用者违反“先锁后用、用完解锁”的约定，Rust 编译器也无法帮我们检查这种逻辑错误。 综上，v1 尽管把数据和锁绑定在一起，但正确使用仍然完全依赖程序员自觉，稍有不慎就可能出错。这显然不符合 Rust 一贯的“编译期保证安全”的理念。有没有办法在编译阶段就防止上述误用呢？这就是我们下一步要做的：引入 RAII 机制，用 Rust 的所有权来管理锁的获取和释放。 RAII（Resource Acquisition Is Initialization，资源获取即初始化）是 C++/Rust 中的核心编程范式，通过将资源的生命周期与对象的生命周期绑定，实现资源的自动管理。其核心思想是：在对象构造函数中获取资源，在析构函数中释放资源，确保资源在任何情况下（包括异常）都能被正确释放。 这个时候，Guard 就可以登场了，我们可以参考标准库一样，在 lock 的时候返回一个 Guard，当这个 Guard 离开作用域的时候，它的 drop 就会被调用，我们可以在里面，执行 unlock 操作，这有 2 个好处： drop(guard) 是要消耗所有权的，所以可以避免重复释放锁； drop(guard) 在变量离开作用域后会被自动调用，所以可以避免忘记释放锁的情况发生。 更新后的版本如下所示： pub struct SpinLockT locked: AtomicBool, value: UnsafeCellT,pub struct SpinLockGuarda, T lock: a SpinLockT,unsafe implT Send for SpinLockT where T: Send unsafe implT Sync for SpinLockT where T: Send implT SpinLockT pub fn new(value: T) - Self Self locked: AtomicBool::new(false), value: UnsafeCell::new(value), pub fn lock(self) - SpinLockGuardT while self.locked.swap(true, Ordering::Acquire) std::hint::spin_loop(); SpinLockGuard::new(self) impla, T SpinLockGuarda, T pub fn new(lock: a SpinLockT) - SpinLockGuarda, T Self lock implT Drop for SpinLockGuard_, T fn drop(mut self) // 释放锁。 self.lock.locked.store(false, Ordering::Release); implT Deref for SpinLockGuard_, T type Target = T; fn deref(self) - Self::Target // Safety: 这里我们已经拿到锁（SpinLockGuard）了， // 所以可以确保数据的存在且独占的。 unsafe *self.lock.value.get() implT DerefMut for SpinLockGuard_, T fn deref_mut(mut self) - mut Self::Target // Safety: 这里我们已经拿到锁（SpinLockGuard）了， // 所以可以确保数据的存在且独占的。 unsafe mut *self.lock.value.get() 我们引入了类型 SpinLockGuard，它包含了一个 SpinLock 的引用，所以我们需要用生命周期 'a 进行标注。 SpinLock 在 lock() 成功时，返回一个 SpinLockGuard。 我们为 SpinLockGuard 实现 drop trait，让其在被 drop 时自动执行 unlock，这样就实现了离开作用域自动 unlock 的功能。 同时为了操作数据的简单性，我们为 SpinLockGuard 实现了 Deref 和 DerefMut 这 2 个 trait。 修改一下我们的测试代码，可以发现更加简洁了，同时有编译器的保护，我们想犯错都难了！ #[cfg(test)]mod tests #[test] fn one_thread_should_work() let lock = SpinLock::new(vec![]); // 临界资源包在锁里面了 let mut data = lock.lock(); data.push(1); // 临界代码区 drop(data); // 主动调用 drop 释放锁。 let data = lock.lock(); print!(:?, *data); // 离开作用域后，这里编译器会自动调用 drop(data) 释放锁。 #[test] fn cross_thread_should_work() let lock = SpinLock::new(vec![]); // 临界资源包在锁里面了 thread::scope(|s| s.spawn(|| let mut data1 = lock.lock(); data1.push(1); // 临界代码区 // data1 离开作用域，自动调用 drop(data1)，释放锁。 ); sleep(Duration::from_millis(100)); let mut data2 = lock.lock(); data2.push(2); ); let data = lock.lock(); print!(:?, *data); 总结 总结一下，在本实战篇中，我们从最初简单的原子标志锁出发，逐步演进，最终实现了一个拥有 RAII 机制的自旋锁 SpinLock。让我们回顾一下这个自旋锁的特点： 忙等待实现： 使用原子变量和循环实现锁的争用等待，而不涉及线程休眠。这样做在临界区很短时可以省去线程切换的开销，但如果锁持有时间较长，会浪费大量 CPU 时间。因此，本实现适合在短临界区或者无操作系统环境（如内核/中断上下文）使用。 RAII 保证解锁： 通过引入 SpinLockGuard 守卫并实现 Drop，我们将解锁操作自动化。开发者无须显式调用解锁函数，避免了因遗忘或异常路径导致的死锁。同时也防止了双重解锁的发生——同一把锁只有一个守卫，Rust 不允许守卫被意外复制或重复释放。 锁与数据绑定： 自旋锁内部直接持有被保护的数据，并通过类型系统将两者关联。任何对数据的访问都必须经由自旋锁提供的方法，这使“未加锁就访问数据”在语法上变得不可能（否则无法拿到数据的引用）。 编译期并发检查： 利用 Rust 的所有权和借用规则，我们实现了一定程度的编译期并发安全检查。只要代码编译通过，就已经避免了绝大多数常见并发错误（数据竞争、未解锁等）。当然，这不意味着可以高枕无忧，我们仍需注意避免死锁等逻辑问题，但 Rust 会提供最大程度的帮助。 同时，我们更进一步地理解原子变量和内存顺序的应用，也结识了一个新的朋友 UnsafeCell，它是 Rust 中同步原语的基础，后面我们还会经常见到。 下篇我们将尝试实现一个非常实用的工具：oneshot-channel（一次性通道），敬请期待！ Happy Coding! Peace~","tags":["rust","并发编程"],"categories":["rust","rust 实战"]},{"title":"RAG 技术概览","path":"/2025/04/13/ai-rag-tech-overview/","content":"RAG 的整个流程可概括为如下图所示，主要分成索引、检索和生成三个部分。 本文提供配套的完整案例，源码可参考：hedon-ai-road/rag-demo，每个 commit 都引入了一个新的技术点，感兴趣的读者可根据 commit 记录一一查探。 文档解析技术 代码示例 PDF 基于规则的开源库 pyPDF2 PyMuPDF pdfminer pdfplumber papermage 基于深度学习的开源库 Layout-parser PP-StructureV2 PDF-Extract-Kit pix2text MinerU marker Gptpdf（基于 LLM API） 商业闭源库 Textln.com Doc2x mathpix 庖丁 PDFlux 腾讯云文档识别 分块策略 代码示例 分块演示工具 3 个关键部分组成： 大小 重叠 拆分 固定大小分块（Fixed Size Chunking） 适用场景： 作为分块策略的基准线； 对大型数据集进行初步分析； 实现简单且可观测性高，分块便于管理； 适用于格式和大小相似的同质数据集，如新闻文章或博客文章。 问题： 不考虑内容上下文，容易导致无意义的文本块； 缺乏灵活性，无法适应文本的自然结构 重叠分块（Overlap Chunking） 使用场景： 需要深入理解语义并保持上下文完整性的文档，如法律文档、技术手册或科研论文； 提升分块内容的连贯性，以提高分析质量。 问题： 计算复杂度增加，处理效率降低； 冗余信息的存储和管理成为负担。 递归分块（Recursive Chunking） 通过预定义的文本分隔符（如换行符 、、句号、逗号、感叹号、空格等）迭代地将文本分解为更小的块，以实现段大小的均匀性和语义完整性。 先按较大的逻辑单元分割，再逐步递归到较小单元，确保在分块大小限制内保留最强的语义片段。 使用场景： 需要逐层分析的文本文档或需要分解成长片段、长段落的长文档，如研究报告、法律文档等。 问题： 在块边界处模糊语义，容易将完整的语义单元切分开。 文档特定分块（Document Specific Chunking） 根据文档的格式（如 Markdown、Latex、或编程语言如 Python 等）进行定制化分割的技术。此方法依据文档的特定格式和结构规则，例如 Markdown 的标题、列表项，或 Python 代码中的函数和类定义等，来确定分块边界。 适用场景： 有特定的文档结构，如编程语言、Markdown、Latex 等结构文档。 问题： 格式依赖性强，不同格式之间的分块策略不通用； 无法处理格式不规范及混合多种格式的情况。 语义分块（Semantic Chunking） 基于文本的自然语言边界（如句子、段落或主题中断）进行分段的技术，需要使用 NLP 技术根据语义分词分句，旨在确保每个分块都包含语义连贯的信息单元。 适用场景： 确保每个文档块的信息完整性且语义连贯； 提高检索结果的相关性和准确性； 适用于复杂文档和上下文敏感的精细化分析。 问题： 需要额外的高计算资源，特别是在处理大型或动态变化的文档数据时； 处理效率降低。 混合分块（Mix Chunking） 在初始阶段使用固定长度分块快速整理大量文档，而在后续阶段使用语义分块进行更精细的分类和主题提取。根据实际业务场景，设计多种分块策略的混合，能够灵活适应各种需求，提供更强大的分块方案。 适用场景： 适用于多层次的精细化分块场景； 数据集动态变化，包含多种文档格式与结构； 平衡处理速度与准确性的场景。 问题： 实现复杂度高； 策略调优难度高； 资源消耗增加。 Embedding 技术 Embedding 嵌入是指将文本、图像、音频、视频等形式的信息映射为高维空间中的密集向量表示。这些向量在语义空间中起到坐标的作用，捕捉对象之间的语义关系和隐含的意义。通过在向量空间中进行计算（例如余弦相似度），可以量化和衡量这些对象之间的语义相似性。 向量检索（Vector Retrieval）是一种基于向量表示的搜索技术，通过计算查询向量与已知文本向量的相似度来识别最相关的文本数据。向量检索的高效性在于，它能在大规模数据集中快速、准确地找到与查询最相关的内容，这得益于向量表示中蕴含的丰富语义信息。 评估指标：（MTEB、C-MTEB） 特定领域的适用性 检索精度 支持的语言 文本块长度 模型大小 检索效率 向量数据库 代码示例 向量数据库是一种专门用于存储和检索多维向量的数据库类型，与传统的基于行列结构的数据库不同，它主要处理高维空间中的数据点。 向量数据库的操作逻辑是基于相似性搜索，即在查询时，应用特定的相似性度量（如余弦相似度、欧几里得距离等）来查找与查询向量最相似的向量。 向量数据库的核心在于其高效的索引和搜索机制。为了优化查询性能，它采用了如哈希、量化和基于图形的多种算法。 层次化可导航小世界（HNSW）：通过在多层结构中将相似向量连接在一起，快速缩小搜索范围。 产品量化（PQ）：通过压缩高维向量，减少内存占用并加速检索。 位置敏感哈希（LSH）：通过哈希函数将相似向量聚集在一起，便于快速定位。 向量数据库的工作流程： 数据处理与向量化 向量存储 向量索引 向量搜索 余弦相似度：主要用于文本处理和信息检索，关注向量之间的角度，以捕捉语义相似性。 欧几里得距离：测量向量之间的实际距离，适用于密集特征集的聚类或分类。 曼哈顿距离：通过计算笛卡尔坐标中的绝对差值之和，适用于稀疏数据的处理。 数据检索 混合检索 代码示例 混合检索（Hybrid Search）通过结合关键词检索和语义匹配的优势，可以首先利用关键词检索精确定位到“订单 12345”的信息，然后通过语义匹配扩展与该订单相关的其他上下文或客户操作的信息，例如“12 开头的订单、包装破损严重”等。这样不仅能够获取精确的订单详情，还能获得与之相关的额外有用信息。 重排序技术 代码示例 重排序技术（Reranking）通过对初始检索结果进行重新排序，改善检索结果的相关性，为生成模型提供更优质的上下文，从而提升整体 RAG 系统的效果。 重排序模型大多是基于双塔或交叉编码架构的模型，在此基础上进一步计算更精确的相关性分数，能够捕捉查询词与文档块之间更细致的相关性，从而在细节层面上提高检索精度。 提示工程 一个提示（prompt）通常包含以下几个元素： 指令（Instruction）：指明模型要执行的特定任务或操作。 上下文（Context）：为模型提供额外信息或背景，可以帮助引导模型生成更准确的响应。 输入数据（Input Data）：我们希望模型回答的问题或感兴趣的输入内容。 输出指示符（Output Indicator）：指定模型的输出类型或格式，例如格式、是否要生成代码、总结文本或回答具体问题。 核心技巧： 具体指令法：具体、细致地告诉大模型要做什么。 示例学习：给出具体详尽的期望示例。 默认回复策略：设定默认回复策略，避免模型产生“幻觉”，让它不知道就说不知道。 任务角色设定：设定身份，可以帮助模型更好地理解任务要求和角色责任，从而输出更加一致、专业的内容。 解释理由法：向模型解释为什么某些任务需要特定的处理方式，帮助其理解任务背景。 文档基础说明：提供文档的背景信息和文本来源。 优化技术 数据清洗和预处理 在 RAG 索引流程中，文档解析之后、文本块切分之前，进行数据清洗和预处理能够有效减少脏数据和噪声，提升文本的整体质量和信息密度。 通过清除冗余信息、统一格式、处理异常字符等手段，数据清洗和预处理过程确保文档更加规范和高质量，从而提高 RAG 系统的检索效果和信息准确性。 处理冗余的模型内容。 消除文档中的额外空白和格式不一致。 去除无用的文档脚注、页眉页脚、版权信息。 查询扩展 查询扩展策略通过大模型从原始查询语句生成多个语义相关的查询，可以覆盖向量空间中的不同区域，从而提高检索的全面性和准确性。 查询扩展的指令模版： 你是一个AI语言模型助手。你的任务是生成五个不同版本的用户问题，以便从向量数据库中检索相关文档。通过从多个角度生成用户问题，你的目标是帮助用户克服基于距离的相似性搜索的一些局限性。请将这些替代问题用换行符分隔。原始问题：查询原文 假设问题： 下面报告中涉及了哪几个行业的案例以及总结各自面临的挑战？ 结果示例： 请问报告中提到的案例涉及了哪些行业？这些行业各自面临的挑战有哪些？报告中有哪些行业的案例被讨论？每个行业在报告中描述的挑战是什么？这个报告中具体提到了哪些行业的案例？能否总结一下这些行业当前面临的主要挑战？该报告中涵盖了哪些行业案例，并对各行业的挑战进行了哪些讨论？在报告中提到的行业案例有哪些？这些行业分别遇到的主要问题和挑战是什么？ 通过这种查询扩展策略，原始问题被分解为多个子查询，每个子查询独立检索相关文档并生成相应的结果。随后，系统将所有子查询的检索结果进行合并和重新排序，效果会更全面更准确。 自查询 自查询策略通过大语言模型自动提取查询中对业务场景至关重要的元数据字段（如标签、作者 ID、评论数量等关键信息），并将这些信息结合到嵌入检索过程中。 自查询的指令模版： 你是一个AI语言模型助手。你的任务是从用户问题中提取关键信息，你的回复应仅包含提取的关键信息。用户问题：查询原文 假设问题： 下面报告中涉及了哪几个行业的案例以及总结各自面临的挑战？ 结果示例： 行业，案例，挑战 提示压缩 提示压缩通过精简上下文、过滤掉不相关的信息，确保系统只处理与查询最相关、最重要的内容。 提示压缩的指令模版： 你是一个AI语言模型助手，负责对检索到的文档进行上下文压缩。你的目标是从文档中提取与用户查询高度相关的段落，并删除与查询无关或噪声较大的部分。你应确保保留所有能够直接回答用户查询的问题核心信息。输入：用户查询：用户的原始查询检索到的文档：检索到的文档内容输出要求：提取与用户查询最相关的段落和信息。删除所有与查询无关的内容，包括噪声、背景信息或扩展讨论。压缩后的内容应简洁清晰，直指用户的核心问题。输出格式：压缩段落1压缩段落2压缩段落3 RAG 效果评估 大模型打分：通过 LLM 对 RAG 的输出进行自动评分。效率高，但是准确性一般。 人工打分：手工针对 RAG 的输出进行逐一打分。更精确、细致的反馈，成本高。 评估指标： CR（Context Relevancy）检索相关性：检索到的信息是否偏离了原始查询。 AR（Answer Relevancy）答案相关性：是否能解决用户的问题，且内容是否逻辑连贯。 F（Faithfulness）可信度：是否存在幻觉或不准确之处。 打分标准： 完美（Perfect）1.0 分 可接受（Acceptable）0.75 分 缺失（Missing）0.5 分 错误（Incorrect）0.25 分 更高级的 RAG GraphRAG GraphRAG 通过构建知识图谱，将实体和实体之间的关系结构化地表示出来，克服了传统 RAG 的复杂推理局限性。 有以下优势： 提高答案的准确性和完整性 提高数据理解和迭代效率 提升可解释性和可追溯性 知识图谱的构建步骤： 实体识别：从文本或数据源中识别出关键实体。 关系抽取：确定实体之间的关系，可能通过自然语言处理技术实现。 三元组生成：将实体和关系表示为 (主体，关系，客体) 的形式。 图谱存储：使用图数据库或专门的存储系统保存知识图谱。 知识图谱的主要成本挑战： 数据收集与清洗成本 知识图谱构建成本 图谱的维护与更新","tags":["ai","rag"],"categories":["ai","rag"]},{"title":"读书笔记丨《Unit Testing Principles, Practices, and Patterns》","path":"/2025/04/09/note-unit-testing/","content":"本篇在上一篇的基础上，梳理下笔者的个人见解，感兴趣的读者可参考原文对比阅读： https://hedon.top/2025/04/09/note-unit-testing-excerpt/https://hedon.top/2025/04/09/note-unit-testing-excerpt/ 揪心疑惑 在撰写单元测试的过程中，你是否曾经被以下问题困扰过？ 为什么要写单元测试？单元测试的目标是什么？ 单元测试的粒度是怎样的？什么叫单元？a class, a function, or a behavior, or an observable behavior? 单测覆盖率真的有用吗？有什么用？又有哪些限制？ 怎样才能写好单元测试？怎样才能写出性价比最高的单元测试？ 如何判断一个单元测试的好坏？有没有具体可供参阅的维度？ 哪些代码需要写单元测试，哪些代码没必要写单元测试？ 单元测试和集成测试的边界是什么？ （单元丨集成）测试到底是要测什么东西？ 单元测试的侧重点是什么？集成测试的侧重点是什么？二者的比例该是怎样的？ 如何使用 Mock？哪些东西是需要 Mock 的？哪些东西是不应该 Mock 的？需要 Mock 的东西，应该在哪个层次进行 Mock？（你的 repository 层需要 Mock 吗？） 为什么你的测试代码很脆弱，总是需要频繁修改，维护起来难度很大？ 如何减少测试结果的假阳性和假阴性？ 四根柱子 对于第 5 个问题，作者提出了 4 个维度： Protection against regressions：防止回归，通过自动化验证代码修改后原有功能不受破坏。 The amount of code that is executed during the test. The complexity of that code. The code’s domain significance. Resistance to refactoring：抗重构性，重构业务代码时，测试代码无需过多变动便可通过用例，证明重构无误。 Tests provide an early warning when you break existing functionality. You become confident that your code changes won’t lead to regressions. Fast feedback：快速反馈。 Maintainability：可维护性。 How hard it is to understand the test. How hard it is to run the test. 对于这 4 个问题，你是否又有以下疑问： 哪个维度是最重要的？ 怎样才能写出满足各个维度的测试代码？ 如果维度之间存在矛盾，如何 trade off？ 为什么要写单元测试？ 三个最重要的原因： 验证你的程序逻辑正确性。 带来更好的代码设计。 因为单元测试能够让你站在使用者的角度去使用暴露的接口，如果接口不好用，逻辑不好测，测试条件不好构建，大概率说明代码的设计本身是有缺陷的，包括但不限于：抽象不合理、逻辑划分不清晰、与其他模块耦合严重等。 使软件项目更可持续发展。 如果你的需求没有发生变化，那原本能运行通过的单测应该一直都能运行，这有助于避免在团队协作中不小心改坏你不知道的代码，也有助于你执行各种重构措施。 这三个原因的重要性是显而易见的，但笔者个人觉得还有一个更深层次的最重要的原因： 你要对你做的事情负责，好的代码一定要先过自己这关。 单元测试的粒度是什么？ 这是一个很有争议的话题，单元测试的「单元」到底是什么？ 一个类？ 一个函数？ 还是多个类组成的一个模块？ 还是多个函数组成的一个大逻辑？ 在《Unit Testing》书中，作者指出：「单元」指的是 an observable behavior，即一个外部系统可观测到的行为。 为什么是可观测行为： 一个帮助客户端实现目标的操作（operation）。 一个帮助客户端实现目标的状态（state）。 换言之，也就是我在撰写单元测试的时候，我就是在使用系统提供的能力，我就是个使用者， 我只要验证你能提供我要的功能，就 OK 了，你背后怎么做，为了这个功能所拆分的类也好，小的辅助函数也好，都不重要，都不属于我要验证的范畴。 所以这更像是黑盒测试（black-box test）。 当然，会有例外，如果你底层有一个特别特别复杂的逻辑，你有必要专门花精力去验证它的逻辑正确性，那是可以针对它撰写专门的白盒测试（white-box test）的。针对这个情况，作者其实也提出了一个观点，对于这个复杂的逻辑，也可以抽成一个单独的模块，由它来提供能力给你当前模块使用。 总结： 优先选择黑盒测试。 对于涉及复杂算法的逻辑，单独撰写白盒测试。 结合覆盖率工具去看哪些代码没被覆盖，然后再站在使用者的角度去思考为什么没被覆盖，是这个分支压根没必要存在，还是还有未考虑到的使用场景。 如何组织单元测试？ 两种结构： AAA: Arrange-Act-Assert GWT: Given-When-Then 其实都是一个思路：准备前置条件→执行待验证代码→验证逻辑正确性。 几个建议： 尽量避免一个单元测试中包含多个 AAA/GWT。 避免在单元测试中使用 if 等分支语句。 命名的时候，尽可能让非程序员也能看懂，即这个命名需要描述一个领域问题。 如何发挥单测的最大价值？ 单元测试用例必须持续不断反复执行验证。 用最小的维护代价提供最大价值的单元测试。 识别一个有价值的测试 撰写一个有价值的测试 验证代码中最重要的部分（领域模型）。 1. 单元测试用例必须持续不断反复执行验证 这里推荐笔者的个人实践： 在 pre-commit 执行增量单元测试，确保本次修改的代码涉及的单测可正确通过。 在 gitlab-ci/github-action 流程中执行全量单元测试，全面覆盖，避免本次修改的代码影响到其他模块的正常功能。同时如果是合并到主分支的请求，加入增量覆盖率阈值检测，不满足阈值的，发送飞书消息卡片进行告警通知。 pre-commit 增量单测 .pre-commit-config.yaml 配置如下： repos: - repo: local hooks: - id: go-unit-tests name: go-unit-tests description: run go tests with race detector entry: bash -c ./script/run_diff_go_test.sh language: golang files: \\.*$ pass_filenames: false run_diff_go_test.sh 脚本如下： #!/bin/bashexport GOTOOLCHAIN=auto# 获取当前改动的 Go 文件changed_files=$(git diff --name-only --cached --diff-filter=d | grep \\.go$)# 如果没有改动的 Go 文件，退出if [ -z $changed_files ]; then echo No Go files changed. exit 0fi# 提取改动文件所在的包路径（使用相对路径），并排除 vendor 目录test_dirs=$(echo $changed_files | xargs -n1 dirname | grep -v ^vendor | sort -u)# 对每个改动的包路径运行 go testfor dir in $test_dirs; do # 检查目录是否存在 if [ ! -d $dir ]; then echo Directory $dir does not exist. Skipping... continue fi # 检查是否存在 go.mod 文件，确保在 Go 模块路径中 if [ -f $dir/go.mod ] || [ -f ./go.mod ]; then echo Running tests in $dir... (cd $dir go test -mod=vendor -gcflags=all=-l -short ./...) if [ $? -ne 0 ]; then echo Tests failed in $dir exit 1 fi else echo Skipping $dir (no go.mod found) fidoneecho All tests passed. gitlab-ci 全量单测 gitlab-ci.yml 配置如下： go-unit-test: stage: go-unit-test script: - sh script/unittest.sh $CI_MERGE_REQUEST_TITLE $GITLAB_USER_EMAIL $CI_PIPELINE_ID $CI_MERGE_REQUEST_TARGET_BRANCH_NAME $CI_JOB_ID rules: - if: $CI_PIPELINE_SOURCE == merge_request_event $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == dev - if: $CI_PIPELINE_SOURCE == merge_request_event $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == release coverage: /coverage: \\d+.\\d+% of statements/ unittest.sh 单测执行脚本如下： #!/bin/bashexport GOPROXY=https://goproxy.cn,directfunction generate_coverage_report gocover-cobertura coverage.out coverage.xml# 使用 gotestsum 执行单元测试# 如果单测执行失败，会发送飞书消息卡片到告警群中if ! gotestsum --junitfile report.xml --post-run-command=./script/send_fs_card.sh \\$1\\ \\$2\\ \\$3\\ \\$5\\ -- ./... -timeout 3s -short -mod=vendor -gcflags=all=-l -coverpkg=./... -coverprofile=coverage.out ; then generate_coverage_report sh script/cal_diff_coverage.sh $4 exit 1fi# 生成单元测试覆盖率报告generate_coverage_report# 如果增量覆盖率不满足阈值，会发送飞书消息卡片到告警群中source script/cal_diff_coverage.sh $4if [[ $4 == release ]]; then source script/check_test_coverage.sh $1 $2 $3fi 其中 cal_diff_coverage.sh 用于计算增量覆盖率： #!/bin/bashexport COVERAGE_PERCENT=0.0# 确保 coverage.xml 文件存在if [ ! -f coverage.xml ]; then echo coverage: 0.0% of statements exit 0fi# 使用 diff-cover 生成覆盖率报告diff-cover coverage.xml --exclude **/docs.go --html-report report.html --compare-branch $1 diff_detail.txt# 检查是否成功生成报告if [ ! -f report.html ]; then echo coverage: 0.0% of statements exit 0fi# 使用 grep 和 awk 提取覆盖率信息COVERAGE=$(grep Coverage: diff_detail.txt | awk print $2)# 如果找到了覆盖率数据，检查是否包含小数点if [ -n $COVERAGE ]; then if [[ $COVERAGE != *.* ]]; then # 如果没有小数点，在百分号前面加上 .0 # 这里这么做的目的是不知道为什么 gitlab ci 无法正确解析下面这个正则 # /coverage: \\d+(.\\d+)?% of statements/ # 只能解析这个 # /coverage: \\d+.\\d+% of statements/ COVERAGE=$COVERAGE/\\%/.0% fi echo coverage: $COVERAGE of statementselse COVERAGE=0.0% echo coverage: 0.0% of statementsfi# 将 COVERAGE 的百分号去掉，只保留数字export COVERAGE_PERCENT=$(echo $COVERAGE | sed s/%//) 2. 用最小的维护代价提供最大价值的单元测试 如何评价一个单元测试价值是否足够大呢？或者，更简单的说法是，如何评价一个单元测试写得好不好？ 可以从 4 个角度进行评估： protection against regressions resistance to refactoring fast feedback maintainability 更具体地说： 2.1 防止回归 代码修改后，原有功能不受影响。 评价指标： 被测试代码执行到的业务代码数量（测试覆盖率）。 业务代码的复杂度。 业务代码的领域重要性。 2.2 抵抗重构 非功能性重构，测试仍能通过，确保功能一致性。 评价指标： 越少的“假阳性”越好。 在重构代码时，引入了破坏性变更，测试代码能否快速反馈，即越少的“假阴性”越好。 测试代码是否为你重构代码提供了足够的信心。 测试代码测试的是业务代码的 observable behavior，而不是其背后的每一个步骤。 2.3 快速反馈 测试代码执行时间越快，则反馈间隔越短，缺陷修复效率和质量就越高。 评价指标： 代码执行速度 2.4 可维护性 测试代码的修改成本，可维护的测试代码更有利于适应需求变更。 评价指标： 测试代码有多难理解？ 测试代码的代码行数有多少？ 测试代码的执行难度有多高？即有多少的外部依赖？ 2.5 如何权衡 单元测试的价值可以通过上述 4 个指标的乘积来进行估算，但现实是，这 4 者，往往无法兼得。那我们如何做权衡呢？ 首先回顾「为什么要写单元测试」，核心目的是为了程序逻辑正确性、使软件项目更可持续发展。所以： 可维护性（maintainability）是不可商量的，必须要撰写可维护的测试代码。 抵抗重构（resistance to refactoring）是不可商量的，我们的测试代码应尽可能对错误的逻辑进行告警，也应避免对正确的逻辑进行误告警。 所以我们能权衡的其实就是 protection againts regressions 和 fast feedback，二者的矛盾很清晰： 如果执行的代码越多，相应的效率就越低。 如果执行的代码太少，那验证的逻辑范围就越小。 为了权衡这二者，业界提出了“测试金字塔”的概念。 单元测试的单位更小，涉及的外部依赖也更少，更加 fast feedback，所以在这个层次我们要撰写更多的测试，去尽可能覆盖更多的单元逻辑。 集成测试、端到端测试的逻辑覆盖范围更大，更加 resistance to refactoring，但是往往会依赖更多的组件，执行的效率也更低，所以在这 2 个层次，我们可以只撰写覆盖最重要（乐观）的业务路径的测试代码，在牺牲有限的执行效率的情况下，尝试更大的防止回归效果。 3. 验证代码中最重要的部分 什么是代码中最重要的部分呢？我们可以将代码分成以下 4 个种类： 领域模型和算法（Domain Model and Algorithms）：领域模型是对业务领域核心概念和逻辑的抽象，算法则是解决特定问题的计算步骤。两者共同构成系统的核心业务逻辑。 琐碎代码（Trivial Code）：实现简单功能、无复杂逻辑的代码片段，通常为工具方法或数据转换层。 控制器（Controllers）：协调业务逻辑与外部交互的中间层，常见于 MVC 或分层架构中。 过度复杂代码（Overcomplicated Code）：既包含核心业务逻辑，又包含控制器逻辑。 作者建议： 永远为 Domain Model and Algorithms 撰写全面细致的单元测试。 永远不为 Trivial Code 撰写单元测试。 为 Controllers 撰写集成测试，而不是单元测试。 避免写 Overcomplicated Code，将其拆分成 Domain Model and Algorithms 和 Controllers 。 如何让代码更容易测试？ 根据不同处理架构的业务代码，可以将测试代码分成以下 3 个种类： output-based：业务代码只产生输出结果，所以只需要验证输出。 state-based：业务代码会修改内部状态或依赖状态，所以需要验证状态变化。 communication-based：业务代码会跟协作方进行交互，所以需要验证交互情况。对于这种场景，我们会使用 mock 工具来进行验证。关于 mock 这个话题，文章后续会进行详细讨论。 我们按照上述 4 个分析维度，对这 3 种测试代码进行比较： protection againts regressions resistance to refactoring fast feedback maintainability output-based ⭐️⭐️⭐️ ⭐️⭐️⭐️ ⭐️⭐️⭐️ ⭐️⭐️⭐️ 最好，不需要外部依赖。 state-based ⭐️⭐️⭐️ ⭐️⭐️ ⭐️⭐️⭐️ ⭐️⭐️ 比较差，需要外部依赖。 communication-based ⭐️⭐️ 过度使用会导致需要到处 mock，而真正执行的业务代码数量很少。 ⭐️ 最差，因为验证交互情况，往往会陷入实现细节，很容易在重构过程中出现误警告。 ⭐️⭐️ 大差不差，但是 mock 工具效率可能会相对低一点点。 ⭐️ 最差，需要引入大量的 mock 工具和 mock 代码。 所以我们应该尽可能写 output-based 测试，减少 communication-based 测试。 可以采取 functional architecture，将代码分成 2 个阶段： 根据业务规则做出决定 根据决定做出行为 为此，在可能的场景下，我们可以尝试通过 2 个步骤来优化我们的测试代码： 使用 mock 来替代外部依赖 out-of-process dependency。 使用 functional architecture 来替代 mock。 聊一下 Mock 在撰写单元测试的过程中，如果业务逻辑依赖的组件不好实例化的时候，我们常常会借助各种 Mock 工具来实现“模拟”功能，使单测更易撰写，这里有一个更准确的词叫 test doubles（测试替身）。 test double 的种类 从大的方面可以分为 2 种： 用于模拟和验证对象间的输出交互（如方法调用次数、参数匹配），则为 mock。 用于模拟输入交互，提供预定义的数据，则为 stub。 更进一步可以分为： mock mock: 由 mock 工具生成。 spy: 手工撰写。 stub stub: 可以通过配置在不同的场景下返回不同的数据。 dummy: 占位符，仅用于填充参数，不参与实际逻辑。 fake: 跟 stub 几乎一样，唯一的区别是 fake 经常用于替代尚未开发或复杂的依赖。 需要注意的是：永远不要去验证（assert）跟 stub 的交互，没必要！ 哪些东西需要 Mock？ 在回答这个问题之前，我们先做下铺垫，聊一下接口的误解、依赖的种类和两种交互的概念。 接口的误解 在谈如何更好地利用 mock 之前，我们先来聊一下接口（interface）的误解。 在业务开发当中，我们经常能看到一些企图进行“优雅”架构设计的代码，上来每一层都定义接口，每一层都使用接口进行交互，反正遇到问题先定义接口再说。 目的有二： 抽象外部依赖，进行解耦。 可以在不修改既有代码的情况下扩展功能，即所谓的开闭原则（Open-Closed principle）。 但这其实存在一些误区，作者在书中指出： 只有一个实现的接口，并不是抽象，也并没有比具体的对象起到太多所谓的解耦作用。 上述第 2 点违反了一个更重要的原则 YAGNI（You are not gonna need it），也就是你所谓的功能扩展大概率是不需要的。 上述做法的唯一好处是什么：使测试成为可能！因为你不隔离掉外部依赖的话，你的单元测试撰写会非常困难，也无法做到 fast feedback。 🙋🏻‍♀️抽象是发现出来的，而不是发明出来的！ 依赖的种类 shared dependency: 一个在测试代码中的共享对象。 out-of-process dependency: 独立于当前应用程序的另外一个进程对象，如数据库、STMP 服务器等。 managed dependency: 仅当前应用程序可访问的依赖（对其他程序、服务是不可见的）。 unmanaged dependency: 除了当前应用，其他应用也可见。 private dependency: 一个私有对象。 两种交互 intra-system communication: 应用程序内部的交互。 inter-system communication: 应用程序之间的交互。 哪些东西需要 Mock？ 铺垫完接口的误解、依赖的种类和两种交互的概念之后，我们来聊一下哪些东西需要 Mock？ 在抉择的时候，需要牢记我们测试粒度和评价指标。 测试粒度：an observable behavior ⭐️⭐️⭐️⭐️⭐️ 评价指标： 防止回归：protection against regressions 抵抗重构：resistance to refactoring 快速反馈：fast feedback 可维护性：maintainability 集合测试粒度和评价指标，Mock 哪些东西可以用一句话来概括： ✅Mock 那些外部可观测到的交互，而尽量避免 Mock 内部的实现细节。 更具体来说： 仅对 unmanaged dependency 应用 mock 对象。因为我们无法预知其他应用会对这些依赖进行什么操作，所以只能隔离开。 对系统最外围的边界进行 mock。只有系统边界，才是可观测行为，内部都是实现细节，对实现细节过多 Mock，意味着破坏了 resistance to refactoring。 尽量只在集成测试中使用 mock，避免在单元测试中使用 mock。 只 mock 属于你的对象，不去 mock 依赖库中的对象。 始终在第三方库之上编写自己的适配器，并对这些适配器进行 mock，而不是 mock底层类型。 仅从库中暴露你所需要的功能。 使用项目的领域语言（domain language）来完成上述操作。 举个例子： 在上图中，右侧是我们的应用程序，它依赖了左下角的 Message bus 这个外部依赖，准确说是 unmanaged dependency。对此，我们为其创建了适配器接口 IBus，在这个通用接口之上，我们又根据具体业务创建了 IMessageBus。 针对这种情况，我们在进行 mock 的时候，只需要 mock IBus 对象，而不是去 mock Message bus 和 IMessageBus。 数据库要不要 Mock？ 这个话题比较有意思，作者的建议是： 如果这个数据库只有你这个应用可以访问，那就不要 mock。 如果这个数据库存在可以被其他应用访问的部分，那就只 mock 这一部分，不去 mock 独属于你应用的那部分。 要践行上述标准，需要做到以下前提： 将数据库的信息也放在源码控制系统中（git），包括： schema reference data（项目启动必须要的初始数据） migration（数据变更记录） 每个开发者有一个单独的数据库（测试环境下） 但数据库变更的时候，不要直接修改，而是要写一条对应 sql 去进行修改，同时将这条 sql 也纳入源码控制系统中。 作者不建议 mock 数据库，包括使用内存数据库替代，如 sqlite 替代 MySQL，核心原因是：你无法保证这些数据库能跟线上环境的行为一致，可能会导致一些无效测试用例，即假阴性。 笔者并不完全采纳这个建议，诚然，如果能做到以上前提，是可以考虑践行的。然而，它的要求很高，收益却相对较小，在单元测试环境下，使用内存数据库进行 mock，在保证了 fast feedback 和 maintainability 的情况下，也能够避免绝大多数的逻辑漏洞了，假阴性的情况会非常少，即便有，也可以交给集成测试和端到端测试去解决。 反面案例 测试私有方法。 暴露私有状态。 泄露领域知识到测试中。 在业务代码中撰写只用于测试的代码。 mock 具体的类。 第 3 点比较有意思，比如下面这个例子： public class CalculatorTests [Fact] public void Adding_two_numbers() int value1 = 1; int value2 = 3; int expected = value1 + value2; // -----The leakage // int expected = 4 // the better one int actual = Calculator.Add(value1, value2); Assert.Equal(expected, actual); 什么叫做泄露领域知识呢？ 比如你要验证一个加法 Add 对不对，但是在测试代码中，你的期望值也是用加法来获得的，这个“加法”就是领域知识，因为这样测的话，就很有可能会出现“负负得正”的情况。 正确的做法是直接断言你预期的最终结果，以确保逻辑符合预期。 Go 实践案例 本章将分享一些笔者在 Go 项目实战过程中的一些实践案例，希望对读者撰写单元测试能提供一些帮助。 依赖 Redis 的逻辑怎么测 可以使用 miniredis，这是一个使用 Go 语言实现的内存版 Redis。 https://github.com/alicebob/miniredishttps://github.com/alicebob/miniredis 可以封装一个函数，用于快速启动 miniredis 并返回客户端对象： func NewMiniRedis() *redis.Client var redisClient *redis.Client var miniRedisClient *miniredis.Miniredis var err error miniRedisClient, err = miniredis.Run() if err != nil panic(err) redisClient = redis.NewClient(redis.Options Addr: miniRedisClient.Addr(), ) return redisClient 这里可能会出现作者提到的不要使用内存数据库替代真实的数据库，因为你无法保证它们的行为一致。 比如这里是单机的，而生产环境可能是集群的，在 Redis Cluster 中，涉及到 lua 脚本和事务的所有 key，都必须保证在同一个 slot 上，在这种情况下，使用 miniredis 是测不出问题的。 依赖 MySQL 的逻辑怎么测 核心挑战： 依赖真实 MySQL 则容易因为网络原因而导致测试失败（不可重复性） 依赖真实 MySQL 会严重影响单侧执行效率 数据预备 数据清洗 单测之间的数据隔离，互不影响 并发安全 为了解决上述问题，提供更优雅的 MySQL 单测解决方案，笔者借助 dolthub/go-mysql-server 和 gorm 的能力，实现了一个 go-mysql-mocker，简称 gmm。 其中： dolthub/go-mysql-server 提供了内存 MySQL 引擎。 gorm 提供了快速建表和插入数据的能力。 https://github.com/hedon954/go-mysql-mockerhttps://github.com/hedon954/go-mysql-mocker 核心功能： 内存版数据库，无网络依赖； 每个单测可单独启动一个数据库，天然做到数据隔离和清洗； 支持 struct、slice、sql stmt、sql file 多种方式进行数据初始化，支持需要前置数据的业务逻辑测试。 随机概率逻辑怎么测 场景：随机抽奖 难点：随机概率的结果是不确定的，直接通过 assert.Equal 是无法写出可稳定重复运行的单测的。 // AssertMapRatioEqual 检查实际计数的比例是否符合预期权重的比例// actual: 实际获得的计数 map[id]count// expected: 预期的权重 map[id]weight// tolerance: 允许的误差范围（如 0.05 表示允许 5% 的误差）func AssertMapRatioEqual(t *testing.T, actual map[int64]int64, expected map[int64]int64, tolerance float64) t.Helper() // 计算总数 var actualTotal, expectedTotal int64 for _, count := range actual actualTotal += count for _, weight := range expected expectedTotal += weight // 检查每个 ID 的比例 for id, expectedWeight := range expected actualCount, exists := actual[id] if !exists t.Errorf(ID %d 在实际结果中不存在, id) continue expectedRatio := float64(expectedWeight) / float64(expectedTotal) actualRatio := float64(actualCount) / float64(actualTotal) if diff := math.Abs(expectedRatio - actualRatio); diff tolerance t.Errorf(ID %d 的比例不符合预期: 期望 %.3f, 实际 %.3f, 差异 %.3f, 超出允许误差 %.3f, id, expectedRatio, actualRatio, diff, tolerance) // 检查是否有多余的 ID for id := range actual if _, exists := expected[id]; !exists t.Errorf(实际结果中存在未预期的 ID: %d, id) 案例： func TestLottery_randOnce(t *testing.T) t.Run(大量抽取应符合权重配置比例, func(t *testing.T) gotCount := make(map[int64]int64) totalCount := int64(10000) for i := int64(0); i totalCount; i++ reward, err := lotteryOnce(0, 0, nil) assert.Nil(t, err) assert.NotNil(t, reward) gotCount[reward.Id] += 1 expectedRatio := map[int64]int64 1: 10, 2: 20, 3: 30, 4: 40, 5: 40, testutil.AssertMapRatioEqual(t, gotCount, expectedRatio, 0.05) ) HTTP 接口怎么测 挑战： 如何快速构建请求体并发送请求？ 如何快速断言异常情况？ 如何快速断言成功情况，并解析出期望的返回值？ 1. 构造请求 // 快速创建请求体 form 表单格式func NewHTTPPostRequest(path string, data any) *http.Request req := httptest.NewRequest(POST, path, NewHTTPBody(data)) req.Header.Set(Content-Type, application/x-www-form-urlencoded) return reqfunc NewHTTPBody(data any) io.Reader values := url.Values v := reflect.ValueOf(data) t := v.Type() if v.Kind() == reflect.Ptr v = v.Elem() t = v.Type() if v.Kind() != reflect.Struct return strings.NewReader(values.Encode()) for i := 0; i t.NumField(); i++ field := t.Field(i) value := v.Field(i) tag := field.Tag.Get(form) if tag == continue // 处理复杂类型（结构体、切片、map） switch value.Kind() case reflect.Struct, reflect.Slice, reflect.Map: jsonBytes, err := json.Marshal(value.Interface()) if err == nil values.Set(tag, string(jsonBytes)) continue else log.Printf(Error marshaling %v: %v, value.Kind(), err) default: // 处理其他类型 values.Set(tag, fmt.Sprintf(%v, value.Interface())) return strings.NewReader(values.Encode()) 2. 发送请求 w := httptest.NewRecorder()sfRouterTest.ServeHTTP(w, request) 3. 断言异常 // AssertRspErr 断言 http 响应异常，expectedErr 为期望的错误信息func AssertRspErr(w *httptest.ResponseRecorder, t *testing.T, expectedErr string) assert.Equal(t, http.StatusOK, w.Code) body := w.Result().Body defer body.Close() rsp, err := FromHTTPResp[any](body) assert.Nil(t, rsp) assert.Equal(t, expectedErr, err.Error()) 4. 断言正确且返回响应值 // FromHTTPResp 从 http 响应中解析出数据func FromHTTPResp[T any](resp io.ReadCloser) (*T, error) body, err := io.ReadAll(resp) if err != nil return nil, err defer func() _ = resp.Close() () var t httpResp[T] err = json.Unmarshal(body, t) if err != nil return nil, err if t.Code != 200 return nil, errors.New(t.Message) return t.Data, nil// AssertRspOk 断言 http 响应成功，并返回响应体 Tfunc AssertRspOk[T any](w *httptest.ResponseRecorder, t *testing.T) *T assert.Equal(t, http.StatusOK, w.Code) body := w.Result().Body defer body.Close() rsp, err := FromHTTPResp[T](body) assert.Nil(t, err) assert.NotNil(t, rsp) return rsp 这里其实就违反了上一张反面案例中的第 3 点”泄露领域知识到测试中“，因为这里接受响应的时候，还是使用的领域对象结构，所以可能会出现负负得正的情况，比如你的对象字段名就是拼写错误了，但是因为你业务逻辑和断言处都是用的一个结构，所以内部形成了循环，就负负得正了，但是真正到了客户端那，就解析失败了。 不过在这个情况下，笔者认为这个情况下的这种风险是可以接受的，远盖不住其带来的效率提升。 5. 组合起来 func SendHTTPRequest[Rsp any](t *testing.T, server HTTPServer, path string, data any, errMsg ...string) *Rsp req := NewHTTPPostRequest(path, data) w := httptest.NewRecorder() server.ServeHTTP(w, req) if len(errMsg) 0 AssertRspErr(w, t, errMsg[0]) return nil else return AssertRspOk[Rsp](w, t) 6. 案例 func Test_GetCollectReward(t *testing.T) t.Run(重复领取, func(t *testing.T) uid := buildUserInfo(UserInfo Got: map[int][]int 1: 1, , , apiTest.svc) _ = testutil.SendHTTPRequest[GetCollectRewardResp](t, routerTest, /get_collect_reward, GetCollectRewardReq UID: uid, CollectID: 1, , 重复领取) // 错误信息 ) t.Run(领取成功, func(t *testing.T) uid := uuid.NewString() rsp := testutil.SendHTTPRequest[GetCollectRewardResp](t, routerTest, /get_collect_reward, GetCollectRewardReq UID: uid, CollectID: 1, , ) assert.NotNil(t, rsp.Reward) // 正确结果 ) 依赖时间的逻辑怎么测 尽量不要依赖时间。 考虑将时间作为参数，避免 time.Now()。 也可以参考： https://hedon.top/2025/03/06/go-lib-synctest/https://hedon.top/2025/03/06/go-lib-synctest/ 并发逻辑怎么测 go test 推荐开启 -race 用于检测并发冲突。 更多可参考： https://hedon.top/2025/03/06/go-lib-synctest/https://hedon.top/2025/03/06/go-lib-synctest/","tags":["读书笔记","单元测试"],"categories":["读书笔记"]},{"title":"书籍摘抄丨《Unit Testing Principles, Practices, and Patterns》","path":"/2025/04/09/note-unit-testing-excerpt/","content":"Learning unit testing doesn’t stop at mastering the technical bits of it, such as your favorite test framework, mocking library, and so on. There’s much more to unit testing than the act of writing tests. You always have to achieve the best return on the time you invest in unit testing, minimizing the effort you put into tests and maximizing the benefits they provide. Achieving both things isn’t an easy task. They grow effortlessly, don’t require much maintenance, and can quickly adapt to their customers’ ever-changing needs. The ratio between the production code and the test code could be anywhere between 1:1 and 1:3. Coverage limitations You can’t guarantee that the test verifies all the possible outcomes of the system under test. No coverage metric can take into account code paths in external libraries. The goal of unit testing lead to a better code design enable sustainable(可持续的) growth of the software project the cost components of writing unit tests: refactoring the test when you refactor the underlying code running the test on each code change dealing with false alarms raised by the test spending time reading the test when you’re trying to understanding how the underlying code behaves a successful test suite must: integrated into the development cycle targets only the most important parts of the code base 👉🏻 domain logic infrastructure code external services and dependencies code that glues everything together provides maximum value with minimum maintenance costs recognize a valuable test (and, by extension, a test of low value) write a valuable test What is a unit test? verifies a single unit of behavior dose it quickly dost it in isolation from other tests An integration test, then, is a test that doesn’t meet one of these criteria. End-to-end tests are a subset of integration tests. How to structure a unit test? Type Components AAA Arrange - Act - Assert GWT Given - When - Then avoid multiple arrange, act, and assert sections. avoid if statements in tests. name the test as if you were describing the scenario to a non-programmer who is familiar with the problem domain. separate words with underscores. structure a test is to make it tell a story about the problem domain Four pillars of a good unit test code is not an asset, it’s a liability. protection against regressions the amount of code that is executed during the test the complexity of that code the code’s domain significance resistance to refactoring the fewer false positives the test generates, the better tests provides an early warning when you break exisiting functionality you become confident that your code changes won’t lead to regressions the more the test is coupled to the implementation details of the system under set(SUT), the more false alarms it generates you need to make sure the test verifies the end result the SUT delivers: its observable behavior, not the steps it takes to do that. the best way to structure a test is to make it tell a story about the problem domain fast feedback maintainability how hard it is to understand the test, which is a function of the test’s size how hard it is to run the test, which is a function of how many out-of-process dependencies the test works with directly The intrinsic connection between the first two attributes Type II Error: if functionality is broken, the test should fail, but if the test also passed, means it is not a good unit test, should if it fails, means it offers protection against regressions. Type I Error: if functionality is correct but the test fails, means that the test dose not test the nature of behavior. The good unit test should always pass when the functionality is correct. This would help us a lot when we try to do refactor. If we refactor the code correctly, but the unit tests always failed, means that the unit tests are not good enough, we need to optimize them. An ideal test value = [0..1] * [0..1] * [0..1] * [0..1](corresponding to the four pillars) black-box and white-box testing choose black-box testing over white-box testing by default. the only exception is when the test covers utility code with high algorithmic complexity use code coverage tools to see which code branches are not exercised, but then turn around and test them as if you know nothing about the code’s internal structure. Mock types of test doubles mock: help to emulate and examine outcoming interactions —— change state mock: generated by tools spy: written manually stub: help to emulate incoming interactions —— get input data stub: can configure to return different values for different scenarios. dummy: a simple, hardcoded value such as a null value or a made-up string. fake: the same as a stub for most purposes, only except for its creation, it is usually implemented to replace a dependency that dose not yet exist never asserting interactions with stubs. Observable behavior expose an operation that helps the client achieve one of its goals. An operation is a method that performs a calculation or incurs a side effect or both. expose a state that helps the client achieve one of its goals. State is the current condition of the system. Whether the code is observable behavior depends on who its client is and what the goals of that client are. Ideally, the system’s public API surface should coincide with its observable behavior, and all its implementation details should be hidden from the eyes of the clients. Mocks and test fragility Intra-system communications are communications between classes inside your application. Inter-system communications are when your application talks to other applications. The use of mocks is beneficial when verifying the communication pattern between your system and external applications. Using mocks to verify communications between classes inside your system results in tests that couple to implementation details and therefore fall short of the resistance-to-refactoring metric. Types of dependencies shared dependency: a dependency shared by test (not production code) out-of-process dependency: a dependency hosted by a process other than the program’s execution process (database, stmp server) private dependency: any dependency that is not shared Styles of unit testing output-based: only need to verify the output. state-based: the underlying code changes its own state, the state of its collaborators, or the state of an out-of-process dependency. communication-based: use mocks to verify communications between the SUT and its collaborators, to verify the communication situations. compare protection against regressions for the most part, they are not very different but overusing the communication-based style can result in shallow tests that verify only a thin slice of code and mock out everything else. fast feedback for the most part, they are not very different communication-based testing can be slightly worse because the cost of mocks. resistance to refactoring state-based is the best one. communication-based is the worse one, because it is the most vulnerable to false alarms. maintainability output-based is the best one, because they do not deal with out-of-process dependencies. state-based is less maintainable because state verification takes up more space than output verification. communication-based is the worst one, it requires setting up test doubles and interaction assertions, and that takes up a lot of space. functional architecture *Functional architecture* maximizes the amount of code written in a purely functional (immutable) way, while minimizing code that deals with side effects. Immutable means unchangeable: once an object is created, its state can’t be modified. This is in contrast to a *mutable* object (changeable object), which can be modified after it is created. Separate two kinds of code: code that make a decision code that acts upon that decision Tips Moving from using an out-of-process dependency to using mocks. Moving from using mocks to using functional architecture Four kinds of code Domain model and algorithms Trivial code Controllers Overcomplicated code Tips always write completed unit tests for domain model the algorithms code never test trivial code write integration test for controllers do not write overcomplicated code, try to separate it into domain model and algorithms and controllers Trade-off domain model testability controller simplicity performance push all external reads and writes to the edges anyway inject the out-of-process dependencies into the domain model split the decision-making process into more granular steps 👈 CanExecute/Execute pattern domain events CanExecute/Execute pattern You can use CanExecute/Execute pattern to balance the performance and testability , but concedes controller simplicity, but it is manageable in most cases. Domain events Domain events help track important changes in the domain model, and then convert those changes to calls to out-of-process dependencies. This pattern removes the tracking responsibility from the controller. extract a DomainEvent base class and introduce a base class for all domain classes, which would contain a collection of such events: ListDomainEvent events Integration tests check as many of the business scenario’s edge cases as possible with unit tests use integration tests to cover one happy path, as well as any edge cases that can’t be covered by unit tests if there’s no one path that goes through all happy paths, write additional integration tests—as many as needed to capture communications with every external system attempt to apply the fail-fast principle as a viable alternative to integration test. two types of out-of-process dependencies managed dependencies: only accessible through your application. it is implement details and should not be mock. unmanaged dependencies: you don’t have full control over it. It is observable behavior and you should mock it. interface misunderstand 🙋🏻‍♀️ Genuine abstractions are discovered, not invented. 👉🏻 For an interface to be a genuine abstraction, it must have at lease two implemtations. The common reasoning behind the use of interfaces is that they help to: Abstract out-of-process dependencies, thus achieving loose coupling. Add new functionality without changing the existing code, thus adhering to the Open-Closed principle Misconceptions: Interfaces with a single implementation are not abstractions and don’t provide loose coupling any more than concrete classes that implement those interfaces. The second reason violates a more foundational principle: YAGNI (You are not gonna need it). The only reason to use interfaces for out-of-process dependencies it is to enable testing! Do not introduce interfaces for out-of-process dependencies unless you need to mock out those dependencies. integration test best practices making domain model boundaries explicit reducing the number of layers in the application eliminating circular dependencies maximozing mock’s value when mocking, always try to verify interactions with unmanaged dependencies at the very edges of your system. Mocking IBus instead of IMessageBus maximizes the mock’s protection against regressions. A call to an unmanaged dependency goes through several stages before it leaves your application. Pick the last such stage. It is the best way to ensure backward compatibility with external systems, which is the goal that mocks help you achieve. In some cases, you can use spy instead of mock for more succinct and expressive. [Fact]public void Changing_email_from_corporate_to_non_corporate() var busSpy = new BusSpy(); var messageBus = new MessageBus(busSpy); var loggerMock = new MockIDomainLogger(); var sut = new UserController(db, messageBus, loggerMock.Object); /* ... */ busSpy.ShouldSendNumberOfMessages(1) .WithEmailChangedMessage(user.UserId, new@gmail.com); mocking best practices applying mocks to unmanaged dependencies only verifying the interactions with those dependencies at the very edges of your system using mocks in integration tests only, not in unit test always verifying the number of calls made to the mock do not rely on production code when making assertions. Use a separate set of literals and constants in tests. mocking only types that you own always write your own adapters on top of third-party libraries and mock those adapters instead of the underlying types. only expose features you need from the library do that using your project’s domain language this guideline dose not apply to in-process dependencies. There is no need to abstract in-memory or managed dependencies. Similarly, there’s no need to abstract an ORM as long as it’s used for accessing a database that isn’t visible to external applications. Testing the database prerequisites keeping the database in the source control system database schemas reference data using a separate database instance for every developer applying the migration-based approach to database delivery applying every modification to the database schema (including reference data) through migrations. Do not modify migrations once they are committed to the source control. If a migration is incorrect, create a new migration instead of fixing the old one. Make exceptions to this rule only when the incorrect migration can lead to data loss. transaction split the Database class into repositories and a transaction: repositories are classes that enable access to and modification of the data in the database. transaction is a class that either commits or rolls back data updates in full. This will be a custom class relying on the underlying database’s transactions to provide atomicity of data modification. tips use at least three transactions or units of work in an integrations test: one per each arrange, act and assert section. your tests should not depend on the state of the database. Your tests should bring that state to the required condition on their own. create two collections for unit and integrations, and then disable test parallelization in the collection with the integration test. clean up data at the beginning of a test write the SQL script manually. It’s simpler and gives you more granular control over the deletion process. the best way to shorten integration is by extracting technical, non-business-related bits into private methods or helper classes. only the most complex or important read operations should be test, disregard the rest. do not test repositories directly, only as part of the overarching integration test suite. Unit testing anti-patterns ⚠️ Do not do the things like below! unit testing private methods Private methods are implementation details! Just test observable behaviors! If the private method is too complex to be tested as part of the public API that uses it, that’s an indication of a missing abstraction. Extract this abstraction into a separate class instead of making the private method public. expose private state leaking domain knowledges to tests public class CalculatorTests [Fact] public void Adding_two_numbers() int value1 = 1; int value2 = 3; int expected = value1 + value2; // -----The leakage // int expected = 4 // the better one int actual = Calculator.Add(value1, value2); Assert.Equal(expected, actual); code pollution Code pollution is adding production code that’s only needed for testing. mocking concrete classes working with time","tags":["单元测试","书籍摘抄"],"categories":["书籍摘抄","单元测试"]},{"title":"一步步推导出 MySQL 数据的底层存储结构","path":"/2025/04/08/mysql-ibd/","content":"以下均以 InnoDB 引擎为基础进行分析。假设我们现在有 3 行数据，如下： 其中： id 是主键索引。 a 和 b 都是数据字段。 tx_id 是隐藏字段，表示事务 id，用于实现 MVCC。 rollback_ptr 是回指针，用于 undo log。 在将数据存储到文件的时候，我们会将这三行数据进行序列化，然后以二进制流的形式存储到文件中。 现在我们要解决第一个问题： 如何按照主键（id）排序？ 在 InnoDB 中，会在每一行的前面，加一个 next_record 字段，用于指向比当前数据 id 大的下一条数据，我们假设一行数据占 20 个字节，那么就如下图所示： 另外，为了便于定位每一行，InnoDB 会在每一行前面再加一个字段 heap_no，它的规则很简单，就是自增，在内部会用于定位一行记录，方便上锁等各种操作。 所以现在的存储结构如下图所示： 现在我们来解决解决第二个问题： 如何快速定位到起点（最小）和终点（最大）？ 在最前面加 2 条特殊的记录： PAGE_NEW_INFIMUM：指向最小记录。 PAGE_NEW_SUPERMUM：最大记录，最大的一个 id 会指向它。 第三个问题： 每次 select * from t where id = ? 都要进行 I/O 操作吗？ 很显然是不行的，效率太低了。这个相信绝大多数读者都知道 ，InnoDB 会以 Page（默认 16KB）为最小单位，一次性将数据从磁盘加载到内存中。为此，需要在最前面再加一条记录，且该记录的前三行分别为： page_no：页号，自增，InnoDB 最多支持 32 位页号，所以存储上限是 16KB * 232 = 64T。 prev_page：指向上一页。 next_page：指向下一页。 如果一个 Page 放不下呢？ 很显然，那就要进行分页，即按照 ID 的顺序进行一分为二，前者取范围 [a, b)，后者取范围 [b, c)。 如何快速定位到数据在哪个 Page 上呢？ 这个时候，我们需要新创建一个 Page，专门用于管理这些数据 Page 的，这个 Page 我们这里暂且称为索引 Page。 其中核心数据就是 2 个： min_id：即当前页存储的最小主键 ID。 page_no：页号，用于定位到 Page。 这是什么呀？这其实就是 B+ 树！在文件层面的存储，是连续存储的，但是为了便于理解，我们可以在逻辑层面将其绘制成 B+ 树的形态。如下图可以看到这其实就是一颗 B+ 树。 在主键索引树上： 叶子节点存储的就是具体某一行的数据（聚簇索引）。 非叶子节点存储的是索引。 每一层的节点，都是一条有序的双向链表。 如果对非主键索引 a 创建索引呢？ 因为要建索引，所以需要先对 a 进行排序，然后针对 a 建立一颗 b+ 树。而且由于 a 是非主键索引，即辅助索引，所以叶子节点存储的是主键的值，用于回表。 假设 a =15 的数据非常多，一个 page 放不下呢？ 会加入主键 ID 作为二维排序来进行分裂： 估算一下一个三层的 B+ 树可以存储多少条数据？ 一个 Page 是 16KB 假设 1 个 Page 可以存放 1000 个 key 假设 1 个 Page 可以存放 200 条记录 基于这种估算： 第 1 层：1 个节点是 1 个 Page，存放 1000 个 key，对应 1000 个分叉 第 2 层：1000 个节点 1000 个 Page，存放 1000*1000 个 Key，对应 1000*1000 个分叉 第 3 层：1000*1000 个 Page，每个 Page 200 条数据，共 1000*1000*200=2 亿条数据 = 16KB*1000*1000=16GB 🐂🐂🐂","tags":["mysql"],"categories":["数据库","mysql"]},{"title":"在 Hexo 博客中优雅地集成 Markmap 思维导图","path":"/2025/03/17/mindmap-for-hexo/","content":"在技术博客写作中，思维导图是一个非常有用的工具，它可以帮助我们更清晰地展示知识结构和概念关系。本文将介绍如何在 Hexo 博客中集成 Markmap，让你能够直接在 Markdown 文件中创建交互式思维导图。 什么是 Markmap？ Markmap 是一个将 Markdown 格式的文本转换为思维导图的开源工具。它允许我们使用熟悉的 Markdown 语法来创建漂亮的、交互式的思维导图。 https://markmap.js.org/https://markmap.js.org/ 实现方案 1. 安装必要依赖 首先，我们需要安装 uuid 包，这是用来给我们每一个思维导图生成一个唯一的 ID： npm install uuid --save 2. 创建自定义标签插件 在 scripts/markmap_tag.js 中创建自定义标签： /** * Markmap Tag Plugin for Hexo */use strict;const v4: uuidv4 = require(uuid);hexo.extend.tag.register( markmap, function (args, content) const id = uuidv4(); return `div class=markmap id=markmap-$id script type=text/template$content /script/divscriptdocument.addEventListener(DOMContentLoaded, () = const template = document.querySelector(#markmap-$id script[type=text/template]); if (template) const content = template.textContent; window.markmap.autoLoader.renderString(content, null, document.querySelector(#markmap-$id)); );/script `; , ends: true ); 3. 添加样式 创建 source/css/markmap.css： /* Markmap Styles */.markmap width: 100%; height: 500px; margin: 20px 0; border: 1px solid #eaeaea; border-radius: 5px; overflow: hidden;.markmap svg width: 100%; height: 100%;/* 响应式设计 */@media (max-width: 768px) .markmap height: 400px; @media (max-width: 480px) .markmap height: 300px; 4. 更新主题配置 在主题配置文件中添加必要的资源引用： inject: head: - link rel=stylesheet href=/css/markmap.css - script src=https://cdn.jsdelivr.net/npm/markmap-autoloader@0.18/script 使用方法 代码块效果% markmap %- 技术栈 - 前端 - Vue.js - React - Angular - 后端 - Node.js - Python - Go - 数据库 - MySQL - MongoDB - Redis% endmarkmap % svg.markmap { width: 100%; height: 500px; } - 技术栈 - 前端 - Vue.js - React - Angular - 后端 - Node.js - Python - Go - 数据库 - MySQL - MongoDB - Redis 工作原理 1. Hexo 插件系统 Hexo 提供了强大的插件系统，允许我们通过 hexo.extend API 来扩展功能 我们使用了 hexo.extend.tag 来注册自定义标签，这是 Hexo 提供的标准扩展点之一 2. 标签插件的工作原理 hexo.extend.tag.register( markmap, function (args, content) const id = uuidv4(); // 生成唯一ID return ` div class=markmap id=markmap-$id script type=text/template $content /script /div ... `; , ends: true ); 当 Hexo 解析到 markmap 标签时，会调用这个注册的函数 content 参数包含了标签之间的所有内容（你的 markdown 结构） ends: true 表示这是一个闭合标签（需要 endmarkmap 结束） 3. Markmap 库的渲染过程 Markmap 库使用 markmap-autoloader 自动处理 markdown 到思维导图的转换 转换过程： Markdown 文本被解析成层级结构 层级结构被转换为 SVG 路径 SVG 被渲染到页面上，并添加交互功能 4. HTML 结构设计 div class=markmap id=markmap-$id script type=text/template $content /script/div 使用 script type=\"text/template\" 来存储原始 markdown 每个思维导图都有唯一 ID，避免页面上多个图表互相干扰 5. JavaScript 初始化 document.addEventListener(DOMContentLoaded, () = const template = document.querySelector( #markmap-$id script[type=text/template] ); if (template) const content = template.textContent; window.markmap.autoLoader.renderString( content, null, document.querySelector(#markmap-$id) ); ); 等待页面加载完成 获取模板中的 markdown 内容 使用 markmap 库渲染思维导图 6. 样式控制 .markmap width: 100%; height: 500px; margin: 20px 0; border: 1px solid #eaeaea; border-radius: 5px; overflow: hidden; 提供响应式布局 确保思维导图在各种屏幕尺寸下都能正常显示 7. 主题集成 在主题配置中注入必要的 CSS 和 JavaScript 确保资源在正确的时机加载","tags":["hexo","markmap"],"categories":["小技术"]},{"title":"为什么 OpenTelemetry 的 SDK 中不支持尾采样 Hook？","path":"/2025/03/13/opentelemetry-tail-sampler/","content":"在分布式追踪系统中，采样策略直接影响着系统的性能和可观测性。OpenTelemetry 作为当前最流行的可观测性框架，其采样机制设计有着深刻的考量。本文将深入探讨 OpenTelemetry 的采样机制，特别是为什么它在 SDK 层面不支持尾采样。 前置采样 vs 尾采样 在讨论 OpenTelemetry 的采样机制前，我们需要理解两种主要的采样策略： 前置采样（Head-based Sampling）： 在链路开始时就决定是否采样 决策一旦做出，整个链路都遵循这个决策 不需要缓存完整的链路数据 尾采样（Tail-based Sampling）： 在链路结束后决定是否保留 可以基于完整链路信息（如总耗时、是否有错误）做决策 需要临时缓存所有链路数据 OpenTelemetry 的采样实现 通过分析 OpenTelemetry Go SDK 的源码，我们可以清晰地看到它采用的是前置采样策略。关键代码如下： func (tr *tracer) newSpan(ctx context.Context, name string, config *trace.SpanConfig) trace.Span // ... 前面的代码 ... // 执行采样决策 samplingResult := tr.provider.sampler.ShouldSample(SamplingParameters ParentContext: ctx, TraceID: tid, Name: name, Kind: config.SpanKind(), Attributes: config.Attributes(), Links: config.Links(), ) // 设置采样标志 if isSampled(samplingResult) scc.TraceFlags = psc.TraceFlags() | trace.FlagsSampled else scc.TraceFlags = psc.TraceFlags() ^ trace.FlagsSampled // ... 后面的代码 ... 这段代码揭示了几个关键点： 采样决策在 span 创建时就已经做出 采样标志通过位操作设置在 TraceFlags 中 这个标志会随着 SpanContext 传播到整个分布式系统 采样标志的传播机制 特别值得注意的是设置采样标志的代码： if isSampled(samplingResult) scc.TraceFlags = psc.TraceFlags() | trace.FlagsSampled else scc.TraceFlags = psc.TraceFlags() ^ trace.FlagsSampled 这段代码使用位操作来设置或清除采样标志： | 操作用于设置采样标志，保留其他标志位不变 ^ 操作用于清除采样标志，同样保留其他标志位不变 这确保了采样决策能够一致地传播到整个分布式链路中。 为什么 OpenTelemetry 不支持尾采样？ 最重要的原因是：在 SDK 中找不到尾巴！因为不知道链路什么时候结束！ 在分布式系统中，一条链路可能跨越多个服务，所以你在某一个服务中，是不知道链路是否结束的，而 OpenTelemetry 也不是一次性上报一整条链路，而是每个 span 独立上报，最后再拼接到一起。 OpenTelemetry 上报原理 独立上报 每个 span 在结束时（调用 span.End()）会被传递给 SpanProcessor SpanProcessor 决定如何处理这个 span（立即导出或批量导出） 导出是独立的，不会等待整个 trace 完成 批处理机制 默认使用 BatchSpanProcessor，它会收集一定数量的 spans 或等待一定时间然后批量导出 但这个批处理与 trace 完整性无关，只是为了效率 Collector 如何实现尾采样 Collector 通过以下方式解决这些问题： 设置等待时间窗口 为每个 trace 设置一个等待期（如 10 秒） 在此期间收集该 trace 的所有 spans 超过等待期后，基于已收集的 spans 做决策 集中式收集 所有服务的 spans 都发送到 Collector Collector 有更全面的视图来关联 spans 专门的资源分配：Collector 作为独立组件，有专门的资源处理这种复杂逻辑，不会影响应用性能。 如何在 OpenTelemetry 生态中实现尾采样？ 虽然 SDK 不直接支持尾采样，但 OpenTelemetry 生态提供了其他方式实现类似功能： 1. 使用 OpenTelemetry Collector Collector 提供了 Tail Sampling Processor，可以在数据聚合层实现尾采样： processors: tail_sampling: decision_wait: 10s num_traces: 100 expected_new_traces_per_sec: 10 policies: - name: error-policy type: status_code status_code: ERROR 2. 结合前置采样和错误捕获 可以实现一个智能的前置采样器，对特定场景（如包含错误属性）强制采样： type SmartSampler struct baseSamplingRate float64func (s *SmartSampler) ShouldSample(p trace.SamplingParameters) trace.SamplingResult // 错误请求必采样 for _, attr := range p.Attributes if attr.Key == error return trace.SamplingResultDecision: trace.RecordAndSample // 其他请求使用基础采样率 if float64(p.TraceID[0])/255.0 s.baseSamplingRate return trace.SamplingResultDecision: trace.RecordAndSample return trace.SamplingResultDecision: trace.Drop 3. 使用专门的后端系统 一些专门的可观测性后端系统提供了尾采样功能： Jaeger 的 Adaptive Sampling SkyWalking 的 Trace Sampling Grafana Tempo 的 Trace Sampling 结论 OpenTelemetry SDK 采用前置采样而非尾采样，是基于分布式系统一致性、性能优化和架构分层等多方面考虑的结果。虽然这意味着无法基于完整链路信息做采样决策，但 OpenTelemetry 生态提供了多种方式来弥补这一限制。 在实际应用中，我们可以： 在 SDK 层使用智能前置采样策略，确保关键链路被采样 在 Collector 层实现尾采样，进一步筛选有价值的链路 结合使用多种采样策略，平衡性能和可观测性 通过这种分层设计，OpenTelemetry 既保证了高效的数据收集，又为高级采样策略提供了可能性，满足了不同场景的需求。 实战案例 笔者实现一个 Go 语言的开源项目 goapm，对多个 Go 语言中常用的组件进行了 trace、log 和 metrics 的集成封装，用于快速在 Go 语言项目中实现可观测性，同时还提供了 goapm-example 实战案例，可供参考。 https://github.com/hedon954/goapmhttps://github.com/hedon954/goapm https://github.com/hedon954/goapm-examplehttps://github.com/hedon954/goapm-example","tags":["opentelemetry","服务监控"],"categories":["服务监控"]},{"title":"读书笔记丨《悟道领域驱动设计》","path":"/2025/03/11/note-ddd-awareness/","content":"思维转变 领域驱动设计（Domain-Driven Design，以下简称 DDD）的核心价值在于其对「业务领域」的深度聚焦。这里的「领域」并非单纯的技术范畴，而是指代软件系统所要映射的现实业务场景及其核心价值主张。DDD 通过建立与业务高度契合的领域模型，使得技术实现与业务本质形成同频共振，从而有效解决复杂业务场景下的认知鸿沟问题。 在 VUCA（Volatile 易变性、Uncertain 不确定性、Complex 复杂性、Ambiguous 模糊性）特征愈发显著的现代商业环境中，任何架构设计都面临固有局限。这种局限性既源于业务需求本身的动态演进，也受制于人类认知的有限性——正如 Eric Evans 在开山之作中强调的“模型永远是对现实的近似抽象”。但正是这种局限性，凸显了 DDD 方法论的战略意义： 它通过\"战略设计\"构建业务全景图，运用限界上下文划定领域边界，通过\"战术设计\"落地聚合根、实体/值对象等模式，形成应对复杂性的结构化解决方案。 需要特别指出的是，DDD 的复杂性并非方法论本身的缺陷，而是其应对现实业务复杂度的必要代价。这种复杂性体现在三个维度： 认知复杂性：要求开发团队与领域专家共建\"通用语言\"，实现业务概念与代码模型的精准映射。 架构复杂性：通过分层架构实现业务逻辑与技术实现的解耦，采用防腐层处理系统集成问题。 演进复杂性：借助子域划分和上下文映射，为持续演进的业务提供可扩展的架构基础。 对于实践者而言，DDD 的价值不在于提供完美无缺的终极方案，而是为 VUCA 环境下的系统建设提供基础性指引。其核心思想——无论是通过限界上下文实现的领域自治，还是通过聚合根维护的业务一致性——都为控制软件熵增提供了可落地的模式库。即便不完全采用 DDD 完整体系，其领域建模思想、分层架构理念等核心要素，仍能显著提升复杂系统的可维护性和演进能力。这种开放包容的哲学，恰是 DDD 历经二十年仍保持生命力的关键所在。 贫血模型 vs. 充血模型 贫血模型：指的是只有属性而没有行为的模型。 充血模型：指的是既有属性又有行为的模型。 笔者过往的实践中，基本上都使用类似于 controller→service→repository[model] 的三层架构： conrtoller 负责暴露对外接口。 service 负责执行所有的业务逻辑。 repository 复杂数据的存储和缓存，包含数据对象 model 的定义。 在这个模式下，基本上所有的核心逻辑都充斥在 service 层中，所以 service 层一般都会非常大，它要扮演多面手，即要负责跟各个模块协作，还要负责处理具体的业务规则，最终完成一个业务行为。这个过程中，model 即为贫血模型，因为逻辑都给 service 处理了，这种架构也称为贫血三层架构。 在 DDD 的理念下，很多的核心业务概念都会被建模为「领域对象」，这些「领域对象」本身就是一种业务规则的体现，所以把业务的处理逻辑，都归属到这些「领域对象」的行为当中了，即所谓的充血模型。 在这个理念下，一个优化后的充血四层架构如下图所示： 充血四层架构 贫血模型推荐场景：业务简单、迭代快速、团队技术栈偏传统（如 Spring Boot+MyBatis）时，避免过度设计。 充血模型推荐场景：业务复杂、需长期演进（如核心交易系统）、团队具备 DDD 经验时，通过实体、值对象、领域服务等战术设计理念降低系统熵增。 混合使用的场景：部分核心领域用充血模型（如订单、支付），非核心模块用贫血模型（如日志、配置），平衡效率与质量。 实际上，充血模型因其状态完整，适合进行状态变更类的操作，以确保业务操作符合领域规则；贫血模型由于其轻量级，更适合作为不会涉及状态变更的操作的数据容器。这其实就是 CQRS 的理念。 概念清单 战术设计 实体 定义：会随着业务变化发生变化的业务概念叫作实体对象。关键点：实体需要唯一表示 值对象 定义：一些对象在表达业务概念时是必须的，可业务并不围绕着它们进行，它们仅是对这些重要业务概念的描述，这一类对象叫作值对象。关键点：值对象的意义取决于属性，只要对象的属性一模一样，那么对象就是相同的。尽量把值对象实现为不可变对象。 领域服务 定义：领域服务自身是没有数据的，只是表达了某种业务计算逻辑，或者业务的某种策略。关键点：领域服务是无状态的。只有在确实表达了一个相对独立的业务概念或者业务策略，并且不能简单地把它归结到某个既有的业务对象上时，才是一个真正的领域服务。 领域事件 定义：领域事件代表从业务专家视角看到的某种重要的事情发生了。关键点：领域事件是一种特殊的值对象。应该根据限界上下文中的通用语言来命名事件：AccountActivited。应该将事件建模成值对象或贫血对象。 聚合 定义：聚合从本质上讲是在基础的构造块上增加了一层边界，用边界把那些紧密相关的对象放到了一起。关键点：紧密相关的对象存在数据一致性问题；缺乏边界时，维护数据一致性是困难的；划分边界的关键在于既不要让整个系统成为一个整体，又让每个单独划分出的聚合具有明确的业务意义；聚合需要关注三条法则：生命周期一致性：如果一个对象在聚合根消失之后仍然有意义，那么说明此时在系统中必然存在能够访问该对象的方法。这和聚合的定义矛盾，所以聚合内的其他元素必然在聚合根消失后失效。问题域一致性：不属于同一个问题域的对象，不应该出现在同一个聚合中。尽量小的聚合：聚合的本质作用是提升对象系统的粒度，确保一致性、降低复杂度。不过，粒度绝不是越大越好。如果聚合的粒度太大，那内部的逻辑复杂度也会大大增加还会影响到复用度。因此，要能够比较容易地断开聚合。 资源库 定义：对于查询、创建、修改、删除数据的操作，领域模型使用“资源库(Repository)”这个概念来承载它们。关键点：一个聚合对应一个资源库，应以聚合根命名资源库，除了聚合根之外的其他对象，都不应该提供资源库对象。 工厂 定义：工厂用于构建聚合。关键点：一个聚合往往包含多个对象，这些对象的数据之间又可能存在联系，如果允许分别创建这些对象，就会让聚合是业务完整性的单元这个定义面临失败。 战略设计 统一语言 定义：与业务专家协作定义全团队通用的术语表，消除沟通歧义。关键点：同一个概念在不同的上下文中可能存在不同的含义；同一个概念在同一上下文中的不同环节，也可能存在不同的含义，需要非常明确清晰的界定，降低沟通成本。 子域 定义：子域是对业务领域的逻辑划分，用于分解复杂问题。通常分为核心子域（业务核心竞争力）、支撑子域（辅助核心业务）和通用子域（可复用的标准化能力）。关键点：因业务目标、团队定位和组织发展阶段等方面的不同，这三个子域的划分并非一成不变，而是会互相转换。 限界上下文 定义：限界上下文本质上是一个自治的小世界，它有完备的职责，还有清晰的边界。关键点：一个子域的一切资产，包括领域模型、数据库、包、可执行程序、接口声明等，都应该封装在限界上下文中，避免跨越边界。如何平衡边界的价值和不利影响，是划分边界时要做的一种重要取舍。一个较为稳妥的策略是考虑认知的渐进特征，不要过早隔离。在已经确定的边界上进行划分，延缓划分那些尚具模糊性的边界，在这些边界逐渐变得清晰时再分离它们。 上下文映射 定义：限界上下文约定了基于领域模型的架构层次的设计分解，而分解必然意味着集成和协作。上下文映射就是对限界上下文之间的协作关系的模式总结。关键点：在边界上完成概念映射是一种基本模式。通过在应用层组装或者使用适配器完成概念映射，可以保持领域概念的清晰，避免领域模型遭到不必要的污染。防腐层模式、标准开放服务模式、客户-供应商模式、追随者模式。 串讲 在应对复杂业务系统时，DDD 通过分治策略将业务领域拆分为多个子域（如电商系统的订单、支付子域），每个子域对应一个限界上下文——这是技术与业务对齐的关键边界，既承载领域模型的实现，也通过上下文映射（如防腐层、共享内核等模式）实现跨子域协作，避免模型污染。 限界上下文内的领域对象是业务逻辑的载体：具备唯一标识和生命周期的实体（如订单实体通过 ID 跟踪状态变化）、描述特征且不可变的值对象（如地址由省市构成，修改需整体替换），以及通过聚合根统一操作保证一致性的聚合（如订单聚合根管理订单项和配送信息）。当业务逻辑跨越多个聚合时，由无状态的领域服务协调（如支付计算需整合订单、账户聚合）。 对象的创建与持久化分别由工厂（封装复杂初始化逻辑）和资源库（隔离存储细节）负责，而领域事件（如订单支付成功事件）则驱动跨上下文的异步协作。 战术设计 factory factory 用于构建复杂的领域对象。 repository 只有聚合根有 repository。 repository 就只提供 load 和 save 功能，且要保证事务一致性。 尽可能提供行级的 repository，而不是表级的 repository，对于表级的 repository，可以抽成一个领域服务。 设计模式 责任链模式 将请求的发送者和接受者解耦，使多个对象都有机会处理请求。 责任链模式的使用要点在于要将维护责任链的代码和业务代码分开。 在 DDD 中使用责任链模式时，应创建一个领域服务，在领域服务中完成责任链的创建和执行。 尽量不要在责任链的处理器中通过 set 修改领域对象（聚合根）的状态，责任链应仅用于某些值的计算，最终将计算结果交给聚合根完成业务操作。 笔者实现了一个快速构建责任链的工具： https://github.com/hedon954/devkit-go/blob/main/designmode/responsibility/builder.gohttps://github.com/hedon954/devkit-go/blob/main/designmode/responsibility/builder.go 策略模式 允许在运行时根据需要选择不同的实现。 在 DDD 中使用策略模式时，通常先定义一个领域服务接口，再在其实现类中完成策略的加载、选择和执行。 注意屏蔽策略模式的实现细节，避免上层关注领域服务内的设计模式细节。 桥接模式 旨在通过解耦抽象和实现，使两者能够独立扩展和变化。 多维解耦机制：桥接模式通过组合/聚合关系替代继承关系，将原本紧密耦合的抽象层（功能定义）与实现层（具体操作）分离例如遥控器（抽象）与电视（实现）的协作，遥控器通过接口控制电视，无需关注具体品牌。 正交扩展能力：支持两个独立变化维度（如消息类型与通知渠道、图形与渲染方式），避免类数量呈指数级增长（M×N 组合问题）。电商物流系统中，新增微信通知渠道时，无需修改所有消息类即可实现扩展。 规约模式 规约模式是一种用于定义业务领域中规则和约束的模式，通常由规约接口（Specification）和验证器（Validator）两个部分组成。 在 DDD 中，规约模式并不是在聚合根进行业务操作之前做前置校验，而是在聚合根完成业务操作之后做后置校验，确保 Repository 保存的聚合根符合业务规则。 适配器模式 将被适配者（Adaptee）的接口转换为目标接口（Target），使原本因接口不兼容而无法协同工作的类能够协同。 在 DDD 中，可以使用适配器模式来实现防腐层，以将外部上下文接口（如开放主机服务）返回的模型转换为本地上下文定义的领域模型，并将本地上下文的操作转换为对外部上下文的操作。可以有效隔离外部上下文的领域模型，避免互相污染。 领域事件 幂等性 领域事件的定义 领域事件是领域模型的组成部分，它通常由聚合根产生，并被其他聚合或者限界上下文订阅和处理，触发相应的业务逻辑。 注意点： 应该根据限界上下文中的通用语言来命名事件：AccountActivited。 应该将事件建模成值对象或贫血对象。 应用： 解耦领域对象之间的关系； 触发其他领域对象的行为； 记录领域内已发生的状态变化； 实现跨聚合的最终一致性； 进行限界上下文集成。 消息体： event_id: , event_type: , entity_id: , event_time: 0, extra_data: 领域事件的生成 应用层创建领域事件。 聚合根创建领域事件。 要避免在聚合根内部调用基础实施发布领域事件，而是生成后返回给应用层，由应用层去发布。 type Entity struct Events []Eventfunc(e *Entity)ResgisterEvent(event Event) e.Events = append(e.Events. event)func(e *Entity) GetEvents() []Event res := e.Events() e.Events = []Event return res 领域事件的发布 直接发布并轮询补偿：为事件存储一个发布状态标识，用于记录是否补发成功。并提供定时任务检索超时未发布成功的事件进行重新发布。 采用事务日志拖尾：引入变更数据捕获组件（Change Data Capture，简称 CDC），捕获数据的变更日志，解析后获得领域事件并发布。 领域事件的订阅 将领域事件订阅者放置在用户接口层 user-interface-subscriber，收到事件后调用应用服务执行业务逻辑。 事件溯源 事件溯源（Event Sourcing）是一种将所有的领域事件（Domain Event）存储到事件存储（Event Store）中，并通过重放历史事件来还原领域对象状态的模式。 核心思想是将系统中所有的状态变更都视为事件，将这些事件以事件顺序记录下来，并存储到事件存储中。这样，可以通过重放这些事件，来还原任意时刻的系统状态。 三种方案： 通过回放所有的历史事件重建聚合根。 通过快照提高重建聚合根的效率。 通过拉链表生成所有事件对应的快照。 拉链表是一种用于处理缓慢变化维度问题的数据结构，它可以有效地处理维度数据的历史变化。在拉链表中，每个记录都有一个开始时间和结束时间，用于描述该记录的存活时间，即该记录的有效期。 拉链法示意图 CQRS CQRS 将系统的操作分为两类： 命令（Command）：负责数据的写操作（增、删、改），不返回数据。 查询（Query）：负责数据的读操作，仅返回结果且不修改数据。 两者的数据模型可独立设计，甚至使用不同的数据库或存储技术。 适用场景 应对高并发读写场景案例 1：B 站点赞系统在日均活跃用户近亿的 B 站，点赞功能通过 CQRS分离读写操作。写入端通过消息队列（如Kafka）异步处理请求，避免数据库锁竞争；查询端通过缓存优化读取性能，显著提升系统吞吐量和稳定性。案例 2：实时答题 PK 游戏高并发的答题得分计算场景中，CQRS 结合事件溯源（EventSourcing）记录每个操作事件，确保读写模型的最终一致性，同时支持复杂战况数据的实时展示。解决复杂查询需求案例 3：电商订单查询随着订单查询需求多样化（如按时间筛选、跨实体聚合数据），CQRS通过独立读模型简化查询逻辑，避免领域模型被复杂查询逻辑污染。案例 4：微服务数据聚合在微服务架构中，CQRS允许通过事件同步跨服务数据到专用读库，避免跨服务联表查询的性能瓶颈（如行程管理服务与用户信息服务的聚合查询）。提升数据模型灵活性案例 5：文本增量更新针对大型文本编辑场景，CQRS拆分读写模型，增量保存修改记录并通过事件合并，减少网络传输数据量，同时支持任意版本的历史数据恢复。 不适用场景 简单 CRUD 系统（如小型管理后台）强一致性要求的金融交易场景（如实时扣款）团队缺乏事件驱动架构经验时 一致性 聚合内事务实现 聚合内事务控制不要放在应用层，会使应用层承担过多的责任。应用层应专注于协调领域对象和基础设施以完成业务操作，不应过多涉及数据访问和事务控制的细节。 聚合内事务控制可以交给 Repository 来实现，采用乐观锁解决并发问题，可以基于版本号和时间戳，一般重试 1-3 次即可。 聚合间事务实现 聚合间控制可以单独建立一个领域服务 Domain Service 来完成。 对于实时性要求不高，仅需最终一致性，可以使用本地消息表或者最大努力通知的方案。 对于实时性一致性要求比较高，可以采用 TCC（Try-Confirm-Cancel） 事务方案。 对于长事务场景，或者涉及外部系统、遗留系统，可以考虑 Saga 事务方案。 Saga 将事务分为多个事务，这些分支事务按照一定的顺序执行。当某个分支事务执行成功后，会通过消息通知下一个分支执行；当某个分支事务执行失败时，会按照正常事务执行顺序的相反方向进行一系列的补偿操作，以确保全局事务的一致性。 战略设计 事件风暴 核心概念与元素 元素名称 颜色标识 说明 领域事件（Domain Event） 橙色 表示已发生的业务事实，以“动词过去式”命名（如“订单已提交”），是事件风暴的核心起点。 命令（Command） 深蓝色 触发领域事件的操作或意图（如“提交订单”），通常由用户或系统触发。 参与者（Actor） 黄色 执行命令的角色，包括用户、部门或外部系统（如“客户”触发支付命令）。 外部系统（External System） 粉色 与当前系统交互的第三方服务（如支付网关回调生成事件）。 策略（Policy） 紫色 业务规则或约束条件（如“库存不足时取消订单”），决定事件触发的逻辑。 读模型（Read Model） 绿色 为查询优化的数据视图（如“用户订单列表”），支持决策展示。 聚合（Aggregate） 大黄色 业务对象集合（如“订单聚合”包含订单项和状态），维护一致性和完整性。 问题（Question） 红色 未达成共识的争议点（如事件定义分歧），需后续专项讨论。 实施流程与步骤 准备工作 参与人员：业务专家、开发、产品、测试等跨职能角色，需领域专家主导。 物料：多色便签、白板、马克笔，线上工具辅助远程协作。 识别领域事件 团队通过头脑风暴罗列所有可能事件（如电商场景的“订单已创建”“库存已扣减”），按时间轴排列，争议事件用红色便签标记并暂存。 补充命令与角色 为每个事件关联触发命令及执行者（如“客户”执行“支付订单”命令生成“支付完成”事件），区分内部操作与外部系统调用。 定义策略与读模型 添加业务规则（如“订单金额 ≥1000 元需审核”）和数据展示需求（如“实时库存看板”）。 构建聚合与划分子域 将相关事件、命令归类为聚合（如“支付聚合”），划分限界上下文（如“订单服务”“库存服务”），明确微服务边界。 注意事项 事件粒度的把控：避免过度细化（如“用户已睁眼\"）或过于宽泛（如“订单已修改”），需聚焦业务关键节点。 争议处理与迭代：对未达成共识的事件标记为“问题”（红色便签），后续专题讨论；定期回顾模型，修正错误或补充遗漏。 技术实现衔接 ：事件风暴的输出需转化为代码模型，例如通过事件溯源（Event Sourcing）持久化事件流，或结合 CQRS 分离读写逻辑。 C4 架构模型 https://c4model.com/https://c4model.com/ 层级 核心目标 受众 关键元素 Context（上下文） 描述系统与外部实体（用户、第三方系统）的交互关系 非技术人员（如业务方、客户） 系统边界、用户角色、外部依赖（如支付网关） Container（容器） 展示系统内部的高阶技术组件（进程级单元） 技术管理者、架构师 Web 应用、数据库、消息队列等独立进程单元，关注技术选型与通信协议（如 REST API、gRPC） Component（组件） 细化容器内部的业务模块与交互逻辑 开发团队 服务、模块、接口（如订单服务、库存服务），强调职责划分与依赖关系 Code（代码） 展示组件实现的代码结构 开发者 类、方法、数据库表（如 UML 类图、ER 图），通常由 IDE 工具自动生成 除了四层核心视图，C4 模型还提供： 部署图：展示容器在物理环境中的分布（如 Kubernetes 集群部署）。 动态图：描述业务流程（如用户下单到支付完成的时序交互）。 系统景观图：多系统协同的全局视图（如企业级中台架构）。 ContextContainerComponentCode 实践案例 参考作者的 ddd-archetype ，笔者实现了一个 Go 版本的 ddd-archetype-go： https://github.com/hedon-go-road/ddd-archetype-gohttps://github.com/hedon-go-road/ddd-archetype-go 整体架构如下：","tags":["读书笔记","ddd"],"categories":["读书笔记"]},{"title":"Go 1.24 新特性解读：使用 testing/synctest 优雅地测试并发代码","path":"/2025/03/06/go-lib-synctest/","content":"在 Go 语言开发中，并发编程一直是其最引人注目的特性之一。然而，如何有效地测试并发代码却常常让开发者感到头疼。Go 1.24 版本引入的实验性包 testing/synctest 为这个问题带来了优雅的解决方案。今天，让我们深入了解这个新特性。 并发测试的传统困境 在介绍新方案之前，我们先看看传统的并发测试面临哪些问题： func TestTraditional(t *testing.T) done := false go func() // 执行某些操作 time.Sleep(100 * time.Millisecond) done = true () // 等待操作完成 time.Sleep(200 * time.Millisecond) if !done t.Fatal(操作未完成) 这种方式存在明显的问题： 时间依赖：需要通过 Sleep 等待，导致测试运行缓慢 不稳定性：在不同环境下可能产生不同结果 精确性差：难以准确把握检查时机 synctest：优雅的解决方案 testing/synctest 包通过两个核心函数改变了这一切： Run(): 创建隔离的测试环境（bubble） Wait(): 等待所有 goroutine 进入稳定状态 让我们看看如何改写上面的测试： func TestWithSynctest(t *testing.T) synctest.Run(func() done := false go func() // 执行某些操作 time.Sleep(100 * time.Millisecond) done = true () synctest.Wait() // 等待所有 goroutine 进入稳定状态 if !done t.Fatal(操作未完成) ) 深入理解 Wait 机制 Wait 的本质 很多开发者初次接触 Wait() 时可能会感到困惑：它到底在等待什么？什么时候会返回？ 想象一个场景：你在拍摄一张全家福，需要等待所有人都找到自己的位置，站好不动，才能按下快门。Wait() 就像这个摄影师，它在等待所有 goroutine（就像照片中的人）都进入一个稳定的状态（站好不动）。 synctest.Run(func() // 类比：三个人要拍全家福 go person1() // 第一个人找位置 go person2() // 第二个人找位置 go person3() // 第三个人找位置 synctest.Wait() // 等待所有人都站好不动 // 这时可以安全地按下快门（检查程序状态）) 为什么需要 Wait？ 在并发程序中，我们经常需要在特定时刻检查程序状态。但是，如果某些 goroutine 还在运行，这个状态可能随时发生变化。Wait() 通过确保所有 goroutine 都进入稳定状态，为我们提供了一个\"快照\"时刻。 synctest.Run(func() result := false go func() // 模拟耗时操作 time.Sleep(1 * time.Second) result = true () synctest.Wait() // 等待 goroutine 进入稳定状态 // 此时 result 的值是确定的，不会突然改变 fmt.Println(result)) 持久阻塞的概念 哪些操作会导致持久阻塞？ channel 操作（同一 bubble 内） time.Sleep sync.WaitGroup.Wait sync.Cond.Wait 哪些操作不算持久阻塞？ 互斥锁操作 外部 I/O 外部 channel 操作 虚拟时钟：测试的神器 synctest 的另一个强大特性是虚拟时钟机制。在 bubble 内部，所有时间相关的操作都使用虚拟时钟，这意味着： synctest.Run(func() // 看似等待24小时 time.Sleep(24 * time.Hour) // 实际上立即执行完成！) 这个特性让我们能够： 快速测试长时间操作 精确控制时间流逝 避免测试的不确定性 实战案例：深入理解 HTTP 100 Continue 测试 背景知识 HTTP 的 100 Continue 机制是一个优化大文件上传的协议特性： 客户端想上传大文件时，先发送带有 \"Expect: 100-continue\" 头的请求 服务器可以决定是否接受这个上传： 如果接受，返回 \"100 Continue\" 如果拒绝，可以直接返回错误状态码 客户端根据服务器的响应决定是否发送文件内容 详细测试实现 func TestHTTPContinue(t *testing.T) synctest.Run(func() // 第一步：建立测试环境 srvConn, cliConn := net.Pipe() defer srvConn.Close() defer cliConn.Close() // 第二步：配置 HTTP 客户端 tr := http.Transport DialContext: func(ctx context.Context, network, address string) (net.Conn, error) return cliConn, nil , ExpectContinueTimeout: 5 * time.Second, // 第三步：准备测试数据 body := request body // 第四步：发送请求 go func() req, _ := http.NewRequest(PUT, http://test.tld/, strings.NewReader(body)) req.Header.Set(Expect, 100-continue) resp, err := tr.RoundTrip(req) if err != nil t.Errorf(请求失败: %v, err) else resp.Body.Close() () // 第五步：验证请求头 req, err := http.ReadRequest(bufio.NewReader(srvConn)) if err != nil t.Fatalf(读取请求失败: %v, err) // 第六步：验证请求体未发送 var gotBody strings.Builder go io.Copy(gotBody, req.Body) synctest.Wait() if got := gotBody.String(); got != t.Fatalf(在发送 100 Continue 之前，意外收到请求体: %q, got) // 第七步：发送 100 Continue srvConn.Write([]byte(HTTP/1.1 100 Continue\\r \\r )) // 第八步：验证请求体 synctest.Wait() if got := gotBody.String(); got != body t.Fatalf(收到的请求体 %q，期望 %q, got, body) // 第九步：完成请求 srvConn.Write([]byte(HTTP/1.1 200 OK\\r \\r )) ) 测试的关键点解析 使用 net.Pipe() 创建内存中的网络连接 避免依赖真实网络 保证测试的可重复性 请求发送过程 在独立的 goroutine 中发送请求 设置 \"Expect: 100-continue\" 头 准备要发送的请求体 验证关键行为 确认请求头正确发送 验证请求体在收到 100 Continue 之前未发送 验证请求体在收到 100 Continue 后正确发送 使用 Wait 的时机 在检查请求体之前调用 Wait 确保所有数据传输操作都已完成或阻塞 获得稳定的程序状态进行验证 使用建议 明确边界：理解什么操作会导致持久阻塞，什么不会 清理资源：确保所有 goroutine 在测试结束前退出 模拟 I/O：使用内存管道替代真实网络连接 合理使用 Wait：在需要检查状态的关键点调用 注意事项 目前是实验性功能，需要设置 GOEXPERIMENT=synctest 不支持测试真实的外部 I/O 操作 互斥锁操作不被视为持久阻塞","tags":["go","单元测试"],"categories":["go"]},{"title":"直播系统推拉流原理","path":"/2025/03/04/live-stream-push-pull/","content":"直播系统推拉流原理概述 直播系统的核心功能是实现主播端视频采集后的实时传输，以及观众端的实时观看。整个过程主要包含：推流、服务器处理、拉流三个环节。 直播系统架构 核心概念解析 1. 推流（Push） 推流是指主播端将视频数据传输到服务器的过程。主要使用 RTMP 协议（Real Time Messaging Protocol）。 比如可能有如下推流 URL 的生成逻辑： public static String generatePushUrl(String pushDomain, String pushKey, String appName, String streamName, long expireTime) String pushUrl = ; // 推流域名未开启鉴权功能的情况下 if (StringUtils.isBlank(pushKey)) pushUrl = rtmp:// + pushDomain + / + appName + / + streamName; else long timeStamp = System.currentTimeMillis() / 1000L + expireTime; String stringToMd5 = / + appName + / + streamName + - + Long.toString(timeStamp) + -0-0- + pushKey; String authKey = md5(stringToMd5); pushUrl = rtmp:// + pushDomain + / + appName + / + streamName + ?auth_key= + Long.toString(timeStamp) + -0-0- + authKey; return pushUrl; 推流地址的组成部分： - rtmp:// - 协议 - pushDomain - 推流域名 - appName - 应用名称 - streamName - 流名称 - auth_key - 鉴权参数（可选） 2. 拉流（Pull） 拉流是观众观看直播的过程。支持多种协议： - RTMP：延迟低（1-3秒） - HTTP-FLV：延迟适中（2-5秒） - HLS(m3u8)：延迟较高（5-30秒） // FLV 格式public static String generalPullUrlFlv(String pullDomain, String pullKey, String appName, String streamName, long expireTime) if (StringUtils.isBlank(pullKey)) return http:// + pullDomain + / + appName + / + streamName + .flv; // ... 鉴权逻辑// HLS 格式public static String generalPullUrlHls(String pullDomain, String pullKey, String appName, String streamName, long expireTime) if (StringUtils.isBlank(pullKey)) return http:// + pullDomain + / + appName + / + streamName + .m3u8; // ... 鉴权逻辑 直播流程 主播开播： 系统生成唯一的 streamId 生成带鉴权的推流地址 主播端推流软件（如 OBS）开始推流 服务器处理： 流媒体服务器接收推流 进行转码、录制等处理 将流分发到 CDN 节点 观众观看： 获取对应格式的拉流地址 通过播放器拉取直播流 实现实时观看 实现建议 选择合适的流媒体服务器： 商业云服务：阿里云直播、腾讯云直播 开源方案：SRS、Nginx-RTMP 根据业务场景选择协议： 普通直播：HTTP-FLV 低延迟场景：RTMP 移动端兼容性要求高：HLS 关注关键指标： 延迟控制 卡顿率 首屏时间 带宽成本 安全鉴权： 防盗链机制","tags":["直播系统"],"categories":["解决方案","直播系统"]},{"title":"网络数据包的完整旅程：从发送到接收的全过程","path":"/2025/03/01/net-data-journey/","content":"不知道你是否曾经好奇你发出的一个网络请求，最终是怎么到达对端，并将你想要的信息返回给你的。本文将通过一个 HTTP 请求与响应，从一个比较宏观的角度来梳理下一个数据包在网络中的旅途，旨在帮助笔者和各位读者建立起对计算机网络模型一个比较全面的认知。 本文参考极客时间《网络架构实战课（谢友鹏）》，再根据笔者的知识面、按照个人理解，补充更多丰富具体的内容。 实战 好，那我们直接开始，我们先使用 curl 来发起一个 HTTP 请求，看看这过程中发生了什么： curl -o /dev/null -v https://example.com 在笔者的 mac 机器上，这行命令的输出如下： 当我们发起请求时，首先会对 example.com 进行域名解析，分别尝试解析到它的 IPv6 和 IPv4。 * IPv6: (none)* IPv4: 23.215.0.138, 96.7.128.198, 23.192.228.80, 23.192.228.84, 23.215.0.136, 96.7.128.175 因为我们使用的是 https 协议，所以会尝试跟这些地址的 443 端口建立 TCP 连接，（如果是 https 则跟 80 端口），并进行 TLS 握手验证，如果成功了，则会建立 TCP 连接。 * Trying 23.215.0.138:443......[TLS handshake]* Connected to example.com (23.215.0.138) port 443 建立连接后，就开始发送 HTTP 请求，这里使用的是 HTTP2 协议。 * using HTTP/2 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0* [HTTP/2] [1] OPENED stream for https://example.com/* [HTTP/2] [1] [:method: GET]* [HTTP/2] [1] [:scheme: https]* [HTTP/2] [1] [:authority: example.com]* [HTTP/2] [1] [:path: /]* [HTTP/2] [1] [user-agent: curl/8.10.1]* [HTTP/2] [1] [accept: */*] [5 bytes data] GET / HTTP/2 Host: example.com User-Agent: curl/8.10.1 Accept: */** Request completely sent off 最后，服务器返回了 HTTP 200 OK 的响应。 [5 bytes data]* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): [265 bytes data]* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): [265 bytes data] HTTP/2 200 content-type: text/html etag: 84238dfc8092e5d9c0dac8ef93371a07:1736799080.121134 last-modified: Mon, 13 Jan 2025 20:11:20 GMT cache-control: max-age=1374 date: Sat, 01 Mar 2025 05:01:03 GMT alt-svc: h3=:443; ma=93600,h3-29=:443; ma=93600,quic=:443; ma=93600; v=43 content-length: 1256 [5 bytes data]100 1256 100 1256 0 0 1172 0 0:00:01 0:00:01 --:--:-- 1172* Connection #0 to host example.com left intact 要进一步了解网络数据包的细节，我们可以通过抓包工具进行分析。你可以使用 tcpdump 抓取与 example.com 的通信数据包。 运行如下命令： sudo tcpdump host example.com -w example.com.pcap 然后再另外一个命令行窗口再次发送请求： curl -o /dev/null -v https://example.com 回到 tcpdump 的窗口并结束监听，我们就会得到 example.com.pcap 的抓包文件，可以通过 Wireshark 软件打开该文件： tcpdump 分析结果 网络分层 通过上述实验，我们可以清晰看到网络是分层的，主流的分层模型有 OSI 七层模型和 TCP/IP 四层模型，它们的对应关系及常见的协议如下图所示： OSI-vs-TCP/IP 我们在 Wireshark 上方随便选择一个数据包，使用鼠标点击下方左侧的每一层，可以在右侧看到对应的层级数据。从链路层到应用层，每一层的数据都是对下一层的进一步封装。 数据包封装 在发送方，用户程序需要传输的数据会经过逐层封装。首先添加应用层的 HTTP Header，然后是传输层的 TCP Header，接着是网络层的 IP Header，最后在链路层添加以太网帧的帧头和帧尾，包括源 MAC 地址、目的 MAC 地址等链路层信息，最终形成网络中传输的完整数据包。 在接收方，数据包会按相反的顺序逐层解封装。接收设备从链路层开始解析数据，依次解读网络层、传输层和应用层的信息，最后将数据传递给接收方的应用程序。 如下图所示： 数据包封装 解析 我们在 Wireshark 中点开下面的每一层，可以看到如下信息，我在图标注了最重要的几个信息： 网络之旅 经过上述实验，我们可以做个小总结： 通过上述实验，我们可以清晰理解数据包的传输过程： HTTP 请求是网络通信的应用层内容，它需要通过各层网络协议的封装才能实现端到端传输。 从发送方角度，数据传输遵循一个明确的逻辑顺序：首先将域名（example.com）解析为 IP 地址，然后基于该 IP 地址和目标端口（443）建立 TCP 连接，接着找到目标 IP 的 MAC 地址，最终由网卡将完整封装的数据包发送到网络中。 从接收方角度，服务器处理数据包的过程是一个自下而上的解封装过程：数据链路层接收到的帧包含源 MAC 地址，网络层解析出 IPv4 地址和协议类型，传输层识别出 TCP 协议和源端口号，最终在应用层获取并处理 HTTP 请求数据。服务器根据这些信息构建响应，并按相反顺序封装返回给客户端。 这种分层处理机制确保了网络通信的灵活性和可靠性，每层只需关注自己的职责，共同完成端到端的数据传输任务。 好，那么这里就有 2 个最关键的问题： 如何通过域名获得 IP 地址？ 如何通过 IP 地址获取 MAC 地址？ DNS 解析 DNS（Domain Name System，域名系统）是互联网的一项核心服务，它允许我们使用易记的域名（如 example.com）而不是数字 IP 地址（如 93.184.216.34）来访问网站。 当你在浏览器中输入一个域名时，DNS 解析按以下步骤进行： 浏览器缓存检查：浏览器首先检查自己的缓存，看是否已经存储了该域名对应的 IP 地址。 操作系统缓存检查：如果浏览器缓存中没有，系统会检查操作系统的 DNS 缓存（如 Windows 的 DNS Client 服务）。 路由器缓存检查：若系统缓存中也没有，请求会被发送到你的路由器，它也维护着一个 DNS 缓存。 ISP DNS 服务器查询：如果以上缓存都未命中，请求会被发送到你的 ISP（互联网服务提供商）的 DNS 服务器。 递归查询：ISP 的 DNS 服务器会执行递归查询： 首先查询根域名服务器（Root DNS Server） 根服务器会引导到顶级域名服务器（TLD DNS Server，如 .com, .net, .org 等） 顶级域名服务器会引导到权威域名服务器（Authoritative DNS Server） 权威服务器会返回该域名的 IP 地址 结果返回与缓存：一旦获取到 IP 地址，它会被沿着查询路径返回，并在各个层级上缓存一段时间（由 TTL 值决定）。 你可以使用以下工具查询 DNS 信息： nslookup：nslookup example.com dig：dig example.com host：host example.com 这些工具可以帮助你了解域名的解析过程和结果。 ➜ ~ host example.comexample.com has address 23.215.0.138example.com has address 23.192.228.84example.com has address 23.215.0.136example.com has address 23.192.228.80example.com has address 96.7.128.175example.com has address 96.7.128.198example.com has IPv6 address 2600:1408:ec00:36::1736:7f31example.com has IPv6 address 2600:1406:3a00:21::173e:2e65example.com has IPv6 address 2600:1406:3a00:21::173e:2e66example.com has IPv6 address 2600:1406:bc00:53::b81e:94c8example.com has IPv6 address 2600:1406:bc00:53::b81e:94ceexample.com has IPv6 address 2600:1408:ec00:36::1736:7f24example.com mail is handled by 0 . 通过 DNS 解析将域名转换为 IP 地址后，网络通信的下一步就是确定如何将数据包发送到目标 IP 地址，这就需要用到 ARP 协议来获取目标设备的 MAC 地址。 穿越客户端局域网 当我们发送一个网络请求时，数据包如何找到离开家庭/办公网络的\"出口\"？ 数据包首先需要解决的是\"该往哪走\"的问题： 问题：我需要直接联系目标设备还是找个\"中介\"？ 解决方案：子网判断 设备会比较目标 IP 与自己的 IP 和子网掩码 就像判断收件人是不是住在同一个小区 问题：如何找到同一网络中的设备？ 解决方案：ARP 协议 类似于小区广播：\"谁是 202 号房的？请告诉我你的门牌号！\" 目标设备回应自己的 MAC 地址（设备的\"身份证号\"） 问题：目标在远方，如何离开本地网络？ 解决方案：默认网关 就像不认识远方收件人的地址，先交给小区门卫（路由器） 数据包头上标注最终目的地 IP，但先送到网关的 MAC 地址 问题：数据如何在本地网络中转发？ 解决方案：交换机的 MAC 地址表 交换机就像小区内的快递员，记住了每家每户的门牌号 它查表后将包裹精确送到对应的门口，不会打扰其他住户 简单来说，数据包在本地网络中的旅程就像是快递先确认收件人是否在同一小区，如果是，直接送达；如果不是，则交给小区出口的保安，由他负责进一步转发。 穿越公网 数据包离开了本地网络，如何在茫茫互联网中找到遥远的目标服务器？ 数据包在互联网上的旅程就像一次跨国旅行： 问题：如何从私人区域进入公共世界？ 解决方案：NAT（网络地址转换） 就像多人共用一个护照出国，本地设备共享一个公网 IP 路由器会记住谁发了什么请求，回程时能送回正确的设备 问题：互联网如此庞大复杂，谁来管理这些网络？ 解决方案：自治系统（Autonomous System, AS） AS 就像互联网世界的\"国家\"或\"独立王国\" 每个 AS 由单一技术管理机构控制（如 ISP、大企业或教育机构） 你的数据包首先进入你的 ISP 所在的 AS，然后可能穿越多个 AS 每个 AS 有唯一的 AS 号（ASN），如 AS7018(ATT) 或 AS8075(Microsoft) 问题：这些\"网络王国\"如何相互通信和合作？ 解决方案：BGP 协议(边界网关协议) BGP 是 AS 之间的\"外交语言\"，用于宣告路由信息 它告诉其他 AS：\"通过我可以到达这些网络\" 路由器根据 BGP 信息，决定数据包应该经过哪些 AS 问题：如何决定数据包在 AS 内部该走哪条路？ 解决方案：内部路由协议 AS 内部使用 OSPF 或 IS-IS 等协议来找到最佳路径 路由器像城市中的交通指挥，根据\"路况\"决定下一个方向 问题：不同运营商之间如何连接？ 解决方案：互联网交换中心（IXP） 就像不同航空公司在大型枢纽机场交换乘客 数据包在 IXP 从一个 AS “转机”到另一个 AS 这减少了路径长度，提高了传输效率 问题：我能知道我的数据经过了哪些地方吗？ 解决方案：路径追踪工具 traceroute/tracert 就像给数据包装上 GPS 你可以看到数据包穿越的不同 AS 和路由器 互联网就像一个巨大的全球快递网络，你的数据包可能穿越多个国家、经过海底电缆，由不同的运营商接力传递，最终到达目的地的网络。 穿越服务端局域网 数据包到达目标所在网络后，如何找到并到达最终的服务器？ 数据包抵达目的地网络，就像国际快递到达目标城市，还需要最后一段\"本地配送\"： 问题：如何确保只有合法请求能进入网络？ 解决方案：防火墙和安全策略 就像机场海关，检查入境者是否符合入境条件 只有合法的数据包才能通过安全检查 问题：大型网站如何处理海量请求？ 解决方案：负载均衡 像大型医院的分诊台，将病人分配到不同的医生处 根据服务器负载、用户位置等因素智能分发请求 问题：如何在数据中心复杂环境中找到目标服务器？ 解决方案：内部路由与最后一跳 ARP 数据中心内部有自己的\"地图\"和\"道路系统\" 最后一个路由器会通过 ARP 找到服务器的具体位置 问题：现代云环境中，服务器可能是虚拟的，怎么处理？ 解决方案：虚拟网络 物理服务器上可能运行多个虚拟机或容器 虚拟交换机将数据包准确送达虚拟环境中的目标应用 这就像国际快递最后的“最后一公里”配送 - 从目的地城市的分拣中心，经过层层筛选，最终送到收件人手中。 总结 网络请求就像一封国际信件的旅程： 本地投递：从你家出发，判断收件人是否在同小区。如不在，交给小区出口的门卫（网关）。 国际运输： 先经过你所在“国家”（你 ISP 的 AS）的海关（NAT） 然后可能穿越多个“国家”（不同的 AS） 各国海关（路由器）通过“国际条约”（BGP）决定包裹走向 有时通过“国际中转站”（IXP）快速转运到其他“国家” 目的地配送： 通过目的地“海关”（防火墙）入境检查 经过“分拣中心”（负载均衡器）分配处理人员 最终通过“本地快递员”（内部路由和交换）送达收件人手中 数据包就这样完成了客户端设备到服务器的全程旅行，然后服务器的响应再沿着类似的路径返回到客户端设备，完成整个请求-响应循环。 参考 极客时间《网络架构实战课》 Difference Between OSI Model and TCP/IP Model","tags":["计算机网络"],"categories":["计算机基础","计算机网络"]},{"title":"解决方案丨游戏后端中的 Push-ACK 机制设计与内存优化","path":"/2025/02/27/solution-push-ack/","content":"引言 在现代在线游戏开发中，服务器与客户端之间实时、可靠的通信机制是游戏体验的基石。作为一名游戏后端开发者，我曾经遇到过这样的场景：更新了一个公会系统的新功能，服务器需要向成千上万个在线玩家推送公会状态变更。短短几小时后，服务器内存使用率飙升至 90%，系统告警不断。问题出在哪里？Push 消息的可靠性机制实现不当导致了内存泄漏。 本文将深入探讨游戏后端中 Push-ACK 机制的设计与实现，特别关注如何避免内存暴涨问题，分享我在多个大型游戏项目中积累的经验与教训。 背景：为什么需要应用层的 ACK 机制？ TCP 协议确实提供了可靠的数据传输保证，包括数据包的序列号、校验和、超时重传等机制。那么，为什么我们还需要在应用层实现额外的 ACK 机制呢？ TCP 可靠性的边界 TCP 只能保证数据被送达到客户端的网络栈，但无法保证： 数据被客户端应用程序正确处理 处理过程中没有出现异常 客户端的业务逻辑正确执行 想象这样一个场景：服务器向玩家推送了一条\"获得稀有装备\"的消息，TCP 确保了数据送达客户端，但如果客户端在处理这个消息时崩溃了呢？对于游戏这类状态敏感的应用，我们需要知道消息是否被成功处理，而不仅仅是成功传输。 业务可靠性需求 实际游戏开发中，不同类型的消息有不同的可靠性需求： 消息类型 示例 可靠性需求 关键状态变更 道具获取、货币变化 极高（必须确认处理） 游戏进程通知 任务更新、成就解锁 高（需要确认） 实时位置同步 玩家位置、NPC 移动 中（新数据可覆盖旧数据） 环境信息 天气变化、背景音乐 低（可接受偶尔丢失） 设计通用的 Push-ACK 机制 一个完善的 Push-ACK 机制需要考虑以下几个方面：消息唯一标识、优先级分级、超时重试、批量确认和失败处理。下面是基于 Go 语言的设计实现： 核心数据结构 // Message 表示服务器推送的消息type Message struct MsgID string `json:msg_id` // 唯一消息标识 MsgType string `json:msg_type` // 消息类型 Timestamp int64 `json:timestamp` // 发送时间戳 Priority int `json:priority` // 优先级：1-高，2-中，3-低 Payload interface `json:payload` // 消息内容 RequiresAck bool `json:requires_ack` // 是否需要确认 Expiration int64 `json:expiration` // 过期时间戳// AckMessage 表示客户端的确认消息type AckMessage struct AckID string `json:ack_id` // 对应原消息ID Status string `json:status` // 状态：success/failed/partial ClientTimestamp int64 `json:client_timestamp` // 客户端处理时间 ErrorCode int `json:error_code` // 错误码 ErrorMessage string `json:error_message` // 错误信息// BatchAckMessage 表示批量确认消息type BatchAckMessage struct BatchAck bool `json:batch_ack` // 批量确认标志 AckIDs []string `json:ack_ids` // 消息ID列表 Status string `json:status` // 状态 ClientTimestamp int64 `json:client_timestamp`// 确认时间// PendingMessageInfo 表示等待确认的消息信息type PendingMessageInfo struct ClientID string // 客户端ID Message *Message // 原始消息 SentTime int64 // 发送时间 RetryCount int // 重试次数 服务器端 Push 管理器实现 // PushManager 负责管理推送消息和确认type PushManager struct pendingMessages map[string]*PendingMessageInfo // 等待确认的消息 clientMessageCount map[string]int // 每个客户端的消息数量 ackTimeout int64 // 确认超时时间(秒) maxRetries int // 最大重试次数 maxPendingPerClient int // 每客户端最大消息数 maxMessageAge int64 // 消息最大生存时间(秒) // 内存监控相关 memoryThresholdMB int64 // 内存阈值(MB) criticalThresholdMB int64 // 危险内存阈值(MB) mutex sync.RWMutex // 保护并发访问 // 网络接口（依赖外部实现） networkLayer NetworkInterface// NewPushManager 创建一个新的推送管理器func NewPushManager(networkLayer NetworkInterface) *PushManager pm := PushManager pendingMessages: make(map[string]*PendingMessageInfo), clientMessageCount: make(map[string]int), ackTimeout: 10, maxRetries: 3, maxPendingPerClient: 1000, maxMessageAge: 300, memoryThresholdMB: 1000, // 1GB criticalThresholdMB: 1500, // 1.5GB networkLayer: networkLayer, // 启动后台任务 go pm.checkTimeoutsLoop() go pm.cleanupLoop() go pm.memoryMonitorLoop() return pm// PushMessage 向客户端推送消息func (pm *PushManager) PushMessage(clientID string, message *Message) bool // 如果不需要确认，直接发送 if !message.RequiresAck return pm.networkLayer.SendToClient(clientID, message) pm.mutex.Lock() defer pm.mutex.Unlock() // 检查客户端消息数是否超限 if pm.clientMessageCount[clientID] = pm.maxPendingPerClient pm.handleQueueOverflow(clientID, message) return false // 存储待确认消息 pm.pendingMessages[message.MsgID] = PendingMessageInfo ClientID: clientID, Message: message, SentTime: time.Now().Unix(), RetryCount: 0, // 更新客户端消息计数 pm.clientMessageCount[clientID]++ // 发送消息 return pm.networkLayer.SendToClient(clientID, message)// ProcessAck 处理客户端的确认消息func (pm *PushManager) ProcessAck(clientID string, ack *AckMessage) bool pm.mutex.Lock() defer pm.mutex.Unlock() info, exists := pm.pendingMessages[ack.AckID] if !exists || info.ClientID != clientID return false // 确认成功，删除消息 delete(pm.pendingMessages, ack.AckID) pm.clientMessageCount[clientID]-- // 如果客户端没有待确认消息了，清理计数器 if pm.clientMessageCount[clientID] = 0 delete(pm.clientMessageCount, clientID) return true// ProcessBatchAck 处理批量确认func (pm *PushManager) ProcessBatchAck(clientID string, batchAck *BatchAckMessage) int pm.mutex.Lock() defer pm.mutex.Unlock() confirmedCount := 0 for _, ackID := range batchAck.AckIDs info, exists := pm.pendingMessages[ackID] if exists info.ClientID == clientID delete(pm.pendingMessages, ackID) pm.clientMessageCount[clientID]-- confirmedCount++ // 如果客户端没有待确认消息了，清理计数器 if pm.clientMessageCount[clientID] = 0 delete(pm.clientMessageCount, clientID) return confirmedCount// 后台任务：超时检查与重试func (pm *PushManager) checkTimeoutsLoop() ticker := time.NewTicker(5 * time.Second) defer ticker.Stop() for range ticker.C pm.checkTimeouts() // 超时检查与重试func (pm *PushManager) checkTimeouts() pm.mutex.Lock() defer pm.mutex.Unlock() now := time.Now().Unix() for msgID, info := range pm.pendingMessages // 检查是否超时 if now - info.SentTime pm.ackTimeout if info.RetryCount pm.maxRetries // 增加重试次数 info.RetryCount++ info.SentTime = now // 重新发送 pm.networkLayer.SendToClient(info.ClientID, info.Message) log.Printf(Retrying message %s to client %s, attempt %d, msgID, info.ClientID, info.RetryCount) else // 超出最大重试次数，放弃并记录 log.Printf(Message %s to client %s failed after %d attempts, msgID, info.ClientID, pm.maxRetries) delete(pm.pendingMessages, msgID) pm.clientMessageCount[info.ClientID]-- // 通知业务层处理失败 go pm.notifyMessageFailed(info.ClientID, info.Message) 解决内存暴涨问题 在大型游戏中，服务器可能同时维护数十万甚至上百万个连接，如果每个连接都有数百条待确认消息，服务器内存很快就会爆满。以下是我在实践中总结的几种高效内存管理策略： 1. 周期性过期消息清理 // 清理过期消息的后台循环func (pm *PushManager) cleanupLoop() ticker := time.NewTicker(1 * time.Minute) defer ticker.Stop() for range ticker.C pm.cleanExpiredMessages() // 清理过期消息func (pm *PushManager) cleanExpiredMessages() pm.mutex.Lock() defer pm.mutex.Unlock() now := time.Now().Unix() expiredCount := 0 for msgID, info := range pm.pendingMessages // 检查消息是否过期 if now - info.SentTime pm.maxMessageAge delete(pm.pendingMessages, msgID) pm.clientMessageCount[info.ClientID]-- expiredCount++ // 记录日志 log.Printf(Cleaned expired message %s to client %s (age: %d seconds), msgID, info.ClientID, now - info.SentTime) if expiredCount 0 log.Printf(Cleanup: Removed %d expired messages, expiredCount) 2. 消息压缩与合并 // CompressMessage 压缩消息以减少内存占用func CompressMessage(message *Message) []byte // 将消息转为JSON jsonData, err := json.Marshal(message) if err != nil log.Printf(Error marshaling message: %v, err) return nil // 使用gzip压缩 var buf bytes.Buffer writer := gzip.NewWriter(buf) _, err = writer.Write(jsonData) if err != nil log.Printf(Error compressing message: %v, err) return nil if err := writer.Close(); err != nil log.Printf(Error closing gzip writer: %v, err) return nil return buf.Bytes()// DecompressMessage 解压缩消息func DecompressMessage(compressed []byte) (*Message, error) reader, err := gzip.NewReader(bytes.NewReader(compressed)) if err != nil return nil, fmt.Errorf(create gzip reader: %w, err) defer reader.Close() var buf bytes.Buffer if _, err := io.Copy(buf, reader); err != nil return nil, fmt.Errorf(decompress data: %w, err) var message Message if err := json.Unmarshal(buf.Bytes(), message); err != nil return nil, fmt.Errorf(unmarshal json: %w, err) return message, nil 3. 分级存储策略 // PushManager 增加分级存储功能type PushManager struct // ... 之前的字段 ... // 内存中存储高优先级消息 memoryPending map[string]*PendingMessageInfo // Redis客户端，用于存储低优先级消息 redisClient *redis.Client redisKeyPrefix string redisExpiry time.Duration// PushMessage 分级存储版本func (pm *PushManager) PushMessage(clientID string, message *Message) bool // 如果不需要确认，直接发送 if !message.RequiresAck return pm.networkLayer.SendToClient(clientID, message) pm.mutex.Lock() defer pm.mutex.Unlock() // 检查客户端消息数量限制 if pm.clientMessageCount[clientID] = pm.maxPendingPerClient pm.handleQueueOverflow(clientID, message) return false pm.clientMessageCount[clientID]++ // 根据优先级选择存储位置 if message.Priority = 2 // 高优先级和中优先级 // 存入内存 pm.memoryPending[message.MsgID] = PendingMessageInfo ClientID: clientID, Message: message, SentTime: time.Now().Unix(), RetryCount: 0, else // 低优先级 // 存入Redis messageInfo := PendingMessageInfo ClientID: clientID, Message: message, SentTime: time.Now().Unix(), RetryCount: 0, jsonData, err := json.Marshal(messageInfo) if err != nil log.Printf(Error marshaling message: %v, err) pm.clientMessageCount[clientID]-- return false redisKey := pm.redisKeyPrefix + message.MsgID err = pm.redisClient.Set(context.Background(), redisKey, jsonData, pm.redisExpiry).Err() if err != nil log.Printf(Error storing message in Redis: %v, err) pm.clientMessageCount[clientID]-- return false // 发送消息 return pm.networkLayer.SendToClient(clientID, message)// ProcessAck 分级存储版本func (pm *PushManager) ProcessAck(clientID string, ack *AckMessage) bool pm.mutex.Lock() defer pm.mutex.Unlock() // 先检查内存中的消息 info, existsInMemory := pm.memoryPending[ack.AckID] if existsInMemory info.ClientID == clientID delete(pm.memoryPending, ack.AckID) pm.clientMessageCount[clientID]-- if pm.clientMessageCount[clientID] = 0 delete(pm.clientMessageCount, clientID) return true // 再检查Redis中的消息 redisKey := pm.redisKeyPrefix + ack.AckID exists, err := pm.redisClient.Exists(context.Background(), redisKey).Result() if err != nil log.Printf(Error checking message in Redis: %v, err) return false if exists == 1 // 获取消息以验证客户端ID jsonData, err := pm.redisClient.Get(context.Background(), redisKey).Bytes() if err != nil log.Printf(Error getting message from Redis: %v, err) return false var messageInfo PendingMessageInfo if err := json.Unmarshal(jsonData, messageInfo); err != nil log.Printf(Error unmarshaling message from Redis: %v, err) return false if messageInfo.ClientID == clientID // 从Redis删除并更新计数 pm.redisClient.Del(context.Background(), redisKey) pm.clientMessageCount[clientID]-- if pm.clientMessageCount[clientID] = 0 delete(pm.clientMessageCount, clientID) return true return false 4. 内存自适应调整 内存自适应调整是我在实际项目中解决突发流量问题的关键策略。它能够根据当前系统负载动态调整消息处理参数，确保系统稳定性。 // 内存监控循环func (pm *PushManager) memoryMonitorLoop() ticker := time.NewTicker(10 * time.Second) defer ticker.Stop() for range ticker.C memoryMB := pm.getMemoryUsageMB() if memoryMB pm.criticalThresholdMB // 紧急情况，进行应急清理 pm.emergencyCleanup(memoryMB) else if memoryMB pm.memoryThresholdMB // 超过警戒线，调整参数 pm.adjustParameters(memoryMB) // 获取当前进程内存使用量（MB）func (pm *PushManager) getMemoryUsageMB() int64 var memStats runtime.MemStats runtime.ReadMemStats(memStats) return int64(memStats.Alloc / 1024 / 1024)// 根据内存使用情况调整参数func (pm *PushManager) adjustParameters(currentMemoryMB int64) pm.mutex.Lock() defer pm.mutex.Unlock() // 计算内存超出比例 excessRatio := float64(currentMemoryMB - pm.memoryThresholdMB) / float64(pm.memoryThresholdMB) // 调整每客户端最大消息数 newMaxPerClient := int(float64(pm.maxPendingPerClient) * (1 - excessRatio*0.5)) if newMaxPerClient 100 newMaxPerClient = 100 // 确保至少保留100条 // 调整消息最大生存时间 newMaxAge := int64(float64(pm.maxMessageAge) * (1 - excessRatio*0.5)) if newMaxAge 60 newMaxAge = 60 // 至少60秒 // 更新参数 pm.maxPendingPerClient = newMaxPerClient pm.maxMessageAge = newMaxAge log.Printf(Memory usage: %d MB, adjusted parameters: maxPending=%d, maxAge=%ds, currentMemoryMB, pm.maxPendingPerClient, pm.maxMessageAge) // 执行一次清理 pm.cleanExpiredMessages()// 紧急清理func (pm *PushManager) emergencyCleanup(currentMemoryMB int64) pm.mutex.Lock() defer pm.mutex.Unlock() log.Printf(CRITICAL: Memory usage at %d MB, performing emergency cleanup, currentMemoryMB) // 大幅降低参数 pm.maxPendingPerClient = 100 pm.maxMessageAge = 60 // 清理低优先级消息 for msgID, info := range pm.memoryPending if info.Message.Priority 1 // 只保留最高优先级 delete(pm.memoryPending, msgID) pm.clientMessageCount[info.ClientID]-- log.Printf(Emergency cleanup completed) 5. 队列溢出处理策略 // 处理队列溢出func (pm *PushManager) handleQueueOverflow(clientID string, newMessage *Message) log.Printf(Queue overflow for client %s, clientID) // 策略1: 根据消息优先级决定是否替换现有消息 if newMessage.Priority == 1 // 高优先级消息 // 查找并替换该客户端的一条低优先级消息 for msgID, info := range pm.memoryPending if info.ClientID == clientID info.Message.Priority 1 // 记录 log.Printf(Replacing low priority message %s with high priority message, msgID) // 删除旧消息 delete(pm.memoryPending, msgID) // 添加新消息 pm.memoryPending[newMessage.MsgID] = PendingMessageInfo ClientID: clientID, Message: newMessage, SentTime: time.Now().Unix(), RetryCount: 0, // 发送新消息 pm.networkLayer.SendToClient(clientID, newMessage) return // 策略2: 丢弃旧消息以腾出空间 // 查找该客户端最旧的消息 var oldestMsgID string var oldestTime int64 = math.MaxInt64 for msgID, info := range pm.memoryPending if info.ClientID == clientID info.SentTime oldestTime oldestMsgID = msgID oldestTime = info.SentTime if oldestMsgID != log.Printf(Dropping oldest message %s for client %s, oldestMsgID, clientID) delete(pm.memoryPending, oldestMsgID) // 添加新消息 pm.memoryPending[newMessage.MsgID] = PendingMessageInfo ClientID: clientID, Message: newMessage, SentTime: time.Now().Unix(), RetryCount: 0, // 发送新消息 pm.networkLayer.SendToClient(clientID, newMessage) else // 极端情况，无法找到可替换的消息 log.Printf(Cannot find message to replace for client %s, clientID) 客户端实现 客户端实现同样关键，特别是批量确认机制能显著减少网络流量： // PushReceiver 客户端推送接收处理器type PushReceiver struct connection Connection // 网络连接接口 processedMsgIDs map[string]int64 // 已处理消息ID及处理时间 pendingAcks []string // 待确认的消息ID ackBatchSize int // 批量确认大小 ackInterval time.Duration // 批量确认间隔 messageHandlers map[string]MessageHandler // 消息处理函数 mutex sync.Mutex // 保护并发访问 stopChan chan struct // 停止信号// MessageHandler 消息处理函数类型type MessageHandler func(payload interface) error// NewPushReceiver 创建推送接收器func NewPushReceiver(conn Connection) *PushReceiver receiver := PushReceiver connection: conn, processedMsgIDs: make(map[string]int64), pendingAcks: make([]string, 0, 100), ackBatchSize: 50, ackInterval: time.Second, messageHandlers: make(map[string]MessageHandler), stopChan: make(chan struct), // 启动批量确认任务 go receiver.ackLoop() // 启动过期消息ID清理任务 go receiver.cleanupLoop() return receiver// RegisterHandler 注册消息处理函数func (r *PushReceiver) RegisterHandler(msgType string, handler MessageHandler) r.mutex.Lock() defer r.mutex.Unlock() r.messageHandlers[msgType] = handler// HandleMessage 处理收到的消息func (r *PushReceiver) HandleMessage(message *Message) r.mutex.Lock() defer r.mutex.Unlock() msgID := message.MsgID // 检查是否已处理过该消息 if _, exists := r.processedMsgIDs[msgID]; exists // 已处理过，再次发送确认 if message.RequiresAck r.pendingAcks = append(r.pendingAcks, msgID) // 如果积累的确认数量超过批量大小，立即发送 if len(r.pendingAcks) = r.ackBatchSize go r.sendBatchAcks() return // 查找处理函数 handler, exists := r.messageHandlers[message.MsgType] if !exists log.Printf(No handler for message type: %s, message.MsgType) // 未知消息类型也需要确认 if message.RequiresAck r.sendErrorAck(msgID, Unknown message type) return // 处理消息 err := handler(message.Payload) if err != nil log.Printf(Error processing message %s: %v, msgID, err) if message.RequiresAck r.sendErrorAck(msgID, err.Error()) return // 记录已处理的消息 r.processedMsgIDs[msgID] = time.Now().Unix() // 如果需要确认，加入待确认队列 if message.RequiresAck r.pendingAcks = append(r.pendingAcks, msgID) // 如果积累的确认数量超过批量大小，立即发送 if len(r.pendingAcks) = r.ackBatchSize go r.sendBatchAcks() // 发送批量确认func (r *PushReceiver) sendBatchAcks() r.mutex.Lock() // 如果没有待确认消息，直接返回 if len(r.pendingAcks) == 0 r.mutex.Unlock() return // 复制当前的待确认ID列表 ackIDs := make([]string, len(r.pendingAcks)) copy(ackIDs, r.pendingAcks) // 清空待确认列表 r.pendingAcks = r.pendingAcks[:0] r.mutex.Unlock() // 创建批量确认消息 batchAck := BatchAckMessage BatchAck: true, AckIDs: ackIDs, Status: success, ClientTimestamp: time.Now().Unix(), // 发送确认 r.connection.Send(batchAck)// 发送错误确认func (r *PushReceiver) sendErrorAck(msgID string, errorMessage string) ack := AckMessage AckID: msgID, Status: failed, ClientTimestamp: time.Now().Unix(), ErrorCode: 1001, ErrorMessage: errorMessage, r.connection.Send(ack)// 批量确认定时器func (r *PushReceiver) ackLoop() ticker := time.NewTicker(r.ackInterval) defer ticker.Stop() for select case -ticker.C: r.sendBatchAcks() case -r.stopChan: return // 清理过期的已处理消息IDfunc (r *PushReceiver) cleanupLoop() // 每小时清理一次 ticker := time.NewTicker(1 * time.Hour) defer ticker.Stop() for select case -ticker.C: r.cleanupProcessedIDs() case -r.stopChan: return // 清理过期的已处理消息IDfunc (r *PushReceiver) cleanupProcessedIDs() r.mutex.Lock() defer r.mutex.Unlock() now := time.Now().Unix() expireTime := int64(86400) // 24小时过期 for msgID, processTime := range r.processedMsgIDs if now - processTime expireTime delete(r.processedMsgIDs, msgID) // Close 关闭推送接收器func (r *PushReceiver) Close() // 发送所有待确认消息 r.sendBatchAcks() // 停止所有后台任务 close(r.stopChan) 实战经验与最佳实践 在多个千万用户级别的游戏项目实践中，我总结了以下几点 Push-ACK 机制的最佳实践： 1. 消息分级是关键 不是所有消息都需要相同级别的可靠性保证。在一个 MMORPG 项目中，我们将消息分为四级： 关键级：直接影响游戏平衡和经济的消息，如道具获取、货币变化 重要级：影响游戏进程的消息，如任务更新、排行榜变动 普通级：一般游戏状态信息，如其他玩家动作、环境变化 低优先级：可以容忍丢失的背景信息，如聊天、天气效果 高级别消息使用完整的 ACK 机制，低级别消息可以简化甚至取消 ACK 需求，这样大大减轻了服务器内存压力。 2. 利用统计指标进行调优 监控以下关键指标： ACK 响应时间分布 消息重试率 每客户端平均待确认消息数 内存使用增长曲线 在一个足球经理类游戏中，通过这些指标我们发现，将 ACK 超时时间从 10 秒调整到 5 秒，并将最大重试次数从 3 次增加到 5 次，可以将消息最终确认率从 99.2%提高到 99.8%，同时减少了 25%的内存使用。 3. 针对不同网络环境优化 移动网络环境差异很大，针对不同网络条件动态调整策略： // 根据网络条件调整参数func (pm *PushManager) adjustForNetworkCondition(clientID string, rtt time.Duration) // 网络条件良好 if rtt 100*time.Millisecond pm.clientTimeouts[clientID] = 3 // 3秒超时 pm.clientRetries[clientID] = 2 // 2次重试 else if rtt 300*time.Millisecond pm.clientTimeouts[clientID] = 5 // 5秒超时 pm.clientRetries[clientID] = 3 // 3次重试 else pm.clientTimeouts[clientID] = 10 // 10秒超时 pm.clientRetries[clientID] = 5 // 5次重试 4. 定期压力测试 在一个大型开放世界游戏中，我们每月进行一次\"混沌测试\"，模拟极端情况： 突发 50%客户端同时掉线然后重连 模拟网络延迟突然从 50ms 增加到 500ms 模拟 10%的确认消息丢失 这种测试让我们发现了很多边缘情况，并建立了更健壮的防御机制。 结论 一个设计良好的 Push-ACK 机制是现代游戏服务器架构的核心组件。它确保了游戏状态的一致性，提升了玩家体验，同时也为运营团队提供了可靠的数据基础。最重要的是，它必须是高性能且资源友好的。 通过采用本文介绍的多级存储、自适应参数调整、消息优先级和过期策略等技术，我们可以构建一个既可靠又高效的推送确认系统，即使在面对数十万并发","tags":["解决方案","游戏后端","push-ack"],"categories":["解决方案","游戏后端"]},{"title":"服务监控丨Prometheus 四大数据类型详解","path":"/2025/02/26/prometheus-data-type/","content":"前言 在微服务和云原生架构的世界中，一套强大的监控系统是保障服务稳定性的基石。Prometheus 作为 CNCF 的明星项目，凭借其简单高效的特性，已成为事实上的云原生监控标准。本文将深入剖析 Prometheus 的四大数据类型及其 PromQL 查询语言，帮助开发团队构建强大的可观测性系统。 结论先行：Prometheus 四大数据类型速览 特性 Counter Gauge Histogram Summary 定义 只增不减的累积计数器 可增可减的瞬时值 观测值分布的分桶统计 客户端计算的分位数统计 重置行为 服务重启时归零 保持当前值 桶计数归零 计数归零 典型应用 请求计数、错误数、流量统计 温度、内存使用、连接数 请求延迟、响应大小 请求延迟、队列等待时间 数据点 单一值 单一值 _bucket、_sum、count {quantile=\"x\"}、_sum、_count 查询重点 rate()、increase() 直接使用、预测函数 histogram_quantile() 直接读取分位数 分布式聚合 可以（sum、rate） 可以（avg、max、min） 可以（百分位也可聚合） 有限（分位数不可聚合） 资源消耗 低 低 中（依赖桶数量） 中（客户端计算） 一、Prometheus 核心数据类型详解 1. Counter（计数器）：持续增长的累积值 Counter 是最简单但也最常用的指标类型，代表一个只增不减的累积数值。每当事件发生，计数器增加；当监控目标重启时，计数器归零。 适用场景： API 请求总数 错误发生次数 处理任务的数量 网络流量字节数 正确的代码实现： // 声明带标签的计数器requestCounter := prometheus.NewCounterVec( prometheus.CounterOpts Name: http_requests_total, Help: Total number of HTTP requests, , []stringmethod, path, status, // 定义标签维度)prometheus.MustRegister(requestCounter)// 使用标签记录请求requestCounter.WithLabelValues(GET, /api/users, 200).Inc() PromQL 查询技巧： # 每秒请求率（5分钟窗口）rate(http_requests_totalstatus=200[5m])# 错误率计算sum(rate(http_requests_totalstatus=~5..[5m])) / sum(rate(http_requests_total[5m]))# 1小时内的请求增量increase(http_requests_total[1h]) 最佳实践： 永远不要直接使用 Counter 的原始值，总是使用 rate() 或 increase() 使用有意义的标签进行多维度分析，但避免高基数标签 Counter 重置（如服务重启）会被 rate() 函数自动处理 2. Gauge（仪表盘）：可变的瞬时值 Gauge 表示一个可增可减的瞬时测量值，反映系统的当前状态。 适用场景： 内存使用量 CPU 使用率 当前活跃连接数 队列深度 温度等物理量 正确的代码实现： // 声明带标签的仪表盘memoryGauge := prometheus.NewGaugeVec( prometheus.GaugeOpts Name: app_memory_usage_bytes, Help: Current memory usage in bytes, , []stringcomponent, instance,)prometheus.MustRegister(memoryGauge)// 设置当前值memoryGauge.WithLabelValues(api-server, instance-1).Set(float64(getCurrentMemoryUsage())) PromQL 查询技巧： # 直接使用当前值app_memory_usage_bytescomponent=api-server# 统计聚合avg_over_time(app_memory_usage_bytes[1h])max_over_time(app_memory_usage_bytes[24h])# 趋势预测（线性回归）predict_linear(app_memory_usage_bytes[6h], 4 * 3600)# 计算变化率(app_memory_usage_bytes - app_memory_usage_bytes offset 1h) / app_memory_usage_bytes offset 1h 最佳实践： Gauge 可以直接使用其瞬时值，不需要像 Counter 那样使用 rate 对于容易波动的指标，考虑使用 avg_over_time 平滑数据 利用 predict_linear 进行容量规划和趋势预测 3. Histogram（直方图）：观测值分布的分桶统计 Histogram 允许对观测值（如请求延迟）进行分布式统计，将数据分散到预定义的桶中，是分析性能分布的理想工具。 自动生成的指标： metric_bucketle=\"upper bound\": 小于等于特定阈值的观测值计数 metric_sum: 所有观测值的总和 metric_count: 观测值总数 适用场景： 请求延迟分布 响应大小分布 批处理任务执行时间 任何需要百分位数分析的场景 正确的代码实现： // 声明带标签的直方图durationHistogram := prometheus.NewHistogramVec( prometheus.HistogramOpts Name: http_request_duration_seconds, Help: HTTP request duration in seconds, Buckets: prometheus.ExponentialBuckets(0.001, 2, 10), // 从1ms开始指数增长 , []stringmethod, path,)prometheus.MustRegister(durationHistogram)// 记录请求延迟durationHistogram.WithLabelValues(GET, /api/users).Observe(responseTime) PromQL 查询技巧： # 计算平均响应时间rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])# 计算P90延迟histogram_quantile(0.9, rate(http_request_duration_seconds_bucket[5m]))# 按API路径分析P95延迟histogram_quantile(0.95, sum by(path, le) (rate(http_request_duration_seconds_bucket[5m])))# 计算SLO：延迟小于100ms的请求比例sum(rate(http_request_duration_seconds_bucketle=0.1[5m])) / sum(rate(http_request_duration_seconds_count[5m])) 最佳实践： 仔细设计桶边界，覆盖关键分位数区域 对于延迟指标，通常使用指数桶比线性桶更合理 利用 histogram_quantile 计算任意分位数 桶的数量会影响存储和性能，权衡精度和开销 4. Summary（摘要）：客户端计算的分位数统计 Summary 与 Histogram 类似，但在客户端直接计算并存储分位数，无需服务器端计算。 自动生成的指标： metricquantile=\"φ\": φ 分位数的值 metric_sum: 所有观测值的总和 metric_count: 观测值总数 适用场景： 需要高精度分位数的场景 客户端计算分位数更高效的情况 对服务器端聚合要求不高的场景 正确的代码实现： // 声明带标签的摘要durationSummary := prometheus.NewSummaryVec( prometheus.SummaryOpts Name: http_request_duration_seconds_summary, Help: HTTP request duration in seconds, Objectives: map[float64]float640.5: 0.05, 0.9: 0.01, 0.99: 0.001, , []stringmethod, path,)prometheus.MustRegister(durationSummary)// 记录请求延迟durationSummary.WithLabelValues(POST, /api/login).Observe(responseTime) PromQL 查询技巧： # 直接读取P99延迟http_request_duration_seconds_summaryquantile=0.99, method=GET, path=/api/users# 计算平均响应时间rate(http_request_duration_seconds_summary_sum[5m]) / rate(http_request_duration_seconds_summary_count[5m])# 每个服务的中位数延迟max by(service) (http_request_duration_seconds_summaryquantile=0.5) 最佳实践与限制： Summary 预计算的分位数不能跨实例聚合（这是关键限制） 适用于分位数精度要求高且实例相对独立的场景 客户端计算分位数会增加应用资源消耗 分位数设置后不可更改，需提前规划好监控需求 二、PromQL 查询语言精通 PromQL 是 Prometheus 的强大武器，掌握它能让我们精确提取所需的监控数据。 1. 基础查询与标签选择 # 基本查询与精确匹配http_requests_totalstatus=200, method=GET# 正则表达式匹配http_requests_totalpath=~/api/v1/.+, method!=OPTIONS# 范围查询（返回时间序列）http_requests_totalstatus=500[5m] 2. 操作符与函数 算术运算符： # 计算内存使用率百分比100 * (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes 聚合函数： # 按服务和路径分组求和sum by(service, path) (rate(http_requests_total[5m]))# 丢弃instance标签求最大值max without(instance) (node_cpu_seconds_total) 瞬时向量函数： # 标签替换label_replace(up, host, $1, instance, (.*):.*)# 按标签分组取topktopk by(path) (5, http_request_duration_seconds_sum / http_request_duration_seconds_count) 3. 复杂查询模式 SLI/SLO 监控： # 服务可用性SLIsum(rate(http_requests_totalstatus=~2..|3..[5m])) / sum(rate(http_requests_total[5m]))# 延迟SLOhistogram_quantile(0.99, sum by(le) (rate(http_request_duration_seconds_bucket[5m]))) 0.3 异常检测： # 相对于历史同期的异常增长rate(http_requests_total[5m]) 2 * avg_over_time(rate(http_requests_total[5m])[1d:5m] offset 1d) 预测分析： # 磁盘空间预测predict_linear(node_filesystem_free_bytesmountpoint=/[6h], 7 * 24 * 3600) 10 * 1024 * 1024 * 1024 三、实战应用场景 1. 服务健康度监控 RED 方法实现： # Rate - 请求率sum by(service) (rate(http_requests_total[5m]))# Error - 错误率sum by(service) (rate(http_requests_totalstatus=~5..[5m])) / sum by(service) (rate(http_requests_total[5m]))# Duration - P95延迟histogram_quantile(0.95, sum by(service, le) (rate(http_request_duration_seconds_bucket[5m]))) 服务依赖健康度： # 数据库查询错误率sum(rate(database_query_errors_total[5m])) / sum(rate(database_queries_total[5m]))# 第三方API调用延迟histogram_quantile(0.99, sum by(api_name, le) (rate(api_request_duration_seconds_bucket[5m]))) 2. 性能瓶颈分析 热点 API 发现： # 延迟最高的10个接口topk(10, histogram_quantile(0.95, sum by(method, path, le) (rate(http_request_duration_seconds_bucket[5m]))))# 请求量最大的接口topk(10, sum by(method, path) (rate(http_requests_total[5m]))) 数据库性能分析： # 平均查询时间趋势rate(db_query_duration_seconds_sum[5m]) / rate(db_query_duration_seconds_count[5m])# 慢查询比例sum(rate(db_query_duration_seconds_bucketle=+Inf[5m])) - sum(rate(db_query_duration_seconds_bucketle=0.1[5m])) / sum(rate(db_query_duration_seconds_bucketle=+Inf[5m])) 3. 容量规划与告警 资源预测： # CPU使用率预测predict_linear(avg by(instance) (rate(node_cpu_seconds_totalmode!=idle[6h])) [3d:], 7 * 24 * 3600) 0.85# 内存压力告警(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes 0.9 流量容量规划： # 带宽使用预测predict_linear(rate(node_network_transmit_bytes_total[12h])[7d:], 30 * 24 * 3600) 四、最佳实践与性能优化 1. 指标命名与标签设计 命名规范： 使用 snake_case 包含单位后缀（_bytes, _seconds, _total） 保持风格一致性 标签最佳实践： // 合理设计标签维度apiLatency := prometheus.NewHistogramVec( prometheus.HistogramOpts Name: api_request_duration_seconds, Help: API request duration in seconds, Buckets: prometheus.ExponentialBuckets(0.001, 2, 10), , []stringservice, endpoint, status_code, // 合理的低基数标签)// 不可变标签使用ConstLabelsprometheus.NewGaugeVec( prometheus.GaugeOpts Name: service_info, Help: Service information, ConstLabels: prometheus.Labelsversion: v2.1.3, environment: production, , []stringinstance,) 2. 客户端性能优化 // 缓存常用标签组合以提高性能getCounter := requestCounter.WithLabelValues(GET, /api/users, 200)for i := 0; i 100; i++ getCounter.Inc() // 重用标签组合，避免重复创建// 批量更新方式var rpcDurations = prometheus.NewSummaryVec( prometheus.SummaryOpts Name: rpc_durations_seconds, Help: RPC latency distributions., Objectives: map[float64]float640.5: 0.05, 0.9: 0.01, 0.99: 0.001, , []stringservice,)func ObserveBatch(durations map[string]float64) for service, duration := range durations rpcDurations.WithLabelValues(service).Observe(duration) 3. 查询优化 # 优化前：高基数查询sum(rate(http_requests_totalpath=~/api/.*[5m])) by (path, method, status)# 优化后：降低基数，按需聚合sum(rate(http_requests_totalpath=~/api/.*[5m])) by (method, status)# 优化聚合顺序（先聚合再求和）sum( avg by(instance) (rate(node_cpu_seconds_totalmode!=idle[5m]))) 五、常见陷阱与解决方案 1. 高基数问题 问题：标签组合过多导致时间序列爆炸 解决方案： 限制标签基数，避免使用 UserID、SessionID 等作为标签 使用label_replace和正则表达式转换高基数标签 考虑使用 Exemplars 而非标签存储高基数数据 2. 数据类型选择误区 Counter vs Gauge：请求数应使用 Counter 而非 Gauge Histogram vs Summary：需要聚合分析请使用 Histogram，精确分位数可选 Summary 3. 查询性能问题 问题：复杂查询导致 Prometheus 高负载 解决方案： 使用记录规则预计算常用查询 合理设置 scrape 间隔，避免过度采集 对高请求量接口使用客户端聚合 总结与展望 Prometheus 的四种数据类型各有所长：Counter 适合累积事件计数，Gauge 适合瞬时状态测量，Histogram 适合分布统计和百分位分析，Summary 适合客户端精确分位数计算。与之配合的 PromQL 提供了强大的数据查询和分析能力，共同构成了完整的监控解决方案。 随着云原生技术的发展，Prometheus 生态也在不断壮大，与 Grafana、Alertmanager、Thanos 等工具集成，能够构建更完善的监控告警平台。在微服务架构中，结合 RED（Rate、Error、Duration）和 USE（Utilization、Saturation、Errors）方法论，可以构建全面的可观测性系统。 无论你是刚开始使用 Prometheus 的新手，还是寻求优化监控系统的资深工程师，希望本文对你理解和应用 Prometheus 有所帮助。记住，好的监控不仅能及时发现问题，更能预测和防范问题，最终服务于业务可靠性和用户体验的提升。 参考资源: Prometheus 官方文档: https://prometheus.io/docs/ Google SRE 书籍: https://sre.google/sre-book/monitoring-distributed-systems/ Prometheus 实战: https://prometheusbook.com/","tags":["服务监控","prometheus"],"categories":["服务监控"]},{"title":"在 Go 项目中实现 JWT 用户认证与续期机制","path":"/2025/02/15/go-action-jwt/","content":"JWT (JSON Web Token) 是一种广泛使用的用户认证方案，因其无状态、跨域支持和灵活性而受到欢迎。本文将结合实际代码，详细讲解如何在 Go 项目中实现 JWT 认证机制，并探讨两种常见的 Token 续期策略：自动续期和 Refresh Token。 1. JWT 基础概念 JWT 由三部分组成：Header、Payload 和 Signature。使用 JWT 进行登录认证的基本工作流程是： 用户登录成功后，服务器生成 JWT。 服务器将 token 返回给客户端。 客户端后续请求携带 token。 服务器验证 token 的有效性。 我们可以在 https://jwt.io/ 网站对 JWT 进行分析，查看其具体的组成成分。 2. 基本准备 在本篇，我们将使用 Go 语言，通过一个完整的案例实现在 HTTP 接口中，使用 JWT 进行用户登录和认证流程。本文假设读者已掌握基本的 Go 语言语法和网络编程经验，并对 Gin 框架有基本的了解。 为了快速响应失败，本文案例中使用了封装好的异常处理机制： package utilsvar (\tErrUser = errors.New()\tErrSys = errors.New())// 定义用户侧错误，会直接将错误内容返回给用户，不打印日志。func UserErr(msg string) error return fmt.Errorf(%w%v, ErrUser, msg)func UserErrf(format string, a ...any) error return fmt.Errorf(%w%v, ErrUser, fmt.Sprintf(format, a...))// 定义系统内部错误，会固定返回 internal server error 给用户，但是会将原始错误信息输出到日志中，便于内部排查。func SystemErr(err error) error return fmt.Errorf(%w%v, ErrSys, err)func SystemErrf(format string, a ...any) error return fmt.Errorf(%w%v, ErrSys, fmt.Sprintf(format, a...))func GinErr(c *gin.Context, req any, err error, msgs ...string) if errors.Is(err, ErrUser) c.JSON(http.StatusOK, err.Error()) return msg := internal server error\tif len(msgs) 0 msg = msgs[0] slog.Error(msg, slog.Any(req, req), slog.String(err, err.Error()),\t)\tc.JSON(http.StatusOK, internal server error) 3. 实现用户认证 在进行实际代码编写之前，你需要先初始化好项目并引入 jwt 依赖： go get -u github.com/golang-jwt/jwt/v5 在代码中使用的时候，可以： import github.com/golang-jwt/jwt/v5 那接下来我们就正式开始我们的功能实现。 3.1 定义 Claims 结构 首先，我们需要定义 JWT 的载荷（Payload）结构，即决定将什么信息存储在 token 当中。 type UserClaims struct jwt.RegisteredClaims UserID uint64 `json:user_id` // 用户ID UserAgent string `json:user_agent` // 用户设备信息 这里我们： 组合了 jwt.RegisteredClaims，它包含了标准的 JWT 字段（如过期时间），帮助我们实现了 jwt.Clamis 接口： type Claims interface GetExpirationTime() (*NumericDate, error)\tGetIssuedAt() (*NumericDate, error)\tGetNotBefore() (*NumericDate, error)\tGetIssuer() (string, error)\tGetSubject() (string, error)\tGetAudience() (ClaimStrings, error) jwt.RegisteredClaims 的实现如下： type RegisteredClaims struct Issuer string `json:iss,omitempty`\tSubject string `json:sub,omitempty`\tAudience ClaimStrings `json:aud,omitempty`\tExpiresAt *NumericDate `json:exp,omitempty`\tNotBefore *NumericDate `json:nbf,omitempty`\tIssuedAt *NumericDate `json:iat,omitempty`\tID string `json:jti,omitempty`func (c RegisteredClaims) GetExpirationTime() (*NumericDate, error) return c.ExpiresAt, nilfunc (c RegisteredClaims) GetNotBefore() (*NumericDate, error) return c.NotBefore, nilfunc (c RegisteredClaims) GetIssuedAt() (*NumericDate, error) return c.IssuedAt, nilfunc (c RegisteredClaims) GetAudience() (ClaimStrings, error) return c.Audience, nilfunc (c RegisteredClaims) GetIssuer() (string, error) return c.Issuer, nilfunc (c RegisteredClaims) GetSubject() (string, error) return c.Subject, nil 添加了自定义字段 UserID 和 UserAgent 用于安全控制。你可以根据自己的业务需求，添加任意非敏感信息到这个结构中。 3.2 登录接口实现 const ( AccessTokenDuration = time.Minute * 15 RefreshTokenDuration = time.Hour * 24 * 7)func (u *UserHandler) LoginJWT(ctx *gin.Context) // 1. 校验用户信息，在本案例中，使用邮箱加密码进行登录 user, err := u.svc.Login(ctx.Request.Context(), req.Email, req.Password) if err != nil utils.GinErr(ctx, req, utils.UserErr(err), login failed) return // 2. 创建 JWT Claims accessClaims := UserClaims UserID: user.ID, UserAgent: ctx.Request.UserAgent(), RegisteredClaims: jwt.RegisteredClaims ExpiresAt: jwt.NewNumericDate(time.Now().Add(AccessTokenDuration)), // 15分钟过期 , // 3. 生成 Access Token accessToken := jwt.NewWithClaims(jwt.SigningMethodHS512, accessClaims) accessTokenStr, err := accessToken.SignedString(AccessTokenKey) if err != nil utils.GinErr(ctx, req, utils.SystemErr(err), generate access token failed) return // 4. 生成 Refresh Token，用于 Token 续期 refreshClaims := RefreshClaims UserID: user.ID, UserAgent: ctx.Request.UserAgent(), RegisteredClaims: jwt.RegisteredClaims ExpiresAt: jwt.NewNumericDate(time.Now().Add(RefreshTokenDuration)), // 7天过期 , refreshToken := jwt.NewWithClaims(jwt.SigningMethodHS512, refreshClaims) refreshTokenStr, err := refreshToken.SignedString(RefreshTokenKey) if err != nil utils.GinErr(ctx, req, utils.SystemErr(err), generate refresh token failed) return // 5. 返回两个 token ctx.Header(x-jwt-token, accessTokenStr) ctx.Header(x-refresh-token, refreshTokenStr) ctx.JSON(http.StatusOK, login success) 3.3 JWT 中间件实现 type LoginJWTMiddlewareBuilder struct whiteList []stringfunc NewLoginJWTMiddlewareBuilder() *LoginJWTMiddlewareBuilder return LoginJWTMiddlewareBuilder whiteList: []string,\tfunc (b *LoginJWTMiddlewareBuilder) IgnorePaths(paths ...string) *LoginJWTMiddlewareBuilder b.whiteList = append(b.whiteList, paths...)\treturn bfunc (b *LoginJWTMiddlewareBuilder) Build() gin.HandlerFunc return func(ctx *gin.Context) // 1. 提取 token authCode := ctx.GetHeader(Authorization) tokenStr := strings.TrimPrefix(authCode, Bearer ) // 2. 解析和验证 token uc := web.UserClaims token, err := jwt.ParseWithClaims(tokenStr, uc, func(token *jwt.Token) (interface, error) return web.AccessTokenKey, nil ) // 3. 验证 token 有效性 if token == nil || !token.Valid ctx.AbortWithStatus(http.StatusUnauthorized) return // 4. 验证 UserAgent if uc.UserAgent != ctx.Request.UserAgent() ctx.AbortWithStatus(http.StatusUnauthorized) return // 5. 设置用户信息到上下文 ctx.Set(user_id, uc.UserID) ctx.Set(claims, uc) 3.4 注册中间件 func initWebServer() *gin.Engine server := gin.Default()\tserver.Use( middleware.CORS(), middleware.NewLoginJWTMiddlewareBuilder(). IgnorePaths(/users/signup). IgnorePaths(/users/login). Build(),\t)\tweb.RegisterRoutes(server)\treturn serverfunc RegisterRoutes(server *gin.Engine) // ...\tuserHandler.RegisterRoutes(server)func (u *UserHandler) RegisterRoutes(server *gin.Engine) ur := server.Group(/users) ur.POST(/login, u.LoginJWT) // ... 4. 在其他接口中使用 Token 的相关信息 func (u *UserHandler) Profile(ctx *gin.Context) // 可以获取 user_id\tuserID := ctx.GetUint64(user_id) // 也可以直接获取整个 claims。 // 这里我们可以选择不进行断言，因为理论上我们的可以保证这里通过断言。 // 如果这里发生 panic 了，则说明我们的内部逻辑没有形成闭环，存在问题。 // panic 可以第一时间暴露问题，然后被解决掉。 // 不过这个时候建议你使用 gin 的 recover 中间件进行全局保护，避免整个服务因为 panic 而宕机。 uc, _ := ctx.Get(claims)\tuserClaims := uc.(*UserClaims) // ... 5. Refresh Token 机制 5.1 添加刷新 Token 接口 func (u *UserHandler) RefreshToken(ctx *gin.Context) // 从请求头获取 Refresh Token refreshTokenStr := ctx.GetHeader(x-refresh-token) if refreshTokenStr == ctx.AbortWithStatus(http.StatusUnauthorized) return // 解析和验证 Refresh Token var refreshClaims RefreshClaims refreshToken, err := jwt.ParseWithClaims(refreshTokenStr, refreshClaims, func(token *jwt.Token) (interface, error) return RefreshTokenKey, nil ) if err != nil || !refreshToken.Valid ctx.AbortWithStatus(http.StatusUnauthorized) return // 验证 User Agent if refreshClaims.UserAgent != ctx.Request.UserAgent() ctx.AbortWithStatus(http.StatusUnauthorized) return // 生成新的 Access Token accessClaims := UserClaims UserID: refreshClaims.UserID, UserAgent: ctx.Request.UserAgent(), RegisteredClaims: jwt.RegisteredClaims ExpiresAt: jwt.NewNumericDate(time.Now().Add(AccessTokenDuration)), , newAccessToken := jwt.NewWithClaims(jwt.SigningMethodHS512, accessClaims) newAccessTokenStr, err := newAccessToken.SignedString(AccessTokenKey) if err != nil utils.GinErr(ctx, nil, utils.SystemErr(err), generate new access token failed) return // 对 Refresh Token 进行续期 refreshClaims := RefreshClaims UserID: user.ID, UserAgent: ctx.Request.UserAgent(), RegisteredClaims: jwt.RegisteredClaims ExpiresAt: jwt.NewNumericDate(time.Now().Add(RefreshTokenDuration)), // 7天过期 , newRefreshToken := jwt.NewWithClaims(jwt.SigningMethodHS512, refreshClaims) newRefreshTokenStr, err := newRefreshToken.SignedString(RefreshTokenKey) if err != nil utils.GinErr(ctx, req, utils.SystemErr(err), generate new refresh token failed) return // 返回新的 Access Token 和续期后的 Refresh Token ctx.Header(x-jwt-token, newAccessTokenStr) ctx.Header(x-refresh-token, newRefreshTokenStr) ctx.JSON(http.StatusOK, token refreshed) 5.2 注册路由 在 RegisterRoutes 方法中添加新路由： func (u *UserHandler) RegisterRoutes(server *gin.Engine) ur := server.Group(/users) ur.POST(/login, u.LoginJWT) ur.GET(/profile, u.Profile) ur.POST(/refresh, u.RefreshToken) 6. 客户端使用流程 登录后获取 Access Token 和 Refresh Token 使用 Access Token 访问受保护资源 当 Access Token 过期时调用 /refresh 接口获取新的 Access Token 使用新的 Access Token 继续访问 刷新 token 的客户端示例代码（笔者并不擅长写前端代码 hhh，所以这是让 ChatGPT 帮忙写的 😄）： async function refreshAccessToken() const response = await fetch(/users/refresh, method: POST, headers: x-refresh-token: localStorage.getItem(refreshToken) ); if (response.ok) const newAccessToken = response.headers.get(x-jwt-token); localStorage.setItem(accessToken, newAccessToken); const newRefreshToken = response.headers.get(x-refresh-token); localStorage.setItem(refreshToken, newRefreshToken); return newAccessToken; // 如果刷新失败，重定向到登录页 window.location.href = /login; 7. Token 续期策略对比 在前面案例中，细心的读者可以观察到我们对 AccessToken 和 RefreshToken 分别采用了 2 种不同的续期策略。 自动续期 优点： 简单易用：在每次请求时自动检查并续期 Token，用户体验流畅。 无额外存储需求：不需要存储 Refresh Token，减少了存储和管理的复杂性 缺点： 安全性较低：如果 Token 被盗用，攻击者可以通过自动续期保持长时间的访问。 Token 过期时间不固定：Token 的有效期会不断延长，难以控制。 Refresh Token 优点： 更高的安全性：即使 Access Token 被盗用，攻击者也无法续期，除非同时获取 Refresh Token。 可控的 Token 生命周期：Access Token 有固定的短期有效期，Refresh Token 有较长的有效期。 支持 Token 撤销：可以实现 Refresh Token 的黑名单机制，支持手动撤销。 缺点： 实现复杂度较高：需要额外的接口和逻辑来处理 Refresh Token。 存储需求：需要安全存储 Refresh Token，可能需要数据库支持。 8. 总结 JWT 实现用户认证的优势在于无状态、跨域支持和灵活性。通过合理使用 JWT 和选择合适的 Token 续期策略，我们可以构建安全、可靠的用户认证系统。希望本文能帮助您在 Go 项目中更好地实现 JWT 认证。","tags":["go","JWT","go 实战"],"categories":["go","go 实战"]},{"title":"深入 Go 语言核心：map 和 slice 的传参有什么不同","path":"/2025/02/14/go-slice-vs-map/","content":"在 Go 开发中，经常会遇到需要在函数中修改 map 或 slice 的场景。虽然它们都支持动态扩容，但在函数传参时的行为却大不相同。今天，让我们通过实例深入理解这个问题。 一个困惑的开始 看这样一个例子： func main() // Map 示例 m := map[string]intold: 1 modifyMap(m) fmt.Println(m) // 输出: map[new:1] // Slice 示例 s := []int1, 2, 3 modifySlice(s) fmt.Println(s) // 输出: [100 2 3]，而不是 [100 2 3 200]func modifyMap(m map[string]int) m[new] = 1 // 会影响原始 map delete(m, old) // 也会影响原始 mapfunc modifySlice(s []int) s[0] = 100 // 会影响原始 slice s = append(s, 200) // 不会影响原始 slice 有趣的是： map 的所有操作都会影响原始数据 slice 的简单索引修改会影响原始数据，但 append 可能不会 为什么会这样？让我们从内部结构开始分析。 内部结构解析 Map 的内部结构 type hmap struct count int // 元素个数 flags uint8 // 状态标志 B uint8 // 桶的对数 B buckets unsafe.Pointer // 指向桶数组的指针 // ... 其他字段 当我们声明一个 map 变量时： m := make(map[string]int)// 实际上 m 是 *hmap，即指向 hmap 结构的指针 Slice 的内部结构 type slice struct array unsafe.Pointer // 指向底层数组的指针 len int // 当前长度 cap int // 当前容量 当我们声明一个 slice 变量时： s := make([]int, 0, 10)// s 是一个完整的 slice 结构体，而不是指针 深入理解传参行为 场景一：简单修改（不涉及扩容） func modifyBoth(m map[string]int, s []int) m[key] = 1 // 通过指针修改原始 map s[0] = 100 // 通过指向相同底层数组的指针修改 图解： Map:main()中的 m ----- hmap... ----- modifyBoth()中的 m(同一个底层结构)Slice:main()中的 s = slicearray: 指向数组1, len: 3, cap: 3 | v [1 2 3] ^modifyBoth()中的 s = slicearray: 指向数组1, len: 3, cap: 3 场景二：涉及扩容的操作 func expandBoth(m map[string]int, s []int) // map 扩容 for i := 0; i 100; i++ m[fmt.Sprintf(key%d, i)] = i // slice 扩容 s = append(s, 200) 图解： Map 扩容过程：Before:main()中的 m ----- hmapbuckets: 指向存储A ^expandBoth()中的 m ---------|After:main()中的 m ----- hmapbuckets: 指向更大的存储B // 同一个 hmap，只是更新了内部指针 ^expandBoth()中的 m ---------|Slice 扩容过程：Before:main()中的 s = slicearray: 指向数组A, len: 3, cap: 3 | v [1 2 3] ^expandBoth()中的 s = slicearray: 指向数组A, len: 3, cap: 3After append:main()中的 s = slicearray: 指向数组A, len: 3, cap: 3 // 保持不变 | v [1 2 3]expandBoth()中的 s = slicearray: 指向数组B, len: 4, cap: 6 // 新的结构体，指向新数组 | v [1 2 3 200] 关键区别解析 传递方式不同： map 传递的是指针，函数内外使用的是同一个 hmap 结构 slice 传递的是结构体副本，函数内的修改发生在副本上 扩容行为不同： map 扩容时，原有的 hmap 结构保持不变，只更新内部的 buckets 指针 slice 扩容时，会创建新的底层数组，并返回一个指向新数组的新 slice 结构体 修改效果不同： map 的所有操作（包括扩容）都会反映到原始数据 slice 的行为分两种情况： 不涉及扩容的修改会影响原始数据（因为指向同一个底层数组） 涉及扩容的操作（如 append）会创建新的底层数组，修改不会影响原始数据 最佳实践 基于以上原理，在编码时应注意： 对于 map： func modifyMap(m map[string]int) m[key] = 1 // 直接修改即可，不需要返回 对于 slice： func modifySlice(s []int) []int // 如果需要 append 或其他可能导致扩容的操作 return append(s, 1)// 使用时s = modifySlice(s) 总结 理解 map 和 slice 的这些差异，关键在于： map 是指针类型，始终指向同一个 hmap 结构 slice 是结构体，包含了指向底层数组的指针 扩容时 map 只更新内部指针，而 slice 需要创建新的底层数组 这种设计各有优势： map 的行为更加统一和直观 slice 的设计提供了更多的灵活性和控制权 在实际编程中，正确理解和处理这些差异，是写出健壮 Go 代码的关键。","tags":["go"],"categories":["go"]},{"title":"读书笔记丨解密 QUIC/HTTP3：未来互联网的基石","path":"/2025/01/15/book-quic-http3/","content":"1. QUIC 产生背景 常见网络协议 UDP TCP SCTP（Stream Control Transmission Protocol）：用于电话网络。 KCP：基于 UDP 在应用层实现可靠性传输，牺牲带宽换取效率。 RTP（Real-time Transport Protocol）：与 RTCP 配合传输实时数据，如交互式音频和视频数据。 RTCP：传输控制信息 RTP：传输实时数据 TSL 版本演化 SSLv2：安全性低 SSLv3：分为握手阶段和数据传输阶段。 握手阶段完成对端点的认证和确定保护数据传输的密钥。 一旦确定了密钥，后面的数据传输和 SSL 协议过程都受到加密和完整性保护。 TSL1.0：基于 SSLv3，存在 CBC（Cipher Block Chaining，密文分组链接）加密和解密模式漏洞，使得主动攻击者可以观察到当前记录的 IV（Intiallization Vector，初始化向量），猜测一个数据库，进行数据注入。 TSL1.1：修复了 TSL1.0 的一些关键安全问题： BC 加密使用每条记录一个的显式 IV； 为了防止 CBC 填充攻击，使用 bad_record_mac 错误码代替 decryption_failed 回复填充错误； 支持传输参数的 IANA（Internet Assigned Numbers Authority，互联网数字分配机构）注册，增加了传输参数的灵活性； 改进了连接关闭过早情况下的连接恢复问题。 有些加密算法还是存在安全漏洞，使用的 MD5 也不安全。 TSL1.2：主要关注了架构灵活性和安全问题。 架构： 客户端可以指定自己支持的签名和 hash 算法列表； 支持非协议固定的算法； 安全： 增加了对 AEAD（Authenticated Encryption with Associated Data 关联数据认证加密）的支持，可以在加密中认证没有加密部分的关键数据，甚至是不在报文中的关键数据，可以保护更大的范围。 规定必须实现密码套件 TLS_RSA_WITH_AES_128_CBC_SHA。 增加了 HMAC-SHA256 密码套件。 删除了包含已废弃算法的 IDEA 和 DES 密码套件。 对 EncryptedPreMasterSecret 版本号进行了更严格的检查。 TSL1.3：除了增加安全性，重点改进了连接速度，首次连接发送数据最低可以 1-RTT，恢复连接发送数据最低可以 0-RTT。 安全： 删除了所有被证明有问题的对称加密算法，只保留了 AEAD 的加密套件。密码套件的概念也已经改变，将认证和密钥交换机制与加密算法和散列（用于密钥导出函数和握手消息认证码）分离。 删除 RSA 和静态 DH 密码套件，因为静态 RSA 加密预主密钥的方式和使用静态 DH 私钥都不能保证前向安全性，很容易泄露密钥。只保留能保证前向安全的密钥交换算法，如使用临时私钥的 ECDHE（Elliptic Curve Diffie-Hellman Ephemeral，椭圆曲线 DH 临时密钥交换算法）和 DHE（Diffie-Hellman Ephemeral, DH 临时密钥交换算法）。 ServerHello 之后的消息都加密传输。 删除了压缩功能。之前版本的压缩功能由于存在被攻击的风险实际上很少使用，而且现代的压缩基本都在应用层实现，比如 HTTP 就自己实现的压缩。 HTTP 版本演化 HTTP0.9：仅支持简单的请求响应，只能访问简单的文本文档。 HTTP1.0：HTTP1 中引入了请求头和响应头，请求时可以指定 HTTP 版本号、用户代理、接收类型等，响应可以指明响应状态、内容长度、内容类型等。 HTTP1.1：增加了重用 TCP 连接（keep-alive）的方法，默认保持连接，除非显式通知关闭连接[插图]。这样可以在一个 TCP 连接上完成多个请求-响应，消除了 TCP 建立的延迟，也避免了新建立的 TCP 连接的慢启动过程。 HTTP1.1 在 HTTP 请求首部中增加了 Host 字段，用来支持共享 IP 地址的虚拟主机服务器。 同时支持了更多的方法，如 PUT、PATCH、DELETE、OPTIONS。 引入分块传输支持动态内容。 引入了更多的缓存控制策略。 支持请求部分内容。 HTTP2：修改了 HTTP1.1 的封装格式，增加了一个二进制分帧层。基于二进制分层，HTTP2 实现了 HTTP 的多路复用。HTTP2 为每个请求分配了一个流标识，服务器响应时带上相同的流标识，客户端就可以方便地将响应与请求关联起来，而不用依赖顺序，从而可以降低延迟和提高吞吐量。 HTTP2 还增加了首部压缩 HPACK（Header Compression for HTTP2，HTTP2 首部压缩算法）。 支持请求优先级。 支持服务器主动推送。 增加了 ALPN（Application-Layer Protocol Negotiation，应用层协议协商）。 支持认证、加密和完整性保护，即 HTTPS。 但多个请求或响应在同一个 TCP 上发送时，仍然受制于 TCP 的队首阻塞问题。 HTTP3：基于 QUIC 协议，底层使用 UDP 实现，摆脱了 TCP 的队首阻塞问题。同时改进了 TCP 中存在的一些其他问题，比如拥塞控制、协议僵化、启动慢、重连慢、安全弱等。 实现了没有队首阻塞的并发。如果 QUIC 丢了一个报文，仅仅影响对应流的交付，不会阻塞其他流。 与 TLS1.3 紧密合作，尽可能的加密。还增加了 QUIC 报文的首部加密，除保证了报文安全性，提高了攻击门槛，还避免了协议僵化。 选择 UDP 作为底层实现。一方面避免了 TCP 的首部阻塞，另一方面互联网中绝大部分的主机和中间件都是 TCP 和 UDP 的天下，所以天然支持。 用户态实现。不依赖于内核，容易单独升级。 低延迟的建立。实现了首次最低 1-RTT 发送应用数据，恢复连接时发送应用数据最低只需 0-RTT。 无缝的连接迁移。QUIC 的连接基于连接标识，改变 IP 或者 UDP 端口号并不影响连接的识别，因此可以实现无缝的连接迁移。但是负载均衡就麻烦了。 改进的流量控制。 协议行为作为负载。 2. QUIC 报文 长首部报文：用于建立 QUIC 连接和建立连接前发送应用数据。 短首部报文：用于在 QUIC 连接建立后发送应用数据和 QUIC 协议内容。 无状态重置报文：当服务器丢失了连接状态但仍然收到该连接的数据包时，可以发送无状态重置报文通知客户端立即终止连接。 QUIC 报文类型 初始报文：客户端使用初始报文来发起连接，服务器使用初始报文和握手报文回应客户端的请求。 0-RTT 报文：用于承载 QUIC 连接之前想要发送的数据，一般用于恢复连接后立即发送数据。 握手报文：用来携带服务器和客户端的 TLS 加密握手信息和确认，载荷一般是 CRYPTO 帧和 ACK 帧。 重试报文：是服务器用来验证客户端地址的报文，可以防止源地址欺骗。 服务器使用重试报文通知客户端按照要求重新发送初始报文，在重试报文中携带重试令牌给客户端，并使用服务器选择的连接标识作为重试报文的源连接标识；客户端需要使用服务器指定的连接标识作为目的连接标识，携带服务器指定的重试令牌，构建新的初始报文，重新发送给服务器。 版本协商报文：当服务器收到包含自己不支持的版本号的初始报文时，就会发送版本协商报文。客户端收到版本协商报文后需要在其中选择一个自己支持的版本号，重新以新版本号发送初始报文。 短首部报文：一般也叫作 1-RTT 报文，连接在协商出 1-RTT 密钥后就可以发送短首部报文，用于携带应用数据。","tags":["quic","http3","计算机网络"],"categories":["计算机基础","计算机网络"]},{"title":"匠心码道丨01 编写优质代码的十大黄金法则","path":"/2024/12/12/clean-code-10-rules/","content":"代码质量的优劣直接影响着项目的可维护性和团队的开发效率。一个经验丰富的开发者不仅要能实现功能，更要善于编写清晰易懂、结构合理的代码。本文将介绍 10 条帮助你编写清晰、易维护且可扩展代码的重要规则。 规则 1. 使用有意义的变量和函数名称 变量、函数和类的命名应该具有描述性和意义。你的代码应该能够清晰地表达其意图，而无需额外的注释来解释。 反面示例： let a = 10;const d = new Date();const res = await api.get();const arr = users.filter(u = u.a === true); 正面示例： let maxRetries = 10;const currentDate = new Date();const userResponse = await api.getUserProfile();const activeUsers = users.filter(user = user.isActive === true); 有意义的命名能讲述代码的故事。读者应该能够仅通过名称就理解变量或函数的用途。 💡实践建议： 使用动词前缀命名函数：getUserProfile()、validateInput()、calculateTotal() 使用名词命名变量：userCount、activeUsers、orderStatus 布尔值使用 is/has/should 等前缀：isValid、hasPermission、shouldUpdate 2. 保持函数简短且专注 函数应该保持简短，并且只做一件事。函数承担的责任越多，测试、调试和理解起来就越困难。 反面示例： def process_order(order): # 多个责任：验证、定价、折扣、配送等 pass 正面示例： def validate_order(order): passdef calculate_total(order): passdef apply_discount(order): pass 每个函数应该只有一个责任。如果你需要用\"和\"来描述函数的功能，那么这个函数可能做得太多了。 💡 最佳实践： 函数建议保持在 20-30 行以内 如果超过 50 行，应该考虑拆分 一个函数最好不要超过 3 个参数 3. 避免深层嵌套 深层嵌套的循环和条件语句会使代码难以理解。通过使用提前返回、函数拆分或将大问题分解为小问题来使代码扁平化。 反面示例： if (user != null) if (user.isActive()) if (order != null) processOrder(order); 正面示例： if (user == null || !user.isActive()) return;if (order == null) return;processOrder(order); 提前返回可以减少读者的认知负担，使代码更简单、更容易理解。 4. 明智地使用注释 注释不应该解释代码做了什么；代码本身应该是自解释的。只在必要时使用注释来解释复杂逻辑背后的\"原因\"，而不是\"是什么\"。 反面示例： // 设置用户状态为激活$user-isActive = true; 正面示例： // 登录成功后将用户标记为激活状态$user-isActive = true; 注释应该增加价值，解释特定实现背后的原因或解释复杂的业务逻辑。 5. 保持一致的格式 一致的代码格式使代码更容易阅读和导航。在项目中使用统一的缩进、间距和对齐方式。 反面示例： function calculate(a,b)return a+b; 正面示例： function calculate(a, b) return a + b; 许多团队使用 Prettier 或 ESLint 等工具来自动格式化并强制执行代码风格规则。 6. 不要重复自己（DRY 原则） 代码重复会导致不一致、bug 和不必要的复杂性。应用 DRY 原则可以保持代码库精简，更易于维护。 反面示例： if ($userType == admin) // 复杂逻辑if ($userType == superadmin) // 相同的复杂逻辑 正面示例： if (userIsAdmin($userType)) // 复杂逻辑 通过将共同逻辑抽象到函数、类或工具中来避免代码重复。 7. 单一责任原则（SRP） 每个类和函数应该只有一个改变的理由。遵循单一责任原则使代码模块化，更容易重构。 反面示例： class User void register(); void login(); void sendEmail(); 正面示例： class User void register(); void login();class EmailService void sendEmail(); 承担太多责任的类更难维护。SRP 使代码更模块化，更容易测试。 8. 避免魔法数字和字符串 魔法数字（或字符串）是没有上下文或解释的硬编码值。使用常量或枚举代替，这样可以增加代码的清晰度。 反面示例： discount = 0.05if user.role == admin: 正面示例： DISCOUNT_RATE = 0.05ADMIN_ROLE = admindiscount = DISCOUNT_RATEif user.role == ADMIN_ROLE: 常量为数字或字符串提供了含义，使代码更容易理解。 9. 编写测试 单元测试和集成测试确保你的代码按预期工作，并且在进行更改时不会出错。编写测试使代码更可靠，长期更易于维护。 反面示例： // 这个方法没有测试public void processOrder(Order order) // 逻辑 正面示例： @Testpublic void testProcessOrder() Order order = new Order(); // 断言 测试应该成为你工作流程的一部分，确保代码无 BUG 且稳定。 10. 保持简单（KISS 原则） KISS（Keep It Simple, Stupid）原则提醒我们简单是关键。复杂的解决方案会导致混淆，更难维护。在面对决策时，选择最简单、最直接的方案来满足需求。 反面示例： // 过度复杂的购物车商品总价计算function calculateTotal(items) let total = 0; let discount = 0; // 复杂的折扣计算逻辑 items.forEach(item = if (item.category === electronics) if (item.price 1000) discount += item.price * 0.1; else if (item.price 500) discount += item.price * 0.05; else if (item.category === books) if (item.quantity 3) discount += item.price * item.quantity * 0.15; total += item.price * item.quantity; ); return total - discount; 正面示例： // 将复杂逻辑拆分成小函数function calculateDiscount(item) if (item.category === electronics) return item.price 1000 ? 0.1 : (item.price 500 ? 0.05 : 0); if (item.category === books item.quantity 3) return 0.15; return 0;function calculateTotal(items) return items.reduce((total, item) = const discount = calculateDiscount(item); const itemTotal = item.price * item.quantity; return total + itemTotal * (1 - discount); , 0); 💡 最佳实践： 将复杂逻辑拆分成小的、容易理解的函数 避免在一个函数中处理过多的条件判断 使用清晰的命名来表达意图 保持函数的单一职责 总结 干净的代码对于可维护性、可读性和协作至关重要。遵循这 10 条规则——使用有意义的命名、保持函数简短、避免魔法数字、编写测试等，将会带来更健壮、更易理解和更易扩展的代码库。编写代码不仅仅是要让它能工作，更要让其他人（包括未来的你）能够轻松理解和扩展。 代码审查清单 在提交代码前，可以使用以下清单进行自查： 变量和函数名称是否具有描述性 函数是否只做一件事 是否存在重复代码 是否有未使用的魔法数字 是否编写了相应的测试 代码格式是否统一 注释是否有价值 嵌套是否过深 参考 top-10-clean-code-rules-every-developer-should-follow","tags":["编程规范","代码质量","最佳实践"],"categories":["匠心码道"]},{"title":"KCP 源码分析与原理总结","path":"/2024/12/01/kcp/","content":"序言 本文很大部分参考了 详解 KCP 协议的原理和实现，非常感谢该文作者的讲解。本文再此基础上，加入了一些笔者的思考和分析图示，以期更好地理解 KCP 的底层原理。 结论先行 KCP 是一个快速可靠协议，能以比 TCP 浪费 10%-20% 的带宽的代价，换取平均延迟降低 30%-40%，且最大延迟降低三倍的传输效果。 TCP 是为流量设计的（每秒内可以传输多少 KB 的数据），讲究的是充分利用带宽。而 KCP 是为流速设计的（单个数据包从一端发送到一端需要多少时间），以 10%-20% 带宽浪费的代价换取了比 TCP 快 30%-40% 的传输速度。TCP 信道是一条流速很慢，但每秒流量很大的大运河，而 KCP 是水流湍急的小激流。 KCP 增加的带宽在哪里？增加的速度又在哪里？ 为什么 KCP 能以比 TCP 浪费 10%-20% 的带宽的代价，换取平均延迟降低 30%-40%？ KCP 核心特性 快速重传： KCP 支持快速重传机制，不像 TCP 那样依赖超时重传。KCP 可以根据接收方返回的确认信息快速判断哪些数据包已经丢失，并迅速进行重传。 选择性确认（Selective Acknowledgment, SACK）： KCP 支持 SACK，这允许接收端告知发送端哪些包已经收到，从而仅重传未被确认接收的数据包，减少不必要的重传。 无连接操作： 基于 UDP 的实现使得 KCP 在传输数据前不需要像 TCP 那样进行三次握手建立连接，这减少了初始的延迟，并使其能在连接性较差的网络环境下更加灵活和快速。 拥塞控制： KCP 实现了类似 TCP 的拥塞控制算法，但更为简化，能够快速适应网络条件的变化，如带宽波动和丢包。 流量控制： KCP 允许调整发送和接收的窗口大小，使得发送方可以根据接收方的处理能力和网络条件调整数据发送速率，优化网络利用率和减少拥塞。 可配置的传输策略： KCP 允许用户根据应用需求调整内部参数，如传输间隔、窗口大小等，以达到最优的传输效率和延迟。 前向错误校正（Forward Error Correction, FEC）： KCP 还可以结合使用 FEC 技术，通过发送额外的冗余数据来恢复丢失的包，进一步提高在高丢包环境下的数据传输可靠性。 为什么 TCP 做不到 KCP 这样？ TCP 作为一种成熟且广泛使用的传输协议，在设计上注重可靠性和通用性，因此在拥塞控制和流量控制方面相对保守，以确保在各种网络条件下都能稳定运行。然而，这些设计上的保守性也导致了 TCP 在某些情况下的灵活性和自适应性不如 KCP。 特性类别 协议 描述 拥塞控制机制 TCP 固定算法（慢启动、拥塞避免等），保守的调整策略（指数和线性增长） KCP 灵活算法，动态调整策略，快速调整窗口大小 重传机制的延迟 TCP 固定重传间隔（RTO），多次确认触发重传，需要主动开启选择性重传（SACK） KCP 快速重传，选择性重传，减少重传延迟 流量控制 TCP 固定流量控制（依赖接收窗口和发送窗口），通用性设计 KCP 自适应流量控制，应用层反馈调整发送窗口和重传策略 应用场景 TCP 广泛应用于各种网络环境，标准化要求高 KCP 优化特定场景（如高丢包率和高延迟网络），灵活实现 1. 拥塞控制机制的固定性 TCP： 固定算法：TCP 的拥塞控制算法，如慢启动（Slow Start）、拥塞避免（Congestion Avoidance）、快速重传（Fast Retransmit）和快速恢复（Fast Recovery），在设计时考虑了广泛的兼容性和可靠性。这些算法虽然有效，但其调整机制相对固定，响应速度较慢。 保守的调整策略：TCP 的拥塞控制算法采用了保守的调整策略，例如指数增长和线性增长，这在高丢包率或高延迟网络中，可能会导致拥塞窗口（cwnd）增长速度较慢，影响传输效率。 KCP： 灵活算法：KCP 的拥塞控制机制更为灵活，可以根据实时网络状况进行快速调整。例如，KCP 的快速重传和选择性重传机制，使其能更快速地响应网络丢包情况。 动态调整策略：KCP 的拥塞窗口调整更为灵活，可以根据网络状况快速增加或减少窗口大小，提高传输效率。 2. 重传机制的延迟 TCP： 固定重传间隔：TCP 使用固定的重传超时（RTO），并随着每次重传逐渐增加（指数回退），这种保守的重传机制在高延迟和高丢包率网络中可能导致重传延迟较长。 多次确认触发重传：TCP 的快速重传需要等待三个重复的 ACK 才能触发，这在丢包率较高的情况下，可能会导致较长的延迟。 KCP： 快速重传：KCP 在检测到丢包后立即进行重传，而不需要等待多个重复的 ACK，这显著减少了重传延迟。 选择性重传：KCP 只重传丢失的数据包，而不是所有未确认的数据包，减少了不必要的重传开销。（TCP 其实也支持选择性重传 SACK） 3. 流量控制的灵活性 TCP： 固定流量控制：TCP 的流量控制主要依赖于接收窗口（rwnd）和发送窗口（swnd），在处理突发流量或变化较大的网络条件时，调整速度较慢。 通用性设计：TCP 作为一种通用协议，其设计必须兼顾各种网络环境，因此在流量控制上相对保守，以确保在任何环境下都能稳定运行。 KCP： 自适应流量控制：KCP 的流量控制机制可以根据实际应用需求进行更细粒度的调整。例如，KCP 可以根据延迟抖动、丢包率等动态参数调整发送速率，确保在不同网络条件下都能保持高效传输。 应用层反馈：KCP 可以根据应用层的实时反馈，动态调整发送窗口和重传策略，进一步优化传输效率。 4. 应用场景的差异 TCP： 广泛应用：TCP 设计用于广泛的网络环境，包括稳定的有线网络和不稳定的无线网络，因此其机制必须足够通用和保守，保证在各种情况下的可靠性。 标准化要求：作为互联网的基础协议，TCP 的各项机制经过严格标准化，任何修改都需要广泛测试和验证，以确保不会影响现有网络的稳定性。 KCP： 特定优化：KCP 设计初衷是优化特定场景下的传输性能，特别是高丢包率和高延迟网络，因此在设计上更加灵活，能够根据实时网络状况进行调整。 灵活实现：KCP 可以根据具体应用需求进行优化，例如在实时通信和在线游戏等场景中，灵活的流量控制和快速重传机制显著提升了传输效率。 结论 虽然 TCP 在拥塞控制和流量控制方面具备基本的动态调整能力，但其保守的设计和标准化要求使得其在高丢包率和高延迟网络中的适应性和灵活性不如 KCP。KCP 通过灵活的拥塞控制、快速重传和自适应流量控制机制，能够更有效地应对不同网络条件下的传输需求，提供更高效的传输性能。 KCP 一定比 TCP 快吗？ 不一定。KCP 并不一定在所有情况下都比 TCP 快。虽然 KCP 在某些特定网络环境（如高丢包率和高延迟的网络）中表现更优异，但在某些情况下，TCP 可能更合适。 1. 网络环境 高丢包率和高延迟网络： KCP：KCP 通过快速重传和选择性重传机制，以及动态调整的窗口和重传间隔，能够更好地应对高丢包率和高延迟网络，减少传输延迟，提高传输效率。 TCP：TCP 的重传机制和保守的拥塞控制在这种环境中可能导致较高的延迟和较低的带宽利用率。 低丢包率和低延迟网络： KCP：在稳定的低丢包率和低延迟网络中，KCP 的频繁重传和控制报文可能会导致额外的带宽开销，未必有明显的性能优势。 TCP：TCP 在这种环境中表现稳定，且由于其带宽开销较小，可能比 KCP 更高效。 2. 带宽利用率 带宽充足的网络： KCP：KCP 由于其频繁的重传和控制报文，可能会占用更多的带宽，但如果带宽充足，这种开销对整体性能影响较小，且其低延迟优势可能更明显。 TCP：TCP 的带宽利用率较高，适合带宽充足的环境。 带宽受限的网络： KCP：KCP 的额外带宽开销在带宽受限的网络中可能会显著影响整体传输效率。 TCP：TCP 的较低带宽开销使其在带宽受限的环境中更有优势。 3. 应用场景 实时应用（如在线游戏、视频会议）： KCP：KCP 的低延迟和快速响应能力使其非常适合实时应用，在这些场景中，传输的及时性比带宽利用率更重要。 TCP：TCP 在这些场景中的表现可能不如 KCP，特别是在高丢包率和高延迟的网络中。 非实时应用（如文件传输、网页浏览）： KCP：KCP 在这些场景中可能不如 TCP 高效，特别是在网络稳定且带宽有限的情况下。 TCP：TCP 的可靠性和高带宽利用率使其非常适合非实时应用。 4. 实现和配置 实现复杂性： KCP：实现和配置 KCP 可能比 TCP 更复杂，需要根据具体应用和网络环境进行优化和调整。 TCP：TCP 是一个成熟的协议，系统和库的支持较好，配置和使用相对简单。 总结 KCP 在某些特定环境和应用场景中确实比 TCP 更快，尤其是高丢包率和高延迟的网络环境，以及对低延迟要求较高的实时应用。但在网络稳定、带宽有限或非实时应用场景中，TCP 可能表现更好。因此，选择使用 KCP 还是 TCP 应根据具体的网络条件和应用需求进行权衡。 前置准备 笔者不想那么快就贴出大段大段的代码进行分析，这可能会使读者不知所云。为了更好地阐述 KCP 的底层原理，笔者的设想是先对原理部分进行概要总结，然后再带着这些结论去分析源码，进一步填充里面的边角细节。 但是呢，为了更好地理解 KCP 的原理，又不得不对涉及源码的一些重要设计，为了避免在原理分析阶段，对源码进行过多的涉及，笔者决定添加这单独的一章内容，对 KCP 的“接口设计”、“报文段”、“KCP 控制块”以及“队列和缓冲区”先进行简要概述，以辅助读者更好地理解后续的内容。 接口设计 KCP 工作简约图 在 kcp.h 文件中，定义了 KCP 最核心的几个接口： // 创建一个新的 KCP 控制对象ikcpcb* ikcp_create(IUINT32 conv, void *user);// 释放一个 KCP 控制对象。void ikcp_release(ikcpcb *kcp);// 设置 KCP 的输出回调函数，这个回调函数在 KCP 需要发送数据时被调用。void ikcp_setoutput(ikcpcb *kcp, int (*output)(const char *buf, int len,\tikcpcb *kcp, void *user));// 从 KCP 的接收队列中接收数据，用于上层从 KCP 中读取数据。int ikcp_recv(ikcpcb *kcp, char *buffer, int len);// 向 KCP 的发送队列中添加数据，用于上层向 KCP 发送数据，KCP 会管理这些数据并负责其可靠传输。int ikcp_send(ikcpcb *kcp, const char *buffer, int len);// 更新 KCP 的内部状态，通常需要定期调用。// 这个函数负责处理 KCP 的超时、重传等操作，需要在一定的时间间隔内反复调用（通常每 10-100 毫秒）。void ikcp_update(ikcpcb *kcp, IUINT32 current);// 判断是否要调用 ikcp_updateIUINT32 ikcp_check(const ikcpcb *kcp, IUINT32 current);// 处理接收到的低层数据包（例如 UDP 包）。int ikcp_input(ikcpcb *kcp, const char *data, long size);// 将缓冲区可以发送的包发送出去，会在 ikcp_update 中被调用。void ikcp_flush(ikcpcb *kcp); ikcp_create: conv: 会话标识符，用于标识两个端点之间的连接。这个标识符在两个通信端点之间必须一致。 user: 用户数据指针，可以传递任意用户数据，这个数据在 KCP 的 output 回调中会被传递回去。 返回值: 一个指向新创建的 KCP 控制块（ikcpcb）的指针。 ikcp_release: 释放一个 KCP 控制对象。 ikcp_setoutput: 设置 KCP 的输出回调函数。 output: 输出回调函数指针。这个回调函数在 KCP 需要发送数据时被调用。 buf: 要发送的数据缓冲区。 len: 数据长度。 kcp: 当前的 KCP 对象。 user: 用户数据。 通过这个回调，KCP 可以将要发送的数据传递给下层的网络层，比如 UDP 套接字。 ikcp_recv: 从 KCP 的接收队列中接收数据。 kcp: KCP 控制对象的指针。 buffer: 用户提供的缓冲区，用于存储接收到的数据。 len: 缓冲区的长度。 返回值: 成功接收的数据大小；如果没有数据可接收，返回负值（例如，EAGAIN）。 这个函数用于上层从 KCP 中读取数据。 ikcp_send: 向 KCP 的发送队列中添加数据。 kcp: KCP 控制对象的指针。 buffer: 要发送的数据缓冲区。 len: 数据的长度。 返回值: 成功发送的数据大小；如果发送失败，返回负值。 这个函数用于上层向 KCP 发送数据，KCP 会管理这些数据并负责其可靠传输。 ikcp_update: 更新 KCP 的内部状态，通常需要定期调用。 kcp: KCP 控制对象的指针。 current: 当前的时间戳（以毫秒为单位）。 这个函数负责处理 KCP 的超时、重传等操作，需要在一定的时间间隔内反复调用（通常每 10-100 毫秒）。 ikcp_input: 处理接收到的低层数据包（例如 UDP 包）。 kcp: KCP 控制对象的指针。 data: 收到的数据缓冲区。 size: 数据的长度。 返回值: 成功处理的数据大小；如果处理失败，返回负值。 ikcp_flush: 刷新待发送的数据。 其中最重要的是这 4 个： ikcp_send: 将数据放在发送队列中等待发送。 ikcp_recv: 从接收队列中读取数据。 ikcp_input: 读取下层协议输入数据，解析报文段，如果是数据，就将数据放入接收缓冲区，如果是 ACK，就在发送缓冲区中标记对应的报文段已送达。 ikcp_flush: 调用输出回调将发送缓冲区的数据发送出去。 这里就先简要介绍到这里，后面在源码分析篇章再对这些接口进行详细分析。 报文段 KCP 的报文段大小为 24 字节，结构如下图所示： 每个字段的含义如下： conv: 连接标识 cmd：报文类型 frg：分片数量，表示随后还有多少个报文属于同一个包 wnd：发送方剩余接收窗口的大小 ts：时间戳 sn：报文编号 una：发送方的接收缓冲区中最小还未收到的报文段的编号，也就是说，比它小的报文段都已全部接收 len：数据段长度 data：数据段，只有数据报文会有这个字段 其中 cmd 共有 4 种报文类型： 数据报文：IKCP_CMD_PUSH 确认报文：IKCP_CMD_ACK 窗口探测报文：IKCP_CMD_WASK 询问对端剩余接收窗口的大小 窗口通知报文：IKCP_CMD_WINS 通知对端剩余接收窗口的大小 在 KCP 中，报文段结构定义在 kcp.h 文件中，如下： struct IKCPSEG\tstruct IQUEUEHEAD node;\tIUINT32 conv;\tIUINT32 cmd;\tIUINT32 frg;\tIUINT32 wnd;\tIUINT32 ts;\tIUINT32 sn;\tIUINT32 una;\tIUINT32 len;\tIUINT32 resendts;\tIUINT32 rto;\tIUINT32 fastack;\tIUINT32 xmit;\tchar data[1];; IKCPSEG 结构还多出了几个字段，这是为了支持 KCP 协议的可靠性和效率： resendts: 记录报文的下次重传时间，用于实现重传机制。如果报文在一定时间内没有被确认收到，就会在这个时间戳之后被重新发送。 rto: 表示当前报文的重传超时时间（RTT 的估计值）。用于计算每个报文的重传时间，如果超过 rto 时间没有收到 ACK，会触发重传。 fastack: 快速重传计数，记录该报文被跳过的次数。如果一个报文的 ACK 连续接收到多个对同一报文的确认，而不是新的报文，会增加这个计数，用于实现快速重传机制。 xmit: 记录报文已经被发送的次数。用于统计一个报文的重传次数，帮助判断传输的可靠性。如果操作 dead_link 次，则会判断为连接失效，KCP 会断开连接。 node: 链表节点，用于将多个 IKCPSEG 结构体链接在一起。KCP 的队列和缓冲区都是循环双链表结构。 这些字段共同作用，帮助 KCP 实现以下功能： 可靠性：通过 sn、una 和 ack 确保数据包按顺序接收和重传。 流量控制：通过 wnd 控制数据流量，避免接收方过载。 高效传输：通过 resendts 和 rto 进行超时和重传控制，fastack 提供快速重传机制。 灵活管理：使用链表节点 node 组织数据，便于内部管理。 KCP 控制块 ikcpcb 上面我们提到的 ikcp_create 和 ikcp_release 就是对 KCP 控制块 ikcpcb 的创建和释放，每个 KCP 连接都对应一个 KCP 控制块。它定义在 kcp.h 中： struct IKCPCB\tIUINT32 conv, mtu, mss, state;\tIUINT32 snd_una, snd_nxt, rcv_nxt;\tIUINT32 ts_recent, ts_lastack, ssthresh;\tIINT32 rx_rttval, rx_srtt, rx_rto, rx_minrto;\tIUINT32 snd_wnd, rcv_wnd, rmt_wnd, cwnd, probe;\tIUINT32 current, interval, ts_flush, xmit;\tIUINT32 nrcv_buf, nsnd_buf;\tIUINT32 nrcv_que, nsnd_que;\tIUINT32 nodelay, updated;\tIUINT32 ts_probe, probe_wait;\tIUINT32 dead_link, incr;\tstruct IQUEUEHEAD snd_queue;\tstruct IQUEUEHEAD rcv_queue;\tstruct IQUEUEHEAD snd_buf;\tstruct IQUEUEHEAD rcv_buf;\tIUINT32 *acklist;\tIUINT32 ackcount;\tIUINT32 ackblock;\tvoid *user;\tchar *buffer;\tint fastresend;\tint fastlimit;\tint nocwnd, stream;\tint logmask;\tint (*output)(const char *buf, int len, struct IKCPCB *kcp, void *user);\tvoid (*writelog)(const char *log, struct IKCPCB *kcp, void *user);; 字段的含义如下，读者可在后续分析过程回过来查阅： 字段名 含义 conv 连接标识符，用于识别一个特定的会话。 mtu 最大传输单元（Maximum Transmission Unit），表示网络层传输数据包的最大字节数。 mss 最大报文段长度（Maximum Segment Size），表示应用层传输数据的最大字节数。 state 连接状态，标识当前的传输状态。 snd_una 未确认的发送序号，表示最早未确认的包的序号。 snd_nxt 下一个发送序号，表示即将发送的包的序号。 rcv_nxt 下一个接收序号，表示期望接收的下一个包的序号。 ts_recent 最近的时间戳，用于延迟测量。 ts_lastack 最近的确认时间戳，用于 RTT 计算。 ssthresh 拥塞避免的慢启动阈值。 rx_rttval RTT 的偏差，用于计算 RTT 的波动。 rx_srtt 平滑的 RTT 值，用于计算平均 RTT。 rx_rto 重新传输超时时间，根据 RTT 动态调整。 rx_minrto 最小的重新传输超时时间。 snd_wnd 发送窗口大小，控制发送流量的窗口。 rcv_wnd 接收窗口大小，控制接收流量的窗口。 rmt_wnd 远端窗口大小，表示对方接收窗口的大小。 cwnd 拥塞窗口大小，控制发送流量的窗口，用于拥塞控制。 probe 探测标志，表示是否需要进行窗口探测。 current 当前的时间戳。 interval 刷新间隔时间，表示定期刷新 KCP 状态的间隔。 ts_flush 下次刷新时间戳，用于确定何时执行下一次状态刷新。 xmit 发送次数，表示数据包重传的次数。 nrcv_buf 接收缓冲区的数据包数量。 nsnd_buf 发送缓冲区的数据包数量。 nrcv_que 接收队列中的数据包数量。 nsnd_que 发送队列中的数据包数量。 nodelay 延迟模式标志，表示是否启用无延迟模式。 updated 更新标志，表示是否需要更新 KCP 状态。 ts_probe 下次探测时间戳，用于窗口探测。 probe_wait 探测等待时间，表示等待多长时间后进行下一次窗口探测。 dead_link 死链标志，表示连接是否已经失效。 incr 增量，用于控制流量的增加速率。 snd_queue 发送队列，用于存储待发送的数据包。 rcv_queue 接收队列，用于存储待处理的数据包。 snd_buf 发送缓冲区，用于存储已经发送但未确认的数据包。 rcv_buf 接收缓冲区，用于存储已经接收到但未处理的数据包。 acklist 确认列表，用于存储待发送的确认序号。 ackcount 确认计数，表示确认列表中的条目数量。 ackblock 确认块大小，表示确认列表的内存分配大小。 user 用户数据指针，用于存储用户自定义的数据。 buffer 缓冲区，用于临时存储发送的数据。 fastresend 快速重传标志，表示启用快速重传功能。 fastlimit 快速重传限制，表示在一个 RTT 内允许的最大重传次数。 nocwnd 无拥塞窗口控制标志，表示是否禁用拥塞窗口控制。 stream 流模式标志，表示是否启用流模式。 logmask 日志掩码，用于控制日志输出的级别。 output 发送数据回调函数，用于发送数据。 writelog 日志回调函数，用于输出日志。 队列和缓冲区 struct IKCPCB ...\tstruct IQUEUEHEAD snd_queue;\tstruct IQUEUEHEAD rcv_queue;\tstruct IQUEUEHEAD snd_buf;\tstruct IQUEUEHEAD rcv_buf; ...;struct IQUEUEHEAD struct IQUEUEHEAD *next, *prev;; KCP 中队列和缓冲区都是循环双链表，链表由宏实现，笔者并不擅长，所以本文就不探讨该链表的实现了，有数据结构基础的笔者应该很好理解这一块。 队列和缓冲区的实现：循环双链表 队列和缓冲区是 KCP 最核心的部分，它们的作用流程大概如下图所示，读者可以自行阅读尝试理解，后续我们会进行详细的分析。 KCP 队列和缓冲区作用流程 原理分析 这一节我们详细讨论 KCP 的整个 ARQ 流程。首先我们会对整体流程进行简要概述，然后详细讨论滑动窗口中的发送和接收过程，接着讨论超时重传和快速重传，在这之后我们会将 KCP 和 TCP 的重传策略进行简单对比，最后介绍一下拥塞控制策略。 1. 整体流程 KCP 全流程 KCP 的全流程如上图所示： 发送方调用 ikcp_send 将发送数据，这个时候会创建报文段实例，并放入 snd_queue 发送队列中。 KCP 会定时调用 ikcp_update 判断是否要调用 ikcp_flush。 调用 ikcp_flush 时会将合适的报文段放入 snd_buf 缓冲区中，具体包括： 发送 ACK 列表中所有 ACK； 根据是否需要发送窗口探测和通知报文，需要则发； 根据发送窗口大小，将适量的报文段从 snd_queue 移入 snd_buf 中； 发送 snd_buf 中的报文，包括新加入的、RTO 内未收到 ACK 的和 ACK 失序若干次的； 根据丢包情况计算 ssthresh 和 cwnd。 发送的时候会调用由 ikcp_setoutput 设置的回调函数，将数据发送到对端。 接收方收到数据后，会调用 ikcp_input，将数据放入 rcv_buf 缓冲区，具体包括： 根据所有报文的 una 将相应的报文标记为已送达； 如果是 ACK，就将相应的报文标记为已送达； 如果是数据报文，就将它放入 rcv_buf，然后将 rcv_buf 中顺序正确的报文移入 rcv_queue 接收队列中，接着将相关信息插入 ACK 列表，在稍后的 ikcp_flush 中会发送相应的 ACK； 如果是窗口探测报文，就标记“需要发送窗口通知”，在稍后的 ikcp_flush 中会发送窗口通知报文； 包括窗口通知报文在内的所有报文都有 wnd 字段，据此更新 rmt_wnd； 根据 ACK 失序情况决定是否进行快速重传； 计算 cwnd。 调用 ikcp_recv 从 rcv_queue 中接收数据。 2. 滑动窗口 发送缓冲区 snd_buf 和接收缓冲区 rcv_buf 中活动的报文都是在滑动窗口之中的。这对于我们理解 KCP 的发送和接收流程非常重要，所有我们先从滑动窗口开始介绍。 滑动窗口实际是一个抽象的概念, 不能简单地认为它是缓冲区的一部分，准确的说，滑动窗口是由队列加缓冲区共同组成的。 2.1 发送 发送窗口 snd_una 和 snd_nxt 会努力往右移动： ikcp_flush 时，会从 snd_queue 中取出报文插入到 snd_nxt 的位置上； 如果 snd_nxt - snd_una = cwnd，则不允许新的报文插入； 当 snd_una 的 ACK 报文到达时，snd_una 就会右移到第一个没有收到 ACK 报文的位置； 发送窗口中未确认到达的报文何时重传？ 报文在一个 RTO 时间内仍未确认到达，就会重传。报文 RTO 初始值是 rx_rto ，会持续增长，速率支持配置。 2.2 接收 接收窗口 每收到一个数据报文, 都会根据它的编号将它插入到 rcv_buf 对应的位置中； 接着检查 rcv_nxt 能否向右移动, 只有当报文的顺序正确且连续才能移动； 在上图的例子中由于 4 号报文的缺失, rcv_nxt 只能处于 4 号位置等待，5, 6 号报文也不能移动到 rcv_queue 中； 等到 4 号报文到达后，才能将 4, 5, 6 号报文一并移动到 rcv_queue 中，同时 rcv_nxt 会右移到 7 号位置。 2.3 案例分析 我们举个简单的例子演示整个 ARQ 的流程。下图中实线箭头表示数据报文，虚线箭头表示 ACK。 KCP ARQ 流程 ① t1 时刻发送方发送 1 号报文, 1 号报文放入发送缓冲区中, snd_una 指向 1, snd_nxt 指向 2. ② t2 至 t3 时刻发送方依次发送 2 至 3 号报文, snd_nxt 依次后移. ③ 1 号报文丢包. ④ t4, t5 时刻接收方收到 3 号和 2 号报文, 放入 rcv_buf 中; 随后回复 3 号和 2 号 ACK. 此时由于 1 号报文缺失, rcv_nxt 始终指向 1. ⑤ 3 号 ACK 丢包. ⑥ t7 时刻发送方收到 2 号 ACK, 将 2 号报文标记为已送达. 此时由于 3 号 ACK 丢包, 3 号报文未标记为已送达. 由于 1 号报文未确认送达, snd_una 亦指向 1. ⑦ t8 时刻 1 号报文超时, 重传. ⑧ t9 时刻接收方收到 1 号报文, 放入 rcv_buf 中; 这时 1, 2, 3 号报文顺序正确, rcv_nxt 右移到 4 号位置. 接收方回复 1 号 ACK, 同时带上 una = 4. ⑨ t10 时刻发送方收到 1 号 ACK, 将 1 号报文标记为已送达. 同时 una 表明 1, 2, 3 号报文均已送达, 因此也将 3 号报文标记为已送达. snd_una 移动到 4. 3. 超时重传 超时重传是当发送的数据包在预定时间内未被确认时，重新发送该数据包的机制。在 KCP 中，这个时间由重新传输超时（RTO）决定。KCP 计算 RTO 初始值的方法是 TCP 的标准方法, 规定在 RFC 6298 中。 这里还是贴出源码讲比较直观： static void ikcp_update_ack(ikcpcb *kcp, IINT32 rtt)\tIINT32 rto = 0;\tif (kcp-rx_srtt == 0) kcp-rx_srtt = rtt; kcp-rx_rttval = rtt / 2; else long delta = rtt - kcp-rx_srtt; if (delta 0) delta = -delta; kcp-rx_rttval = (3 * kcp-rx_rttval + delta) / 4; kcp-rx_srtt = (7 * kcp-rx_srtt + rtt) / 8; if (kcp-rx_srtt 1) kcp-rx_srtt = 1; rto = kcp-rx_srtt + _imax_(kcp-interval, 4 * kcp-rx_rttval);\tkcp-rx_rto = _ibound_(kcp-rx_minrto, rto, IKCP_RTO_MAX); 这个计算过程笔者就不做详细介绍了，代码里面的公式读者可以尝试自行画图进行理解，这里就不花大篇幅画公式了，下面我尝试以更通俗易懂的话语解释 RTO，只需要理解它在做什么，为什么这么做，就可以了，个人觉得对公式的细节可以暂且忽略。 3.1 RTO 计算目的 KCP 的 RTO 计算是为了确定在多长时间内未收到确认（ACK）时，应该重新发送数据包。这段时间被称为重传超时时间（RTO）。计算 RTO 的目的是在网络条件变化的情况下，既能快速响应数据丢失，也能避免不必要的重传，从而保持高效的传输。 3.2 RTO 计算涉及的变量解释 RTT 和 SRTT 的概念: RTT（Round-Trip Time）: 是从发送一个数据包到收到其确认（ACK）所花的时间。 SRTT（Smoothed RTT）: 是 RTT 的加权平均值，它代表了 RTT 的一个更稳定的估计值。SRTT 的目的是减少 RTT 的短期波动对 RTO 的影响。 RTT 变化值（RTT variance）：网络传输时间并不总是固定的，有时会因为网络拥塞或其他原因出现波动。我们通过计算 RTT 变化值（RTT variance）来估计这种波动的大小。 为什么需要 SRTT 和 RTT 变化值： SRTT 给我们一个平均的 RTT 估计值。 RTT 变化值告诉我们网络的波动性。如果波动很大，我们希望 RTO 更大，以免因为短暂的网络延迟就触发不必要的重传。 3.3 RTO 计算步骤 1. 初始化：初次计算时，我们没有历史 RTT 值，所以直接用第一次测量的 RTT 来初始化 SRTT，并将 RTT 变化值设为 RTT 的一半。 2. 更新 SRTT 和 RTT 变化值: 每次我们测量新的 RTT，就用它来更新 SRTT 和 RTT 变化值。 更新 SRTT：我们不直接替换旧的 SRTT，而是用一个平滑的方式（即加权平均），使得 SRTT 逐渐靠近新 RTT，但又不会剧烈变化。 更新 RTT 变化值：计算新的 RTT 与 SRTT 的差值，用这个差值来更新 RTT 变化值，使其反映当前网络波动的大小。 3. 计算 RTO: 用 SRTT 加上四倍的 RTT 变化值来计算 RTO，这样可以确保 RTO 足够长，能涵盖大部分的网络波动。 我们还要确保 RTO 不小于一个最小值（rx_minrto），以防止 RTO 过小导致频繁重传；也不能大于一个最大值（IKCP_RTO_MAX），以防止 RTO 过大影响响应速度。 4. RTO 计算效果 稳定的传输: SRTT 提供了一个稳定的平均 RTT 估计，使得 RTO 能适应网络的长期变化。 适应网络波动: RTT 变化值使得 RTO 能够应对网络的短期波动，减少因短暂延迟而导致的重传。 快速响应: RTO 设置合理后，能够在数据丢失时快速重传，保持传输的高效和及时性。 通过这样的计算方式，KCP 能够在不同的网络条件下，自动调整重传策略，从而在保证数据可靠性的同时，保持较高的传输效率。 4. 快速重传 在网络传输中，数据包可能会由于网络拥塞、丢包等原因而丢失。超时重传依赖于重传超时时间（RTO）来判断是否需要重传，这可能会导致响应延迟。而快速重传通过检测重复的确认包（ACK）来快速判断数据包的丢失，并立即触发重传，显著缩短了数据丢失的恢复时间。 KCP 快速重传 4.1 何时快速重传？ 每个报文的 fastack 记录了它检测到 ACK 失序的次数，每当 KCP 收到一个编号为 sn 的 ACK 时，就会检查 snd_buf 中编号小于 sn 且未确认送达的报文，并将其 fastack 加 1。 可以通过配置 fastresend 指定失序多少次就执行快速重传。 每次调用 ikcp_flush 都会重传 snd_buf 中 fastask = fastresend 的报文。 4.2 无限快速重传吗？ 每个报文的 xmit 记录它被传输的次数，可以配置 fastlimit 规定传输次数小于 fastlimit 的报文才能执行快速重传。 5. 比较 TCP 的超时重传和快速重传 TCP 也实现了类似的机制，但在复杂性和应用场景上有所不同。 5.1 TCP 的超时重传 1. RTT 估算: TCP 通过接收确认包来估算 RTT，并使用 RTT 的变化范围来计算 RTO。 TCP 使用 Jacobson/Karels 算法进行 RTT 估算和 RTO 计算： // SRTT and RTTVAR calculationRTTVAR = (1 - β) * RTTVAR + β * |RTTsample - SRTT|SRTT = (1 - α) * SRTT + α * RTTsampleRTO = SRTT + 4 * RTTVAR 其中，SRTT 是平滑的 RTT，RTTVAR 是 RTT 的变化范围，α 和 β 是权重因子。 2. 重传策略: 如果在 RTO 时间内未收到 ACK，TCP 会重传未确认的数据包。 每次重传，RTO 值会按照指数增长（指数退避算法）。 3. 拥塞控制: TCP 使用复杂的拥塞控制机制，如慢启动、拥塞避免等，来调整发送窗口和传输速率。 5.2 TCP 的快速重传 当接收到三个重复的 ACK 时，TCP 会立即重传丢失的数据包，而不等待 RTO 超时。 快速重传后，TCP 进入快速恢复状态，调整拥塞窗口，避免拥塞窗口过度收缩。 5.3 比较分析 特性 KCP TCP RTT 估算 基于加权移动平均，较为简单 使用 Jacobson/Karels 算法，复杂但精确 RTO 计算 简化的计算公式 基于 RTT 的复杂计算 重传机制 超时重传和快速重传 超时重传和快速重传 拥塞控制 简单的拥塞控制，适合低延迟应用 复杂的拥塞控制，适合广泛的传输场景 适用场景 实时应用，如游戏、视频会议 通用应用，如文件传输、HTTP 实现复杂度 较为简单，易于理解和实现 复杂，需处理更多的网络状态和控制 可靠性 依赖于用户自定义的重传和控制策略 内置可靠性和流控制机制 响应速度 高效快速，适用于低延迟和高吞吐量场景 可靠但响应速度较慢，适合稳定传输场景 KCP 和 TCP 都提供了可靠的传输机制，但它们适用于不同的应用场景。KCP 设计简单，适合对延迟敏感的实时应用，而 TCP 拥有完善的拥塞控制和可靠性机制，适合广泛的网络应用。 6. 拥塞控制 拥塞控制是网络传输协议中的一个重要机制，用于防止发送过多的数据包导致网络拥塞。在 KCP 中，拥塞控制相对简单，主要通过发送窗口（snd_wnd）和拥塞窗口（cwnd）来管理数据发送速率。 6.1 三种策略 KCP 有 3 种拥塞控制的策略： 慢启动（slow start） 拥塞避免（congestion avoidance） 快速恢复（fast recovery） 慢启动：先将 cwnd 设置为 1，随后平均每经过一个 RTT 时间，cwnd = cwnd * 2，直到阈值 ssthresh。 拥塞避免：cwnd 到 ssthresh 后，cwnd 呈线性增长。 当慢启动或者拥塞避免造成 丢包 后，就采取相应的退让策略： fastack = fastresend - 发生快速重传：将 ssthresh = cwnd / 2，cwnd = ssthresh + fastresend 进入快恢复。 current = resentts - 超时重传：ssthresh = ssthresh / 2，cwnd = 1，进入慢启动。 拥塞控制中 cwnd 和 ssthresh 的变化情况 6.2 核心概念 KCP 的拥塞控制基于以下几个核心概念： 发送窗口 (snd_wnd)：表示发送端在未收到接收端确认之前，允许发送的数据包的数量。它类似于 TCP 中的发送窗口，控制了数据流的速率。 接收窗口 (rcv_wnd)：表示接收端能够处理的最大数据包数量。发送端通过接收端的窗口大小来调整自己的发送速率。 远端窗口 (rmt_wnd)：表示接收端的窗口大小，发送端会根据这个值调整自己的发送窗口，以避免发送的数据超出接收端的处理能力。 拥塞窗口 (cwnd)：用于控制传输中的数据包数量。它基于网络的拥塞情况动态调整，以避免网络拥塞。 慢启动阈值 (ssthresh)：用于确定拥塞控制的模式。当 cwnd 小于 ssthresh 时，KCP 处于慢启动模式，否则进入拥塞避免模式。 6.3 窗口探测（Window Probing） 在某些情况下，接收端的窗口可能会被关闭（即 rmt_wnd 为 0），这意味着接收端无法接收任何新的数据。为了应对这种情况，KCP 实现了窗口探测机制： 当 rmt_wnd 为 0 时，KCP 不会立即停止发送数据，而是会定期发送一个探测包，以检测接收端窗口是否已经打开。 这个探测包会触发接收端返回一个 ACK，其中包含最新的接收窗口大小信息。 6.4 调节和配置 KCP 的拥塞控制机制提供了一些配置参数，用户可以通过调整这些参数来优化传输性能： snd_wnd: 发送窗口大小，用户可以根据应用的需求调整该值，以控制数据发送的最大量。 rcv_wnd: 接收窗口大小，表示接收端能够处理的最大数据包数量。 ssthresh: 慢启动阈值，初始值通常设置为较大的一个常量，用户可以根据网络情况调整。 cwnd: 拥塞窗口大小，初始值通常设置为 1，随传输情况动态调整。 7. 比较 TCP 的拥塞控制 7.1 四个阶段 TCP 拥塞控制有四个关键阶段 慢启动（Slow Start）： 目的：快速探测网络的可用带宽。 机制：当一个连接刚建立或者从丢包恢复时，cwnd（拥塞窗口）从一个较小的值（通常是 1 个 MSS，即最大报文段大小）开始，并以指数增长的方式增加。 过程：每次收到一个 ACK，cwnd 增加一个 MSS，使得 cwnd 每 RTT 增加一倍，直到 cwnd 达到慢启动阈值（ssthresh）。 拥塞避免（Congestion Avoidance）: 目的：逐步探测网络的最大容量，并避免拥塞。 机制：当 cwnd 达到或超过 ssthresh 时，TCP 进入拥塞避免阶段，此时 cwnd 以线性增长的方式增加。 过程：每个 RTT，cwnd 增加 1/cwnd 个 MSS，这种增长方式较为保守，旨在防止过度发送导致的拥塞。 快速重传（Fast Retransmit）: 目的：快速响应丢包，提高传输效率。 机制：当发送端收到三个重复的 ACK 时，立即重传被确认丢失的数据包，而不等待 RTO 超时。 过程：快速重传的目的是迅速恢复丢失的数据包，从而减少因丢包导致的等待时间。 快速恢复（Fast Recovery）: 目的：在拥塞后快速恢复到适当的传输速率。 机制：在快速重传后，TCP 不会直接进入慢启动，而是保持 cwnd 的一部分，以较快的速度恢复到拥塞避免状态。 过程：将 ssthresh 设置为当前 cwnd 的一半，cwnd 被临时减小，然后在接收新 ACK 时快速增加 cwnd，直到恢复到 ssthresh 为止。 7.2 比较分析 特性 TCP KCP 实现复杂度 复杂，包含多个阶段和算法 简单，主要通过窗口大小控制 拥塞检测 通过 RTT 估算和 ACK 检测丢包 主要通过 ACK 和窗口大小检测丢包 响应速度 响应相对较慢，适合稳定传输 响应较快，适合实时性高的传输 适应性 能适应广泛的网络条件 适应性较好，但更适合低延迟网络 配置灵活性 较为固定，依赖于系统配置和优化 提供更多的配置选项，用户可根据需求调整 应用场景 适用于各种需要可靠传输的应用 适用于实时性要求高的应用，如游戏和视频会议 窗口调整 慢启动、拥塞避免、快速重传、快速恢复等机制 主要通过发送窗口和拥塞窗口调整 丢包响应 丢包时通过减小 cwnd 和 ssthresh 来调整 丢包时迅速调整 cwnd 和重传 拥塞控制策略 慢启动、拥塞避免、快速重传、快速恢复等多种策略 主要通过调整 cwnd 和 ssthresh 进行简单控制 优点 稳定可靠、机制全面、应用广泛 实现简单、响应快、灵活性高、适合实时应用 缺点 复杂、响应慢、初始阶段保守 无法应对更加复杂的网络状况、应用场景有限 TCP 和 KCP 都有各自的拥塞控制机制，适用于不同的应用场景。TCP 提供了复杂而全面的拥塞控制，适合于各种网络条件下的可靠传输，而 KCP 提供了简单高效的控制机制，适合于低延迟和高响应速度的实时应用。选择使用哪种协议取决于具体的应用需求和网络环境。 源码分析 1. 核心数据结构 1.1 IKCPSEG 报文段结构 struct IKCPSEG struct IQUEUEHEAD node; // 链表节点 IUINT32 conv; // 会话ID IUINT32 cmd; // 命令类型 IUINT32 frg; // 分片序号 IUINT32 wnd; // 窗口大小 IUINT32 ts; // 时间戳 IUINT32 sn; // 序列号 IUINT32 una; // 待接收的下一个包序号 IUINT32 len; // 数据长度 IUINT32 resendts; // 重传时间戳 IUINT32 rto; // 超时重传时间 IUINT32 fastack; // 快速重传计数器 IUINT32 xmit; // 传输次数 char data[1]; // 数据; 1.2 IKCPCB 控制块 struct IKCPCB // === 基础配置 === IUINT32 conv; // 会话ID，用于标识一个会话 IUINT32 mtu; // 最大传输单元，默认1400字节 IUINT32 mss; // 最大报文段大小，默认mtu-24字节 IUINT32 state; // 连接状态，0=正常，-1=断开 // === 发送和接收序号 === IUINT32 snd_una; // 第一个未确认的包序号 IUINT32 snd_nxt; // 下一个待发送的包序号 IUINT32 rcv_nxt; // 待接收的下一个包序号 // === 时间戳相关 === IUINT32 ts_recent; // 最近一次收到包的时间戳 IUINT32 ts_lastack; // 最近一次收到ACK的时间戳 IUINT32 ssthresh; // 慢启动阈值，默认为IKCP_THRESH_INIT(2) // === RTT相关 === IINT32 rx_rttval; // RTT的变化量 IINT32 rx_srtt; // 平滑后的RTT IINT32 rx_rto; // 超时重传时间，初始为IKCP_RTO_DEF(200ms) IINT32 rx_minrto; // 最小重传超时时间，默认为IKCP_RTO_MIN(100ms) // === 窗口相关 === IUINT32 snd_wnd; // 发送窗口大小，默认32 IUINT32 rcv_wnd; // 接收窗口大小，默认128 IUINT32 rmt_wnd; // 远端窗口大小，默认128 IUINT32 cwnd; // 拥塞窗口大小，初始为0 IUINT32 probe; // 探测标志，用于窗口探测 // === 时间相关 === IUINT32 current; // 当前时间 IUINT32 interval; // 内部更新时间间隔，默认100ms IUINT32 ts_flush; // 下次刷新时间 IUINT32 xmit; // 总重传次数 // === 队列计数器 === IUINT32 nrcv_buf; // 接收缓存中的包数量 IUINT32 nsnd_buf; // 发送缓存中的包数量 IUINT32 nrcv_que; // 接收队列中的包数量 IUINT32 nsnd_que; // 发送队列中的包数量 // === 配置标志 === IUINT32 nodelay; // 是否启用nodelay模式，0=不启用 IUINT32 updated; // 是否调用过update // === 探测相关 === IUINT32 ts_probe; // 下次探测时间 IUINT32 probe_wait; // 探测等待时间 // === 链路控制 === IUINT32 dead_link; // 最大重传次数，默认为IKCP_DEADLINK(20) IUINT32 incr; // 可发送的最大数据量 // === 数据队列 === struct IQUEUEHEAD snd_queue; // 发送队列 struct IQUEUEHEAD rcv_queue; // 接收队列 struct IQUEUEHEAD snd_buf; // 发送缓存 struct IQUEUEHEAD rcv_buf; // 接收缓存 // === ACK相关 === IUINT32 *acklist; // ACK列表 IUINT32 ackcount; // ACK数量 IUINT32 ackblock; // ACK列表大小 // === 用户相关 === void *user; // 用户数据指针 char *buffer; // 临时缓存 // === 快速重传相关 === int fastresend; // 触发快速重传的重复ACK个数 int fastlimit; // 快速重传次数限制，默认IKCP_FASTACK_LIMIT(5) // === 其他配置 === int nocwnd; // 是否关闭拥塞控制，0=不关闭 int stream; // 是否为流模式，0=消息模式(默认)，1=流模式 int logmask; // 日志掩码，控制日志输出级别 // === 回调函数 === // 数据输出回调，用于发送数据 int (*output)(const char *buf, int len, struct IKCPCB *kcp, void *user); // 日志输出回调 void (*writelog)(const char *log, struct IKCPCB *kcp, void *user);; 这个结构体可以大致分为几个主要部分： 基础配置：包含基本的会话标识和传输单元大小设置 序号追踪：用于追踪发送和接收的包序号 时间管理：包含各种时间戳和定时器 窗口控制：实现流量控制和拥塞控制 队列管理：管理数据的发送和接收 ACK 处理：处理确认包 配置选项：各种功能开关和参数设置 回调函数：用于数据输出和日志记录 2. 核心函数 在进入具体的核心函数分析之前，需要先点明 2 点，kcp 的实现者期望其尽可能地简单和减少依赖，所以数据的输出甚至是当前时间都是由使用者来设置的，即 kcp 本身是不依赖于机器时钟的。具体体现在下面 2 个函数： //---------------------------------------------------------------------// set output callback, which will be invoked by kcp//---------------------------------------------------------------------void ikcp_setoutput(ikcpcb *kcp, int (*output)(const char *buf, int len,\tikcpcb *kcp, void *user))\tkcp-output = output;//---------------------------------------------------------------------// update state (call it repeatedly, every 10ms-100ms), or you can ask// ikcp_check when to call it again (without ikcp_input/_send calling).// current - current timestamp in millisec.//---------------------------------------------------------------------void ikcp_update(ikcpcb *kcp, IUINT32 current) ... 2.1 ikcp_send 发送数据 ikcp_send 是应用层接口，负责将用户数据分片并加入到发送队列（snd_queue）。 ikcp_send //---------------------------------------------------------------------// user/upper level send, returns below zero for error//---------------------------------------------------------------------int ikcp_send(ikcpcb *kcp, const char *buffer, int len)\tIKCPSEG *seg;\tint count, i;\tint sent = 0;\t// mtu: 最大传输单元\t// mss: 最大报文段大小\t// mss = mtu - 包头长度(24)\tassert(kcp-mss 0);\tif (len 0) return -1;\t// append to previous segment in streaming mode (if possible)\t// 如果是流模式，则将数据追加到前一个分段中（如果可能）\tif (kcp-stream != 0) // 如果当前发送队列不为空，且前一个分段未满，则将数据追加到前一个分段中 if (!iqueue_is_empty(kcp-snd_queue)) IKCPSEG *old = iqueue_entry(kcp-snd_queue.prev, IKCPSEG, node); if (old-len kcp-mss) int capacity = kcp-mss - old-len; int extend = (len capacity)? len : capacity; seg = ikcp_segment_new(kcp, old-len + extend); assert(seg); if (seg == NULL) return -2; // 将新的 seg-node 放入 snd_queue 中等待发送 iqueue_add_tail(seg-node, kcp-snd_queue); // 把上一个报文的数据拷贝过来 memcpy(seg-data, old-data, old-len); if (buffer) memcpy(seg-data + old-len, buffer, extend); buffer += extend; seg-len = old-len + extend; seg-frg = 0; len -= extend; iqueue_del_init(old-node); // 释放之前老数据的 kcp node ikcp_segment_delete(kcp, old); sent = extend; if (len = 0) return sent; // 1. 非流模式，不追加到上一个报文后面\t// 2. 流模式，但是上一个报文已满，则创建新的报文\t// 计算需要的报文数量，kcp 会对数据进行分段传输\tif (len = (int)kcp-mss) count = 1;\telse count = (len + kcp-mss - 1) / kcp-mss;\t// 接收窗口位置不够，则暂停发送\tif (count = (int)IKCP_WND_RCV) if (kcp-stream != 0 sent 0) return sent; return -2; if (count == 0) count = 1;\t// 发送所有的报文段\tfor (i = 0; i count; i++) int size = len (int)kcp-mss ? (int)kcp-mss : len; seg = ikcp_segment_new(kcp, size); assert(seg); if (seg == NULL) return -2; if (buffer len 0) memcpy(seg-data, buffer, size); seg-len = size; seg-frg = (kcp-stream == 0)? (count - i - 1) : 0; iqueue_init(seg-node); // 将报文段放入 snd_queue 中 iqueue_add_tail(seg-node, kcp-snd_queue); kcp-nsnd_que++; if (buffer) buffer += size; len -= size; sent += size; return sent; 2.2 ikcp_input 接收数据 ikcp_input 负责处理从网络接收到的原始 KCP 数据包，它会处理协议层面的数据，包括 ACK、窗口控制等协议信息，并将接收到的数据放入 KCP 的内部接收缓冲区（rcv_buf 和 rcv_queue）。 2.3 ikcp_recv 获取数据 ikcp_recv 是应用层函数，供上层应用调用以获取完整的消息数据，它从 KCP 的接收队列(rcv_queue)中读取已经排序好的数据，处理分片重组，确保返回完整的消息。 ikcp_recv //---------------------------------------------------------------------// user/upper level recv: returns size, returns below zero for EAGAIN// 从 rcv_queue 中获取数据//---------------------------------------------------------------------int ikcp_recv(ikcpcb *kcp, char *buffer, int len)\tstruct IQUEUEHEAD *p;\tint ispeek = (len 0)? 1 : 0;\tint peeksize;\tint recover = 0;\tIKCPSEG *seg;\tassert(kcp);\t// 如果 rcv_queue 为空，则直接返回\tif (iqueue_is_empty(kcp-rcv_queue)) return -1;\t// 如果 len 0，则说明是 peek 操作，准备只查看数据\tif (len 0) len = -len;\t// 计算 rcv_queue 中数据的大小\tpeeksize = ikcp_peeksize(kcp);\t// 无法获得大小，返回 -2\tif (peeksize 0) return -2;\t// 数据过大，返回 -3\tif (peeksize len) return -3;\t// nrcv_que: rcv_queue 的长度\t// rcv_wnd: 接收窗口的大小\t// 如果 nrcv_que = rcv_wnd，则需要进行快恢复\t// 因为 nrcv_que = rcv_wnd，说明接收窗口已经满了，\t// 这个时候需要发送 IKCP_CMD_WINS 告诉发送方窗口大小，\t// 这个时候发送方需要进行快恢复，减小数据传输，以尽快释放接收窗口\tif (kcp-nrcv_que = kcp-rcv_wnd) recover = 1;\t// merge fragment\t// 将多个片段合并成一个完整的片段\t// 合并后，将合并后的片段从 rcv_queue 中删除\tfor (len = 0, p = kcp-rcv_queue.next; p != kcp-rcv_queue; ) int fragment; seg = iqueue_entry(p, IKCPSEG, node); p = p-next; if (buffer) memcpy(buffer, seg-data, seg-len); buffer += seg-len; len += seg-len; fragment = seg-frg; if (ikcp_canlog(kcp, IKCP_LOG_RECV)) ikcp_log(kcp, IKCP_LOG_RECV, recv sn=%lu, (unsigned long)seg-sn); if (ispeek == 0) iqueue_del(seg-node); ikcp_segment_delete(kcp, seg); kcp-nrcv_que--; if (fragment == 0) break; assert(len == peeksize);\t// move available data from rcv_buf - rcv_queue\t// 尝试将 rcv_buf 中编号连续的数据，移动到 rcv_queue 中\t// 移动后，将移动的数据从 rcv_buf 中删除\twhile (! iqueue_is_empty(kcp-rcv_buf)) seg = iqueue_entry(kcp-rcv_buf.next, IKCPSEG, node); if (seg-sn == kcp-rcv_nxt kcp-nrcv_que kcp-rcv_wnd) iqueue_del(seg-node); kcp-nrcv_buf--; iqueue_add_tail(seg-node, kcp-rcv_queue); kcp-nrcv_que++; kcp-rcv_nxt++; else break; // 快恢复\tif (kcp-nrcv_que kcp-rcv_wnd recover) // 在ikcp_flush 中返回 IKCP_CMD_WINS // 通知本段窗口大小给对端 kcp-probe |= IKCP_ASK_TELL; return len; 2.4 ikcp_update 定时时钟 前面我们看了 ikcp_send 、ikcp_input 和 ikcp_recv 三个核心流程的函数，其中的一些细节，你可以回到本文前面的「原理分析」再对照源码仔细阅读。 在前面的原理分析中，我们提到，为了提高传输和处理数据的效率，kcp 设计了队列和缓冲区，同时为了实现可靠性，kcp 也提供了 ACK 和重试、拥塞控制等机制，这些事情都是周期定时去处理的。这里是由 ikcp_update 函数去处理的。 ikcp_update 是 KCP 的定时器函数，负责以固定间隔调用 ikcp_flush 处理数据发送和协议更新，是 KCP 的\"心跳\"机制。 //---------------------------------------------------------------------// update state (call it repeatedly, every 10ms-100ms), or you can ask// ikcp_check when to call it again (without ikcp_input/_send calling).// current - current timestamp in millisec.//---------------------------------------------------------------------void ikcp_update(ikcpcb *kcp, IUINT32 current)\tIINT32 slap;\tkcp-current = current;\tif (kcp-updated == 0) kcp-updated = 1; kcp-ts_flush = kcp-current; // 计算间隔\tslap = _itimediff(kcp-current, kcp-ts_flush);\tif (slap = 10000 || slap -10000) kcp-ts_flush = kcp-current; slap = 0; // 达到调用间隔，则执行 ikcp_flush 进行接收数据或发送数据\tif (slap = 0) kcp-ts_flush += kcp-interval; if (_itimediff(kcp-current, kcp-ts_flush) = 0) kcp-ts_flush = kcp-current + kcp-interval; ikcp_flush(kcp); 这个函数很简单，根据注释所说，通常情况下会每 10ms~100ms 执行一次，然后核心是去调用 ikcp_flush 函数，所有的逻辑都在里面。 2.5 ikcp_flush 定时处理 如上所述，ikcp_flush 是 KCP 的核心发送函数，负责将发送队列 snd_queue 中的数据移入发送缓存 snd_buf 并通过 output 回调发送出去，同时处理 ACK 发送、快速重传、超时重传和窗口探测等协议细节。 ikcp_flush void ikcp_flush(ikcpcb *kcp)\tIUINT32 current = kcp-current;\t// 当前时间\tchar *buffer = kcp-buffer; // 临时缓冲区\tchar *ptr = buffer;\tint count, size, i;\tIUINT32 resent, cwnd;\tIUINT32 rtomin;\tstruct IQUEUEHEAD *p;\tint change = 0; // 是否执行过快速重传\tint lost = 0; // 是否执行过超时重传\tIKCPSEG seg;\t// 检查是否已调用 ikcp_update\tif (kcp-updated == 0) return;\t// 初始化一个段用于构建各种控制包\tseg.conv = kcp-conv; // 连接标识\tseg.cmd = IKCP_CMD_ACK; // 报文类型：IKCP_CMD_ACK 表示确认报文\tseg.frg = 0; // 分片数量，表示随后还有多少个报文属于同一个包\tseg.wnd = ikcp_wnd_unused(kcp); // 发送方剩余接收窗口的大小\tseg.una = kcp-rcv_nxt; // 发送方的接收缓冲区中最小还未收到的报文段的编号，也就是说，编号比它小的报文段都已全部接收\tseg.len = 0; // 数据段长度\tseg.sn = 0; // 报文编号\tseg.ts = 0; // 时间戳\t// flush acknowledges\t// ① 发送 ACK 队列中的所有 ACK\tcount = kcp-ackcount;\tfor (i = 0; i count; i++) size = (int)(ptr - buffer); // buffer 中累计的数据将要超过 mtu 的时候 // 就调用 ikcp_output 将数据发送出去 if (size + (int)IKCP_OVERHEAD (int)kcp-mtu) ikcp_output(kcp, buffer, size); ptr = buffer; // 从 ACK 列表中取出 sn(报文编号)和 ts(时间戳) ikcp_ack_get(kcp, i, seg.sn, seg.ts); // 将 ACK 报文写入 buffer ptr = ikcp_encode_seg(ptr, seg); // ② ACK 队列已清空\tkcp-ackcount = 0;\t// probe window size (if remote window size equals zero)\t// 对端剩余接收窗口大小为 0，则意味着可能需要发送窗口探测报文：IKCP_CMD_WASK\tif (kcp-rmt_wnd == 0) // 根据 ts_probe 和 probe_wait 确定当前时刻是否需要发送探测报文 // probe_wait: 等待发送探测报文的时间，IKCP_PROBE_INIT=7s, IKCP_PROBE_LIMIT= if (kcp-probe_wait == 0) kcp-probe_wait = IKCP_PROBE_INIT; // 7s 后去发探测报文 kcp-ts_probe = kcp-current + kcp-probe_wait; else if (_itimediff(kcp-current, kcp-ts_probe) = 0) if (kcp-probe_wait IKCP_PROBE_INIT) kcp-probe_wait = IKCP_PROBE_INIT; kcp-probe_wait += kcp-probe_wait / 2; if (kcp-probe_wait IKCP_PROBE_LIMIT) kcp-probe_wait = IKCP_PROBE_LIMIT; kcp-ts_probe = kcp-current + kcp-probe_wait; kcp-probe |= IKCP_ASK_SEND; // 设置是否需要去发送 IKCP_ASK_SEND else kcp-ts_probe = 0; kcp-probe_wait = 0; // flush window probing commands\t// ③ 如果需要，则发送窗口探测报文：IKCP_CMD_WASK\tif (kcp-probe IKCP_ASK_SEND) seg.cmd = IKCP_CMD_WASK; size = (int)(ptr - buffer); if (size + (int)IKCP_OVERHEAD (int)kcp-mtu) ikcp_output(kcp, buffer, size); ptr = buffer; ptr = ikcp_encode_seg(ptr, seg); // flush window probing commands\t// ④ 如果需要，则发送窗口通知报文：IKCP_CMD_WINS\tif (kcp-probe IKCP_ASK_TELL) seg.cmd = IKCP_CMD_WINS; size = (int)(ptr - buffer); if (size + (int)IKCP_OVERHEAD (int)kcp-mtu) ikcp_output(kcp, buffer, size); ptr = buffer; ptr = ikcp_encode_seg(ptr, seg); kcp-probe = 0;\t// calculate window size\t// ⑤ 计算当前窗口大小\tcwnd = _imin_(kcp-snd_wnd, kcp-rmt_wnd);\tif (kcp-nocwnd == 0) cwnd = _imin_(kcp-cwnd, cwnd);\t// move data from snd_queue to snd_buf\t// 5.1 如果符合发送的条件，则创建新的 newseg 并放入 snd_buf 的尾部\twhile (_itimediff(kcp-snd_nxt, kcp-snd_una + cwnd) 0) IKCPSEG *newseg; if (iqueue_is_empty(kcp-snd_queue)) break; newseg = iqueue_entry(kcp-snd_queue.next, IKCPSEG, node); iqueue_del(newseg-node); iqueue_add_tail(newseg-node, kcp-snd_buf); kcp-nsnd_que--; kcp-nsnd_buf++; newseg-conv = kcp-conv; newseg-cmd = IKCP_CMD_PUSH; newseg-wnd = seg.wnd; newseg-ts = current; newseg-sn = kcp-snd_nxt++; newseg-una = kcp-rcv_nxt; newseg-resendts = current; newseg-rto = kcp-rx_rto; newseg-fastack = 0; newseg-xmit = 0; // calculate resent\t// 失序多少次就快速重传。如果 fastresend 大于 0，则取其值；否则，设为最大值 0xffffffff。\tresent = (kcp-fastresend 0)? (IUINT32)kcp-fastresend : 0xffffffff;\t// 最小超时重传时间。如果 nodelay 为 0，则为 rx_rto 的八分之一，否则为 0。\trtomin = (kcp-nodelay == 0)? (kcp-rx_rto 3) : 0;\t// flush data segments\tfor (p = kcp-snd_buf.next; p != kcp-snd_buf; p = p-next) // 从 snd_buf 取出一个报文 IKCPSEG *segment = iqueue_entry(p, IKCPSEG, node); int needsend = 0; // 条件1：第一次发送的报文，直接发送 if (segment-xmit == 0) // 该报文的 xmit 传输次数 needsend = 1; segment-xmit++; segment-rto = kcp-rx_rto; segment-resendts = current + segment-rto + rtomin; else if (_itimediff(current, segment-resendts) = 0) // 条件2：且重传时间到了，则重传 needsend = 1; segment-xmit++; kcp-xmit++; if (kcp-nodelay == 0) segment-rto += _imax_(segment-rto, (IUINT32)kcp-rx_rto); else IINT32 step = (kcp-nodelay 2)? ((IINT32)(segment-rto)) : kcp-rx_rto; segment-rto += step / 2; segment-resendts = current + segment-rto; lost = 1; else if (segment-fastack = resent) // 条件3：达到快速重传次数，则重传 if ((int)segment-xmit = kcp-fastlimit || kcp-fastlimit = 0) needsend = 1; segment-xmit++; segment-fastack = 0; segment-resendts = current + segment-rto; change++; if (needsend) int need; segment-ts = current; segment-wnd = seg.wnd; segment-una = kcp-rcv_nxt; size = (int)(ptr - buffer); need = IKCP_OVERHEAD + segment-len; if (size + need (int)kcp-mtu) ikcp_output(kcp, buffer, size); ptr = buffer; ptr = ikcp_encode_seg(ptr, segment); if (segment-len 0) memcpy(ptr, segment-data, segment-len); ptr += segment-len; // 如果某个数据包的重传次数超过阈值，则标记连接断开。 if (segment-xmit = kcp-dead_link) kcp-state = (IUINT32)-1; // flash remain segments\tsize = (int)(ptr - buffer);\tif (size 0) ikcp_output(kcp, buffer, size); // update ssthresh\t// 1. 如果发生了快速重传，让 ssthresh 减半，进入快恢复\tif (change) IUINT32 inflight = kcp-snd_nxt - kcp-snd_una; kcp-ssthresh = inflight / 2; if (kcp-ssthresh IKCP_THRESH_MIN) kcp-ssthresh = IKCP_THRESH_MIN; kcp-cwnd = kcp-ssthresh + resent; kcp-incr = kcp-cwnd * kcp-mss; // 2. 如果发生了超时重传，则让 ssthresh 减半，然后 cwnd = 1，进入慢启动\tif (lost) kcp-ssthresh = cwnd / 2; if (kcp-ssthresh IKCP_THRESH_MIN) kcp-ssthresh = IKCP_THRESH_MIN; kcp-cwnd = 1; kcp-incr = kcp-mss; // 兜底，cwnd 至少为 1\tif (kcp-cwnd 1) kcp-cwnd = 1; kcp-incr = kcp-mss; 参考 KCP repo 详解 KCP 协议的原理和实现","tags":["kcp","tcp","网络"],"categories":["计算机基础","计算机网络"]},{"title":"Rust 入门丨01 类型系统概述","path":"/2024/11/28/rust-01-type-system/","content":"在 Rust 编程世界中，绝大部分的特性和能力都离不开 Rust 强大的类型系统，所以在这个系列的第 1 篇我们先来对 Rust 的类型系统做一个全局概述，希望可以帮助你建立起对 Rust 的基本印象。在后续的实践过程中，我推荐你可以经常回来思考下为什么 Rust 要构建这样的类型系统，在每一个分支点是如何做出决策的，这些决策又体现在代码的哪些地方。相信这样可以帮助你更好地入门 Rust。 废话不多说，进入正文。 什么是类型系统？ 在进入 Rust 类型系统讨论之前，我们先尝试占在更高的角度，即整个编程语言界的角度去思考，什么是类型系统？ 编程语言的类型系统是指一套规则，用于定义和管理程序中数据的类型。类型系统的主要目的是帮助捕获程序中的错误，提高代码的可靠性和可读性。 类型系统可以根据多种特性进行分类，主要包括以下几个方面： 静态类型和动态类型： 静态类型：在编译时检查变量类型。例如，Java、C++ 和 Haskell 都是静态类型语言。在这些语言中，变量的类型必须在编译时确定，这样可以在编译阶段捕获许多类型错误。 动态类型：在运行时检查变量类型。例如，Python、Ruby 和 JavaScript 是动态类型语言。在这些语言中，变量的类型是在程序运行时确定的，这提供了更大的灵活性，但也可能导致运行时错误。 强类型和弱类型： 强类型：严格限制不同类型之间的操作。例如，Python 和 Java 是强类型语言。强类型系统通常不允许隐式类型转换，这意味着在进行不同类型之间的操作时，必须显式地进行类型转换。 弱类型：允许更多隐式类型转换。例如，JavaScript 和 Perl 是弱类型语言。在这些语言中，编译器或解释器会在需要时自动进行类型转换，这可能导致难以预料的行为。 显式类型和隐式类型： 显式类型：程序员必须明确声明每个变量的类型。例如，Java 和 C++ 要求在声明变量时指定其类型。 隐式类型：编译器或解释器会根据上下文自动推断变量的类型。例如，Python 和 JavaScript 使用隐式类型，程序员不需要显式声明变量类型。 子类型和多态： 子类型：一种类型系统允许一种类型作为另一种类型的子集。例如，在面向对象编程中，子类是父类的子类型。 多态：允许一个接口被多种不同类型实现。多态性有多种形式，包括参数多态（如泛型）和子类型多态（如继承）。 类型推断： 类型推断是指编译器自动确定表达式的类型，而无需明确的类型注释。例如，Haskell 和 Scala 使用类型推断来减少程序员的负担，同时保持静态类型的安全性。 代数数据类型和类型构造： 代数数据类型（ADT）是通过组合其他类型来构造新类型的机制，常见于函数式编程语言，如 Haskell 和 OCaml。ADT 包括产品类型（如元组）和和类型（如枚举）。 结构类型和名义类型： 结构类型：基于对象的结构来确定类型的兼容性。例如，TypeScript 和 Go 使用结构类型系统。 名义类型：基于名称来确定类型的兼容性。例如，Java 和 C++ 使用名义类型系统。 这里我梳理了一张图，供你参考： 编程语言类型系统 注：本图参考了陈天老师在 Rust 训练营课程上提供的教案并进行了增改。 Rust 类型系统 Rust 为了在提供高性能的同时保证内存安全和线程安全，花了大量力气构建了一个强大的类型系统。 基于之前提到的七个方面，我们来梳理下 Rust 的类型系统： 静态类型：Rust 是静态类型语言，这意味着变量的类型在编译时就被确定。这种设计使得 Rust 在编译阶段就可以捕获许多类型错误，从而提高代码的安全性和性能。 fn main() let x: i32 = 10; // 明确指定类型 强类型：Rust 是强类型语言，它严格限制不同类型之间的操作。Rust 不允许隐式类型转换（例如，不能自动将整数转换为浮点数），需要显式地使用 as 进行类型转换。这种严格性有助于避免许多常见的编程错误。 fn main() let x: i32 = 5; let y: f64 = 10.0; // 错误：不能将 i32 隐式转换为 f64 // let sum = x + y; // 正确：需要显式转换 let sum = x as f64 + y; println!(Sum = , sum); 显式类型和类型推断：虽然 Rust 是显式类型语言，要求在某些情况下声明变量类型，但它也具有强大的类型推断能力。编译器可以根据上下文推断出大多数变量的类型，减少了程序员的负担。例如： let mut v = vec![];v.push(5u8); // 结合这里，Rust 编译器可以推断出 v 的类型是 Vecu8 子类型和多态：Rust 支持泛型和 trait，这是一种多态性的实现方式。trait 类似于接口，允许定义类型可以实现的一组方法。泛型允许定义函数、结构体和枚举时使用占位类型，从而实现代码的重用和灵活性。 // 这里 std::fmt::Display 就是一个 trait，目前，你可以先简单理解为 trait 就是接口fn print_valueT: std::fmt::Display(value: T) println!(, value);fn main() print_value(42); // 42 默认为 i32，标准库为其是实现了 Display trait print_value(Hello, world!); // str 也实现了 Display trait 类型推断：Rust 的类型推断系统非常强大，能够根据代码上下文自动推断变量和表达式的类型。这使得代码更简洁，同时保持了类型安全性。 代数数据类型和类型构造：Rust 支持代数数据类型，通过枚举（enum）和结构体（struct）来实现。枚举允许定义一个类型，该类型可以是几种不同的变体之一，每个变体可以携带不同的数据。 enum OptionT Some(T), None, 结构类型和名义类型：Rust 使用名义类型系统。每个类型都有一个显式的名称，类型的兼容性基于名称而不是结构。这意味着即使两个结构体有相同的字段，它们也被视为不同的类型，除非通过特征或显式转换来实现兼容性。 除此之外，Rust 的类型系统还提供了其他非常强大且有用的特效，如所有权和借用、生命周期以及模式匹配。 所有权和借用（Ownership and Borrowing）： Rust 的类型系统与其所有权模型紧密结合。所有权模型通过所有权、借用和生命周期的概念来管理内存，从而在无垃圾回收器的情况下确保内存安全。 fn main() let s = String::from(Hello); let len = calculate_length(s); // 借用 println!(The length of is ., s, len);fn calculate_length(s: String) - usize s.len() 生命周期（Lifetimes）： Rust 使用生命周期标注来跟踪引用的有效范围，确保引用在使用时始终有效。这是 Rust 类型系统中一个独特的特性，帮助防止悬空引用和数据竞争。 fn longesta(x: a str, y: a str) - a str if x.len() y.len() x else y fn main() let string1 = String::from(long string is long); let string2 = xyz; let result = longest(string1.as_str(), string2); println!(The longest string is , result); 模式匹配： Rust 提供强大的模式匹配功能，尤其是在处理枚举和复杂数据结构时，使得代码更具表达力和安全性。 enum Message Quit, Move x: i32, y: i32 , Write(String), ChangeColor(i32, i32, i32),fn process_message(msg: Message) match msg Message::Quit = println!(Quit the application); Message::Move x, y = // 模式匹配能根据数据类型直接拆解出来，使用起来非常方便 println!(Move to coordinates: (, ), x, y); Message::Write(text) = println!(Text message: , text); Message::ChangeColor(r, g, b) = println!(Change color to RGB(, , ), r, g, b); Rust 的类型系统通过上述特性实现了高效、安全和灵活的编程模型，适合系统编程和高性能应用。它在编译期捕获许多潜在错误，使得运行时更为安全可靠。 当然，如果你之前没有学习过 Rust，那这些概念和代码对你来说大概率是云里雾里，不要着急，我们先建立起一个大概的印象就行了。这里我针对 Rust 类型系统梳理了一张图，你可以在以后的学习中时常回来看看： Rust 类型系统 注：本图参考了陈天老师在 Rust 训练营课程上提供的教案并进行了增改。 本篇就到这里，下篇我们将介绍 Rust 的数据类型，enjoy coding~","tags":["rust"],"categories":["rust","rust 入门"]},{"title":"Rust 训练营总结丨第三次入门 Rust","path":"/2024/11/26/rust-bootcamp/","content":"缘起 2023 年我给自己定了很多个目标，最终的结果是每个都做了一些事情，但是没有一个是做得比较彻底的，印证了《孙子兵法》的那句：“无所不备，则无所不寡”。 在 2023.10.23 出于好奇，我订阅了《Rust 语言从入门到实战》的专栏，跟着课程的更新节奏学习完了整个专栏。 虽然我第一次入门 Rust 失败了，但也被 Rust 的种种特性所吸引。我是个特别喜欢“痛苦前置”的人，而 Rust 编译器\"睚眦必报\"的编译器检查正给予了我被虐的爽感，编译通过后程序的稳定运行也符合我追求成为一位“靠谱”工程师的愿景。 加之我的主力语言是 Go，一门应用编程语言，所以我一直希望学习一门系统编程语言，以期将来有能力窥探一些底层的细节原理。C/C++ 太古老了，特性太多了，大神太多了，我怎么学都不可能赶得上别人，嘿嘿，学个新的，大家都没学过，这不就舒服了么。 后来极客时间决定开设《Rust 训练营》，讲师是陈天老师，我去搜了关于陈天老师的一些资料，看了一些他写的文章和技术分享视频，甚至油管上还有他之前面试的视频。OK，这个人得到了我的认可，我想跟这样的人交个朋友，哪怕只是加个微信，至少我多了个口子，得以窥探精英阶层人士的生活一角。 结合 2023 年的教训，2024 年年初我就给自己制定了一年的目标，只有一个，就是踏踏实实、完完整整学习完整个 Rust 训练营，其他所有事情和目标，都要为其让步。 其实是 2 个目标 hhh，另外一个目标是：完成人生的第一场半程马拉松。 筑基 为了更好服务于《Rust 训练营》，在 1-4 月份，我花了差不多 3 个多月的时间啃下了《Rust 程序设计（第二版）》，对整个 Rust 的语言特性建立了更加完善的体系基础，也多奠定了一些基础，当然，这是我第二次入门 Rust 失败。 修炼 4 月 18 号开营，本来是预计 7 月份结营的，不过陈天老师分享的欲望刹不住车，硬是“拖堂”到了 11 月 22 号。事实上，这是有点难受的，一个事情拖太久，思维上很容易疲惫，懒惰也愈难克服。不过从消费者的角度，这是赚翻了，毕竟，学着学着，花呗的 12 期无息分期也差不多要还完了。 所以，其实一个 1095 的程序员，在 4.20 到 11.22 是可以花 279 小时 54 分钟学完 202 讲课程的。 即使你将来不使用 Rust，相信你学完这门课程后也能成为一位更好的软件工程师。 —— 陈天 是的，在学习中，更多时候感受到的不仅仅是在学习 Rust，而是在重学软件工程，我开始切身接触优秀的软件开发具备了哪些不可或缺的流程。为了效仿这些优秀的思想和实践，在实际工作中，今年我做了一些尝试： 引入更丰富的 CI/CD 流程，尽可能发挥机器的能力，让机器不厌其烦地做那些的重复劳动，而这些不起眼的重复劳动，却能以最小代码为我们排查出最多难以发现的“失误” BUG。 开始学习写单测，开始学习如何将代码写得能单测、易单测，学习着如何将那些不能单测的 💩 代码改造成可单测的代码，也将单测运行加入了 CI/CD 的流程中。在单测多次帮我揪出那些我意识不到的不小心改错的逻辑的时候，我才切身感受到单测的作用，也真正理解了“写单测并不会影响开发效率，如果影响了，那也是提高了开发效率”。幸运的是，截至目前（11.27），我已经连续 2 次，在上千行代码的需求开发中，提测阶段和线上发布阶段，都是 0 Bug，运气不错。 引入监控系统，在指标上，存储层、应用层、业务层和网关层进行分层监控，在开发时，从业务无关组件（goapm），到业务相关通用组件，最后再到应用程序特定组件的分阶段分层次开发，开始学习着“先解决业务背后的领域问题，顺带解决业务问题”。 开始思考一些架构层面的东西，开始思考一些代码组织、接口契约、领域模块划分的问题，以期写出质量更好的代码。 为了支撑上面这些事情，今年我又顺带读了一些书，我是个很少读书的人，因为我总觉得：“读书好慢”。而且我读书也确实很慢，主要是，很困 😅。然而，当我回望来时路，一切却都在我的意料之外。 hedon 2024 的书单 这个时候我才知道： 慢就是快 少就是多 历劫 rust-road 这些书其实都不在我的计划之内，因为 2024 我只有一个目标：完成 Rust 训练营的学习。它们只不过是我完成既定计划之余的加餐罢了。 而幸好我只有一个目标，所以才能有更多时间和精力去应对跟随训练营学习中的一些困难： 晚上 9 点下班，真累啊，休息下吧，真不想学了。 工作了一周，真累啊，周末要不就休息吧，真不想学了。 编译器报错好多啊，算了，要不直接 copy 现成的代码吧。 这知识点在讲啥啊，算了，先不懂装懂吧，后面还那么多课，先赶进度再说。 前端和客户端的知识，好像跟我没啥关系，算了，不听了，过过过。 单测我就不写了，浪费时间。 学完咯，感觉没啥好总结的，算了，下一个吧。 .... 运气不错，上述的 n 多种情况，至少在 50-70% 的时候，我能做到： 学一下再说，累了再停。 下午出去玩，早上先学了再说。 算了，狠点，盲写，自己尝试解决一下，咦，也就那么回事。写完后再对比下，哦，其实这块没听懂。 弄懂再说，多听几遍课，重新看几遍书，再搜一些相关博客，哦，这个知识点是这个意思，读书百遍其义自见原来是这味？ 算了，试试现在 LLM 是否如吹的那么牛，嗯，好像用 LLM 来实现前端和客户端的基础功能还真可以，也没那么无聊嘛。 算了，先试着写下单测吧。哦，我的代码这么难测啊，哦，这行代码怎么就犯蠢了呢，哦，花不了多少时间嘛。 要不还是总结下吧，哦，原来这个地方是这个意思，哦，原来还讲到了这个点。 所以这个时候我又知道了： 慢就是快 少就是多 小成 ➜ hedon-rust-road lltotal 0drwxr-xr-x 21 wangjiahan staff 672B Nov 27 18:26 aicommdrwxr-xr-x 23 wangjiahan staff 736B Sep 11 13:55 chatdrwxr-xr-x 17 wangjiahan staff 544B Sep 11 18:31 chatappdrwxr-xr-x 26 wangjiahan staff 832B Nov 27 18:26 crmdrwxr-xr-x 22 wangjiahan staff 704B Nov 27 18:26 dinodrwxr-xr-x 16 wangjiahan staff 512B Nov 27 18:29 error-infodrwxr-xr-x 18 wangjiahan staff 576B Sep 4 19:00 hackernewsdrwxr-xr-x 22 wangjiahan staff 704B Sep 12 15:54 hedon-botdrwxr-xr-x 9 wangjiahan staff 288B Nov 27 18:29 httpiedrwxr-xr-x 13 wangjiahan staff 416B Aug 22 10:40 inverted-index-concurrencydrwxr-xr-x 7 wangjiahan staff 224B Nov 27 18:28 json-macrodrwxr-xr-x 26 wangjiahan staff 832B Sep 3 19:30 learn-ffidrwxr-xr-x 8 wangjiahan staff 256B Nov 27 18:29 learn-proc-macrodrwxr-xr-x 7 wangjiahan staff 224B Nov 27 18:30 mandelbrotdrwxr-xr-x 10 wangjiahan staff 320B Aug 22 10:40 matrix-multidrwxr-xr-x 7 wangjiahan staff 224B Nov 27 18:29 pest-parser-collectiondrwxr-xr-x 19 wangjiahan staff 608B Nov 27 18:27 r-redisdrwxr-xr-x 21 wangjiahan staff 672B Aug 22 10:40 rclidrwxr-xr-x 17 wangjiahan staff 544B Aug 22 10:40 simple-chatdrwxr-xr-x 17 wangjiahan staff 544B Aug 22 10:40 simple-shortenerdrwxr-xr-x 21 wangjiahan staff 672B Aug 22 10:40 taotiedrwxr-xr-x@ 18 wangjiahan staff 576B Nov 27 15:38 thumbordrwxr-xr-x 19 wangjiahan staff 608B Aug 29 10:56 winnow-parser-collection➜ hedon-rust-road tokei -t rust=============================================================================== Language Files Lines Code Comments Blanks=============================================================================== Rust 336 25451 21615 644 3192 |- Markdown 53 546 0 476 70 (Total) 25997 21615 1120 3262=============================================================================== Total 336 25451 21615 644 3192=============================================================================== 看老师画了那么多牛逼的图，要不“邯郸学步”模仿一下吧。故而又忍着“下一个吧”的念头，梳理了下这几个月到底做了些什么。 rcli r-redis macro-json macro-error-info rust-ecosystem crm taotie dino aicomm 归元 知是行之始，行是知之成。 遇事不决，可问春风。春风不语，既随本心。 2025 见！","tags":["rust"],"categories":["rust","总结","2024"]},{"title":"Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序","path":"/2024/11/11/rust-memory-order/","content":"系列文章： Rust 原理丨聊一聊 Rust 的 Atomic 和内存顺序 👈 本篇 Rust 原理丨从汇编角度看原子操作 Rust 实战丨手写一个 SpinLock Rust 实战丨手写一个 oneshot channel Rust 实战丨手写一个 Arc Rust 原理丨操作系统并发原语 Rust 实战丨手写一个 Mutex Rust 实战丨手写一个 Condvar Rust 实战丨手写一个 RwLock Atomic 在 Rust 的 std::sync::atomic 模块中包含了无锁并发编程的原子化类型，与通常的算术运算符和逻辑运算符不同，原子化类型会暴露执行原子化操作的方法，单独的加载、存储、交换和算术运算都会作为一个单元安全地进行，哪怕其他线程也在执行操作同一内存的原子化操作也没问题。 Rust 提供了以下几种原子化类型： AtomicIsize 和 AtomicUsize 是与单线程 isize 类型和 usize 类型对应的共享整数类型。 AtomicI8、AtomicI16、AtomicI32、AtomicI64 及其无符号变体（如 AtomicU8）是共享整数类型，对应于单线程中的类型 i8、i16 等。 AtomicBool 是一个共享的 bool 值。 AtomicPtr 是不安全指针类型 *mut T 的共享值。 这些类型都会以下几类核心功能： Load 、Store: 存取值 Fetch-and-Modify: 获取并修改 Compare-and-Exchange: 比较并交换 下面我们对上述提到的几种核心功能进行举例。 Load Store load: 从原子化类型中获取起对应的基本数据类型的值。 store: 将一个基本数据类型的值存储到其对应的原子化类型中。 在下面的例子中，我们使用 AtomicUsize::new(0) 初始化了一个原子类型，它对应的基本数据类型是 usize。 我们起了一个子线程，在 for 循环中不断地使用 store 函数修改 num_done 的值，然后在主线程中使用 load 获取起对应的值，当发现值为 100 时，就退出循环，进程结束。 得益于原子化类型的并发安全特性，所以这里两个线程对 num_done 进行并发读写都是安全的。 fn main() let num_done = AtomicUsize::new(0); let main_thread = thread::current(); thread::scope(|s| s.spawn(|| for i in 0..100 sleep(Duration::from_millis(10)); num_done.store(i + 1, std::sync::atomic::Ordering::Relaxed); // store 存储 main_thread.unpark(); ); loop let n = num_done.load(std::sync::atomic::Ordering::Relaxed); // load 获取 if n == 100 break; println!(Working... n/100 done); thread::park_timeout(Duration::from_millis(1)); ); println!(Done!); 这里我们暂且忽略 std::sync::atomic::Ordering::Relaxed 这个参数的含义，在后续的「内存顺序」章节会进行详细阐述。 Fetch-and-Modify Fetch-and-Modify 操作用于在获取当前值的同时对其进行修改。这类操作包括 fetch_add、fetch_sub、fetch_and、fetch_or、fetch_xor 等。 我们将上面的例子修改一下，不再是直接 store 一个值，而是不断进行加 1 操作： fn main() let num_done = AtomicUsize::new(0); thread::scope(|s| s.spawn(|| for _ in 0..100 num_done.fetch_add(1, std::sync::atomic::Ordering::Relaxed); // 使用 fetch_add 进行加 1 ); loop let n = num_done.load(std::sync::atomic::Ordering::Relaxed); if n == 100 break; println!(Working... n/100 done); ); println!(Done!); Compare-and-Exchange Compare-and-Exchange 是一种条件更新操作，只有在当前值等于预期值时才会更新。 下面的例子中我们实现了一个函数 allocate_new_id，它支持在并发环境下分配新的 id，这里我们使用了 compare_exchange(id, id+1) 进行条件更新，只有当 id 没有发生变化的时候，才运行对其进行加 1，这就保证了在并发下，只有一个线程可以成功执行该语句，从而保证 id 的递增性和唯一性。 fn allocate_new_id() - u32 static NEXT_ID: AtomicU32 = AtomicU32::new(0); let mut id = NEXT_ID.load(std::sync::atomic::Ordering::Relaxed); loop assert!(id 1000, Too many IDs!); match NEXT_ID.compare_exchange( // 只有 id 没有发生变化，才允许进行加 1 id, id + 1, std::sync::atomic::Ordering::Relaxed, std::sync::atomic::Ordering::Relaxed, ) Ok(_) = return id, Err(v) = id = v, 在 Rust 中，原子化类型还提供了另外一个函数：compare_exchange_weak，它与 compare_exchange 的主要区别在于它们在失败时的行为： compare_exchange:只会在实际值不等于期望值时失败。提供更强的保证，但可能性能较低。适用于不在循环中的单次比较交换操作。compare_exchange_weak:即使实际值等于期望值时也可能失败（称为“虚假失败”或“spuriousfailure”）。性能可能更好，因为允许在某些架构上生成更高效的代码。最适合在循环中使用，因为需要处理可能的虚假失败。在实际应用中:如果操作在循环中,使用 compare_exchange_weak通常更好。如果是单次操作,使用 compare_exchange 更合适。在某些平台上，这两个操作可能没有性能差异,但compare_exchange_weak 的行为仍然可能不同。这种区别的存在是因为在某些 CPU架构上,允许虚假失败可以生成更高效的机器码。比如在 ARM架构上，compare_exchange_weak 可以直接映射到单个LL/SC（Load-Link/Store-Conditional）指令。 硬件原理 在一些处理器架构中，当一个 CPU 执行需要原子性的操作时，它可以通过锁定内存总线来确保在操作完成之前，其他 CPU 无法访问相关的内存地址。 基本工作流程如下： CPU 发出 LOCK 信号 └── 激活处理器的 LOCK# 引脚 └── 获得总线的独占访问权 └── 执行原子操作 └── 释放 LOCK 信号 └── 其他处理器可以访问内存 主流的有 2 种锁定机制： 总线锁定（Bus Locking）：总线锁定是一种机制，它通过锁定内存总线来确保在执行原子操作时，其他处理器无法访问内存。这种方法虽然简单，但会导致总线的其他操作被阻塞，从而影响系统性能。 优点：- 绝对的原子性保证- 适用于所有内存位置缺点：- 性能开销大- 会阻塞其他 CPU 对内存的访问 缓存锁定（Cache Locking）：现代处理器通常使用缓存锁定来实现原子操作。缓存锁定通过锁定处理器的缓存行来实现，而不是锁定整个总线。这种方法可以减少对总线的影响，提高系统的并发性能。 优点：- 性能更好- 不会完全阻塞内存访问条件：- 数据必须在缓存行中- 缓存行必须是独占状态 缓存锁定通常依赖于缓存一致性协议（如 MESI 协议）来确保在多个处理器之间的数据一致性。通过这些协议，处理器可以在本地缓存中执行原子操作，并在必要时与其他处理器同步。 MESI 协议即： M (Modified)：已修改E (Exclusive)：独占S (Shared)：共享I (Invalid)：无效操作流程：1. 检查数据是否在缓存中2. 如果在，将状态改为 Exclusive3. 执行原子操作4. 通知其他 CPU 使其缓存失效 不同的架构有不同的锁定方式： x86/x64：使用 LOCK 前缀 ARM：使用 exclusive load/store 指令 PowerPC：使用 load-linked/store-conditional 以下是 x86 汇编的一个示例： ; 原子加法操作lock add dword ptr [memory], 1; 比较并交换lock cmpxchg dword ptr [memory], eax 为了充分利用缓存锁定的优势，我们在编写代码时，可以有以下的性能考虑： 缓存行对齐，避免伪共享 use std::sync::atomic::AtomicI32, Ordering;// 在 Rust 中，可以使用 #[repr(align(N))] 属性来确保结构体或变量的对齐方式，以避免伪共享。// 伪共享是指多个线程访问不同的变量，但这些变量共享同一个缓存行，从而导致不必要的缓存一致性流量。#[repr(align(64))]struct AlignedCounter counter: AtomicI32,fn main() let counter = AlignedCounter counter: AtomicI32::new(0), ; // 使用 counter.counter.fetch_add(...) 进行操作 避免频繁的总线锁定 use std::sync::atomic::AtomicI32, Ordering;fn main() let counter = AtomicI32::new(0); // 不好的做法：频繁的原子操作 for _ in 0..1000 counter.fetch_add(1, Ordering::SeqCst); // 更好的做法：本地累加后一次性更新 let mut local_sum = 0; for _ in 0..1000 local_sum += 1; counter.fetch_add(local_sum, Ordering::SeqCst); Rust 实战查看汇编 笔者使用的是 ARM64 架构的 macbook。 use std::sync::atomic::AtomicI64, Ordering;use std::thread;static ATOMIC: AtomicI64 = AtomicI64::new(0);fn main() let t1 = thread::spawn(|| ATOMIC.store(10086, Ordering::Release); ); let t2 = thread::spawn(|| let val = ATOMIC.load(Ordering::Acquire); println!(val); ); t1.join().unwrap(); t2.join().unwrap(); 使用 rustc 编译并输出汇编代码： rustc -O --emit asm src/main.rs 代码中我特地设置了 10086 这个特殊的值，这是为了可以在输出的 main.s 文件中快速找到 store 对应的位置： __ZN3std3sys9backtrace28__rust_begin_short_backtrace17h750d7a3a9c81fc67E:\t.cfi_startprocLloh8:\tadrp\tx8, __ZN4main6ATOMIC17hd0b0dbf92e477148E.0@PAGELloh9:\tadd\tx8, x8, __ZN4main6ATOMIC17hd0b0dbf92e477148E.0@PAGEOFF\tmov\tw9, #10086 ; 将值 10086 移入寄存器\tstlr\tx9, [x8] ; Store-Release 指令，原子地存储值\t; InlineAsm Start\t; InlineAsm End\tret\t.loh AdrpAdd\tLloh8, Lloh9\t.cfi_endproc 在这个代码中，stlr 就是 Store Release 的意思，另外一个关键字是 ladpr，表示 Load Acquire 的意思，通过这个关键字，你可以找到 load 对应的汇编代码： Lloh11:\tadd\tx8, x8, __ZN4main6ATOMIC17hd0b0dbf92e477148E.0@PAGEOFF\tldapr\tx8, [x8] ; ; Load-Acquire 指令，原子地加载值\tstr\tx8, [sp, #8] Go 实战查看汇编 笔者使用的是 ARM64 架构的 macbook。 package mainimport (\tsync/atomic)func main() data := atomic.Int64\tgo func() data.Store(10086)\t()\tgo func() a := data.Load() println(a)\t() 使用如下命令，可以输出优化后的汇编代码： go build -gcflags=-S -ldflags=-w main.go 2 assembly.txt 查看输出的文件，我们同样搜索 10086，可以快速找到 store 的位置： 0x0008 00008 (/Users/wangjiahan/go/go1.23.2/src/sync/atomic/type.go:109)\tMOVD\t$10086, R10x000c 00012 (/Users/wangjiahan/go/go1.23.2/src/sync/atomic/type.go:109)\tSTLR\tR1, (R0) 可以看到，这里同样也是使用了 STLR 指令。接着我们看第 14 行代码的位置对应的汇编：可以发现这里使用的 LDAR 指令，也就是 Load Acuqire。 0x001c 00028 (/Users/wangjiahan/goStudy/go-atomic/main.go:14)\tHINT\t$00x0020 00032 (/Users/wangjiahan/go/go1.23.2/src/sync/atomic/type.go:106)\tLDAR\t(R0), R00x0024 00036 (/Users/wangjiahan/go/go1.23.2/src/sync/atomic/type.go:106)\tMOVD\tR0, main..autotmp_6-8(SP) 内存顺序 在了解了 Rust Atomic 的基本用法和基本原理之后，我们回过头来谈一谈原子操作参数中的 std::sync::atomic::Ordering::Relaxed，这个就是本篇的主题：内存顺序。内存顺序要解决的核心问题是如何合理地限制单一线程中的代码执行顺序，使得在不使用锁的情况下，既能最大化利用 CPU 的计算能力，又能保证多线程环境下不会出现逻辑错误。 指令乱序 CPU 和编译器都会在保证程序运行结果不发生改变的前提下，尽一切可能让我们的程序运行得尽可能快。 fn f(a: mut i32, b: mut i32) *a += 1; *b += 1; *a += 1; 像上述代码，编译器完全可以优化成下面的代码，从而提高程序的运行效率： fn f(a: mut i32, b: mut i32) *a += 2; *b += 1; 在这个过程中，就可能会出现指令重排，甚至是代码重写，不过这带来了指令乱序的问题，即程序的实际执行顺序跟我们的代码顺序是不一致的。 不过，编译器保证的是在单线程环境下，执行的结果最终一致，所以，指令乱序在单线程环境下完全是允许的。对于编译器来说，它只知道：在当前线程中，数据的读写以及数据之间的依赖关系。但是，编译器并不知道哪些数据是在线程间共享，而且是有可能会被修改的。而这些是需要开发人员去保证的。 内存模型 为了解决指令乱序带来的并发问题，Rust 采用了内存模型（Memory Model）这一概念。这个概念主要借鉴自 C++11 中引入的内存模型，它定义了在多线程环境下内存访问的行为规范。 内存模型的核心目标是在以下三方面之间取得平衡： 正确性保证：确保多线程程序的行为是可预测和一致的。 性能优化：允许编译器和 CPU 在不违反正确性的前提下进行优化。 跨平台兼容：提供一个统一的抽象层，使代码可以在不同的硬件架构上正确运行。 具体来说，内存模型： 为开发者提供了清晰的规则，说明在多线程环境下，什么样的内存访问行为是合法的，什么样的行为会导致未定义行为。 为编译器开发者提供了明确的标准，指导他们在不同平台上实现必要的内存同步原语。 通过定义不同的内存顺序级别（如 Relaxed、Release/Acquire、SeqCst 等），让开发者可以根据需要选择合适的同步强度。 这种抽象让开发者可以专注于并发逻辑本身，而不必过分关 注底层硬件的具体实现细节。 Sequenced-Before 在讨论内存顺序之前，我们需要先对 2 个重要关系术语进行简单阐述，分别是 Sequenced-Before 和 Happens-Before。 Sequenced-Before 描述的是单个线程内的操作顺序。它基于程序的源代码顺序，表示在同一线程中，一个操作在程序中出现在另一个操作之前。 具体来说，如果操作 A sequenced-before 操作 B，那么： 数据依赖关系：如果 B 依赖于 A 的结果，那么 A 一定会在 B 之前执行。例如： let x = 1; // 操作 Alet y = x + 1; // 操作 B - 依赖于 A 的结果 原子操作的顺序：对同一个原子变量的操作会保持程序顺序。例如： X.fetch_add(5, Relaxed); // 一定先执行X.fetch_add(10, Relaxed); // 一定后执行 独立操作的可重排性：如果两个操作之间没有数据依赖关系，且操作的是不同的变量，那么它们可能会被重排序。例如： X.store(1, Relaxed); // 这两个操作可能会被重排序Y.store(2, Relaxed); // 因为它们操作的是不同的变量 Happens-Before Happens-Before 则描述了跨线程的操作顺序。它定义了不同线程中的操作之间的可见性和顺序关系。如果操作 A Happens-Before 操作 B，那么 A 的内存写入对 B 是可见的。 典型的 Happens-Before 有： 同一线程内，如果先调用 f()，再调佣 g()，则 f() happens-before g()，其实这就是 sequenced-before。 spawing happens-before joining。 lock happens-before unlock。 举个例子： static X: AtomicI32 = AtomicI32::new(0);fn main() X.store(1, Relaxed); let t = thread::spawn(f); X.store(2, Relaxed); t.join().unwrap(); X.store(3, Relaxed);fn f() let x = X.load(Relaxed); assert!(x == 1 || x == 2); 上面这个例子的执行顺序如下图所示，因为 spawn happens-before join，所以我们可以确定的执行顺序是：“store 1 to X”→“store 2 to X”→“store 3 to X”。而 load from X 介于 spawn 和 join 之间，且没有进行任何其他的内存顺序限制，所以它和 store 2 to X 之间的顺序是不确定的，但是可以肯定的是，它一定在 store 3 to X 之前，所以 assert!(x == 1 || x == 2); 是永远成立的。 到这里，相信不少读者已经能够理解为什么需要内存顺序这个东西了，核心问题就是在于 store 2 to X 和 load from X 的执行顺序是否会影响我们的业务逻辑，如果不会，那么我们可以指定最松散的内存顺序要求，如果会，那么我们就要利用指定合适的内存顺序来使得其按照我们的预期顺序进行执行，从而保证业务逻的正确。 Rust 内存顺序 Rust 支持五种内存顺序（Ordering），从最松散到最严格依次为： 内存顺序 说明 保证 适用场景 示例 Relaxed 最宽松的内存顺序 - 仅保证操作的原子性- 不提供任何同步保证- 不建立 happens-before 关系 - 简单计数器- 性能要求极高且确定不需要同步- 已通过其他方式确保同步 counter.fetch_add(1, Ordering::Relaxed) Release 用于存储操作 - 之前的内存访问不会被重排到此操作之后- 与 Acquire 配对使用可建立 happens-before 关系 - 生产者-消费者模式- 发布共享数据- 初始化完成标志 data.store(42, Ordering::Release) Acquire 用于加载操作 - 之后的内存访问不会被重排到此操作之前- 与 Release 配对使用可建立 happens-before 关系 - 生产者-消费者模式- 获取共享数据- 检查初始化标志 data.load(Ordering::Acquire) AcqRel 同时包含 Acquire 和 Release 语义 - 结合了 Acquire 和 Release 的所有保证- 用于读改写操作 - 需要双向同步的原子操作- 锁的实现- 复杂的同步原语 value.fetch_add(1, Ordering::AcqRel) SeqCst 最严格的内存顺序 - 包含 AcqRel 的所有保证- 所有线程看到的所有 SeqCst 操作顺序一致- 提供全局的顺序一致性 - 需要严格的全局顺序- 不确定使用哪种顺序时- 对性能要求不高的场景 flag.store(true, Ordering::SeqCst) 在 C++ 中，其实还有另外一种内存顺序 Consume，它是 Acquire 的一个更弱的版本： Acquire: 保证后续的所有读写操作不会重排到这个操作前面 Consume: 只保证后续与这个操作结果相关的读写操作不会重排到这个操作前面 理论上，Consume 在某些架构上可以提供比 Acquire 更好的性能，因为它只需要对数据依赖的操作进行同步。 然而，由于以下原因，Rust 选择不支持 Consume 顺序： 实现复杂性：很多编译器实现者发现正确实现 Consume 语义非常困难。 性能收益不确定：在实践中，大多数编译器都将 Consume 视为 Acquire 来处理。 标准困惑：C++ 标准委员会也承认当前的 Consume 语义定义存在问题，正在考虑重新设计。 选择建议： 不确定选择哪种顺序时：使用 SeqCst（最安全但性能最低）或咨询有经验的开发者性能优化时：先使用 SeqCst 开发在性能测试后，根据需要降低到 Release/Acquire只有在确实需要时才使用 Relaxed常见组合：Release 写 + Acquire 读：最常见的生产者-消费者模式AcqRel：用于原子的读改写操作Relaxed：用于简单的计数器场景 下面我们来对每种内存顺序进行举例阐述。 Relaxed Relaxed 是最宽松的内存顺序，它只保证了原子操作在并发下的安全性，但不保证执行顺序。 考虑如下代码： static X: AtomicI32 = AtomicI32::new(0);fn a() X.fetch_add(5, Relaxed); X.fetch_add(10, Relaxed);fn b() let a = X.load(Relaxed); let b = X.load(Relaxed); let c = X.load(Relaxed); let d = X.load(Relaxed); println!(a b c d); // 这个输出不一定fn main() thread::scope(|s| s.spawn(a); s.spawn(b); ); println!(:?, X.load(Relaxed)); // 最终结果一定是 15 基于我们上面提到的 sequenced-before 规则，我们可以确定 a 和 b 两个线程内的 happens-before 规则，但是二者之间的 happens-before 是无法确定的，但是我们可以确定最后的结果是 15。下图展示了上述代码的执行顺序示意图： 虽然两个线程之间的 happens-before 是无法确定的，但是我们可以确定 X 的变化顺序：0→5→15。所以线程 b 输出 0 0 0 0、0 0 5 15 和 0 15 15 15 都是可能的，而永远不可能输出 0 5 0 15 或 0 0 10 15 类似的结果。 但是如果是这样子的话，就不一定了： static X: AtomicI32 = AtomicI32::new(0);fn a1() X.fetch_add(5, Relaxed);fn a2() X.fetch_add(10, Relaxed);fn b() let a = X.load(Relaxed); let b = X.load(Relaxed); let c = X.load(Relaxed); let d = X.load(Relaxed); println!(a b c d); // 这个输出不一定fn main() thread::scope(|s| s.spawn(a1); s.apawn(a2); s.spawn(b); ); println!(:?, X.load(Relaxed)); // 最终结果一定是 15 上面这个例子，X 的变化顺序可以是 0→5→15，也可以是 0→10→15，这取决于哪个 fetch_add 先被执行。 再举个例子： static DATA: AtomicI32 = AtomicI32::new(0);static READY: AtomicBool = AtomicBool::new(false);fn main() thread::scope(|s| // 线程 A - 写入者 s.spawn(|| DATA.store(123, Ordering::Relaxed); // ① 准备数据 READY.store(true, Ordering::Relaxed); // ② 发出数据就绪信号 ); // 线程 B - 读取者 s.spawn(|| while !READY.load(Ordering::Relaxed) // ③ 等待数据就绪信号 thread::yield_now(); assert_eq!(DATA.load(Ordering::Relaxed), 123); // ④ 获取数据，这里断言一定成功吗？ ); ); 上面这个例子中，线程 A 执行了： DATA.store(123, Ordering::Relaxed); // 准备数据READY.store(true, Ordering::Relaxed); // 发出数据就绪信号 这是 2 个没有依赖关系的原子操作，且使用的是 Relaxed 内存顺序，所以对于线程 B 来说，这 2 个操作的顺序是不确定的。所以是很可能在 READY.load(Ordering::Relaxed) 返回 true 的时候，DATA.load(Ordering::Relaxed) 依旧还是 0。 那如何确保这个断言一定成功呢？那就需要“升级”一下了~ 这个时候就轮到 Release 和 Acquire 的出场了。 Release Acquire Release 和 Acquire 一般成对出现，它们共同建立了线程间的同步关系： Release: 作用于写操作（store），确保该操作之前的所有内存访问不会被重排到这个 Release 操作之后。 Acquire: 作用于读操作（load），确保该操作之后的所有内存访问不会被重排到这个 Acquire 操作之前。 当一个线程通过 Acquire 读取到另一个线程通过 Release 写入的值时，会建立一个 happens-before 关系：线程 A 中 Release 写入之前的所有内存写操作，对于线程 B 中 Acquire 读取之后的所有内存读操作都是可见的。 修改一下上面的例子： static DATA: AtomicI32 = AtomicI32::new(0);static READY: AtomicBool = AtomicBool::new(false);fn main() thread::scope(|s| s.spawn(|| DATA.store(123, Ordering::Relaxed); READY.store(true, Ordering::Release); // 这里改为 release ); s.spawn(|| while !READY.load(Ordering::Acquire) // 这里改为 acquire thread::yield_now(); assert_eq!(DATA.load(Ordering::Relaxed), 123); // 必定成功 ); ); 如上图所示，在这个例子中： Release-Acquire 同步确保了 READY 的写入和读取之间建立了 happens-before 关系 由于 DATA 的写入在 READY 的 Release 写入之前，而 DATA 的读取在 READY 的 Acquire 读取之后 因此可以保证线程 B 一定能看到线程 A 写入的值 123 更进一步，我们通过观察，可以发现 DATA 都没必要使用 Atomic 类型，因为由 READY 建议的 happens-before 规则已经能保证对 DATA 的读写不可能并发执行了。不过因为 Rust 的类型系统并不允许跨线程进行非原子类型的读写操作，所以这里我们需要使用 unsafe 才能使编译通过，但通过我们之前的分析，我们可以确保下面这段代码是安全的： static mut DATA: u64 = 0;static READY: AtomicBool = AtomicBool::new(false);fn main() thread::spawn(|| // Safety: 此时没有其他线程访问 DATA， // 因为我们还没有设置 READY 标志 unsafe DATA = 123 ; READY.store(true, Release); // 在这个存储操作之前的所有内存操作 .. ); while !READY.load(Acquire) // .. 在这个加载操作返回 true 后都是可见的 thread::sleep(Duration::from_millis(100)); println!(waiting...); // Safety: 没有线程会修改 DATA，因为 READY 已经被设置 println!(, unsafe DATA ); 释放序列（Release Sequence） 我们再来看一段代码示例：use std::sync::atomic::AtomicU8, thread;static mut DATA: Veci64 = vec![];static FLAG: AtomicU8 = AtomicU8::new(0);fn thread_1() unsafe DATA.push(42); FLAG.store(1, std::sync::atomic::Ordering::Release);fn thread_2() let mut expected = 1; // memory_order_relaxed is okay because this is an RMW, // and RMWs (with any ordering) following a release form a release sequence while FLAG .compare_exchange( expected, 2, std::sync::atomic::Ordering::Relaxed, std::sync::atomic::Ordering::Relaxed, ) .is_err() expected = 1 fn thread_3() while FLAG.load(std::sync::atomic::Ordering::Acquire) 2 // if we read the value 2 from the atomic flag, we see 42 in the vector unsafe assert_eq!(DATA[0], 42); // will never fire fn main() thread::scope(|s| s.spawn(thread_1); s.spawn(thread_2); s.spawn(thread_3); );这段代码是参考 cppreference而翻译成 Rust 代码的，在上述代码中，即使 thread_2中我们使用的是 Relaxed， 这段代码中的assert_eq!(DATA[0], 42)也是一定成功的。为什么呢？这涉及到一个重要的概念——释放序列（ReleaseSequence）：对某个原子对象 M的一段连续修改序列的定义，用于保证acquire-加载 能够同步到对应的release-存储，形成 happens-before关系。具体来说：起始于一次释放操作（release operation）该释放操作是对原子对象 M 的一次写操作，且其内存语义为release、acq_rel 或 seq_cst。这条操作在修改顺序（modificationorder）中作为释放序列的头部。后续紧随其后的所有修改自头部释放操作之后，凡是在 M的修改顺序中紧跟出现的原子操作，且满足以下之一，皆被纳入同一释放序列：同一线程对 M 执行的任意原子写操作；任意线程对 M 执行的“读-改-写”（RMW）原子操作（如fetch_add、compare_exchange 等）。只要序列中没有出现其它线程的普通（非RMW）store，就形成一个最大连续子序列，这就是完整的释放序列。序列中继发的这些写或 RMW 操作本身无需再指定memory_order_release（它们即便是relaxed），也都被“挂到”最初那次 release 操作上，从而被后续的 acquire加载所“看到”。在这段代码中：当 thread_2 的 RMW操作成功的时候，说明 FLAG 是 1，即thread_1 已经执行了 release操作，这个时候：thread_1 的 release 操作建立了同步点thread_2 的 RMW操作自动成为释放序列的一部分当 thread_3 通过 acquire 看到值 2时，它能看到整个释放序列的所有修改。因此能保证看到 DATA 中的 42。所以在这种场景下使用 relaxed 既安全又高效，因为：它是释放序列的一部分不需要额外的同步开销仍然能保证正确的内存顺序为什么这样设计呢？原子性保证：RMW操作本身就是原子的，不会产生数据竞争连续性：每个 RMW操作都直接或间接地基于前一个操作的结果因果关系：形成了一个清晰的修改链条性能考虑：中间的 RMW 操作不需要额外的同步开销 Sequentially Consistent SeqCst 是最严格的内存顺序，它包括获取 release 和 acquire 的所有保证，还保证了全局一致的操作顺序。简单理解就是，你代码的顺序是怎么样，实际的执行顺序就是什么样。 我们来看一段代码： use std::sync::atomic::Ordering::SeqCst;static A: AtomicBool = AtomicBool::new(false);static B: AtomicBool = AtomicBool::new(false);static mut S: String = String::new();fn main() let a = thread::spawn(|| A.store(true, SeqCst); if !B.load(SeqCst) unsafe S.push(!) ; ); let b = thread::spawn(|| B.store(true, SeqCst); if !A.load(SeqCst) unsafe S.push(!) ; ); a.join().unwrap(); b.join().unwrap(); 在这段代码中，两个线程都是希望将自己的原子变量设置为 true，从而阻止另外一个线程对 S 进行 push 操作，其实就类似于锁。因为这里使用了 SeqCst，所以代码的执行顺序是跟代码编写顺序是一致的，那么就可能出现以下 3 种执行情况： seqcst-memory-order 即：同一时刻，最多只可能有一个线程会对 S 进行操作。 内存屏障 除了内存顺序（Memory Order），还有另外一种方式可以控制程序的执行顺序，就是内存屏障（Memory Barrier）。内存屏障是一种底层的同步原语，它能强制处理器按照特定的顺序执行内存操作。内存屏障通过阻止或限制指令重排序，来确保内存操作的可见性和顺序性。 基本概念 内存屏障主要分为以下几种类型： Load Barrier（读屏障） 确保在屏障之前的所有读操作都执行完成 防止后续读操作被重排到屏障之前 对应 Acquire 语义 Store Barrier（写屏障） 确保在屏障之前的所有写操作都执行完成 防止后续写操作被重排到屏障之前 对应 Release 语义 Full Barrier（全屏障） 同时包含读屏障和写屏障的功能 防止任何内存操作的重排序 对应 SeqCst 语义 即下面这 2 种实现方式是等价的： 所以到这里，我们可以更好地理解为什么 release 是阻止其前面的内存访问越过它，而 acquire 是阻止其后面的内存访问越过它了。因为有个 fence 在前面或后面拦着！ 但是一般来说，下面的写法相比上面的写法会有一丢丢的性能损失，因为这会增加一些额外的处理指令。那 fence 的用武之地是什么呢？ 可以同时对多个原子操作进行 fench； 可以根据条件判断，选择是否进行 fench。 举个例子： 这个例子的关键点是： 如果线程 2 中的任何一个 load 操作观察到了线程 1 中对应的 store 操作的值： 比如 A.load() 读到了值 1，或 B.load() 读到了值 2，或 C.load() 读到了值 3 那么：线程 1 中的 release fence 就会 happens-before 线程 2 中的 acquire fence。这意味着线程 1 中 release fence 之前的所有内存操作对线程 2 中 acquire fence 之后的操作都是可见的。 这展示了内存屏障的一个重要优势：一个屏障可以同时为多个原子操作建立同步关系，而不需要在每个原子操作上都使用 Release/Acquire 内存序。这在某些场景下可能会更高效。 用更通俗的话说：这就像在线程 1 设置了一个\"检查点\"（release fence），在线程 2 也设置了一个\"检查点\"（acquire fence），只要线程 2 看到了线程 1 在其检查点之后做的任何一个改动，那么线程 1 检查点之前的所有操作对线程 2 的检查点之后都是可见的。 硬件实现 不同的处理器架构实现内存屏障的方式不同： ; x86/x64MFENCE ; 全屏障LFENCE ; 读屏障SFENCE ; 写屏障; ARMDMB ; 数据内存屏障DSB ; 数据同步屏障ISB ; 指令同步屏障 与内存顺序的关系 Rust 的内存顺序实际上是通过内存屏障来实现的： // Release 写入会插入 Store Barrieratomic.store(42, Ordering::Release); // 编译器会在此处插入 Store Barrier// Acquire 读取会插入 Load Barrierlet x = atomic.load(Ordering::Acquire); // 编译器会在此处插入 Load Barrier// SeqCst 操作会插入 Full Barrieratomic.store(42, Ordering::SeqCst); // 编译器会在此处插入 Full Barrier 注意：直接使用内存屏障是非常底层的操作，通常我们应该使用 Rust提供的高级抽象（如原子类型和它们的内存顺序）来实现同步。内存屏障的知识主要用于理解这些高级抽象的工作原理。 Go Atomic 熟悉 Go 语言的读者应该会意识到在使用 Go 语言的原子类型的时候，好像都没见过 Memory Order 这个东西，如下： package mainimport (\tsync/atomic)func main() data := atomic.Int64\tdata.Add(1)\tdata.And(2)\tdata.Or(3)\tdata.Swap(4)\tdata.Store(5)\tdata.Load()\tdata.CompareAndSwap(6, 7) 在 atomic/doc.go 源码中我们可以看到这段话： // The load and store operations, implemented by the LoadT and StoreT// functions, are the atomic equivalents of return *addr and// *addr = val.//// In the terminology of [the Go memory model], if the effect of// an atomic operation A is observed by atomic operation B,// then A “synchronizes before” B.// Additionally, all the atomic operations executed in a program// behave as though executed in some sequentially consistent order.// This definition provides the same semantics as// C++s sequentially consistent atomics and Javas volatile variables.//// [the Go memory model]: https://go.dev/ref/mem Go 语言设计者认为让程序员选择内存序会增加复杂性和出错的可能，所以为了程序的简单性和可预测性，直接就使用了最安全的 Seq-Cst 内存顺序了。 the Go memory model 中还提了一句： If you must read the rest of this document to understand the behavior of your program, you are being too clever.Dont be clever. 这也呼应了 Go 的设计理念： Share memory by communicating; dont communicate by sharing memory. 所以总结一下： Go 的原子操作采用了最强的顺序一致性内存序； 这是一个有意识的设计选择，为了简单性和可预测性； 如果你需要更细粒度的内存序控制，那么 Go 可能不是最佳选择； Go 更推荐使用 channels 和其他同步原语来进行并发控制。 参考 Rust Atomics And Lock 聊一聊内存模型与内存序 cppreference the Go memory model","tags":["go","rust","内存顺序","内存屏障","并发控制","atomic","happens-before"],"categories":["rust","rust 底层原理"]},{"title":"Rust 实战丨SSE(Server-Sent Events)","path":"/2024/06/06/rust-action-sse/","content":"📌 SSE（Server-Sent Events）是一种允许服务器向客户端浏览器推送信息的技术。它是 HTML5 的一部分，专门用于建立一个单向的从服务器到客户端的通信连接。SSE 的使用场景非常广泛，包括实时消息推送、实时通知更新等。 SSE 的本质 严格地说，HTTP无法做到服务器主动推送信息。但是，有一种变通方法，就是服务器向客户端声明，接下来要发送的是流信息（streaming）。 也就是说，发送的不是一次性的数据包，而是一个数据流，会连续不断地发送过来。这时，客户端不会关闭连接，会一直等着服务器发过来的新的数据流，视频播放就是这样的例子。本质上，这种通信就是以流信息的方式，完成一次用时很长的下载。 SSE 就是利用这种机制，使用流信息向浏览器推送信息。它基于 HTTP 协议，目前除了 IE/Edge，其他浏览器都支持。 特点 持续连接：与传统的 HTTP 请求不同，SSE 保持连接开放，服务器可以随时发送消息。 文本数据流：SSE 主要传输文本数据，这些数据以特定的格式流式传输，使得每条消息都是简单的文本格式。 内置重连机制：浏览器会自动处理连接中断和重连，包括在重连请求中发送最后接收的事件 ID，以便服务器从正确的位置恢复发送事件。 简单的客户端处理：在浏览器中，使用 JavaScript 的 EventSource 接口处理 SSE 非常简单，只需几行代码即可监听服务器发来的事件。 工作原理 建立连接：客户端通过创建一个 EventSource 对象请求特定的 URL 来启动 SSE 连接。这个请求是一个标准的 HTTP 请求，但会要求服务器以特定方式响应。 服务器响应：服务器响应必须设置 Content-Type 为 text/event-stream，然后保持连接打开。 发送消息：服务器可以通过持续发送数据格式为特定事件流的消息来推送更新。每个消息包括一个可选的事件类型、数据和一个可选的 ID。 数据：实际的消息内容，以 data: 开头，多行数据以双换行符 结束。 事件类型：允许客户端根据事件类型来监听，以 event: 开头。 ID：如果连接中断，客户端将发送包含上次接收的最后一个 ID 的 Last-Event-ID 头，以便服务器从断点继续发送数据。 实战 客户端 !DOCTYPE htmlhtml head titleSSE Test/title /head body h1Server-Sent Events Test/h1 div id=events/div script // 确保这里的URL匹配你的服务器地址和端口 var eventSource = new EventSource(http://localhost:8000/events); eventSource.onmessage = function (event) console.log(New event:, event.data); document.getElementById(events).innerHTML += event.data + br; ; /script /body/html Rust 服务端 Rust 实现演示 依赖： anyhow = 1.0.86axum = version = 0.7.5 chrono = 0.4.38futures-core = 0.3.30tokio = version = 1.38.0, features = [macros, rt-multi-thread, ] tokio-stream = 0.1.15tower-http = version = 0.5.2, features = [cors] 代码： use std::time::Duration;use axum:: response::sse::Event, Sse, routing::get, Router,;use tokio::net::TcpListener, time::interval;use tokio_stream::wrappers::IntervalStream, StreamExt;use tower_http::cors::Any, CorsLayer;#[tokio::main]async fn main() - anyhow::Result() let cors = CorsLayer::new() .allow_headers(Any) .allow_origin(Any) .allow_headers(Any) .allow_credentials(false); let listener = TcpListener::bind(0.0.0.0:8000).await?; let app = Router::new().route(/events, get(sse_handler)).layer(cors); axum::serve(listener, app).await?; Ok(())async fn sse_handler() - Sseimpl futures_core::StreamItem = ResultEvent, axum::Error let interval = interval(Duration::from_secs(1)); let stream = IntervalStream::new(interval).map(|_| let data = format!( , chrono::Local::now().to_rfc2822()); Ok(Event::default().data(data)) ); Sse::new(stream) Go 服务端 Go 实现演示 package mainimport (\tfmt\tlog\tnet/http\ttime)func sseHandler(w http.ResponseWriter, r *http.Request) // 设置头部信息，确保允许跨域，并且告诉浏览器这是一个事件流\tw.Header().Set(Content-Type, text/event-stream)\tw.Header().Set(Cache-Control, no-cache)\tw.Header().Set(Connection, keep-alive)\tw.Header().Set(Access-Control-Allow-Origin, *)\t// 不断发送消息\tfor // 生成服务器时间，并发送给客户端 now := time.Now() // 生成消息，格式为 data: content msg := fmt.Sprintf(data: %s , now.Format(time.DateTime)) // 发送消息 if _, err := fmt.Fprintf(w, msg); err != nil log.Println(write error:, err) break // 刷新响应缓冲，确保即时发送 flusher, ok := w.(http.Flusher) if !ok log.Println(Streaming unsupported!) break flusher.Flush() // 每秒发送一次 time.Sleep(1 * time.Second)\tfunc main() http.HandleFunc(/events, sseHandler)\tlog.Println(Server started on port 8000...)\tlog.Fatal(http.ListenAndServe(:8000, nil))","tags":["go","rust","sse"],"categories":["rust","rust 实战"]},{"title":"Rust 实战丨通过实现 json! 掌握声明宏","path":"/2024/05/28/rust-action-macro-json/","content":"在 Rust 编程语言中，宏是一种强大的工具，可以用于在编译时生成代码。json! 是一个在 Rust 中广泛使用的宏，它允许我们在 Rust 代码中方便地创建 JSON 数据。 声明宏（declarative macros）是 Rust 中的一种宏，它们使用 macro_rules! 关键字定义。 本文将参考《Rust 程序设计（第二版）》，通过实现 json! 宏，深入理解声明宏的工作原理。 结论先行 本文我们将构建一个 json! 宏，它支持我们以字符串 JSON 风格的语法来编写 Json 值。如下面这个例子： let students = json![ name: Hedon Wang, class_of: 2022, major: Software engineering\t, name: Jun Lei, class_of: 1991, major: Computor science\t] 完整代码 实现 json! 定义 Json enum 首先我们需要思考一下 Json 结构是什么样子的？主要是以下 3 种模式： name: hedon, age: 18, school: name: Wuhan University, address: Hubwi Wuhan [ name: hedon , name: john ] null 为此我们定义一个 Json 结构的枚举： #[derive(Clone, PartialEq, Debug)]pub enum Json Null, Boolean(bool), Number(f64), String(String), Array(VecJson), Object(HashMapString, Json), 你应该可以感到非常奇妙，使用一个这么简单的枚举，居然就可以表示所有的 Json 结构了。遗憾的是，现在这个结构编写 Json 值的语法相当冗长。 let people = Json::Object(HashMap::from([ (name.to_string(), Json::String(hedon.to_string())), (age.to_string(), Json::Number(10.0)), (is_student.to_string(), Json::Boolean(true)), ( detail.to_string(), Json::Object(HashMap::from([ (address.to_string(), Json::String(beijing.to_string())), (phone.to_string(), Json::String(1234567890.to_string())) ])) )])) 我们期望可以以下面这种方式来声明 Json 变量，这看起来就清爽许多了。 let students = json!([ name: Jim Blandy, class_of: 1926, major: Tibetan throat singing , name: Jason Orendorff, class_of: 1702, major: Knots ]); 猜想 json! 我们可以预见 Json 宏内部将会有多条规则，因为 JSON 数据有多种类型：对象、数组、数值等。事实上，我们可以合理地猜测每种 JSON 类型都将有一条规则： macro_rules! json (null) = Json::Null ; ([ ... ]) = Json::Array(...) ; ( ... ) = Json::Object(...) ; (???) = Json::Boolean(...) ; (???) = Json::Number(...) ; (???) = Json::String(...) ; 然而这不太正确，因为宏模式无法区分最后 3 种情况，稍后我们会讨论如何处理。至于前 3 种情况，显然它们是以不同的语法标记开始的，所以这几种情况比较好处理。 实现 Null 我们先从最简单的 Null 分支开始，先编写如下测试用例： #[cfg(test)]mod tests use super::*; #[test] fn test_null_json() let json = json!(null); assert_eq!(json, Json::Null); 想要通过上述测试用例非常简单，我们只需要在 macro_rules! 支持中匹配这种情况即可： #[macro_export]macro_rules! json (null) = Json::Null ; #[macro_export] 注解是 Rust 中的一个属性，用于指示这个宏应该被导出到调用者的作用域中，这样其他模块也可以使用它。 macro_rules! 宏定义了一个自定义的宏。在这里，它创建了一个名为 json 的宏，用于生成 JSON 数据。 宏定义中 (null) 是匹配模式。这意味着当你调用 json! 宏并传递 null 作为参数时，将会触发这个规则。 = 符号用于指示匹配模式后的代码块。在这里，它指定了当匹配 (null) 时应该生成的代码块。 Json::Null 是一个 JSON 类型的枚举值，表示 JSON 中的 null 值。这个宏的目的是将传入的 null 转换为 Json::Null。 实现 Boolean/Number/String 我们先准备如下测试用例： #[test]fn test_boolean_number_string_json() let json = json!(true); assert_eq!(json, Json::Boolean(true)); let json = json!(1.0); assert_eq!(json, Json::Number(1.0)); let json = json!(hello); assert_eq!(json, Json::String(hello.to_string())); 通过观察分析，它们其实都是同一种模式： Boolean/Number/String 分析 现在需要解决的问题就是，如何将这 3 种模式进行统一，这样在 macro_rules! 中才可以统一匹配模式并进行代码生成。 这里我们其实需要做的就是将 bool、f64 和 str 转为对应的 Json 类型。那就需要用到标准库中的 From trait 了。 做法很简单，我们实现如下代码： impl Frombool for Json fn from(value: bool) - Self Json::Boolean(value) impl Fromstr for Json fn from(value: str) - Self Json::String(value.to_string()) impl Fromf64 for Json fn from(value: f64) - Self Json::Number(value) 然后完善我们的 json!，目前的实现如下： #[macro_export]macro_rules! json (null) = Json::Null ; ($value: tt) = Json::from($value) ; 这里我们使用 $value作 为变量来承接匹配到的元素，其类型为 tt ，表示任意的语法标记树。具体可以参考：片段类型。 这时运行上述测试用例，是没有问题的： PASS [ 0.004s] json-macro tests::test_boolean_number_string_jsonPASS [ 0.004s] json-macro tests::test_null_json 美中不足的是，JSON 结构中的数字类型，其实不一定是 f64，也可以是 i32、u32、f32 或其他的数字类型，如果我们要为这全部的数字类型都实现到 Json 的 From trait，那就多冗余。 这个时候我们又可以实现一个宏，用于快速生成 impl FromT for Json 。这个实现比较简单，本文就不赘述了，代码如下： #[macro_export]macro_rules! impl_from_for_primitives ( $( $type: ty ) * ) = $( impl From$type for Json fn from(value: $type) - Self Json::Number(value as f64) )* 然后我们只需要用下面这一行代码，就可以为所有的数字类型实现 From trait 了： impl_from_for_primitives!(u8 u16 u32 u64 i8 i16 i32 i64 f32 f64 isize usize); 记得这个时候你要删除上面手动实现的 impl Fromf64 for Json，不然会有 impl 冲突错误。 再次运行测试，也是可以通过的。 实现 Array 准备如下测试用例： #[test]fn test_array_json() let json = json!([1, null, string, true]); assert_eq!( json, Json::Array(vec![ Json::Number(1.0), Json::Null, Json::String(string.to_string()), Json::Boolean(true) ]) ) 要匹配 [1, null, \"string\", true]这个模式，笔者的分析过程如下： 首先是外面的两个中括号 [ 和 ] ； 再往里，是一个重复匹配的模式，以 , 分割，可以匹配 0 到任意多个元素，所以是 $( ,*) ，具体可以参考：重复模式； 最里面就是第 2 步要匹配的元素了，我们先用 $element 作为变量来承接每一个元素，其类型为 tt ，表示任意的语法标记树。 分析完匹配的表达式后，我们就可以得到： ([ $( $element:tt ), * ]) = /* TODO */ 我们要生成的代码长这个样子： Json::Array(vec![ Json::Number(1.0), Json::Null, Json::String(string.to_string()), Json::Boolean(true)]) 其实就是一个 vec!，然后里面每个元素都是一个 Json，如此递归下去。 即可以得到代码生成部分的逻辑为： Json::Array(vec![$(json!($element)),* ]) Json::Array 宏分析 综上，我们实现的代码如下： #[macro_export]macro_rules! json (null) = Json::Null ; ([ $( $element: tt),* ]) = Json::Array(vec![ $( json!($element)), * ]) ; ($value: tt) = Json::from($value) ; 运行测试用例： PASS [ 0.003s] json-macro tests::test_null_jsonPASS [ 0.003s] json-macro tests::test_boolean_number_string_jsonPASS [ 0.004s] json-macro tests::test_array_json 实现 Object 写好如下测试用例，这次我们顺带把 Null、Boolean、Number 和 String 带上了： #[test]fn test_object_json() let json = json!( null: null, name: hedon, age: 10, is_student: true, detail: address: beijing, phone: 1234567890 ); assert_eq!( json, Json::Object(HashMap::from([ (name.to_string(), Json::String(hedon.to_string())), (age.to_string(), Json::Number(10.0)), (is_student.to_string(), Json::Boolean(true)), ( detail.to_string(), Json::Object(HashMap::from([ (address.to_string(), Json::String(beijing.to_string())), (phone.to_string(), Json::String(1234567890.to_string())) ])) ) ])) ) 对比预期的 json! 宏内容和展开后的代码： Json::Object 宏分析 完善我们的 macro_rules! json ： #[macro_export]macro_rules! json (null) = Json::Null ; ([ $( $element: tt),* ]) = Json::Array(vec![ $( json!($element)), * ]) ; ( $( $key:tt : $value:tt ),* ) = Json::Object(HashMap::from([ $( ( $key.to_string(), json!($value) ) ), * ])) ; ($value: tt) = Json::from($value) ; 运行测试用例： PASS [ 0.004s] json-macro tests::test_object_jsonPASS [ 0.005s] json-macro tests::test_array_jsonPASS [ 0.004s] json-macro tests::test_null_jsonPASS [ 0.005s] json-macro tests::test_boolean_number_string_json 至此，我们就完成了 json! 宏的构建了！完整源码可见：完整代码 Peace! Enjoy coding~ 附录 重复模式 在 实现 Array 中，我们匹配了这样一个模式： ([ $( $element:tt ), * ]) = /* TODO */ 其中 $($element:tt), *) 就是一个重复模式，其可以进一步抽象为 $( ... ),* ，表示匹配 0 次或多次，以 , 分隔。 Rust 支持以下全部重复模式： 模式 含义 $( … ) * 匹配 0 次或多次，没有分隔符 $( … ), * 匹配 0 次或多次，以逗号分隔 $( … ); * 匹配 0 次或多次，以分号分隔 $( … ) + 匹配 1 次或多次，没有分隔符 $( … ), + 匹配 1 次或多次，以逗号分隔 $( … ); + 匹配 1 次或多次，以分号分隔 $( … ) ? 匹配 0 次或 1 次，没有分隔符 即： * 表示 0 次或多次 + 表示 1 次或多次 ? 表示 0 次或 1 次 可在上述 3 者之前加入分隔符 片段类型 在 实现 Array 中，我们匹配了这样一个模式： ([ $( $element:tt ), * ]) = /* TODO */ 这里我们将 $element 指定为 tt，这个 tt 就是宏中的一种片段类型。 tt 能匹配单个语法标记树，包含： 一对括号，如 (..)、[..]、或 .. ，以及位于其中的所有内容，包括嵌套的语法标记树。 单独的非括号语法标记，比如 1926 或 Knots 。 所以为了匹配任意类型的 Json ，我们选择了 tt 作为 $element 的片段类型。 macro_rules! 支持的片段类型如下所示： 片段类型 匹配（带例子） 后面可以跟 ······ expr 表达式：2 + 2, \"udon\", x.len() =,; stmt 表达式或声明，不包括任何尾随分号（很难用，请尝试使用 expr 或 block） =,; ty 类型：String, Vec, (str, bool), dyn Read + Send =,; = path 路径：ferns, ::std::sync::mpsc =,; = pat 模式：_, Some(ref x) =,= item 语法项：struct Point { x: f64, y: f64 }, mod ferns; 任意 block 块：{ s += \"ok\"; true } 任意 meta 属性的主体：inline, derive(Copy, Clone), doc=\"3D models.\" 任意 literal 字面量值：1024, \"Hello, world!\", 1_000_000f64 任意 lifetime 生命周期：'a, 'item, 'static 任意 vis 可见性说明符：pub, pub(crate), pub(in module::submodule) 任意 ident 标识符：std, Json, longish_variable_name 任意 tt 语法标记树：;, =, {}, [0 1 (+ 0 1)] 任意 完整代码 use std::collections::HashMap;#[derive(Debug, Clone, PartialEq)]#[allow(unused)]enum Json Null, Boolean(bool), String(String), Number(f64), Array(VecJson), Object(HashMapString, Json),impl Frombool for Json fn from(value: bool) - Self Json::Boolean(value) impl Fromstr for Json fn from(value: str) - Self Json::String(value.to_string()) impl FromString for Json fn from(value: String) - Self Json::String(value) #[macro_export]macro_rules! impl_from_for_primitives ( $( $type: ty ) * ) = $( impl From$type for Json fn from(value: $type) - Self Json::Number(value as f64) )* impl_from_for_primitives!(u8 u16 u32 u64 i8 i16 i32 i64 f32 f64 isize usize);#[macro_export]macro_rules! json (null) = Json::Null ; ([ $( $element: tt),* ]) = Json::Array(vec![ $( json!($element)), * ]) ; ( $( $key:tt : $value:tt ),* ) = Json::Object(HashMap::from([ $( ( $key.to_string(), json!($value) ) ), * ])) ; ($value: tt) = Json::from($value) ;#[cfg(test)]mod tests use super::*; #[test] fn test_null_json() let json = json!(null); assert_eq!(json, Json::Null); #[test] fn test_boolean_number_string_json() let json = json!(true); assert_eq!(json, Json::Boolean(true)); let json = json!(1.0); assert_eq!(json, Json::Number(1.0)); let json = json!(hello); assert_eq!(json, Json::String(hello.to_string())); #[test] fn test_object_json() let json = json!( null: null, name: hedon, age: 10, is_student: true, detail: address: beijing, phone: 1234567890 ); assert_eq!( json, Json::Object(HashMap::from([ (null.to_string(), Json::Null), (name.to_string(), Json::String(hedon.to_string())), (age.to_string(), Json::Number(10.0)), (is_student.to_string(), Json::Boolean(true)), ( detail.to_string(), Json::Object(HashMap::from([ (address.to_string(), Json::String(beijing.to_string())), (phone.to_string(), Json::String(1234567890.to_string())) ])) ) ])) ) #[test] fn test_array_json() let json = json!([1, null, string, true]); assert_eq!( json, Json::Array(vec![ Json::Number(1.0), Json::Null, Json::String(string.to_string()), Json::Boolean(true) ]) )","tags":["rust","宏","元编程"],"categories":["rust","rust 实战"]},{"title":"xgo 原理探索","path":"/2024/05/23/go-xgo-explore/","content":"Go 单测 mock 方案 Mock 方法 原理 依赖 优点 缺点 接口 Mock 为依赖项定义接口，并提供接口的 Mock 实现。 需要定义接口和 Mock 实现。 灵活，遵循 Go 的类型系统；易于替换实现。 需要更多的样板代码来定义接口和 Mock 实现。 Monkey Patching（bouk/moneky） 直接修改函数指针的内存地址来实现对函数的替换。 内存保护；汇编代码。 强大，可以 Mock 任何函数，甚至第三方库的函数。 复杂，容易出错；线程不安全；依赖系统指令集。 bouk/monkey 弊端 bouk/monkey 🐒 monkey 的核心功能是能够在运行时替换某个函数的实现。 原理： 函数指针替换：在 Go 语言中，函数的地址存储在内存中。bouk/monkey 通过直接修改函数指针的内存地址来实现对函数的替换。 汇编代码：使用了汇编代码来实现对函数入口的跳转。这些汇编代码会在函数被调用时，将执行流重定向到新的函数实现。 内存保护：为了修改内存中的函数指针，bouk/monkey 需要临时修改内存页面的保护属性（例如，将页面设为可写）。在修改完毕后，它会恢复原来的保护属性。 反射与 unsafe 包：利用 Go 的反射机制和 unsafe 包，bouk/monkey 可以获取并操作函数的底层实现细节。 实现步骤： 保存原函数：在替换函数之前，bouk/monkey 会保存原始函数的指针，以便在需要时恢复或调用原始函数。 生成跳转代码：bouk/monkey 生成一段汇编跳转代码，这段代码会在函数调用时，将执行流跳转到新的函数实现。 修改函数指针：使用 unsafe 包，bouk/monkey 修改目标函数的入口地址，指向生成的跳转代码。 恢复内存保护：在完成上述修改后，恢复内存页面的保护属性。 有以下几个弊端： 如果启用了内联，Monkey 有时无法修补函数。尝试在禁用内联的情况下运行测试，例如: go test -gcflags=-l。同样的命令行参数也可以用于构建。 Monkey 不能在一些面向安全的操作系统上工作，这些操作系统不允许同时写入和执行内存页。目前的方法并没有真正可靠的解决方案。 线程不安全的。 依赖指令集。 先看 xgo 怎么用 xgo 😈 代码结构如下： .├── greet.go└── greet_test.go 现在在 greet.go 中有一个函数 greet： func greet(s string) string return hello + s 在真实的生产环境中，greet 可能要复杂得多，它可能会依赖各种第三方 API，也可能会依赖数据库等多种外部组件。所以在测试的时候，我们希望对其进行 mock，使其返回一个固定的值，便于我们撰写单元测试。 xgo 参考了 go-monkey 的思想，但是不从 修改指令 这个途径入手，而是另辟蹊径，从 代码重写 的角度实现了 mock 的能力。 为了使用 xgo，我们需要先安装 xgo 这个命令： go install github.com/xhd2015/xgo/cmd/xgo@latest 同时在我们的项目中需要引入 xgo 依赖： go get github.com/xhd2015/xgo/runtime/mock 我们编写的 greet_test.go 如下： package xgo_useimport (\ttesting\tgithub.com/xhd2015/xgo/runtime/mock)func TestOriginGreet(t *testing.T) res := greet(world)\tif res != hello world t.Fatalf(greet() = %q; want %q, res, hello world)\tfunc TestMockGreet(t *testing.T) mock.Patch(greet, func(s string) string return mock + s\t)\tres := greet(world)\tif res != mock world t.Fatalf(greet() = %q; want %q, res, mock world) 可以看到在 TestMockGreet 这个单元测试中，我们将 greet 进行了 mock，返回 \"mock \" + s。 mock.Patch(greet, func(s string) string return mock + s) 为了使用 xgo 的能力，我们在执行单元测试的时候，需要运行以下命令： xgo test -v ./ 输出大致如下： ➜ xgo-use git:(master) xgo test -v ./xgo is taking a while to setup, please wait...=== RUN TestOriginGreet--- PASS: TestOriginGreet (0.00s)=== RUN TestMockGreet--- PASS: TestMockGreet (0.00s)PASSok xgo-explore/xgo-use (cached) xgo 的核心原理 xgo 的核心原理是利用 go build -toolexec 的能力。 运行以下命令： go help build 找到 toolexec 的相关说明： -toolexec cmd args a program to use to invoke toolchain programs like vet and asm. For example, instead of running asm, the go command will run cmd args /path/to/asm arguments for asm. The TOOLEXEC_IMPORTPATH environment variable will be set, matching go list -f .ImportPath for the package being built. 一言以蔽之：-toolexec 允许对 go 工具链进行拦截，包括 vet、asm、compile 和 link。 这种技术也被称为：插桩（stubbing）、增强（instrumentation）和代码重写（rewriting）。 -toolexec 示意图（来源：https://blog.xhd2015.xyz/zh/posts/xgo-monkey-patching-in-go-using-toolexec/） 基于上述分析，xgo 提出了 代码重写 的思路，实现了 在编译过程中插入拦截器代码 的功能： xgo 在 go build 中的作用位置（来源：https://blog.xhd2015.xyz/zh/posts/xgo-monkey-patching-in-go-using-toolexec/） 所以上述我们的 greet.go 文件中的源代码： func greet(s string) string return hello + s 经过 xgo 编译后最终实际编译的代码如下： import runtimefunc greet(s string) (r0 string) stop, post := runtime.__xgo_trap(Greet, s, r0) if stop return defer post() return hello + s greet 函数重写变化示意图（来源：https://blog.xhd2015.xyz/zh/posts/xgo-monkey-patching-in-go-using-toolexec/） 如图所示，一旦函数被调用，它的控制流首先转移到 Trap，然后一系列拦截器将根据其目的检查当前调用是否应该被 Mock、修改、记录或停止。 如果 greet 注册了 mock 函数，那么就会在 __xgo_trap 中调用 mock 的函数，并将返回值设置到 r0 上进行返回，而跳过原始的执行逻辑。 第 1 步：死代码实现 ➜ 01-deadcode git:(master) tree.├── greet.go├── greet_test.go└── mock.go 我们先从最简单的实现开始，采用侵入性代码实现 xgo 的核心功能，这里我们还用不到 -toolexec。 代码结构如上所示，在 mock.go 中，我们有如下代码： var mockFuncs = sync.Mapfunc RegisterMockFunc(funcName string, fun interface) mockFuncs.Store(funcName, fun) mockFuncs: 用于承载函数与 mock 函数的对应关系，其中 key 为函数名称，value 为 mock 函数。我们使用 sync.Map 来保证并发安全。 RegisterMockFunc 用于为指定的 funcName 注册 mock 函数。 在 greet.go 中，我们有一个 Greet 函数： func Greet(s string) string return hello + s 如果我们要对其支持 mock，那么需要修改其实现为： func Greet(s string) string fun, ok := mockFuncs.Load(Greet)\tif ok f, ok := fun.(func(s string) string) if ok return f(s) return hello + s 在修改后的代码中，我们先判断是否存在 mock 函数，如果存在，则执行 mock 函数，否则执行原始逻辑。 现在我们在 greet_test.go 中编写测试代码： func TestMockGreet(t *testing.T) RegisterMockFunc(Greet, func(s string) string return mock + s\t)\tres := Greet(world)\tif res != mock world t.Fatalf(Greet() = %q; want %q, res, mock world)\tfunc TestOriginGreet(t *testing.T) res := Greet(world)\tif res != hello world t.Fatalf(Greet() = %q; want %q, res, hello world) 执行测试： # 单独执行 TestMockGreet➜ 01-deadcode git:(master) ✗ go test -v -run TestMockGreet=== RUN TestMockGreet--- PASS: TestMockGreet (0.00s)PASSok xgo-explore/01-deadcode 0.103s# 单独执行 TestOriginGreet➜ 01-deadcode git:(master) ✗ go test -v -run TestOriginGreet=== RUN TestOriginGreet--- PASS: TestOriginGreet (0.00s)PASSok xgo-explore/01-deadcode 0.102s# 一起执行➜ 01-deadcode git:(master) ✗ go test -v -run $Test$=== RUN TestMockGreet--- PASS: TestMockGreet (0.00s)=== RUN TestOriginGreet greet_test.go:20: Greet() = mock world; want hello world--- FAIL: TestOriginGreet (0.00s)FAILexit status 1FAIL xgo-explore/01-deadcode 0.102s 我们会发现单独执行都是 ok 的，不过一起执行的话 TestOriginGreet 就失败了，这是因为先执行了 TestMockGreet，这个时候已经往 mockFunc 中注册了 mock 函数了，所以 TessOriginGreet 就执行失败了。 这里需要在协程层面上做 mock 隔离，xgo 的思路是在编译时注入 getg() 函数来获取当前协程信息从而实现在注册 mock 函数时进行协程隔离。本文将聚焦在 xgo 的核心原理 代码重写 上，故暂时不考虑这一块。 Ok，那么短短几行代码，我们就将 xgo 的最核心思想给展示出来了。可以看到，xgo 的核心思想是往源代码中加入 合法的 Go 代码，所以不涉及指令重写，故而只要你的机器能执行 Go 程序，天然就支持 mock 功能，这就天然达到了架构无关的兼容性了。同时我们也使用了 sync.Map 来保证了并发安全。 第 2 步：死代码拦截器 ➜ 02-deadcode-interceptor git:(master) tree.├── greet.go├── greet_test.go└── mock.go 在第 1 步中，这段代码我觉得有点冗长了： fun, ok := mockFuncs.Load(Greet)if ok f, ok := fun.(func(s string) string) if ok return f(s) 参考 xgo 的函数签名，我们对其进行优化，在 mock.go 中加入一个 丐版拦截器： // mock.gofunc InterceptMock(funcName string, arg string, result *string) bool fn, ok := mockFuncs.Load(funcName)\tif ok f, ok := fn.(func(s string) string) if ok *result = f(arg) return true return false 对应 greet.go 中 Greet 函数就修改为： func Greet(s string) (res string) if InterceptMock(Greet, s, res) return res return hello + s 这看起来就清爽多了。再次执行测试代码，一样是可以通过的。 ➜ 02-deadcode-interceptor git:(master) go test -v -run TestOriginGreet=== RUN TestOriginGreet--- PASS: TestOriginGreet (0.00s)PASSok xgo-explore/02-deadcode-interceptor 0.331s➜ 02-deadcode-interceptor git:(master) go test -v -run TestMockGreet=== RUN TestMockGreet--- PASS: TestMockGreet (0.00s)PASSok xgo-explore/02-deadcode-interceptor 0.103s 第 3 步：toolexec 初探 ➜ 03-toolexec-static git:(master) tree.├── cmd│ └── mytool│ └── mytool.go├── greet.go├── main.go├── mock.go└── script.sh 这里 mock.go 没有任何变化。我们期望使用 -toolexec 来修改源代码，以实现 mock 无源代码侵入的特性，所以我们在 greet.to 中将 Greet 函数恢复为只关注实际功能的样子： func Greet(s string) (res string) return hello + s 同时为了更好地测试使用 -toolexec 编译后的运行结果，这里将 greet_test.go 删除了并新增了 main.go 文件，内容如下： func main() res := Greet(world)\tif res != hello world log.Fatalf(Greet() = %q; want %q, res, hello world) RegisterMockFunc(Greet, func(s string) string return mock + s\t)\tres = Greet(world)\tif res != mock world log.Fatalf(Greet() = %q; want %q, res, mock world) log.Println(run successfully) 那么 -toolexec 要执行的命令怎么实现呢？在 Google 搜索 go toolexec 你会看到官方给出的一个案例：toolexec.txt。 核心部分在最下面，参考这个示例，我们来实现自己的 toolexec： mkdir -p cmd/mytooltouch cmd/mytool/mytool.go 在mytool.go 中，我们先写这么点代码，看一下会输出什么。 func main() tool, args := os.Args[1], os.Args[2:]\tif len(args) 0 args[0] == -V=full // dont do anything to infuence the version full output. else if len(args) 0 fmt.Printf(tool: %s , tool) fmt.Printf(args: %v , args) // 继续执行之前的命令\tcmd := exec.Command(tool, args...)\tcmd.Stdout = os.Stdout\tcmd.Stderr = os.Stderr\tif err := cmd.Run(); err != nil log.Fatalf(run command error: %v , err) 这里我们企图输出执行的工具 tool 及传给它的参数 args。由于 -V=full 的作用是在终端输出版本信息，所以我们要跳过它，避免产生干扰。输出日志后，我们暂且先继续执行原始的命令，不对编译过程做其他的干扰。 Ok，现在就来看看这个 -toolexec 到底做了什么，在 03-toolexec-static 目录下执行以下命令： # 清除缓存，一直使用最新的编译结果go clean -cache -modcache -i -r# 编译 mytoolgo build ./cmd/mytool# 编译业务程序go build -toolexec=./mytool -o main 因为这几个命令经常会用到，所以我们可以将其封装到 script.sh 文件中： touch script.shchmod +x script.sh 内容如下： #!/bin/bashgo clean -cache -modcache -i -rgo build ./cmd/mytoolgo build -toolexec=./mytool -o main 执行上述命令后，可以看到以下输出： ➜ 03-toolexec-static git:(master) ./script.sh# xgo-explore/03-toolexec-statictool: /opt/homebrew/Cellar/go/1.22.3/libexec/pkg/tool/darwin_arm64/compileargs: [-o $WORK/b001/_pkg_.a -trimpath $WORK/b001= -p main -lang=go1.22 -complete -buildid PcS9clqF_ny_Ds5N0i_s/PcS9clqF_ny_Ds5N0i_s -goversion go1.22.3 -c=4 -shared -nolocalimports -importcfg $WORK/b001/importcfg -pack ./greet.go ./main.go ./mock.go]# xgo-explore/03-toolexec-statictool: /opt/homebrew/Cellar/go/1.22.3/libexec/pkg/tool/darwin_arm64/linkargs: [-o $WORK/b001/exe/a.out -importcfg $WORK/b001/importcfg.link -buildmode=pie -buildid=KgnnCoU_6enHkOm-T62Z/PcS9clqF_ny_Ds5N0i_s/H80dtgGZw1L8mTtVqJBf/KgnnCoU_6enHkOm-T62Z -extld=cc $WORK/b001/_pkg_.a] 可以看到执行了 compile 和 link 两个工具，compile 是编译过程，将生成 .out 文件，而 link 是将多个 .out 文件链接成一个可执行文件。这是很经典的编译过程，如果对 Go 语言的编译过程感兴趣，也可以参考官方的 Go Compile Readme，或者笔者撰写的 Go1.21.0 程序编译过程。 这里我们需要重点关注的是 compile 命令，它是负责编译源代码的，涉及到的源代码文件会通过 -pack ./greet.go ./main.go ./mock.go 传递给 compile 命令。 结合 -toolexec 的帮助信息： -toolexec cmd args a program to use to invoke toolchain programs like vet and asm. For example, instead of running asm, the go command will run cmd args /path/to/asm arguments for asm. The TOOLEXEC_IMPORTPATH environment variable will be set, matching go list -f .ImportPath for the package being built. 我们只需要在执行 compile 命令之前，在 cmd args 这个环节，进行 代码重写 就可以实现我们想要的功能了。 我们现在是要对 greet.go 里面的 Greet 函数进行重写，先看看之前的代码： package mainfunc Greet(s string) (res string) return hello + s 重写后的代码应该跟我们之前 第 2 步 是一样的： package mainfunc Greet(s string) (res string) if InterceptMock(Greet, s, res) return res return hello + s 这里有 n 多种方式可以做到，现在笔者决定使用最暴力的方式，直接临时创建一个包含这段代码的文件 tmp.go，并替换掉传给 compile 的参数，即将 -pack ./greet.go ./main.go ./mock.go 替换为 -pack tmp.go ./main.go ./mock.go 综上，cmd/mytool/mytool/go 实现的代码如下： func main() tool, args := os.Args[1], os.Args[2:]\tif len(args) 0 args[0] == -V=full // dont do anything to infuence the version full output. else if len(args) 0 if filepath.Base(tool) == compile index := findGreetFile(args) if index -1 f, err := os.Create(tmp.go) if err != nil log.Fatalf(create tmp.go error: %v , err) defer f.Close() defer os.Remove(tmp.go) _, _ = f.WriteString(newCode) args[index] = tmp.go fmt.Printf(tool: %s , tool) fmt.Printf(args: %v , args) // 继续执行之前的命令\tcmd := exec.Command(tool, args...)\tcmd.Stdout = os.Stdout\tcmd.Stderr = os.Stderr\tif err := cmd.Run(); err != nil log.Fatalf(run command error: %v , err)\tfunc findGreetFile(args []string) int for i, arg := range args if strings.Contains(arg, greet.go) return i return -1var newCode = `package mainfunc Greet(s string) (res string) if InterceptMock(Greet, s, res) return res return hello + s` 这里我先使用 findGreetFile 来查找 greet.go 文件所处的参数位置，如果找到了，则生成新的 tmp.go 文件，并替换参数，最后在 本次 compile 命令执行完毕后，删除 tmp.go，“毁尸灭迹”。 执行 ./script.sh 重新编译： ➜ 03-toolexec-static git:(master) ✗ ./script.sh# xgo-explore/03-toolexec-statictool: /opt/homebrew/Cellar/go/1.22.3/libexec/pkg/tool/darwin_arm64/compileargs: [-o $WORK/b001/_pkg_.a -trimpath $WORK/b001= -p main -lang=go1.22 -complete -buildid PcS9clqF_ny_Ds5N0i_s/PcS9clqF_ny_Ds5N0i_s -goversion go1.22.3 -c=4 -shared -nolocalimports -importcfg $WORK/b001/importcfg -pack tmp.go ./main.go ./mock.go]# xgo-explore/03-toolexec-statictool: /opt/homebrew/Cellar/go/1.22.3/libexec/pkg/tool/darwin_arm64/linkargs: [-o $WORK/b001/exe/a.out -importcfg $WORK/b001/importcfg.link -buildmode=pie -buildid=KgnnCoU_6enHkOm-T62Z/PcS9clqF_ny_Ds5N0i_s/H80dtgGZw1L8mTtVqJBf/KgnnCoU_6enHkOm-T62Z -extld=cc $WORK/b001/_pkg_.a] 输出的结果中可以看到已经将 compile 的参数替换为 -pack tmp.go ./main.go ./mock.go 了。 现在我们来执行生成的程序文件，可以看到是执行成功的。 ➜ 03-toolexec-static git:(master) ✗ ./main2024/05/23 17:53:52 run successfully 如果我们不使用 -toolexec，是执行不成功的： ➜ 03-toolexec-static git:(master) ✗ go clean -cache -modcache -i -r➜ 03-toolexec-static git:(master) ✗ go build -o main➜ 03-toolexec-static git:(master) ✗ ./main2024/05/23 17:54:33 Greet() = hello world; want mock world 第 4 步：使用 AST 在函数前插入代码 ➜ 04-toolexec-ast git:(master) ✗ tree.├── cmd│ └── mytool│ └── mytool.go├── greet.go├── main.go├── mock.go└── script.sh 暴力替换源代码文件的方式可能是不太优雅哈，假如我们的 greet.go 内容改成下面这样： package mainfunc Greet(s string) (res string) return hello + sfunc Greet2(s string) (res string) return hello 2 + s 如果我们想对 Greet2 也进行 代码重写，那就需要修改前面 newCode 字段的内容，而且它是写死的，确实不太优雅。现在我们正式来面对这件事，对比修改后的函数： func Greet(s string) (res string) if InterceptMock(Greet, s, res) return res return hello + s 其实就是在每个函数前加上这么一段： if InterceptMock(Greet, s, res) return res 了解过编译原理的读者应该可以想到，我们可以通过操作源代码的 AST 结构，往函数的开头插入这段代码即可。如果我们先不考虑参数和返回值的话，那这段代码我们需要替换的地方就是函数名称了，所以它的结构如下： if InterceptMock($funcName, s, res) return res 这里我们需要用到几个标准库工具： go/ast: 包定义了 Go 编程语言的抽象语法树（AST），核心有以下几种类型： File: 表示一个 Go 源文件。 Decl: 表示一个声明，包括函数声明、变量声明、类型声明等。 Stmt: 表示一个语句。 Expr: 表示一个表达式。 go/token: 定义了处理 Go 源代码的词法元素的基础设施，包括位置、标记和标识符等。这个包提供了用于管理源代码位置的信息，可以帮助定位代码中的特定部分。 go/parser: 将一个 .go 文件以解析成 AST 结构。 go/printer: 提供了将 AST 格式化并输出为 Go 源码的功能 修改后的 cmd/mytool/mytool.go 代码如下： func main() tool, args := os.Args[1], os.Args[2:]\tif len(args) 0 args[0] == -V=full // dont do anything to infuence the version full output. else if len(args) 0 if filepath.Base(tool) == compile index := findGreetFile(args) if index -1 filename := args[index] f, err := os.Create(tmp.go) defer f.Close() defer os.Remove(tmp.go) if err != nil log.Fatalf(create tmp.go error: %v , err) _, _ = f.WriteString(insertCode(filename)) args[index] = tmp.go fmt.Printf(tool: %s , tool) fmt.Printf(args: %v , args) // 继续执行之前的命令\tcmd := exec.Command(tool, args...)\tcmd.Stdout = os.Stdout\tcmd.Stderr = os.Stderr\tif err := cmd.Run(); err != nil log.Fatalf(run command error: %v , err)\tfunc findGreetFile(args []string) int for i, arg := range args if strings.Contains(arg, greet.go) return i return -1func insertCode(filename string) string fset := token.NewFileSet()\tfast, err := parser.ParseFile(fset, filename, nil, parser.AllErrors)\tif err != nil log.Fatalf(parse file error: %v , err) for _, decl := range fast.Decls fun, ok := decl.(*ast.FuncDecl) if !ok continue f, err := os.Create(tmp2.go) if err != nil log.Fatalf(create tmp2.go error: %v , err) _, _ = f.WriteString(fmt.Sprintf(newCodeFormat, fun.Name.Name)) f.Close() tmpFset := token.NewFileSet() tmpF, err := parser.ParseFile(tmpFset, tmp2.go, nil, parser.AllErrors) if err != nil log.Fatalf(parse tmp2.go error: %v , err) fun.Body.List = append(tmpF.Decls[0].(*ast.FuncDecl).Body.List, fun.Body.List...) os.Remove(tmp2.go) var buf bytes.Buffer\tprinter.Fprint(buf, fset, fast)\tfmt.Println(buf.String())\treturn buf.String()var newCodeFormat = `package mainfunc TmpFunc() if InterceptMock(%s, s, res) return res ` 核心的修改在于 insertCode 函数： 使用 parser.ParseFile 将源代码文件解析成 AST 结构； 遍历 AST 结构，找到所有的声明（Decl）结构，并使用 decl(.ast.FuncDecl) 找到所有的函数； FuncDecl struct Doc *CommentGroup // associated documentation; or nil Recv *FieldList // receiver (methods); or nil (functions) Name *Ident // function/method name Type *FuncType // function signature: type and value parameters, results, and position of func keyword Body *BlockStmt // function body; or nil for external (non-Go) functionBlockStmt struct Lbrace token.Pos // position of List []Stmt Rbrace token.Pos // position of , if any (may be absent due to syntax error) 查看 ast.FuncDecl 的结构后，可以得出下一步就是往 FuncDecl.Body.List 列表前面插入一些 Stmt； 笔者没找到类似 parseStmt 方法，所以取了个巧，我定义了一段代码的 format，里面的 %s 会使用 fun.Name.Name 获取函数名并进行替换。 var newCodeFormat = `package mainfunc TmpFunc() if InterceptMock(%s, s, res) return res ` 创建一个临时文件 tmp2.go 并写入格式化后的代码，然后再次调用 parser.ParseFile 得到解析这段代码的抽象语法树结构 tmpF 了； 然后通过 tmpF.Decls[0].(*ast.FuncDecl).Body.List 就可以得到 TmpFunc 中的语句 Stmt 了； 将其加在源代码函数的前面即可：fun.Body.List = append(tmpF.Decls[0].(*ast.FuncDecl).Body.List, fun.Body.List...)； 然后再使用 go/printer 将修改后的 AST 输出为新文件内容。 通过上述步骤，我们就可以为 greet.go 中的每个函数前面都插入打桩代码了。 修改 main.go 里面的内容，加入对 Greet2 的测试： func main() res := Greet(world)\tif res != hello world log.Fatalf(Greet() = %q; want %q, res, hello world) RegisterMockFunc(Greet, func(s string) string return mock + s\t)\tres = Greet(world)\tif res != mock world log.Fatalf(Greet() = %q; want %q, res, mock world) log.Println(run greet 1 successfully)\tRegisterMockFunc(Greet2, func(s string) string return mock 2 + s\t)\tres = Greet2(world)\tif res != mock 2 world log.Fatalf(Greet2() = %q; want %q, res, mock 2 world) log.Println(run greet 2 successfully) 执行脚本： ./script.sh 输出应该还是跟之前是一样的，我们运行生成的可执行函数，得到如下结果那就说明我们又成功进了一步了~ ➜ 04-toolexec-ast git:(master) ✗ ./main2024/05/23 20:03:22 run greet 1 successfully2024/05/23 20:03:22 run greet 2 successfully 第 5 步：使用 reflect 反射动态获取参数和返回值名称 ➜ 05-toolexec-general git:(master) ✗ tree.├── cmd│ └── mytool│ └── mytool.go├── greet.go├── main.go├── mock.go└── script.sh 接下来我们来处理函数签名中的参数和返回值部分，我们的样板代码中，写死了参数的名称和返回值的名称，现在我们需要来动态获取函数参数的名称和返回值的名称，如果返回值没有名称，那我们还需要手动设置名称。 我们将 greet.to 修改为以下内容： func Greet(s string) (res string) return hello + sfunc Greet2(s2 string) (res2 string) return hello 2 + s2func Greet3(s3 string) string return hello 3 + s3 函数的信息当然都在前面获得的 ast.FuncDecl 结构中，再次观察其结构： FuncDecl struct Doc *CommentGroup // associated documentation; or nil Recv *FieldList // receiver (methods); or nil (functions) Name *Ident // function/method name Type *FuncType // function signature: type and value parameters, results, and position of func keyword Body *BlockStmt // function body; or nil for external (non-Go) function 通过注释就可以知道 Type 字段就包含了参数和返回值的相关信息，查看 FuncType 结构，如下： FuncType struct Func token.Pos // position of func keyword (token.NoPos if there is no func) TypeParams *FieldList // type parameters; or nil Params *FieldList // (incoming) parameters; non-nil Results *FieldList // (outgoing) results; or nil Params：函数参数 Results：函数返回值 查看 FieldList 结构，可知参数列表和返回值列表都在相应的 List 字段中，而其中的 Names 字段就是参数的名称了。 type FieldList struct Opening token.Pos // position of opening parenthesis/brace/bracket, if any\tList []*Field // field list; or nil\tClosing token.Pos // position of closing parenthesis/brace/bracket, if anytype Field struct Doc *CommentGroup // associated documentation; or nil\tNames []*Ident // field/method/(type) parameter names; or nil\tType Expr // field/method/parameter type; or nil\tTag *BasicLit // field tag; or nil\tComment *CommentGroup // line comments; or nil 补充一下，这里为什么 Names 类型是 []*Ident 呢？因为函数有以下的命名方式： func hello(s1, s2 string) (r1, r1 string) 那么在当下，只有 1 个参数和只有 1 个返回值的情况下，我们就可以通过 fun.Type.Params.List[0].Names[0].Name 来获取参数名称，也可以通过 fun.Type.Results.List[0].Names 来获取返回值名称，如果返回值没有名称，那我们就为其设置名称 __xgo_res_1 并写回源 AST 结构。这样就都有名称，就很好处理了。 经上分析， cmd/mytool/mytool.go 中我们只需要修改 insertCode 部分，修改的结果如下： func insertCode(filename string) string fset := token.NewFileSet()\tfast, err := parser.ParseFile(fset, filename, nil, parser.AllErrors)\tif err != nil log.Fatalf(parse file error: %v , err) for _, decl := range fast.Decls fun, ok := decl.(*ast.FuncDecl) if !ok continue f, err := os.Create(tmp.go) if err != nil log.Fatalf(create tmp.go error: %v , err) _, _ = f.WriteString(newCode(fun)) f.Close() tmpFset := token.NewFileSet() tmpF, err := parser.ParseFile(tmpFset, tmp.go, nil, parser.AllErrors) if err != nil log.Fatalf(parse tmp.go error: %v , err) fun.Body.List = append(tmpF.Decls[0].(*ast.FuncDecl).Body.List, fun.Body.List...) os.Remove(tmp.go) var buf bytes.Buffer\tprinter.Fprint(buf, fset, fast)\tfmt.Println(buf.String())\treturn buf.String()func newCode(fun *ast.FuncDecl) string /* Doc:nil Names:[s] Type:string Tag:nil Comment:nil Doc:nil Names:[res] Type:string Tag:nil Comment:nil Doc:nil Names:[s2] Type:string Tag:nil Comment:nil Doc:nil Names:[res2] Type:string Tag:nil Comment:nil Doc:nil Names:[s3] Type:string Tag:nil Comment:nil Doc:nil Names:[] Type:string Tag:nil Comment:nil\t*/\t// 函数名称\tfuncName := fun.Name.Name\t// 参数列表\targName := fun.Type.Params.List[0].Names[0].Name\t// 返回值列表\tresNames := fun.Type.Results.List[0].Names\tif len(resNames) == 0 resNames = append(resNames, ast.IdentName: _xgo_res_1) fun.Type.Results.List[0].Names = resNames resName := resNames[0].Name\treturn fmt.Sprintf(newCodeFormat, funcName, argName, resName, resName)var newCodeFormat = `package mainfunc TmpFunc() if InterceptMock(%s, %s, %s) return %s ` 现在我们就可以动态获取参数名称和返回值名称了。 修改我们的 main.go，以测试所有的情况： func main() res := Greet(world)\tif res != hello world log.Fatalf(Greet() = %q; want %q, res, hello world) RegisterMockFunc(Greet, func(s string) string return mock + s\t)\tres = Greet(world)\tif res != mock world log.Fatalf(Greet() = %q; want %q, res, mock world) log.Println(run greet 1 successfully)\tRegisterMockFunc(Greet2, func(s string) string return mock 2 + s\t)\tres = Greet2(world)\tif res != mock 2 world log.Fatalf(Greet2() = %q; want %q, res, mock 2 world) log.Println(run greet 2 successfully)\tRegisterMockFunc(Greet3, func(s string) string return mock 3 + s\t)\tres = Greet3(world)\tif res != mock 3 world log.Fatalf(Greet3() = %q; want %q, res, mock 3 world) log.Println(run greet 3 successfully) 执行编译脚本： ./script.sh 执行编译产生的可执行程序，输出如下就说明我们又成功进了一大步~ ➜ 05-toolexec-general git:(master) ✗ ./main2024/05/23 20:15:08 run greet 1 successfully2024/05/23 20:15:08 run greet 2 successfully2024/05/23 20:15:08 run greet 3 successfully 第 6 步：支持多参数和多返回值 ➜ 06-toolexec-multi git:(master) ✗ tree.├── cmd│ └── mytool│ └── mytool.go├── greet.go├── main.go├── mock.go└── script.sh 本文的最后一步，我们来面对一下多参数和多返回值的问题。假设我们又如下函数： func Pair1(s1, s2 string) (res string) return pair 1 + s1 + + s2 这个时候我们 代码重写 后应该长什么样子呢？可以是下面这样的： func Pair1(s1, s2 string) (res string) if InterceptMock(Pair1, s1, s2, res) return res return pair 1 + s1 + + s2 按照这个思路，下面这个函数呢？ func Pair2(s1, s2 string) (res1, res2 string) return pair 1 + s1, pair 2 + s2 那就是这样的？ func Pair2(s1, s2 string) (res1, res2 string) if InterceptMock(Pair2, s1, s2, res1, res2) return res1, res2 return pair 1 + s1, pair 2 + s2 这种思路当然也能实现，换一种更优雅的思路呢？既然是一个列表，那么就可以用切片来承载，也就是可以是这样的： func Pair2(s1, s2 string) (res1, res2 string) if InterceptMock(Pair2, []interfaces1, s2, []interfaceres1, res2) return res1, res2 return pair 1 + s1, pair 2 + s2 那我们就可以抽象出插入代码的模板了： if InterceptMock($funcName, []interface$paramList, []interface$returnListWith) return $returnListWithout 为了实现这个，我们需要先修改一下 mock.go 中的 InterceptMock 函数： func InterceptMock(funcName string, args []interface, results []interface) bool mockFn, ok := mockFuncs.Load(funcName)\tif !ok return false in := make([]reflect.Value, len(args))\tfor i, arg := range args in[i] = reflect.ValueOf(arg) mockFnValue := reflect.ValueOf(mockFn)\tout := mockFnValue.Call(in)\tif len(out) != len(results) panic(mock function return value number is not equal to results number) for i, result := range results reflect.ValueOf(result).Elem().Set(out[i]) return true 拦截器的具体实现如下： 判断是否注册了 mock 函数，没有则直接返回； 将所有参数都放到 []refect.Value 中； 通过反射 refect.ValueOf 获取 mockFn 的值； 调用 mockFnValue.Call() 来执行函数，并返回结果列表； 遍历传进来的返回值引用列表，调用 reflect.ValueOf(result).Elem().Set(out[i]) 将返回值设置回去。 现在我们来修改我们的 -toolexec 工具，来根据函数的 AST 结构，获取参数列表和返回值列表，生成代插入的模板代码，并将其插入到每个函数的开头。这次在 cmd/mytool/mytool.go 中，我们只需修改 newCode 函数： func insertCode(filename string) string fset := token.NewFileSet()\tfast, err := parser.ParseFile(fset, filename, nil, parser.AllErrors)\tif err != nil log.Fatalf(parse file error: %v , err) for _, decl := range fast.Decls fun, ok := decl.(*ast.FuncDecl) if !ok continue f, err := os.Create(tmp.go) if err != nil log.Fatalf(create tmp.go error: %v , err) _, _ = f.WriteString(newCode(fun)) f.Close() tmpFset := token.NewFileSet() tmpF, err := parser.ParseFile(tmpFset, tmp.go, nil, parser.AllErrors) if err != nil log.Fatalf(parse tmp.go error: %v , err) fun.Body.List = append(tmpF.Decls[0].(*ast.FuncDecl).Body.List, fun.Body.List...) os.Remove(tmp.go) var buf bytes.Buffer\tprinter.Fprint(buf, fset, fast)\tfmt.Println(buf.String())\treturn buf.String()func newCode(fun *ast.FuncDecl) string // 函数名称\tfuncName := fun.Name.Name\t// 参数列表\targs := make([]string, 0)\tfor _, arg := range fun.Type.Params.List for _, name := range arg.Names args = append(args, name.Name) // 返回值列表\treturns := make([]string, 0)\treturnRefs := make([]string, 0)\treturnNames := fun.Type.Results.List[0].Names\tif len(returnNames) == 0 for i := 0; i fun.Type.Results.NumFields(); i++ fun.Type.Results.List[0].Names = append(fun.Type.Results.List[0].Names, ast.IdentName: fmt.Sprintf(_xgo_res_%d, i+1)) for _, re := range fun.Type.Results.List[0].Names returns = append(returns, re.Name) returnRefs = append(returnRefs, +re.Name) return fmt.Sprintf(newCodeFormat, funcName, strings.Join(args, ,), strings.Join(returnRefs, ,), strings.Join(returns, ,))var newCodeFormat = `package mainfunc TmpFunc() if InterceptMock(%s, []interface%s, []interface%s) return %s\t` 思路跟之前第 5 步大同小异，不过是用遍历的方式来支持多个参数和多个返回值罢了。 现在我们为 greet.go 添加更多的测试函数，代码如下： func Greet(s string) (res string) return hello + sfunc Greet2(s2 string) (res2 string) return hello 2 + s2func Greet3(s3 string) string return hello 3 + s3func Pair1(s1, s2 string) (res string) return pair 1 + s1 + + s2func Pair2(s1, s2 string) (res1, res2 string) return pair 1 + s1, pair 2 + s2func Other(i int, s string, f float64) string return fmt.Sprintf(int: %d, string: %s, float: %f, i, s, f) 为了测试，我们再次修改 main.go，使其覆盖所有的情况： func main() RegisterMockFunc(Other, func(i int, s string, f float64) string return fmt.Sprintf(mock %d %s %.2f, i, s, f)\t)\tres := Other(1, hello, 3.14)\tif res != mock 1 hello 3.14 log.Fatalf(Other() = %q; want %q, res, mock 1 hello 3.14) log.Println(run other successfully)\tRegisterMockFunc(Pair1, func(s1, s2 string) string return mock 1 + s1 + + s2\t)\tres = Pair1(hello, world)\tif res != mock 1 hello world log.Fatalf(Pair1() = %q; want %q, res, mock 1 hello world) log.Println(run pair1 successfully)\tRegisterMockFunc(Pair2, func(s1, s2 string) (string, string) return mock 2 + s1, mock 2 + s2\t)\tres1, res2 := Pair2(hello, world)\tif res1 != mock 2 hello || res2 != mock 2 world log.Fatalf(Pair2() = %q, %q; want %q, %q, res1, res2, mock 2 hello, mock 2 world) log.Println(run pair2 successfully)\tres = Greet(world)\tif res != hello world log.Fatalf(Greet() = %q; want %q, res, hello world) RegisterMockFunc(Greet, func(s string) string return mock + s\t)\tres = Greet(world)\tif res != mock world log.Fatalf(Greet() = %q; want %q, res, mock world) log.Println(run greet 1 successfully)\tRegisterMockFunc(Greet2, func(s string) string return mock 2 + s\t)\tres = Greet2(world)\tif res != mock 2 world log.Fatalf(Greet2() = %q; want %q, res, mock 2 world) log.Println(run greet 2 successfully)\tRegisterMockFunc(Greet3, func(s string) string return mock 3 + s\t)\tres = Greet3(world)\tif res != mock 3 world log.Fatalf(Greet3() = %q; want %q, res, mock 3 world) log.Println(run greet 3 successfully) 编译代码： ./script.sh 执行生成的可执行程序，如果有以下输出，那我们就又成功进了一大大步了~ ➜ 06-toolexec-multi git:(master) ✗ ./main2024/05/23 20:31:10 run other successfully2024/05/23 20:31:10 run pair1 successfully2024/05/23 20:31:10 run pair2 successfully2024/05/23 20:31:10 run greet 1 successfully2024/05/23 20:31:10 run greet 2 successfully2024/05/23 20:31:10 run greet 3 successfully 更进一步 通过上面 6 个简单的小阶段，我们就已经把 xgo 最最核心的功能给实现了，在一些小场景下还勉强能用？🤡 我们来看看包含测试代码和样例函数，总共用了多少代码： ➜ 06-toolexec-multi git:(master) ✗ tokei .=============================================================================== Language Files Lines Code Comments Blanks=============================================================================== Go 4 281 224 11 46 Shell 1 5 3 1 1=============================================================================== Total 5 286 227 12 47=============================================================================== 短短 224 行代码，这是一个非常了不起的成就！ 当然，优秀的读者肯定可以发现我们这个 丐版 xgo 有太多的不足和缺陷了。这是必然的，我们来看看 xgo 截止 1.0.37 版本，总共有多少行代码： ➜ xgo git:(master) tokei .=============================================================================== Language Files Lines Code Comments Blanks=============================================================================== BASH 1 104 81 11 12 CSS 1 153 118 5 30 Go 369 33232 26836 2588 3808 JavaScript 1 170 146 10 14 JSON 2 435 435 0 0 PowerShell 1 28 16 3 9 Shell 3 288 251 4 33 SVG 1 41 41 0 0 Plain Text 7 192 0 174 18------------------------------------------------------------------------------- HTML 1 19 16 3 0 |- JavaScript 1 6 6 0 0 (Total) 25 22 3 0------------------------------------------------------------------------------- Markdown 17 1455 0 1083 372 |- Go 8 820 635 72 113 |- JSON 1 80 80 0 0 (Total) 2355 715 1155 485=============================================================================== Total 404 36117 27940 3881 4296=============================================================================== 光 Go 代码就有 26836 行了。所以可知 xgo 的作者是做了很多的付出和努力的。不过我们用了不到百分之一的代码量，就将 xgo 最核心的原理展示得淋漓尽致了，感兴趣的读者可以进一步阅读 xgo 的源码，可以进一步探索如何抽象出更通用更简洁更易扩展的 interceptor，如何支持协程隔离，如何优化依赖管理，以及如何实现其他的 trace、coverage 功能。再次为 xgo 打 call 👏！ 参考 xgo repo xgo: 基于代码重写实现 Monkey Patch 和 Trace go compile README xgo: 在 go 中使用-toolexec 实现猴子补丁","tags":["go","单元测试","开源项目"],"categories":["go","开源项目"]},{"title":"Kafka 负载均衡挑战及解决思路","path":"/2024/05/20/kafka-load-balance/","content":"本文转载自 Agoda Engineering，介绍了在实际应用中，如何应对 Kafka 负载均衡所遇到的各种挑战，并提出相应的解决思路。本文简要阐述了 Kafka 的并行性机制、常用的分区策略以及在实际操作中遇到的异构硬件、不均匀工作负载等问题。通过深入分析这些挑战，并提供具体的解决方案，本文旨在帮助读者更好地理解和应用 Kafka 的负载均衡技术，从而提高系统的整体性能和稳定性。 以下大部分内容翻译自原文 how-we-solve-load-balancing-challenges-in-apache-kafka，并已获得原作者同意。 思维导图 Kafka 负载均衡解决方案 Kafka 并行性 Kafka 通过分区来实现并行性，如下图所示，生产者（Producer）产生的消息会按照一定的分区策略分配到多个分区（Partition）中，消费组中的每个消费者会分别负责消费其中的若干个分区。 Kafka 分区演示 分区策略： 轮询（Round Robin）：默认情况下，Kafka 使用轮询策略将消息均匀地分配到所有分区。 哈希（Key Hashing）：如果消息有分区键，Kafka 会对键进行哈希计算，将消息分配到特定的分区。 自定义分区策略：开发者可以实现自定义的分区器（Partitioner）逻辑，以满足特定需求。 如果要使用轮询或者哈希策略来达到“负载均衡”的目的，那么需要满足以下 2 个假设： 消费者拥有相同的处理能力， 消息的工作量相等。 然而，在实践中，这些假设往往不成立。 现实挑战 1. 异构硬件 不同代的服务器硬件性能不同，导致处理速率存在差异。例如，使用不同代硬件进行处理的基准显示性能存在显着差异： 不同服务器处理速率差异举例 2. 每条 Kafka 消息的工作负载不均匀 下图显示了在一个时间窗口内到达的 12 条消息。在这里，生产者向该主题中的六个分区中的每一个发布两条消息。因此，每个 worker 消耗来自 2 个分区的数据，这意味着每个 worker 需要处理 4 条消息。 使用循环分区器和循环分配器来分发消息的先前供应系统的演示。每个 worker 都分配有相同数量的消息。 不同的消息可能需要不同的处理步骤集。例如，处理消息可能涉及调用第三方 HTTP 端点，并且不同的响应大小或延迟可能会影响处理速率。此外，对于涉及数据库操作的应用程序，其数据库查询的延迟可能会根据查询参数而波动，从而导致处理速率发生变化。 3. 过度配置问题 由于工作负载和处理效率不同，为了达到系统吞吐量的需求，可能会出现过度配置问题，从而导致资源浪费。 假设我们的高吞吐量和低吞吐量的处理速率分别为 20 msg/s 和 10 msg/s（根据表 1 中的数据进行简化）。使用两个较快的处理器和一个较慢的处理器，我们预计总容量为 20+20+10 = 50 条消息/秒。但是，当保持消息的循环分配时，我们无法达到此容量。下图显示了如果流量持续达到每秒 50 条消息时会发生什么情况。 如果传入流量保持在 50 条消息/秒，则慢速处理器无法处理总体消息 1/3 的负载，从而导致累积延迟。为了避免高延迟，向该系统添加了额外的资源以维持处理。 从这个例子中我们可以看到，我们的处理器服务一次最多只能接受 30 条消息，以防止滞后并确保及时传递更新。 在这种情况下，要实际每秒处理 50 条消息，我们必须总共扩展到 5 台机器，以保证及时处理所有消息。由于这种不适当的分配逻辑（66.7％的过度配置），我们会向该系统过度配置额外的两台机器。 为了每秒处理 50 条消息，我们需要扩展到五台机器以确保及时处理所有消息。由于这种不适当的分配逻辑（66.7% 的过度配置），这会导致向该系统过度配置两台额外的机器。 静态解决方案 1. 在相同的 Pod（机器）上部署 考虑控制服务部署中使用的硬件类型以缓解问题。如果您在虚拟机上部署服务并拥有充足的资源和性能相同的硬件，则此方法是可行的。 然而，由于成本效益和灵活性下降，在私有云环境中通常不建议采用这种策略，主要是因为同时升级所有现有硬件可能具有挑战性。如果它非常适合您的情况，则可以使用Kubernetes 关联性将 Pod 分配给某些类型的节点。 2. 加权负载均衡 如果容量是可预测的并且大部分时间保持静态，则为不同的消费者分配不同的权重可以帮助最大限度地利用可用资源。例如，在为表现较好的消费者赋予更高的权重后，我们可以将更多流量路由给这些消费者。 动态解决方案 虽然我们可以估计消息的容量和工作负载来设计静态规则来确定加权负载平衡策略，但由于以下几个因素，这种方法在实际生产环境中可能并不总是可行： 消息的工作负载并不统一，这使得估计机器容量变得困难。 依赖关系（例如网络和第三方连接）不稳定，有时会导致实际处理中的容量发生变化。 该系统经常添加新功能，增加额外的维护工作以保持权重更新。 为了解决这些问题，我们可以动态监控每个分区中的当前滞后并根据当前流量状况做出相应响应。 有 2 种思路： 生产者角度：使用自定义算法根据滞后的消息数量来确定每个分区的流量，这种生产者称为滞后感知生产者（Lag-aware Producer）。 消费者角度：这些消费者旨在监控当前滞后的消息数量，并可以在必要时取消订阅以触发负载重新平衡。通常，可以采用自定义的重新平衡策略来调整分区分配。这种消费者称为滞后感知消费者（Lag-aware Comsumer）。 1. 从生产者角度出发 如此图所示，生产者可以使用自定义算法根据滞后确定每个分区的流量。为了减少对 Kafka 代理的调用次数，系统可以维护一个内部延迟缓存，而不是在发布每条消息之前调用 Kafka 代理。 在此示例中，分区 4 和 6 的延迟比其他分区高得多。应减少从内部生产者发送到这些分区的流量。 使用滞后数据，定制的算法被设计为向经历高滞后的分区发布更少的流量，向低滞后的分区发布更多流量，以平衡每个分区上的工作负载。当滞后平衡且稳定时，此方法应确保消息的均匀分布。 不适用情况： 纯消费者应用程序：您的应用程序不控制消息生成。 多个消费者组：当生成的消息被多个消费者组消费时，生产者可能会为其他消费者组产生不必要的倾斜负载，因为滞后只是特定于一个消费者组的信息。 相同队列长度算法 该算法将每个分区滞后视为处理的队列大小。获取滞后信息后，它会发布适当数量的消息以填充短队列。此方法更适合由于异构硬件而导致的倾斜滞后分布，其中高性能 Pod（机器）在大多数情况下能够更快地处理。 相同队列长度算法的演示。最初，不同队列的长度不同。该算法尝试生成不同数量的消息，以在所有队列中实现相同的队列长度。这里，队列长度和 Kafka lag 是同一个概念，代表尚未处理的消息数量 异常值检测算法 该算法利用统计方法来确定所有分区的上离群值，并暂时停止那些慢速离群值的发布过程。在原文章中，针对 Agoda 的特定需求，他们提出了 IQR（四分位距）和 STD（标准差）异常值检测算法。算法流程图如下所示。 异常值检查算法流程 慢速分区：（已关闭）由于存在延迟，这些分区的消息生成已停止。 好的分区：（打开）照常发布并均匀分发到所有好的分区。 OK 分区：（观察/半开放）为了提高性能不佳的机器的性能，当系统尝试将慢速分区提升为良好分区时，会添加一个观察期。通过仅生成一小部分消息并进行观察，可以将该观察阶段优化为“半开放”状态。当滞后获取间隔相对较长时，半开放是有益的，因为它可以防止消费者延迟等待传入消息而更新的滞后数据尚未查询的情况。 2. 从消费者角度出发 这里 Adoga 提出的思路是：遇到高延迟的实例可以主动取消订阅主题以触发重新平衡。在重新平衡期间，可以使用自定义的分配器来平衡所有消费者实例之间的分区。 触发重新平衡的成本非常昂贵，因为急切的重新平衡会停止消费者组中的所有处理。Kafka 2.4 中引入的增量协作再平衡协议已经最大限度地减少了性能影响，允许更频繁的再平衡以更好地分配每个分区上的负载。 为了增强重新分配的灵活性，分区的数量应该大于 worker 的数量。这一比率应根据应用程序而有所不同，并假设一个工作线程至少可以处理来自一个分区的负载以避免饥饿。 在此示例中，工作程序 3 在速度较慢的硬件上运行，导致分区 5 和 6 出现更高的延迟。因此，工作程序 3 可能会主动取消订阅主题以触发重新平衡并更有效地重新分配分区。在此示例中，应实现自定义分配器以根据机器指标和滞后信息重新分配分区。 总结 本文从 Kafka 并行性的一般实现出发，探讨了 Kafka 实现负载均衡在现实实践中可能遇到的各种挑战，并从静态调整和动态调整两个方面给出了解决思路，特别注重讨论了动态调整策略，并分别从生产者和消费者的角度提出了解决方案。 总之，通过在 Kafka 中实现负载均衡，可以有效地将工作负载分配到可用资源之间，从而显著提高服务性能。具体的算法和策略需要根据实际情况进行选择和调整。","tags":["kafka","中间件","消息队列"],"categories":["kafka"]},{"title":"学习记录：用 Go 自制解释器 Monkey","path":"/2024/05/12/monkey-language/","content":"词法分析 TDD：测试驱动开发 先写测试用例，再进行词法分析逻辑的完善。 语法分析 递归下降语法分析伪代码 function parseProgram() program = newProgramASTNode() advanceTokens() for (currentToken() != EOF_TOKEN) statement = null if (currentToken() == LET_TOKEN) statement = parseLetStatement() else if (currentToken() == RETURN_TOKEN) statement = parseReturnStatement() else if (currentToken() == IF_TOKEN) statement = parseIfStatement() if (statement != null) program.Statements.push(statement) advanceTokens() return programfunction parseLetStatement() advanceTokens() identifier = parseIdentifier() advanceTokens() if currentToken() != EQUAL_TOKEN parseError(no equal sign!) return null advanceTokens() value = parseExpression() variableStatement = newVariableStatementASTNode() variableStatement.identifier = identifier variableStatement.value = value return variableStatementfunction parseIdentifier() identifier = newIdentifierASTNode() identifier.token = currentToken() return identifierfunction parseExpression() if (currentToken() == INTEGER_TOKEN) if (nextToken() == PLUS_TOKEN) return parseOperatorExpression() else if (nextToken() == SEMICOLON_TOKEN) return parseIntegerLiteral() else if (currentToken() == LEFT_PAREN) return parseGroupedExpression() // [...]function parseOperatorExpression() operatorExpression = newOperatorExpression() operatorExpression.left = parseIntegerLiteral() operatorExpression.operator = currentToken() operatorExpression.right = parseExpression() return operatorExpression() 递归下降分析法 let x=5 let stmt AST structure return 5 return stmt AST structue 普拉特解析","tags":["go","编译原理"],"categories":["go","go 实战"]},{"title":"时间处理基础：Rust 的 chrono 库教程","path":"/2024/05/11/rust-crate-chrono/","content":"在开发过程中，我们经常有对时间和日期处理的需求。不论是日历应用、日程安排、还是时间戳记录，准确的时间数据处理都是必不可少的。Rust 社区提供的 chrono 库以其强大的功能和灵活的接口，在 Rust 开发者中广受欢迎。本文将简单介绍 chrono 库，展示如何利用它来精确处理和转换时间和日期，帮助你在任何 Rust 项目中都能高效地管理时间。 版本 chrono: 0.4.38 结论先行 chrono 各种时间类型转换图 时间相关概念 概念 理解 UNIX 时间戳（UNIX Timestamp） 也称为 POSIX 时间或 Epoch 时间，是自 1970 年 1 月 1 日（UTC 时区）以来经过的秒数，不计入闰秒。这是一种非常通用的时间表示方法，在编程中广泛使用，因为它可以简化时间差的计算。 UTC（协调世界时） 全称为协调世界时（Coordinated Universal Time），是目前国际上广泛采用的时间标准。它基本上与格林威治平均时（GMT）相同，但在技术上更加精确，因为它使用原子钟来保持时间准确。世界各地的时间都是以 UTC 为基础，加上或减去一定的小时数来定义的。 时区（Time Zone） 时区是地球上划分的标准时间区域。由于地球自西向东旋转，每向东移动一定角度，当地的太阳时间就会相应地提前。世界被分成了 24 个时区，每个时区通常相差一小时。时区允许地区内的人们能在大致相同的时间内，经历类似的日夜更替模式。 UTC+8 UTC+8 是 UTC 时间加上 8 小时的时间区。中国大陆就是位于这个时区。例如，当 UTC 时间为 00:00 时，UTC+8 的时间就是 08:00。 chrono 关键类型 类型 含义 适用场景 DateTimeTz 一个带有时区的日期和时间类型，其中 Tz 是实现了 TimeZone 特质的类型，如 Utc 和 Local 。这意味着 DateTime 考虑了时区的影响，可以表示全球任意地点的精确时间。 广泛用于需要考虑时区转换的场景，如存储用户的本地时间或在不同地区之间转换时间。 NaiveDateTime 一个“天真的”日期和时间，即不包含任何时区信息的日期和时间。这种类型仅仅表示一个日历日期和一天中的时间，而没有任何关于地理或政治时区的数据。 对于一些时区不重要的场景非常有用，比如记录电影的发行日期或历史事件的日期。 NaiveDate 仅表示一个日历日期，不包括时间或时区信息。 它用于处理只需要日期而不关心具体时间的场景，如生日、节日等。 NaiveTime 是一个只表示一天中时间的类型，它不包含日期或时区信息。 这个类型适用于需要处理具体某个时间点（如开会时间、日常活动的开始时间）但不需要日期数据的情景。 chrono 时区类型 chrono 支持多种时区类型，方便进行全球时间的转换和计算： Utc: 用于处理协调世界时。 Local: 代表服务器或用户的本地时区。 FixedOffset: 允许定义任意的小时和分钟偏移量，适合固定偏移的时间计算。 常用功能 获取当前时间 let local_datetime: DateTimeLocal = Local::now();let utc_datetime: DateTimeUtc = Utc::now(); DateTime 转 String println!(, local_datetime.to_rfc2822()); // Sun, 12 May 2024 00:15:55 +0800println!(, local_datetime.to_rfc3339()); // 2024-05-12T00:15:55.325058+08:00println!(, local_datetime.to_string()); // 2024-05-12 00:15:55.325058 +08:00println!(, local_datetime.format(%Y-%m-%d %H:%M:%S)) // 2024-05-12 00:15:55 String 转 DateTime 字符串带时区信息，使用 DateTime::parse_from_str(s, f)。 let format_withzone = %Y-%m-%d %H:%M:%S %z;let datetime_withzone_str = 2024-01-01 00:00:00 +08:00;let local_datetime = DateTime::parse_from_str(datetime_withzone_str, format_withzone).unwrap(); 字符串无时区信息，使用 NaiveDateTime::parse_from_str(s, f)。 let format = %Y-%m-%d %H:%M:%S;let datetime_str = 2024-01-01 00:00:00;let local_datetime = NaiveDateTime::parse_from_str(datetime_str, format) .unwrap() .and_local_timezone(Local) // 转为带时区的 DateTime .unwrap(); DateTime 转 timestamp let local_datetime = Local::now();println!(seconds: , local_datetime.timestamp()); // 1715444324println!(millis: , local_datetime.timestamp_millis()); // 1715444338610println!(micros: , local_datetime.timestamp_micros()); // 1715444338610873println!(nacos: , local_datetime.timestamp_nanos_opt().unwrap()); // 1715444338610873000 timestamp 转 DateTime let utc_datetime: DateTimeUtc = DateTime::from_timestamp(1704139200, 0).unwrap(); // 默认是 Utclet local_datetime: DateTimeLocal = DateTime::from_timestamp(1704139200, 0).unwrap().into(); // 使用 into() 转为 Local 时区转换 use chrono::DateTime, FixedOffset, Utc;fn main() let utc_date_time: DateTimeUtc = Utc::now(); let fixed_offset = FixedOffset::east(8 * 3600); // 转为 utc+8 东八区 let local_date_time = utc_date_time.with_timezone(fixed_offset); println!(Local time in UTC+8: , local_date_time); 时间计算 时间加减： use chrono::Duration, Local;let now = Local::now();let yesterday = now - Duration::hours(24); chrono time duration methods 时间间隔： use chrono::Duration, Local;let now = Local::now();let yesterday = now - Duration::hours(24);let hour_interval = (now - yesterday).num_hours(); chrono time interval methods 总结 通过本文的详细介绍和实用示例，我们了解了如何使用 Rust 的 chrono 库来精确处理时间和日期。chrono 不仅支持复杂的时区计算和全球时间管理，还提供了方便的日期时间解析和格式化工具，以及灵活的时间运算功能。掌握了这些技能后，你将能够在任何需要精确时间数据处理的 Rust 应用中，提供稳定和高效的解决方案。 时间是每个程序的基石，而 chrono 就是那把能够操纵时间的魔杖。 希望本文能对你有帮助，peace! enjoy coding~ 参考： chrono crate rust-working-with-date-and-time 作图： https://excalidraw.com/","tags":["rust"],"categories":["rust","rust 常用库"]},{"title":"epoll","path":"/2024/04/28/epoll/","content":"前言 epoll 是一种 I/O 多路复用技术，主要用于高性能的网络服务器中，特别是在处理大量并发连接时。它是 Linux 特有的，自 Linux 内核 2.5.44 版本引入，并在后续版本中不断优化。epoll 能够帮助服务器高效地管理数以千计的客户端连接，是 select 和 poll 方法的现代替代品。 本文不对 epoll 的源码进行分析，仅做原理上的总结，方便快速查阅回顾。各大论坛很多大佬都对 epoll 的源码进行了详尽的分析，感兴趣的读者可以看「参考」篇章。 主要特点 效率高: 相较于 select 和 poll，epoll 可以更高效地处理大量的并发连接。select 和 poll 的效率随着监视的文件描述符数量增加而线性下降，而 epoll 则不会因为监视的文件描述符数量增加而显著降低效率。 扩展性好: epoll 使用一种称为事件通知的机制，只会处理那些真正发生了事件的文件描述符。这意味着系统不必重新检查所有文件描述符，从而大大减少了不必要的 CPU 开销。 支持边缘触发和水平触发: epoll 支持 Edge Triggered 和水平触发 Level Triggered 两种模式。边缘触发模式只在文件描述符状态改变时才通知应用程序，适用于非阻塞 I/O；而水平触发模式则在有事件可读或可写时都会通知应用程序，更容易使用但效率略低。 结论先行 epoll flow chart 工作原理 epoll 的工作可以分为三个主要步骤： 创建 epoll 实例: 使用 epoll_create 函数创建一个 epoll 实例。 添加/修改/删除文件描述符: 使用 epoll_ctl 函数将新的文件描述符添加到 epoll 实例中，或者修改、删除已存在的文件描述符。这些操作与文件描述符的数量无关，因此执行速度非常快。 等待事件发生: 使用 epoll_wait 函数等待事件的发生。这个函数可以同时监控多个文件描述符，当指定的文件描述符上发生了注册的事件时，函数返回，并告知哪些文件描述符上发生了事件。 ET LT 在 epoll 中，边缘触发（ET, Edge Triggered）和水平触发（LT, Level Triggered）是两种不同的事件通知方式，它们定义了操作系统如何通知应用程序文件描述符上的 I/O 事件。 这两种模式的主要区别在于何时以及如何多次通知应用程序关于某个文件描述符的事件。 水平触发（Level Triggered） 定义: 在水平触发模式下，只要文件描述符上有未处理的 I/O 事件存在，epoll_wait 就会通知应用程序。这意味着，如果数据可读取但未被完全读取，epoll_wait 会在下次调用时再次返回该文件描述符。 行为: 这种模式更容易编程，因为应用程序可以不用担心在一个操作中处理所有数据。如果数据还在，epoll_wait 会继续通知你。 适用场景: 更适合那些简单的应用或者对实时性要求不是非常高的应用，因为它简化了处理逻辑。 边缘触发（Edge Triggered） 定义: 在边缘触发模式下，只有状态变化时（例如从无数据到有数据），epoll_wait 才会通知应用程序。一旦通知了应用程序某事件发生，除非有新的数据到达或状态再次发生变化，否则不会再次通知应用程序该事件。 行为: 这要求应用程序必须立即处理所有事件，因为之后不会再收到关于这些事件的通知。这意味着应用程序必须循环读取或写入，直到数据被完全处理完，以确保不遗漏任何事件。 适用场景: 适合需要高性能的场景，因为它减少了事件处理的次数，但要求程序必须更加小心地管理 I/O 操作。 比较和选择 性能: 边缘触发通常提供更高的性能，因为它减少了系统调用的次数和不必要的事件处理。 编程复杂性: 边缘触发模式编程比水平触发复杂，因为需要确保每次事件被彻底处理，并且更容易遇到如“惊群效应”（多个进程或线程被同一个事件唤醒）等问题。 可靠性: 水平触发因为其简单的行为模式，在可靠性处理上更为直接和容易。 通常，选择哪种模式取决于应用的具体需求、预期的负载以及开发者对事件处理逻辑的控制程度。高性能服务器通常选择边缘触发模式，以最大化其效率，而简单的或者低负载应用可能会更倾向于使用水平触发，以简化开发和调试过程。 数据结构 epoll 使用 2 种关键的数据结构来维护和跟踪文件描述符（FD）和事件： 红黑树（Red-Black Tree）: 用于存储所有注册的文件描述符及其事件。红黑树是一种自平衡二叉搜索树，能够在对数时间内完成插入、删除和查找操作，这使得管理大量文件描述符变得高效。 就绪列表（Ready List）: 当事件发生（如可读、可写等）并被内核检测到时，相应的 FD 会被添加到一个就绪列表中。这个列表仅包含实际有事件发生的文件描述符，从而减少了 epoll_wait 调用的处理时间。 工作细节 epoll data structure 通过调用 epoll_create() 函数创建并初始化一个 eventpoll 对象。 通过调用 epoll_ctl() 函数把被监听的文件句柄 (如 socket 句柄) 封装成 epitem 对象并且添加到 eventpoll 对象的红黑树中进行管理。 通过调用 epoll_wait() 函数等待被监听的文件状态发生改变。 当被监听的文件状态发生改变时（如 socket 接收到数据），会把文件句柄对应 epitem 对象添加到 eventpoll 对象的就绪队列 rdllist 中。并且把就绪队列的文件列表复制到 epoll_wait() 函数的 events 参数中。 唤醒调用 epoll_wait() 函数被阻塞（睡眠）的进程。 事件监听 内核中的事件监听和回调机制是通过高效的事件驱动模型实现的，而不是简单的循环检查（如在用户空间中的轮询）。这种机制利用了现代操作系统的中断和回调系统，以及针对异步事件的优化处理策略。 以下是这个过程的详细解释： 1. 中断和中断处理 在硬件层面，大多数 I/O 操作（如网络通信、磁盘 I/O）都是通过中断驱动的。当一个 I/O 设备准备好数据或需要服务时，它会产生一个中断信号，这个信号被发送到 CPU。CPU 响应中断，并执行一个预定的中断处理程序（Interrupt Service Routine, ISR），该程序是由设备的驱动程序提供的。 2. 事件和回调 在 ISR 中，与设备相关的事件（例如网络包的接收、硬盘读取完成）会被检测到，并且可以在此阶段调用特定的回调函数。这些回调函数是在设备驱动或相关的内核模块中定义的，用来通知内核其他部分或者相关的进程有关事件的发生。 3. 文件描述符的回调机制 对于 epoll 等 I/O 多路复用技术，内核为每个文件描述符维护了一个事件处理机制。当文件描述符被创建时，相关的设备或资源会注册一组回调函数，这些函数会在特定的操作（如读、写、错误）上被触发。例如，一个网络套接字可能会在数据到达时触发一个“可读”事件的回调。 4. epoll 的事件绑定 当一个文件描述符被加入到 epoll 监听队列中，epoll 会利用这些回调来获得事件通知。epoll 操作相关的代码会将一个额外的回调函数绑定到这些文件描述符上。当文件描述符的状态改变时（如数据可读），这个回调函数将被触发，然后它会将相应的文件描述符标记为“就绪”，并放入 epoll 的就绪队列。 5. 事件通知和唤醒 当 epoll_wait 被调用且有事件就绪时，内核会检查就绪队列，并将这些事件传递给等待的进程。如果没有事件就绪，进程将被挂起直到有事件发生。事件的发生会触发内核调度程序唤醒相应的进程。 6. 效率和性能 这种基于中断的事件通知机制意味着内核不需要不断循环检查每个文件描述符的状态，从而极大地提高了效率。事件只有在实际发生时才被处理，且处理通常是由硬件中断直接触发的，这使得整个系统更加响应快速，减少了无效的 CPU 使用。 这种设计使得 Linux 内核在处理大量并发 I/O 操作时能够保持高效和稳定，适合构建高性能的网络服务和应用。 中断 中断机制是计算机硬件和操作系统核心功能之一，它允许外设或硬件异步地通知 CPU 需要处理某些事件。中断机制的实现并不依赖于类似于 for 循环的轮询检查，而是建立在更为直接和高效的硬件和处理器架构支持之上。 当 CPU 接收到中断信号时，它是通过一套内建于硬件的协调机制来识别和响应中断的。这个过程涉及硬件电路设计、处理器架构和操作系统的中断管理功能。 以下是 CPU 如何知道有中断发生，并且如何处理这一中断的详细步骤： 中断信号的检测和响应 中断请求线（IRQ）：外部设备通过连接到处理器的一个特定的硬件线路（IRQ）发送中断信号。这个线路直接与处理器内的中断控制单元（Interrupt Controller）相连。 中断控制器：大多数现代计算机系统使用一个或多个中断控制器来管理中断信号。中断控制器的任务是接收来自各种外部设备的中断请求，并将这些请求优先级排序后发送给 CPU。 中断向量：当中断控制器接收到一个中断信号后，它会根据中断源确定一个中断向量。这个向量是一个数字，指向中断向量表中对应的入口，该入口包含了处理该中断的中断服务例程（ISR）的地址。 CPU 如何处理中断 当前指令的完成：当 CPU 接收到中断控制器发出的中断信号时，它首先会完成当前执行的指令。这是为了保证程序的状态能够正确保存，从而在中断处理完毕后可以无缝地恢复执行。 保存上下文：一旦当前指令执行完毕，CPU 会自动保存当前的程序状态，包括程序计数器（PC）、寄存器和其他必要的状态信息。这些信息通常被推送到当前的栈上。 跳转到 ISR：CPU 使用中断向量来访问中断向量表，找到与中断号对应的中断服务例程（ISR）的地址，并跳转到该地址开始执行 ISR。这个过程是自动的，由处理器的内部机制控制。 执行 ISR：中断服务例程会执行必要的操作来处理中断，比如读取数据缓冲区、清除设备状态或发送信号等。 恢复上下文并返回：一旦 ISR 执行完成，处理器会从栈上恢复之前保存的程序状态，并将控制权返回到被中断的程序，继续执行。 硬件支持 这一过程大量依赖于处理器的硬件支持，如中断向量表通常是固定在处理器的特定内存地址上的。此外，现代处理器如 x86 架构还提供了更高级的功能，比如支持多重中断控制器和高级可编程中断控制器（APIC）等。 这种基于硬件的中断响应机制允许 CPU 快速有效地处理各种外部事件，确保系统的响应性和稳定性。 参考 图解 | 深入揭秘 epoll 是如何实现 IO 多路复用的！ 一图总结 epoll 的总体工作流程 scalable-io-events-vs-multithreading-based Epoll 实现原理 网络编程之 epoll 源码深度剖析","tags":["epoll","网络编程","非阻塞 i/o"],"categories":["计算机基础","计算机网络"]},{"title":"Rust 实战丨并发构建倒排索引","path":"/2024/04/23/rust-action-inverted-index-concurrency/","content":"引言 继上篇 Rust 实战丨倒排索引，本篇我们将参考《Rust 程序设计（第二版）》中并发编程篇章来实现高并发构建倒排索引。 本篇主要分为以下几个部分： 功能展示：展示我们最终实现的 2 个工具的效果（构建索引、搜索功能） 阅读源码：阅读书中源码的实现，理清大体思路。 构建索引：实战构建索引的每个具体环节，并对核心逻辑进行解释和阐述缘由。 搜索功能：这是书中未曾提供的功能，笔者根据自身理解，对齐上篇提供的功能，实现了一个搜索功能。 能学到： Rust 各种迭代器的使用 Rust 文件常用操作 Rust 字符串常用操作 Rust channel 实战 Rust 并发编程 多路合并文件实际应用 使用 byteorder 进行位操作 使用 clap 进行 CLI 开发 终端高亮输出 深入理解倒排索引高性能的核心细节 阅读建议 本篇内容较为冗长，涉及到的细节讲解可能比较啰嗦，推荐直接阅读源码，然后对不理解的地方再来本篇对应的章节进行阅读。 完成源码位于：https://github.com/hedon-rust-road/inverted-index-concurrency 版本声明 Rust: 1.76 byteordrr: 1.5.0 clap: 4.5.0 运行环境：macbookPro Apple M2 Max 功能展示 create.rs Usage: create [OPTIONS] FILENAMES...Arguments: FILENAMES...Options: -s, --single-threaded Default false -h, --help Print help 指定文件目录，构建索引，可以使用 -s 使用单线程构建，默认使用并发构建。 执行示例如下： ➜ inverted-index-concurrency git:(master) ✗ cargo run --bin create ./texts Finished dev [unoptimized + debuginfo] target(s) in 0.08s Running `/Users/wangjiahan/rust-target/debug/create ./texts`indexed document 0:./texts/text1.txt, 22 bytes, 5 wordsindexed document 1:./texts/text3.txt, 27 bytes, 5 wordsindexed document 2:./texts/text2.txt, 39 bytes, 6 wordsword count: 16351 bytes main, 736 bytes totalwrote file ./tmp00000001.dat search.rs Usage: search --index-file INDEX_FILE --term TERMOptions: -i, --index-file INDEX_FILE Specify index file path -t, --term TERM Specify search term -h, --help Print help 指定索引文件和搜索词来进行搜索。 执行示例如下： search.rs 执行示例 阅读源码 书中的源码位于：fingertips 第一部分我们先来阅读源码，书中展示了这样一张图： 索引构建器管道，其中箭头表示通过通道将值从一个线程发送到另一个线程（未展示磁盘 I/O） 从这张图我们大概可以猜想本案例中构建并发索引的过程可能是： 读取文件内容； 根据文件内容构建索引； 多个索引进行合并； 将索引写入文件； 多个索引文件进行合并。 按照这个思路的指引，我们打开源码，从 main.rs 的 main() 出发： fn main() let mut single_threaded = false; let mut filenames = vec![]; // 命令行参数解析 let mut ap = ArgumentParser::new(); ap.set_description(Make an inverted index for searching documents.); ap.refer(mut single_threaded).add_option( [-1, --single-threaded], StoreTrue, Do all the work on a single thread., ); ap.refer(mut filenames).add_argument( filenames, Collect, Names of files/directories to index. \\ For directories, all .txt files immediately \\ under the directory are indexed., ); ap.parse_args_or_exit(); // 构建索引 match run(filenames, single_threaded) Ok(()) = Err(err) = println!(error: , err), 解析命令行参数，这里使用 argparse 这个比较古老的 crate 来解析，现在一般是使用 clap。 single_threaded: 是否使用单线程，默认是多线程。 filenames: 指定的文本文件或目录。 run 函数执行构建索引。 看一下 run： /// Generate an index for a bunch of text files.fn run(filenames: VecString, single_threaded: bool) - io::Result() let output_dir = PathBuf::from(.); let documents = expand_filename_arguments(filenames)?; if single_threaded run_single_threaded(documents, output_dir) else run_pipeline(documents, output_dir) 单线程：run_single_threaded 多线程：run_pipeline 先从简单看，单线程，忽略掉源码中定义的特殊数据结构，可以发现跟我们上篇介绍的简单版倒排索引思路基本是一致的，只不过本案例中数据是从文件中读，最后又会将索引写入到文件中。 fn run_single_threaded(documents: VecPathBuf, output_dir: PathBuf) - io::Result() let mut accumulated_index = InMemoryIndex::new(); let mut merge = FileMerge::new(output_dir); let mut tmp_dir = TmpDir::new(output_dir); // 迭代每个文本文件 for (doc_id, filename) in documents.into_iter().enumerate() // 打开文件，并将内容读取到 `text` 上 let mut f = File::open(filename)?; let mut text = String::new(); f.read_to_string(mut text)?; // 构建索引 let index = InMemoryIndex::from_single_document(doc_id, text); accumulated_index.merge(index); if accumulated_index.is_large() // 当索引足够大的时候，将其写到文件中 let file = write_index_to_tmp_file(accumulated_index, mut tmp_dir)?; merge.add_file(file)?; accumulated_index = InMemoryIndex::new(); // 将最后一个索引写入到文件中 if !accumulated_index.is_empty() let file = write_index_to_tmp_file(accumulated_index, mut tmp_dir)?; merge.add_file(file)?; merge.finish() 再来看本文的重头戏，多线程： fn run_pipeline(documents: VecPathBuf, output_dir: PathBuf) - io::Result() // 将构建索引分为 5 个过程 let (texts, h1) = start_file_reader_thread(documents); let (pints, h2) = start_file_indexing_thread(texts); let (gallons, h3) = start_in_memory_merge_thread(pints); let (files, h4) = start_index_writer_thread(gallons, output_dir); let result = merge_index_files(files, output_dir); // 等待所有线程执行完毕 let r1 = h1.join().unwrap(); h2.join().unwrap(); h3.join().unwrap(); let r4 = h4.join().unwrap(); r1?; r4?; result 首先将索引构建分成 5 个阶段： 1. start_file_reader_thread 就是从文件中读取文本信息，并将其扔进 ReceiverString channel 中，传到下一个阶段。 fn start_file_reader_thread( documents: VecPathBuf,) - (ReceiverString, JoinHandleio::Result()) let (sender, receiver) = channel(); let handle = spawn(move || for filename in documents let mut f = File::open(filename)?; let mut text = String::new(); // 读取文件内容 f.read_to_string(mut text)?; if sender.send(text).is_err() break; Ok(()) ); (receiver, handle) 2. start_file_indexing_thread 从第 1 步传过来的文本信息中调用 InMemoryIndex::from_single_document 构建索引。 fn start_file_indexing_thread( texts: ReceiverString,) - (ReceiverInMemoryIndex, JoinHandle()) let (sender, receiver) = channel(); let handle = spawn(move || for (doc_id, text) in texts.into_iter().enumerate() // 构建索引 let index = InMemoryIndex::from_single_document(doc_id, text); if sender.send(index).is_err() break; ); (receiver, handle) 3. start_in_memory_merge_thread 将第 2 步构建的单一索引进行合并，并将合并后的索引传到下一个阶段。 fn start_in_memory_merge_thread( file_indexes: ReceiverInMemoryIndex,) - (ReceiverInMemoryIndex, JoinHandle()) let (sender, receiver) = channel(); let handle = spawn(move || let mut accumulated_index = InMemoryIndex::new(); for fi in file_indexes // 将索引进行合并 accumulated_index.merge(fi); if accumulated_index.is_large() // 如果索引大小到达阈值，则传到下一阶段 if sender.send(accumulated_index).is_err() return; accumulated_index = InMemoryIndex::new(); if !accumulated_index.is_empty() let _ = sender.send(accumulated_index); ); (receiver, handle) 4. start_index_writer_thread 将第 3 步传来的内存索引写入到临时文件中。 fn start_index_writer_thread( big_indexes: ReceiverInMemoryIndex, output_dir: Path,) - (ReceiverPathBuf, JoinHandleio::Result()) let (sender, receiver) = channel(); let mut tmp_dir = TmpDir::new(output_dir); let handle = spawn(move || for index in big_indexes // 将索引写入临时文件中 let file = write_index_to_tmp_file(index, mut tmp_dir)?; if sender.send(file).is_err() break; Ok(()) ); (receiver, handle) 5. merge_index_files 将临时文件进行合并，生成最终的索引文件。 fn merge_index_files(files: ReceiverPathBuf, output_dir: Path) - io::Result() let mut merge = FileMerge::new(output_dir); for file in files merge.add_file(file)?; merge.finish() 这 5 个步骤跟书中给出的示意图基本一致，我们再来看 run_pipeline 是如何合并并行的： // 使用 join() 等待所有线程完成let r1 = h1.join().unwrap();h2.join().unwrap();h3.join().unwrap();let r4 = h4.join().unwrap();// 阶段 2 和阶段 3 都是纯内存操作，不会有错误// 阶段 1 是读文件，阶段 4 是写文件，所以有可能会报错r1?;r4?; run_pipeline 示意图 源码阅读部分差不多就到这了，大的思想架构你应该都能 Get 到了，其中每个数据结构的具体实现细节，我们在后面的实战中进行拆解。 构建索引 代码结构 书中源码代码结构如下所示： ➜ fingertips git:(master) ✗ tree.├── Cargo.lock├── Cargo.toml├── LICENSE-MIT├── README.md├── src│ ├── index.rs│ ├── main.rs│ ├── merge.rs│ ├── read.rs│ ├── tmp.rs│ └── write.rs 书中给出的源码并没有实现使用构建好的索引文件进行搜索的功能，笔者将在此基础上实现该功能，所以对代码结构进行了简单的调整： ➜ inverted_index git:(master) ✗ tree.├── Cargo.lock├── Cargo.toml├── index.bat├── src│ ├── bin│ │ ├── create.rs│ │ └── search.rs│ ├── index.rs│ ├── lib.rs│ ├── merge.rs│ ├── read.rs│ ├── tmp.rs│ └── write.rs└── texts ├── text1.txt ├── text2.txt └── text3.txt 可以看到我将核心代码从 bin 改成了 lib ，这是为了支持我后面要实现的两个 bin: create: 构建索引，基本上就是源代码中的 main.rs search: 基于生成的索引文件实现搜索功能 texts 是我提供的文本文件样例。 src 目录中的代码阅读顺序及功能划分如下： index: 定义了内存索引数据结构 InMemoryIndex，实现了从文件内容中构建内存索引的基本逻辑，也实现了从索引文件重建内存索引的功能。 tmp: 定义了临时目录数据结构 TmpDir，用于存放临时索引文件。 write: 定义了索引文件写入器 IndexFileWriter，实现了将 InMemoryIndex 写入文件中的逻辑。 merge: 定义了文件合并器 FileMerge，用于合并 TmpDir 的所有索引文件。 read: 定义了索引文件读取器 IndexFileWrite，实现了解析索引文件的逻辑。 项目准备 cargo new --lib inverted_index_concurrency Cargo.toml [package]name = inverted-index-concurrencyversion = 0.1.0edition = 2021license = mitauthors = [hedon]description = a tool to concurrently build an inverted index.[[bin]]name=createpath=src/bin/create.rs[[bin]]name=searchpath=src/bin/search.rs[dependencies]byteorder = 1.5.0clap = version = 4.5.4, features = [derive] lib.rs pub mod index;pub mod merge;pub mod read;pub mod tmp;pub mod write; 在 lib.rs 中我们将这 5 个 mod 公开出去，这样就可以给 bin 目录中的 crate.rs 和 search.rs 使用了。 index.rs 完整源码：index.rs 第一部分是内存索引的构建。 tokenize 我们先定义一个分词函数： fn tokenize(text: str) - Vec(str, usize, usize) let mut res = Vec::new(); let mut token_start = None; for (idx, ch) in text.char_indices() match (ch.is_alphanumeric(), token_start) (true, None) = token_start = Some(idx), // 每个单词的开始 (false, Some(start)) = // 每个单词的结尾 res.push((text[start..idx], start, idx - 1)); token_start = None _ = if let Some(start) = token_start res.push((text[start..], start, text.len() - 1)) res 这个分词函数跟书中源码提供的不一样，为了实现文本高亮，我们需要记录每个分词在原文本中的起始位置和结束位置。它的核心逻辑如下： 通过 char_indices() 获取 text 的字符迭代器，这是一种懒加载的方法，避免一次性将所有 char 加载到内存中。 匹配 (ch.is_alphanumeric(), token_start)： 如果是 (true, None) 则表示这是一个单词的开始，我们纪录其开始的位置 Some(idx)； 如果是 (false, Some(idx)) 则表示这是一个单词的结束，我们将其加入到 res 中，并记录起始位置和结束位置。 其他情况，不做处理，要么是非法字符，要么是处于单词中间。 从这个简单的理解中，你应该可以感受到 Rust 中 match pattern 的强大和便捷了，666 👍🏻 struct: InMemoryIndex 在 index.rs 中，我们定义了三个数据结构： pub struct InMemoryIndex pub word_count: usize, pub terms: HashMapString, VecHit, pub docs: HashMapusize, Document,pub struct Document pub id: u32, pub path: PathBuf,pub type Hit = Vecu8; Document: 文档封装。 id: 文档 id，唯一标识符。 path: 源文件路径。 Hit: 它是一个字节数组，我们按照小端序进行存储，它的存储结构如下： [0..3] 存储一个 HITS_SEPERATOR = -1，表示一个 Hit 的开始。 [4..7] 存储一个 u32 的 document_id。 后面每 8 个 u8 会存在一个 u32 的 start_pos 和一个 u32 的 end_pos。 InMemoryIndex: 内存索引。 word_count: 包含的单词（word/term）个数，记录它是为了判断索引是否过大，以便对索引进行分片存储。 terms: 存储 word 到 Hits 的映射，每个 word 是一个搜索项。 docs: 存储了 document_id 到文档的映射，用于查询原始文档信息。 接下来我们来为 InMemoryIndex 实现一系列方法，因为我们期望使用小端序存储 Hit 中的数据，所以我们需要引入 byteorder 这个 crate: cargo add byteorder 具体实现可参考源码，核心逻辑是 from_single_document 和 merge。 from_single_document from_single_document 的核心逻辑在这一段，它其实跟我们之前实现的简易版倒排索引很相似： for (token, start_pos, end_pos) in tokens.iter() let hits = index.terms.entry(token.to_string()).or_insert_with(|| let mut hits = Vec::with_capacity(4 + 4 + 4 + 4); hits.write_i32::LittleEndian(Self::HITS_SEPERATOR) .unwrap(); hits.write_u32::LittleEndian(document_id).unwrap(); vec![hits] ); hits[0].write_u32::LittleEndian(*start_pos as u32).unwrap(); hits[0].write_u32::LittleEndian(*end_pos as u32).unwrap(); index.word_count += 1; 遍历每个 token 和它在文本中的位置。 对于每个 token，尝试在索引的 map 中查找一个现有的条目。如果不存在，则创建一个新的 Hit 记录，并初始化它： 创建一个新的 Hit 向量，预留 24 字节的容量，这是因为至少要存储 1 个分隔符、1 个 document_id、1 个 start_pos 和 1 个 end_pos。 首先写入 HITS_SEPERATOR 和 document_id（使用小端序）。 向对应的 Hit 向量中添加当前单词的位置。 累加处理的单词总数到 index.word_count。 这里给个示例，希望可以帮助你理解 InMemoryIndex 的内存结构： InMemoryIndex│├── word_count: usize│├── terms: HashMapString, VecHit│ ││ ├── Key: example (String)│ │ └── Value: VecHit│ │ ├── [HITS_SEPERATOR, Document ID: 1, Positions: [10, 19, 30, 39]] (Hit)│ │ └── [HITS_SEPERATOR, Document ID: 2, Positions: [15, 25]] (Hit)│ ││ └── Key: test│ └── Value: VecHit│ └── [HITS_SEPERATOR, Document ID: 1, Positions: [20, 24, 50, 69]] (Hit)│└── docs: HashMapu32, Document ├── Key: 1 (u32) │ └── Value: Document id: 1, path: path/to/file1.txt └── Key: 2 └── Value: Document id: 2, path: path/to/file2.txt merge merge 是用于合并多个 InMemoryIndex，起到批处理的目的。 pub fn merge(mut self, other: InMemoryIndex) for (term, hits) in other.terms self.terms.entry(term).or_default().extend(hits) self.word_count += other.word_count; self.docs.extend(other.docs); 实现完了 InMemoryIndex 后，我们就可以先来完成 create.rs 的 run_pipeline 的前 3 个阶段了。 step1: start_file_reader_thread 读取文件信息：我们需要在独立的线程中依次打开给定的文件列表，并将文件内容读取到一个 String 中，并利用 channel 传送出去。 fn start_file_reader_thread( documents: VecPathBuf,) - (Receiver(PathBuf, String), JoinHandleio::Result()) let (sender, receiver) = channel(); let handler = spawn(move || for filename in documents let mut f = File::open(filename.clone())?; let mut text = String::new(); f.read_to_string(mut text)?; if sender.send((filename, text)).is_err() break; Ok(()) ); (receiver, handler) step2: start_file_indexing_thread 构建索引：通过 channel 从第 1 阶段中获取文档文本信息，通过 from_single_document 构建索引 InMemoryIndex 后，将索引通过 channel 传送出去。 fn start_file_indexing_thread( docs: Receiver(PathBuf, String),) - (ReceiverInMemoryIndex, JoinHandle()) let (sender, receiver) = channel(); let handler = spawn(move || for (doc_id, (path, text)) in docs.into_iter().enumerate() let index = InMemoryIndex::from_single_document(doc_id as u32, path, text); if sender.send(index).is_err() break; ); (receiver, handler) step3: start_in_memory_merge_thread 合并索引：通过 channel 从第 2 阶段中获得构建的 InMemoryIndex 并将其合并成大索引，然后通过 channel 传送出去。 fn start_in_memory_merge_thread( indexes: ReceiverInMemoryIndex,) - (ReceiverInMemoryIndex, JoinHandle()) let (sender, receiver) = channel(); let handle = spawn(move || let mut accumulated_index = InMemoryIndex::new(); for i in indexes accumulated_index.merge(i); if accumulated_index.is_large() if sender.send(accumulated_index).is_err() return; accumulated_index = InMemoryIndex::new(); if !accumulated_index.is_empty() let _ = sender.send(accumulated_index); ); (receiver, handle) 补充：为什么采用这种“复杂”的方式来存储数据呢？可否使用 JSON 或者 Protobuf 呢？ 选择如何组织和存储数据，特别是在实现一个搜索引擎或数据库索引时，是一个关键决策，这会直接影响到程序的性能、可维护性以及扩展性。在这些情况下，使用像byteorder 这样的低级数据格式存储索引信息可能比使用 JSON 或Protobuf 等高级格式更有优势。读写速度：二进制格式：直接操作二进制格式通常比解析文本或半结构化的数据格式（如JSON）要快，因为它减少了解析时间和内存使用。在二进制格式中，数据通常是紧密打包的，没有额外的格式标记（如JSON 中的花括号和逗号），这减少了磁盘 I/O 需求。文本/半结构化格式：例如JSON，每次读取时都需要解析文本，转换数据类型，这会增加 CPU的负担，尤其是在大规模数据处理时。空间效率：二进制格式：使用最少的字节表示数据，例如使用定长的整数存储文档ID 和位置索引，不仅节省空间，还能提高缓存利用率。文本/半结构化格式：文本格式需要存储额外的字符来标识数据（例如引号和键名），这增加了存储需求。适用场景：二进制格式：非常适合需要高性能和大数据处理的后端系统，如搜索引擎和数据库索引。这种格式可以有效地支持快速的数据读取和写入，特别是在资源受限的环境中（如嵌入式系统或低延迟应用）。JSON/Protobuf：更适合需要跨平台兼容性和易于调试的应用场景。例如，在Web 应用中使用 JSON 作为数据交换格式，可以简化前后端的集成和测试。 tmp.rs 完成内存索引的构建后，我们需要将构建过程中产生的大索引先临时落盘，后面再进行合并。为了临时存储这些数据文件，我们需要将他们放在一个临时目录中，为此，我们定义了 TmpDir 数据结构： #[derive(Clone)]pub struct TmpDir dir: PathBuf, n: usize, dir: 目录 n: 自增器，用于区分临时文件命名 接下来为 TmpDir 实现 2 个方法： impl TmpDir pub fn newP: AsRefPath(dir: P) - TmpDir TmpDir dir: dir.as_ref().to_owned(), n: 1, pub fn create(mut self) - io::Result(PathBuf, BufWriterFile) let mut r#try = 1; loop let filename = self .dir .join(PathBuf::from(format!(tmp:08x.dat, self.n))); self.n += 1; match fs::OpenOptions::new() .write(true) .create_new(true) .open(filename) Ok(f) = return Ok((filename, BufWriter::new(f))), Err(exc) = if r#try 999 exc.kind() == io::ErrorKind::AlreadyExists // keep going else return Err(exc); r#try += 1; new 方法是 TmpDir 的构造函数，其中我们将 n 设置为 1，即文件名从 1 开始生成。dir.as_ref().to_owned() 接受一个可能是任何类型的路径，将其标准化为一个 Path 类型的引用，然后再复制这个引用，创建一个完全独立的、拥有所有权的 PathBuf 对象， create 方法是在 TmpDir 目录下创建一个临时文件。 write.rs 完整源码：write.rs 准备好内存索引和临时文件，那我们就需要实现将内存索引写入到文件中的功能了。 struct: InMemoryIndex 我们先来分析一下如何将 InMemoryIndex 落盘。首先 InMemoryIndex 的结构如下： pub struct InMemoryIndex pub word_count: usize, pub terms: HashMapString, VecHit, pub docs: HashMapu32, Document,pub struct Document pub id: u32, pub path: PathBuf, 其中 word_count 不需要存储，我们可以计算出来。那我们就需要存储索引 map 和文档原数据 docs。为了能精确定位到各个数据，我们需要： terms: 写入 VecHit docs: 写入 docs 中的每个 Document 写入 id 写入 path 大小 写入 path 而为了快速定位到每个 term 和 doc 的位置，我们需要下面几个值，这几个值将组合起来辅助我们快速定位 terms 或 docs，我们后面会将其称为 Entry，它包含以下几个值： term: 索引单词。为了统一，如果 term 为空，则表示当前表示的是 doc，否则为 terms。 df: term 的出现次数。为了统一，如果 df 为 0，则表示当前表示的是 doc，否则为 terms。 offset: 对应的 terms 或 docs 在文件中的偏移。 nbytes: 对应的 terms 或 docs 的总长度。 所以文件的内存结构大概如下： 文件区域 描述 指向内容 头部 （8 字节） 包含一个指向目录表开始位置的偏移量。 header 主条目 这些条目按顺序紧密存储，没有额外的元数据。这部分包含实际的数据条目。 terms + docs 目录表 存储在文件的最后，包括每个条目的术语信息、文档频率、偏移和大小。 entries 示意图如下： 索引文件内存结构示意图 为此我们定义了 IndexFileWriter，它专门用于将 InMemoryIndex 写入到临时文件中，定义如下： /// A structure to manage writing to an index file efficiently.pub struct IndexFileWriter offset: u64, writer: BufWriterFile, contents_buf: Vecu8, offset: 用于追踪文件中当前的写入位置。 writer: 一个缓冲写入器，它包装了一个文件，用于输出操作。 contents_buf: 一个向量，用来存储内容条目，在全部写入文件之前暂存在这个缓冲区。 接下来我们为 IndexFileWriter 实现几个方法： new: 这是一个构造函数，它初始化文件并设置初始偏移量。在文件的开始处写入一个占位符作为头部，这个头部最终会存储主数据区的大小。 write_document: 用于将一个文档以二进制格式写入到文件中，同时更新偏移量。 write_main: 这个方法接受一段数据，并将它写入文件中，同时更新偏移量。 write_contents_entry: 将一个内容 Entry 追加到内部的缓冲区中。Entry 包括一个术语、文档频率、术语数据的起始偏移和大小，它用于快速定位 terms 或 docs。 finish: 完成文件写入过程，将内部缓冲区的内容写入文件，并更新文件头部的主数据大小。 new 我们先来看构造方法： pub fn new(mut f: BufWriterFile) - io::ResultIndexFileWriter const HEADER_SIZE: u64 = 8; f.write_u64::LittleEndian(0)?; // content start Ok(IndexFileWriter offset: HEADER_SIZE, writer: f, contents_buf: vec![], ) new 分为以下几步： 定义头部大小：const HEADER_SIZE: u64 = 8;：定义一个常量 HEADER_SIZE，其值为 8 字节，这表示文件头部的大小。这个头部将用于后续在文件的开始处写入主数据区的起始位置。 写入头部占位符：f.write_u64::LittleEndian(0)?;：在文件的开始处写入一个 8 字节的占位符，这个值是以小端字节序（LittleEndian）存储的。初始时这里写入的是 0，意味着“主数据区的起始位置未知”，这个值在后续的 finish 函数中会被更新。 返回一个新的 IndexFileWriter 实例：Ok(IndexFileWriter offset: HEADER_SIZE, writer: f, contents_buf: vec![], )：构造并返回一个 IndexFileWriter 实例。这个实例的 offset 字段被初始化为 HEADER_SIZE（8 字节），表示实际数据将从文件的第 17 个字节开始写入。writer 字段就是传入的文件写入器，contents_buf 是一个新的空向量，用于临时存储内容条目数据。 为什么这样设计？ 这个实现方式有几个设计上的考虑：预留头部空间：通过在文件开始处预留 8字节空间来存储主数据区的大小，这样做可以在数据写入完成后，方便地回填这个信息。这是文件格式设计中常见的做法，允许读取者快速定位主数据区和内容索引区。使用小端字节序：小端字节序是一种在二进制文件中常用的字节序，尤其是在Windows平台下。使用小端字节序可以提高文件的兼容性，并且对于多数处理器架构来说，小端字节序的读写操作更为高效。灵活的数据写入：通过将 writer 和contents_buf组合使用，这个结构体可以灵活地处理不同的数据写入需求。writer直接写入文件，适合连续大块数据的写入；而 contents_buf用于聚集多个小片段的数据，可以在最后统一写入，减少磁盘操作次数。总的来说，这个构造函数的实现为高效和灵活的文件写操作提供了良好的基础，同时通过合理的错误处理和数据组织方式，确保了程序的健壮性和高性能。 write_main Hit 本身就是一个 Vecu8， 将其写入文件很简单，调用 write_all，即可，我们为其封装 write_main 方法： pub fn write_main(mut self, buf: [u8]) - io::Result() self.writer.write_all(buf)?; self.offset += buf.len() as u64; Ok(()) write_document 为了将 Docuemnt 本以二进制结构写入到文件中，我们需要拆分成几个部分： 文件 id 文件路径大小 文件路径 为此我们为 IndexFileWriter 封装了 write_document： pub fn write_document(mut self, doc: Document) - io::Result() self.writer.write_u32::LittleEndian(doc.id)?; self.writer .write_u64::LittleEndian(doc.path.as_os_str().len() as u64)?; self.writer.write_all(doc.path.as_os_str().as_bytes())?; self.offset += 4 + 8 + doc.path.as_os_str().len() as u64; Ok(()) write_contents_entry Entry 的数据量一般较小，我们会先写入缓冲中，后面再一次性刷盘，为此我们为 IndexFileWriter 封装了 write_contents_entry： /// Appends a content entry to the internal buffer.////// # Arguments/// * `term` - The term associated with the entry/// * `df` - Document frequency for the term/// * `offset` - Offset where the term data starts in the file/// * `nbytes` - Number of bytes of the term datapub fn write_contents_entry(mut self, term: String, df: u32, offset: u64, nbytes: u64) self.contents_buf.write_u64::LittleEndian(offset).unwrap(); self.contents_buf.write_u64::LittleEndian(nbytes).unwrap(); self.contents_buf.write_u32::LittleEndian(df).unwrap(); let bytes = term.bytes(); self.contents_buf .write_u32::LittleEndian(bytes.len() as u32) .unwrap(); self.contents_buf.extend(bytes); finish 刷盘的过程我们封装在 finish 中： pub fn finish(mut self) - io::Result() let contents_start = self.offset; self.writer.write_all(self.contents_buf)?; self.writer.seek(SeekFrom::Start(0))?; self.writer.write_u64::LittleEndian(contents_start)?; Ok(()) write_index_to_tmp_file 综合下来，我们就可以实现最核心的函数 write_index_to_tmp_file 了： pub fn write_index_to_tmp_file(index: InMemoryIndex, tmp_dir: mut TmpDir) - io::ResultPathBuf let (filename, f) = tmp_dir.create()?; let mut writer = IndexFileWriter::new(f)?; let mut index_as_vec: Vec_ = index.terms.into_iter().collect(); index_as_vec.sort_by(|(a, _), (b, _)| a.cmp(b)); for (term, hits) in index_as_vec let df = hits.len() as u32; let start = writer.offset; for buffer in hits writer.write_main(buffer)?; let stop = writer.offset; writer.write_contents_entry(term, df, start, stop - start); // if term == df == 0 type = document for (_, doc) in index.docs let start = writer.offset; writer.write_document(doc)?; let stop = writer.offset; writer.write_contents_entry(.to_string(), 0, start, stop - start) writer.finish()?; println!(wrote file :?, filename); Ok(filename) 我们在临时目录中创建一个临时文件，并初始化 IndexFileWriter； 将索引的 terms 转换成一个向量并按照键排序； 对于每个 term，计算文档频率（df），记录开始和结束位置，然后调用 write_main 方法将数据写入文件，然后使用 write_contents_entry 方法写入 Entry 的元数据到目录表； 对于 index.docs 中的每个文档，计算起止位置，并使用一个特殊的条目（空字符串作为条目名和 0 作为文档频率）标记在文件中； 最后我们使用 finish 将缓存中所有的 Entry 刷盘，并设置 entries 的起始位置。 文件的内存结构如上面给出的图一样，这里我们可以再看一次： 索引文件内存结构示意图 step4: start_index_writer_thread 实现了将内存索引写入到文件的功能后，我们就可以继续在 create.rs 中实现下一个流程了： fn start_index_writer_thread( big_indexes: ReceiverInMemoryIndex, output_dir: Path,) - (ReceiverPathBuf, JoinHandleio::Result()) let (sender, receiver) = channel(); let mut tmp_dir = TmpDir::new(output_dir); let handle = spawn(move || for i in big_indexes println!(word count: , i.word_count); let file = write_index_to_tmp_file(i, mut tmp_dir)?; if sender.send(file).is_err() break; Ok(()) ); (receiver, handle) 在 start_index_writer_thread 流程中，我们将构建好的内存索引一个个写入到文件中，并将生成的文件句柄传入下一个流程。 merge.rs 完整源码：merge.rs 前面 start_index_writer_thread 是将一个个 InMemoryIndex 写入到 TmpDir 临时目录中。现在我们要将这些临时文件合并成一个最终的索引文件，以优化查询效率和节省存储空间。 srtuct: FileMerge 我们定义一下结构： pub struct FileMerge output_dir: PathBuf, tmp_dir: TmpDir, stacks: VecVecPathBuf, output_dir: 用于存储最终合并文件的输出目录。 tmp_dir: 前面 tmp.rs 定义的结构，用于管理合并过程中产生的临时文件。 stacks: 这是一个二维向量，每个内部向量代表一个合并“层”，存储了该层待合并的文件路径。 关于 stacks，再多说两点： 多级合并策略: FileMerge 使用一个多层合并策略，这种策略在处理大量文件时尤为有效。基本思想是，当一层的文件数量达到一个预设的阈值（NSTREAMS）时，这些文件会被合并成一个新的文件，新文件则被推送到上一层。这种层级式的处理方式可以显著减少最终合并步骤需要处理的文件数量，从而优化性能。 动态扩展：使用 VecVecPathBuf 允许动态地添加新的合并层，这在处理不确定数量的文件时非常有用。向量的灵活性意味着无需预先知道将处理多少文件，它可以根据实际需要进行扩展。 接下来我们会为 FileMerge 实现 2 个方法： add_file: 添加一个文件到合并栈中，并使用多级合并策略进行合并。 finish: 执行最后的合并操作，生成最终的索引文件，输出到 output_dir 中。 add_file 首先我们来看add_file，它的实现如下： pub fn add_file(mut self, mut file: PathBuf) - io::Result() // 从第一层开始检查 let mut level = 0; // 使用循环来处理文件的添加和可能的合并。 loop // 如果当前的 level （层级）不存在于 stacks 中， // 就在 stacks 中添加一个新的空向量。 // 这是为了存放该层级的文件。 if level == self.stacks.len() self.stacks.push(vec![]); // 将当前的文件添加到对应层级的向量中。 self.stacks[level].push(file); // 如果这个级别的堆栈已满，就合并这个级别的文件。 // 如果没满，则不进行合并，直接退出。 if self.stacks[level].len() NSTREAMS break; // 创建一个新文件来存储合并结果，并更新堆栈。 let (filename, out) = self.tmp_dir.create()?; // 初始化一个空的 to_merge 向量， // 然后使用 mem::swap 交换当前层级的文件列表和这个空向量， // 这样 to_merge 向量就包含了需要合并的文件， // 而当前层级变为空，可以用来存放新的合并文件。 let mut to_merge = vec![]; mem::swap(mut self.stacks[level], mut to_merge); // 调用 merge_streams 函数将 to_merge 中的文件合并到新创建的文件中。 merge_streams(to_merge, out)?; // 将合并后得到的新文件路径赋值给 file 变量，用于下一轮循环。 file = filename; // level 加一，表示移动到下一个层级。 level += 1; Ok(()) 这个方法通过层级的方式管理文件合并，每个层级可以有多个文件，但数量上限为 NSTREAMS。如果某层满了，就将该层的文件合并成一个新文件，并将这个新文件移动到上一层继续参与合并。这种设计有效地将多个文件逐步合并成一个文件，同时控制内存和 I/O 资源的使用。 其中 merge_streams 就是具体的合并过程，它的实现如下： fn merge_streams(files: VecPathBuf, out: BufWriterFile) - io::Result() // 从索引文件中构建 IndexFileReader 列表 let mut streams: VecIndexFileReader = files .into_iter() .map(|p| IndexFileReader::open_and_delete(p, true)) .collect::io::Result_()?; // 针对输出文件生成一个 IndexFileWriter 用于写入索引信息 let mut output = IndexFileWriter::new(out)?; // 用于记录当前写入的位置（或者数据偏移量）。 let mut point: u64 = 0; // 记录还有数据未处理的文件流数量，用 peek() 方法检查。 let mut count = streams.iter().filter(|s| s.peek().is_some()).count(); // 只要 count 大于0，表示还有文件未完全处理，就继续循环。 while count 0 let mut term = None; let mut nbytes = 0; let mut df = 0; // 这段代码通过遍历每个文件流，使用 peek() 方法预览每个文件的当前数据条目 for s in streams match s.peek() None = Some(entry) = // term 是空的，则说明这是表示 doc 的 entry。 // 直接退出 for 循环，因为 doc 的 entry 没有顺序且唯一，不会进行累加。 if entry.term.is_empty() term = Some(entry.term.clone()); nbytes = entry.nbytes; df = entry.df; break; // term 不是空的，则说明这是表示 terms 的 entry。 // 选择词条最小的一个（字典序），并且累加其出现的频次和字节大小。 // 这是多路归并的核心，确保输出文件是有序的。 if term.is_none() || entry.term *term.as_ref().unwrap() term = Some(entry.term.clone()); nbytes = entry.nbytes; df = entry.df else if entry.term == *term.as_ref().unwrap() nbytes += entry.nbytes; df += entry.df let term = term.expect(bug in algorithm); // 对于每个文件流，如果当前数据条目与选择的 term 相同， // 则将该条目写入输出文件，并更新该流的读取位置。 for s in mut streams if s.is_at(term) s.move_entry_to(mut output)?; if s.peek().is_none() count -= 1; if term.is_empty() break; output.write_contents_entry(term, df, point, nbytes); point += nbytes Ok(()) 这里涉及到了一个新的结构 IndexFileReader，它是索引文件的读取器，我们将在 read.rs 中实现它。这里先不展开，你只需要知道： IndexFileReader::open_and_delete(p, true): 打开一个索引文件，并根据传入的参数判断是否要删除这个文件，在合并过程中，因为都是临时文件，所以我们会指定为删除文件。但是在后面从索引文件中重建 InMemoryIndex 的时候，我们不希望删除原始的索引文件。 s.peek(): 查看下一个 Entry，它的返回值是 OptionEntry。 s.move_entry_to(mut output): 将 s.peek() 指向的 Entry 写入到 output 文件中，并移动到一下 Entry。 总结下来，这个函数实现多路归并的核心部分，它将多个索引文件合并成一个单一的有序文件。 finish 我们再来看 FileMerge 的另外一个方法 finish： pub fn finish(mut self) - io::Result() // 初始化一个临时向量 tmp，用来暂存需要合并的文件路径。 // 这个向量的容量设置为 NSTREAMS，这是预先定义的常量，表示一次可以合并的最大文件数。 let mut tmp = Vec::with_capacity(NSTREAMS); // 方法遍历 self.stacks 中的每个堆栈。每个堆栈代表一个合并层级，包含若干待合并的文件。 for stack in self.stacks // 对于每个堆栈，方法使用 .into_iter().rev() 迭代器反向遍历文件， // 以确保按正确的顺序处理（先进后出）。 for file in stack.into_iter().rev() // 将文件逐个添加到 tmp 向量中。 tmp.push(file); // 当 tmp 的长度达到 NSTREAMS 时， // 调用 merge_reversed 函数进行合并。 if tmp.len() == NSTREAMS merge_reversed(mut tmp, mut self.tmp_dir)?; // 对于剩余文件进行最终的合并。 if tmp.len() 1 merge_reversed(mut tmp, mut self.tmp_dir)?; // 最后应该只有一个最终文件 assert!(tmp.len() == 1); match tmp.pop() // 对文件进行重命名 Some(last_file) = fs::rename(last_file, self.output_dir.join(MERGED_FILENAME)), None = Err(io::Error::new( io::ErrorKind::Other, no ducuments were parsed or none contained any words, )), 这里涉及到了另外一个函数 merge_reversed： fn merge_reversed(filenames: mut VecPathBuf, tmp_dir: mut TmpDir) - io::Result() filenames.reverse(); let (merge_filename, out) = tmp_dir.create()?; let mut to_merge = Vec::with_capacity(NSTREAMS); mem::swap(filenames, mut to_merge); merge_streams(to_merge, out)?; filenames.push(merge_filename); Ok(()) 它其实就是将 filenames 翻转，清空并将内容转移到 to_merge，然后调用 merge_streams 合并，并将合并后的文件重新放回被清空的 filenames，也就是我们在 finish 中声明的 tmp 变量。 为什么这里需要翻转 filenames？ 假设 NSTREAMS = 3，我们执行 add_file，从file1 到 file8，那么过程如下：ActionStack 0Stack 1NotesAdd file1file1Add file2file1, file2Add file3file1, file2, file3Merge S1(empty)merge1merge1 is the result of merging file1-file3Add file4file4merge1Add file5file4, file5merge1Add file6file4, file5, file6merge1Merge S2(empty)merge1, merge2merge2 is the result of merging file4-file6Add file7file7merge1, merge2Add file8file7, file8merge1, merge2Trigger merge because 8 files are reached最后我们获得的结果是：stack0stack1file7, file8merge1, merge2按照文件的添加顺序，我们期望在 finish中合并的顺序应该是：merge1, merge2, file7, file8。所以我们遍历stacks 的时候，从第 1 层开始遍历的话，我们就需要反向遍历rev()，这个时候我们组成的 tmp 就是：file8,file7, merge2, merge1。最后我们传入 merge_reversed的时候，再进行 reverse()，就可以获得我们期望的顺序 merge1,merge2, file7, file8。 回过头来，我们总结一下 finish：这个方法通过多级合并的方式，逐层处理并最终合并所有文件到一个文件。这个方法确保在多个文件频繁合并的环境中，能有效地管理和减少临时存储使用，并保持合并操作的效率。通过最后的重命名操作，它还处理了文件的最终存放，确保合并结果的正确性和可用性。 实现了 merge.rs 的相关内容，我们就可以来实现 create.rs 中的最后一步了。 step5: merge_index_files 我们将第 4 阶段构建的临时文件合并成一个最终的索引文件并输出到 output_dir 目录中。 fn merge_index_files(files: ReceiverPathBuf, output_dir: Path) - io::Result() let mut merge = FileMerge::new(output_dir); for file in files merge.add_file(file)?; merge.finish() run_pipeline 至此，我们就完成了并发构建倒排索引的 5 个步骤了，对其进行组织，就可以实现我们的并发构建函数 run_pipeline： fn run_pipeline(documents: VecPathBuf, output_dir: PathBuf) - io::Result() // Launch all five stages of the pipeline. let (texts, h1) = start_file_reader_thread(documents); let (pints, h2) = start_file_indexing_thread(texts); let (gallons, h3) = start_in_memory_merge_thread(pints); let (files, h4) = start_index_writer_thread(gallons, output_dir); let result = merge_index_files(files, output_dir); // Wait for threads to finish, holding on to any errors that they encounter. let r1 = h1.join().unwrap(); h2.join().unwrap(); h3.join().unwrap(); let r4 = h4.join().unwrap(); // Return the first error encountered, if any. // (As it happens, h2 and h3 cant fail: those threads // are pure in-memory data processing.) r1?; r4?; result read.rs 完整源码：read.rs 在 merge.rs 中，我们还剩最后一个结构没有解析，那就是 IndexFileReader，它是索引文件的读取器。 struct: IndexFileReader pub struct IndexFileReader pub terms_docs: BufReaderFile, entries: BufReaderFile, next: OptionEntry,pub struct Entry pub term: String, pub df: u32, pub offset: u64, pub nbytes: u64, 我们在 IndexFileReader 结构体中定义两个 BufReaderFile ，这是为了有效管理和操作索引文件中的不同数据段。具体来说，这种设计使得代码能够更加灵活和高效地处理索引文件中的“主数据区”和“内容表区”。 即用来分别处理下图的 termsdoc 和 entries 两个区域： 索引文件内存结构示意图 这有几个好处： 独立的文件指针：每个 BufReaderFile 维护自己的文件读取位置（文件指针）。这意味着读取或搜索内容表时，不会影响主数据区的文件指针，反之亦然。这样可以避免频繁地重新定位文件指针，提高文件操作的效率。 缓冲读取：BufReader 提供了缓冲读取功能，可以减少直接对硬盘的读取次数，从而优化读取性能。对于需要频繁读取小块数据的索引操作，使用缓冲读取可以显著提高效率。 并行操作：在多线程环境中，可能需要同时读取主数据区和内容表区。使用两个独立的 BufReader 实例可以简化并行读取的管理，每个读取操作都可以在不干扰另一个操作的情况下独立进行。 Entry 就是我们在 write.rs 中 write_contents_entry 时传入的参数，这里我们将其封装成一个 struct，再次回顾下这几个字段的含义： term: 索引单词。为了统一，如果 term 为空，则表示当前表示的是 doc，否则为 terms。 df: term 的出现次数。为了统一，如果 df 为 0，则表示当前表示的是 doc，否则为 terms。 offset: 对应的 terms 或 docs 在文件中的偏移。 nbytes: 对应的 terms 或 docs 的总长度。 read_entry 这里我们重点解释一下 read_entry 方法，其他的都比较简单，请在源码中查找。 fn read_entry(f: mut BufReaderFile) - io::ResultOptionEntry // 获取偏移值 let offset = match f.read_u64::LittleEndian() Ok(value) = value, Err(err) = if err.kind() == io::ErrorKind::UnexpectedEof return Ok(None); else return Err(err); ; // 读取 nbytes let nbytes = f.read_u64::LittleEndian()?; // 读取 df let df = f.read_u32::LittleEndian()?; // 读取 term_len，并初始化一块内存 bytes 用来读取完整的 term let term_len = f.read_u32::LittleEndian()? as usize; let mut bytes = vec![0; term_len]; f.read_exact(mut bytes)?; let term = match String::from_utf8(bytes) Ok(s) = s, Err(_) = return Err(io::Error::new(io::ErrorKind::Other, unicode fail)), ; // 返回构建的 Entry Ok(Some(Entry term, df, offset, nbytes, )) 结合下面这张图，很容易理解 read_entry 就是前面 write_contents_entry 的逆向过程。 entries 区域布局，每个 entry 紧贴排布 create.rs 完整源码：create.rs 至此，我们就分析完并发构建索引的整个过程了，在 create.rs 中，我们使用 clap 命令解析框架来构建一个 CLI 工具用以支持构建索引，我们同时支持单线程构建和并发构建，具体可看完整源码。 如果对 clap 不熟悉的读者，可参考：深入探索 Rust 的 clap 库：命令行解析的艺术 #[derive(Parser)]struct Opts #[arg(short, long, default_value_t = false, help = Default false)] single_threaded: bool, #[arg(required = true)] filenames: VecString,fn main() let opts = Opts::parse(); match run(opts.filenames, opts.single_threaded) Ok(()) = Err(err) = println!(error: , err), 搜索功能 在《Rust 程序设计（第二版）》中，作者并没有实现搜索功能，笔者对其进行扩展，目标是对标我们前篇所构建的 Rust 实战丨倒排索引。这个搜索功能，会根据现有的索引文件重建内存索引 InMemoryIndex，支持指定 term 进行搜索，并将包含这个 term 的文件在响应的位置中进行高亮显示并输出到终端。 search.rs 完整源码：search.rs 程序入口如下所示，比较简单，就不赘述了。 #[derive(Parser)]struct Opts #[arg(short, long, required = true, help = Specify index file path)] index_file: String, #[arg(short, long, required = true, help = Specify search term)] term: String,fn main() - io::Result() let opts = Opts::parse(); let index = InMemoryIndex::from_index_file(opts.index_file)?; index.search(opts.term)?; Ok(()) 这里有 2 个核心逻辑： InMemoryIndex::from_index_file: 根据索引文件重建内存索引。 index.search(term): 搜索。 index.rs 我们在 index.rs 中为 InMemoryIndex 实现上述 2 个方法。 from_index_file pub fn from_index_fileP: AsRefPath(filename: P) - io::ResultInMemoryIndex let mut index = InMemoryIndex::new(); // 获取 IndexFileReader let mut reader = IndexFileReader::open_and_delete(filename, false)?; // 依次解析每个 Entry while let Some(entry) = reader.iter_next_entry() if entry.term.is_empty() entry.df == 0 // 当前 Entry 指向的是一个 Document。 // 通过 terms_docs 读取 Document 所在位置并进行解析。 reader.terms_docs.seek(io::SeekFrom::Start(entry.offset))?; let doc_id = reader.terms_docs.read_u32::LittleEndian()?; let path_len = reader.terms_docs.read_u64::LittleEndian()?; let mut path = vec![0u8; path_len as usize]; reader.terms_docs.read_exact(mut path)?; index.docs.insert( doc_id, Document id: doc_id, path: vec_to_pathbuf(path), , ); else // 当前 Entry 指向的是一个 terms。 // 通过 terms_docs 读取 terms 所在位置并进行解析。 let mut hits = vec![]; reader.terms_docs.seek(io::SeekFrom::Start(entry.offset))?; let mut data = vec![0u8; entry.nbytes as usize]; reader.terms_docs.read_exact(mut data)?; let mut cursor = Cursor::new(data); let mut i = entry.df; let mut has_hit = false; let mut quit = false; while i 0 !quit let mut hit = Vec::with_capacity(4 + 4 + 4); // cannot use vec![0;12] loop if let Ok(item) = cursor.read_i32::LittleEndian() // the start of next hit if item == Self::HITS_SEPERATOR has_hit hits.push(hit); i -= 1; index.word_count -= 2; hit = Vec::with_capacity(4 + 4 + 4); has_hit = true; hit.write_u32::LittleEndian(item as u32).unwrap(); index.word_count += 1; else quit = true; if !hit.is_empty() hits.push(hit); index.word_count -= 2; break; index.terms.insert(entry.term, hits); index.word_count /= 2; Ok(index) search pub fn search(self, term: str) - io::Result() // 获取 term 出现的位置 let m: OptionVecVecu8 = self.terms.get(term); if m.is_none() println!(can not found in all documents, term); return Ok(()); let hits = m.unwrap(); // 遍历每个出现的位置 for hit in hits let mut cursor = Cursor::new(hit); let _ = cursor.read_i32::LittleEndian().unwrap(); // 获取文档原始信息 let document_id = cursor.read_u32::LittleEndian().unwrap(); let doc = self.docs.get(document_id); if doc.is_none() println!(cannot found document , document_id); continue; let doc = doc.unwrap(); // hits 存储的内容：[HITS_SEPERATOR, document_id, start_pos1, end_pos1, ...] // 解析 term 出现在 doc 中的每个位置 let mut poss = Vec::with_capacity(hits.len() / 4); let mut pos = TokenPos::default(); let mut has_pos = false; while let Ok(p) = cursor.read_u32::LittleEndian() if !has_pos pos.start_pos = p; has_pos = true; else pos.end_pos = p; poss.push(pos); pos = TokenPos::default(); has_pos = false; // 对每个出现的位置进行高亮处理 let result = highlight_file(doc.path.clone(), mut poss)?; // 输出高亮后的结果 println!( :?: , doc.path, result); Ok(()) 至此，我们就实现了高并发构建索引和根据索引进行搜索的功能，本篇某些部分可能比较复杂，篇幅也比较冗长，笔者在阅读书中原实现的时候，也是获益颇丰，想不到一个简单的倒排索引竟涉及这么多的处理细节。也希望本篇文章能对感兴趣的读者有些许帮助。 peace! enjoy coding~ 绘图工具 https://excalidraw.com/ 参考资料 维基百科·倒排索引 Rust 程序设计（第二版）","tags":["rust","并发编程","倒排索引","通道"],"categories":["rust","rust 实战"]},{"title":"Rust 实战丨倒排索引","path":"/2024/04/15/rust-action-inverted-index-demo/","content":"引言 倒排索引（Inverted Index）是一种索引数据结构，用于存储某个单词（词项）在一组文档中的所有出现情况的映射。它是搜索引擎执行快速全文搜索的核心技术，也广泛用于数据库中进行文本搜索。我们熟知的 ElasticSearch 最核心底层原理便就是倒排索引。 倒排索引的基本原理是将文档中的词汇进行反转，形成倒排列表。 在倒排列表中，每个词汇都对应一个文档标识符的列表，这些标识符指明了该词汇出现在哪些文档中。 通过查询倒排列表，可以快速地找到包含特定词汇的文档。 本文将使用 Rust 语言来实现一个简单的倒排索引，包括倒排索引的构建和搜索过程。在下一篇文章中，笔者会基于《Rust 程序设计（第二版）》并发编程篇章，解读该书作者是如何基于 Rust 通道实现更优秀、更高性能的倒排索引。 可以学到 倒排索引的原理、优势和使用 常用 crate：colored、regex Rust HashMap Rust 迭代器 开发思路 倒排索引构建过程 一个简单的倒排索引开发思路大概如上图所示： 读取文档 分词 构建每个词到每个文档的映射 开发过程 完整源码位于：inverted_index。 最终效果 fn main() let mut index = InvertedIndex::new(); index.add(1, Rust is safe and fast.); index.add(2, Rust is a systems programming language.); index.add(3, Programming in Rust is fun.); // query Rust let results = index.query(Rust); for result in results println!(, result); println!(); // query Programming let results = index.query(Programming); for result in results println!(, result); 执行： cargo run 输出： inverted index 输出示例 版本声明 [package]name = inverted_indexversion = 0.1.0edition = 2021[dependencies]colored = 2.1.0regex = 1.10.4 项目准备 首先我们创建项目： cargo new inverted_index 准备依赖： cargo add regexcargo add colored colored: 终端高亮，后面我们将实现搜索词的高亮显示，使结果更美观。 regex: 正则库，用于实现不区分大小写替换匹配到的搜索词。 实现过程 首先我们定义两个数据结构： struct Document id: usize, content: String,struct InvertedIndex indexes: HashMapString, Vecusize, documents: HashMapusize, Document,impl InvertedIndex fn new() - InvertedIndex InvertedIndex indexes: HashMap::new(), documents: HashMap::new(), Document: 封装原始文档 IndexedIndex: 我们将构建的倒排索引 接下来我们要实现 2 个辅助函数，一个是 tokenize，用于将原始的文档信息拆分成独立的词（word/term），另一个是 hightlight，用于将匹配到的文本进行替换，使其在中断可以以紫色输出。 tokenize 实现如下： fn tokenize(text: str) - Vecstr text.split(|ch: char| !ch.is_alphanumeric()) .filter(|c| !c.is_empty()) .collect()#[test]fn tokenize_test() assert_eq!( tokenize(This is hedons tokenize function.), vec![This, is, hedon, s, tokenize, function] ) highlight 实现如下： fn highlight(term: str, content: str) - String let regex = Regex::new(format!(r(?i), term)).unwrap(); let highlighted_content = regex .replace_all(content, |caps: regex::Captures| caps[0].to_string().purple().to_string() ) .to_string(); highlighted_content#[test]fn highlight_test() assert_eq!( highlight(programming, I like programming with Rust Programming), I like \\u1b[35mprogramming\\u1b[0m with Rust \\u1b[35mProgramming\\u1b[0m ); 现在我们可以为 InvertedIndex 实现构建索引的方法 add 了，它会接收原始文档，对其进行分词，并将记录每个分词和文档 id 的映射。 impl InvertedIndex fn add(mut self, doc_id: usize, content: str) let content_lowercase = content.to_lowercase(); let words = tokenize(content_lowercase); for word in words self.indexes .entry(word.to_string()) .or_insert(vec![]) .push(doc_id) self.documents.insert( doc_id, Document id: doc_id, content: content.to_string(), , ); 然后我们再实现对应的根据分词 term 搜索原始文档的方法： impl InvertedIndex fn query(self, term: str) - VecString let term_lowercase = term.to_lowercase(); if let Some(doc_ids) = self.indexes.get(term_lowercase) doc_ids .iter() .filter_map(|doc_id| self.documents .get(doc_id) .map(|doc| highlight(term_lowercase, doc.content)) ) .collect() else Vec::new() 这样一个简单的倒排索引构建和搜索功能就完成了，具体的执行效果你可以回到前面的「最终效果」进行查阅。 总结预告 本文实现的倒排索引虽然非常简单，但是也基本体现了倒排索引的最核心思想和应用方式了。在《Rust 程序设计（第二版）》的并发编程篇章中，该书提出了使用通道 channel 来并发构建倒排索引，同时给出了更加丰富和优雅的实现。在下篇文章中，笔者将阅读这部分的源码，解析并重现当中的实战过程，并进行适当扩展。 peace! enjoy coding~ 绘图工具 https://excalidraw.com/ 参考资料 维基百科·倒排索引 Rust 程序设计（第二版）","tags":["rust","倒排索引"],"categories":["rust","rust 实战"]},{"title":"深入浅出 Go 语言的 defer 机制","path":"/2024/03/28/go-defer/","content":"Go 语言以其简洁的语法和强大的并发支持而闻名。在这些特性中，defer 语句是 Go 语言提供的一项独特功能，它允许我们推迟函数的执行直到包含它的函数即将返回。这个简单而强大的机制不仅可以帮助我们处理资源释放和错误处理，还能让代码更加简洁和安全。本文将深入浅出地介绍 defer 的工作原理，探究其背后的机制，并通过丰富的案例来展示它的实际应用。 笔者本来以为 Go 语言的 defer 其实东西不多，就是类似于“栈”的操作罢了，无非就是用于释放资源、后进先出而已。但是最近在阅读完《深入理解 Go 语言》、《Go 底层原理剖析》和《Go 语言设计与实现》中关于 defer 的篇章。发现其中隐含的道道和坑还是比较有意思的，特此整理这篇文章，希望能对 Go defer 原理感兴趣的读者带来一些帮助。 本文具体会包含以下内容： defer 机制简介：介绍 defer 关键字的基本概念和它在 Go 语言中的作用。 defer 的工作原理：深入探讨 defer 在函数执行结束时如何工作的细节。 defer 的执行顺序：解释 defer 语句是如何按照后进先出（LIFO）的顺序执行的。 参数预计算和值传递：讨论 defer 语句中参数是如何被预先计算和传递的。 环境变量和闭包：探讨 defer 如何与闭包一起工作，以及如何捕获和影响环境变量。 defer 与错误处理：说明如何利用 defer 和 recover 进行错误处理和异常捕获。 defer 的实现细节：深入分析 defer 的不同实现策略，包括堆上分配、栈上分配和开放编码。 版本声明 Go1.22 思维导图 Go defer 核心要点 对于后面将要分析的各种各样的情况，在分析的时候只要遵循以下几个核心点，基本上就不会跑偏： 延迟执行：在函数结束时执行，包括正常返回或遭遇 panic。 栈式执行顺序：后定义的 defer 先执行（LIFO）。 参数预计算：defer 语句定义时即计算并固定参数值。 值传递原则：defer 拷贝参数，使用定义时的值。 环境变量捕获：在 defer 中可以跟一个闭包，闭包可以捕获环境变量，当然这包括具名返回值。 特别说明的是，虽然我们通常将 defer 想象为使用栈进行管理，但是实际实现上，defer 并不都是存放在栈上的，我们后面会具体分析到。这种实现细节通常对于编写正确的 Go 代码并不重要，但了解这一点对于深入理解语言内部机制可能是有帮助的。 基本用法 在 Go 语言中，defer 语句通常用于确保一个函数调用在程序执行结束时发生，常见的用例包括文件关闭、锁释放、资源回收等。 func readFile(filename string) error f, err := os.Open(filename) if err != nil return err // 确保文件在函数返回时关闭 defer f.Close() // ... 处理文件 ... return nil 在上面的例子中，defer f.Close() 保证了无论 readFile 函数如何返回（正常返回或发生错误），f.Close() 都会被调用，从而避免了资源泄露。 执行顺序 defer 的执行顺序是先进后出，即“栈”操作。这里借用刘丹冰老师的一张图来演示这个过程： Go defer 执行顺序 我们可以通过以下代码进行验证： func func1() fmt.Println(func1...)func func2() fmt.Println(func2...)func func3() fmt.Println(func3...)func main() defer func1()\tdefer func2()\tdefer func3() 输出如下： func3...func2...func1... 参数求值与陷阱 关于 defer 参数这一块，是一个比较容易出错的地方。我们先来看一个例子，你可以分析下它的输出会是什么？ func printI(i int) fmt.Println(printI i:, i)func main() i := 10\tdefer printI(i * 10)\ti = i + 1\tfmt.Println(main i:, i) 按照我们之前总结的核心点：参数预计算：defer 语句定义时即计算并固定参数值。具体来说，在把 defer 压入“栈”时，会同时压入函数地址和函数形参，也就是会在这个时候就把参数先算好。所以在执行到第 7 行代码的时候，就会把 i*10 算好，然后同 printI 一同压入到延迟执行栈中。 所以最后的结果就是： main i: 11printI i: 100 关于参数值传递，笔者这里再举两个例子进行比较，体会后你应该就理解了。 第一个例子中，defer 后面参数是指针，本质上值传递，但是拷贝的是指针，所以在 defer 中修改的东西，最后会反馈到指针指向的对象，所以对 testUser 的返回值是有影响的。 type User struct\tname stringfunc testUser() *User user := User\tuser.name = name-1\tdefer func(u *User) u.name = name-defer\t(user)\tuser.name = name-2\treturn userfunc main() user := testUser()\tfmt.Println(user)// name-defer 第二个例子中，我们传入的就是结构体示例本身了，因为值传递，即拷贝了一份新的 user，所以闭包内的修改对外面是不产生影响的。 type User struct name stringfunc testUser() User user := User\tuser.name = name-1\tdefer func(u User) u.name = name-defer\t(user)\tuser.name = name-2\treturn userfunc main() user := testUser()\tfmt.Println(user)// name-2 环境变量捕获 将上面的一个例子进行简单修改，会输出什么呢？ func printI(i int) fmt.Println(printI i:, i)func main() i := 10\tdefer func() printI(i * 10)\t()\ti = i + 1\tfmt.Println(main i:, i) 这个时候其实没有参数，所以会直接将下面闭包压入延迟栈中。 func() printI(i * 10) 而闭包是可以捕获环境变量的，所以在 main return 后，defer 可以捕获到 i 的值，为更新后的 i+1，最后再进行 printI(i * 10)。 所以输出结果是： main i: 11printI i: 110 所以说，defer 后面的闭包，是可以捕获环境变量的，如果这个变量是返回值的话，那么理所应当也是可以对其产生作用的，如： func getI() (i int) i = 1\tdefer func() i *= 10\t()\treturn 20func main() fmt.Println(getI()) 这段代码中，getI 的返回值是有名字的 i，getI 执行了 return 20，其实就是将 i 设置为 20，所以在执行到 defer 闭包的时候，捕获到了 i=20，并将其进行了修改。所以最终输出： 200 错误处理与 defer 我们都知道 Go 程序中遇到 panic 就会中断后面的执行流程直接返回，这个时候我们可以在 defer 中结合 recover 来捕获这个 panic，从而保护程序不崩溃。 如： func panicAndRecover() defer func() if err := recover(); err != nil fmt.Println(err) ()\tfmt.Println(函数中正常流程)\tpanic(出现异常)\tfmt.Println(panic 后的语句永远执行不到)func main() panicAndRecover()\tfmt.Println(正常回到 main)// 函数中正常流程// 出现异常// 正常回到 main 更进一步，如果我们在 defer 中也有 panic 呢？请思考下列代码： func panicAndRecover() defer func() fmt.Println(第 1 个入栈的 defer) if err := recover(); err != nil fmt.Println(最终捕获的 panic:, err) ()\tdefer func() fmt.Println(第 2 个入栈的 defer) panic(第 2 个入栈的 defer 发生 panic)\t()\tfmt.Println(panicAndRecover 函数中正常流程)\tpanic(panicAndRecover 出现异常)\tfmt.Println(panic 后的语句永远执行不到)func main() panicAndRecover()\tfmt.Println(正常回到 main) 上述代码中，我们在 panicAndRecover 强行抛出 panic，由于 defer 先进后出，所以我们会先执行第 2 个 defer，其中也发生了 panic，我们在第 1 个 defer 中对 panic 进行 recover，最终的现象是只捕获到了后面抛出的 panic： panicAndRecover 函数中正常流程第 2 个入栈的 defer第 1 个入栈的 defer最终捕获的 panic: 第 2 个入栈的 defer 发生 panic正常回到 main 这是为什么呢？ 在 Go 语言中，panic 函数实际上是创建了一个 panic 对象，并抛出这个对象。 当一个 panic 发生并开始向上传播时，Go 运行时会检查每个 defer。如果 defer 中包含 recover 调用，并且它被执行，那么 recover 会捕获当前的 panic，并且防止它继续向上传播。如果 defer 中再次发生 panic，那么原来的 panic 就不会被 recover 捕获，因为 defer 函数已经退出了。在这种情况下，新的 panic 会导致程序崩溃，因为没有更多的 defer 函数去 recover 这个新的 panic。 这说明了 Go 程序中不允许同时有多个活跃的 panic 存在，这个设计确保了在任何给定的时刻，只有一个 panic 能够被处理。这样做有几个原因： 简化错误处理： 如果同时存在多个 panic，就会变得非常复杂去确定如何处理它们，尤其是在它们之间存在依赖关系的时候。一个 panic 应该表示一个不可恢复的错误，如果有多个这样的错误同时存在，程序的状态可能会变得非常不确定。 保持一致性： panic 通常表示程序中出现了严重错误，可能会破坏程序的一致性或安全性。如果允许多个 panic 同时存在，就很难保证程序状态的一致性，因为不同的 panic 可能需要回退不同的操作。 避免资源泄漏： defer 语句用于确保资源被释放，例如文件和锁。如果在处理一个 panic 的过程中，又发生了另一个 panic，可能会导致 defer 语句中剩余的清理代码无法执行，从而引起资源泄漏。 控制流程清晰： panic 和 recover 的设计使得错误的控制流程清晰且可预测。一旦一个 panic 被 recover 捕获，程序可以选择是否继续执行，或者是通过重新 panic 来终止程序。这种决策过程在多个 panic 情况下会变得复杂且难以管理。 因此，在 Go 的设计中，不允许同时存在多个活跃的 panic。一旦发生 panic，它必须被 recover 处理，否则程序将会终止。这确保了错误处理的清晰性和程序的稳定性。 defer 放在哪 defer 实际上不一定是放在栈上的，截止 Go1.22，defer 其实用 3 种分配策略： 堆上分配 栈上分配 开放编码 执行机制 在 ssa.go 文件中，我们可以找到 state.stmt()，这个函数是负责在 Go 程序编译过程中中间代码生成阶段时对不同语句的处理过程，其中对于 ODEFER 即 defer 语句的处理逻辑如下： // stmt converts the statement n to SSA and adds it to s.func (s *state) stmt(n ir.Node) s.stmtList(n.Init())\tswitch n.Op() case ir.ODEFER: n := n.(*ir.GoDeferStmt) if base.Debug.Defer 0 var defertype string if s.hasOpenDefers defertype = open-coded else if n.Esc() == ir.EscNever defertype = stack-allocated else defertype = heap-allocated base.WarnfAt(n.Pos(), %s defer, defertype) ... 可以看到，总共有 3 种分配策略： open-coded: s.hasOpenDefers == true stack-allocated: n.Esc() == ir.EscNever heap-allocated: 默认 默认是堆分配，在 Go1.13 以前，也只有堆分配这一种策略，不过该实现的性能较差。Go 语言在 1.13 中引入栈上分配的结构体，减少了 30% 的额外开销，并在 1.14 中引入了基于开放编码的 defer，使得该关键字的额外开销几乎可以忽略不计。 本文中不对具体的分配机制进行分析，这一块会比较复杂，笔者本身也不是很感兴趣，便决定对此不过分深究，感兴趣的读者推荐详细阅读《Go 语言设计与实现》中关于 defer 关键字的分析：https://draveness.me/golang/docs/part2-foundation/ch05-keyword/golang-defer/。 本文只讨论什么情况下会使用什么分配策略。由于堆分配是默认的，我们就不作分析了，具体来看看 s.hasOpenDefers == true 和 n.Esc() == ir.EscNever 什么时候会成立。 栈上分配 我们先来看栈上分配，要满足栈上分配，则需要满足 n.Esc() == ir.EscNever。 const (\tEscUnknown = iota\tEscNone // Does not escape to heap, result, or parameters.\tEscHeap // Reachable from the heap\tEscNever // By construction will not escape.) 当 n 的逃逸分析结果是 ir.EscNever，则表明该 defer 语句从不逃逸（不会在函数调用结束后仍然被引用），这种情况下 defer 将被分配到栈上（stack-allocated）。否则，如果 defer 逃逸了，就会被分配到堆上（heap-allocated）。 那 defer 语句什么时候会逃逸呢？ 在 Go 中，一个变量的逃逸意味着它的生命周期超出了当前函数的范围。在函数内定义的变量通常分配在栈上，而在堆上分配内存需要更复杂的管理。在一些情况下，编译器可能会选择将变量分配在堆上，这种情况下我们称之为逃逸。 对于 defer 语句，如果它引用了函数外的变量，这个 defer 就会逃逸。例如： var x = 10func someFunction() defer func() fmt.Println(x) // 这里引用了外部变量 x () 在这个例子中，defer 函数内部引用了 x 这个外部变量，因此 defer 语句需要确保 x 在 defer 函数执行时仍然有效。为了满足这个条件，编译器可能会将 x 分配在堆上，而不是栈上。 开放编码 先给结论，在开发过程中，要使用开放编码策略，你只需要关注以下 4 点即可： 函数的 defer 数量不能超过 8 个； 函数的 defer 关键字不能在循环中执行； 函数的 defer 中不能发生逃逸； 函数的 return 语句与 defer 语句的乘积小于或者等于 15 个； Ok，下面是具体的分析过程。 借助 Goland 的能力，将鼠标光标放在 s.hasOpenDefers 上，按住 Command 加点击鼠标，可以看到该属性的使用情况： s.hasOpenDefers 可以看到该属性的判断逻辑都在 ssa.go 文件中的 buildssa() 函数中。去掉一些无关的代码，核心逻辑如下： // buildssa builds an SSA function for fn.// worker indicates which of the backend workers is doing the processing.func buildssa(fn *ir.Func, worker int) *ssa.Func ... // ①\ts.hasOpenDefers = base.Flag.N == 0 s.hasdefer !s.curfn.OpenCodedDeferDisallowed()\tswitch // ②\tcase base.Debug.NoOpenDefer != 0: s.hasOpenDefers = false\tcase s.hasOpenDefers (base.Ctxt.Flag_shared || base.Ctxt.Flag_dynlink) base.Ctxt.Arch.Name == 386: // ③ // Dont support open-coded defers for 386 ONLY when using shared // libraries, because there is extra code (added by rewriteToUseGot()) // preceding the deferreturn/ret code that we dont track correctly. s.hasOpenDefers = false // ④\tif s.hasOpenDefers len(s.curfn.Exit) 0 // Skip doing open defers if there is any extra exit code (likely // race detection), since we will not generate that code in the // case of the extra deferreturn/ret segment. s.hasOpenDefers = false // ⑤\tif s.hasOpenDefers // Similarly, skip if there are any heap-allocated result // parameters that need to be copied back to their stack slots. for _, f := range s.curfn.Type().Results().FieldSlice() if !f.Nname.(*ir.Name).OnStack() s.hasOpenDefers = false break // ⑥\tif s.hasOpenDefers s.curfn.NumReturns*s.curfn.NumDefers 15 // Since we are generating defer calls at every exit for // open-coded defers, skip doing open-coded defers if there are // too many returns (especially if there are multiple defers). // Open-coded defers are most important for improving performance // for smaller functions (which dont have many returns). s.hasOpenDefers = false ...\treturn s.f 可以看到总共有 6 个条件，我已在注释中进行标注，我们来进行逐一分析： ① base.Flag.N == 0 s.hasdefer !s.curfn.OpenCodedDeferDisallowed() 如果base.Flag.N 等于 0 且当前函数有延迟调用且没有禁止开放式延迟，那么设置s.hasOpenDefers为true。 在 Go 编译器中，-N标志通常用于禁用优化。在这段代码中，如果base.Flag.N等于 0，意味着没有禁用优化，因此编译器可能会尝试使用更高级的优化技术，比如开放式延迟（open-coded defers）。 OpenCodedDeferDisallowed() 即禁用开放编码，它的实现如下： const funcOpenCodedDeferDisallowed // cant do open-coded defersfunc (f *Func) OpenCodedDeferDisallowed() bool return f.flagsfuncOpenCodedDeferDisallowed != 0 按住 Command 后点击 funcOpenCodedDeferDisallowed 可以看到只有 funcOpenCodedDeferDisallowed(b) 可以修改它的值。 funcOpenCodedDeferDisallowed 我们来看看哪个地方会调用 funcOpenCodedDeferDisallowed()，并将 funcOpenCodedDeferDisallowed 设置为 true： 将 funcOpenCodedDeferDisallowed 设置为 true 的地方 调用它的地方在 stmt.go 文件中的 walkStmt() 函数，具体如下： // The max number of defers in a function using open-coded defers. We enforce this// limit because the deferBits bitmask is currently a single byte (to minimize code size)const maxOpenDefers = 8// The result of walkStmt MUST be assigned back to n, e.g.////\tn.Left = walkStmt(n.Left)func walkStmt(n ir.Node) ir.Node ...\tswitch n.Op() ... case ir.ODEFER: n := n.(*ir.GoDeferStmt) ir.CurFunc.SetHasDefer(true) ir.CurFunc.NumDefers++ if ir.CurFunc.NumDefers maxOpenDefers // Dont allow open-coded defers if there are more than // 8 defers in the function, since we use a single // byte to record active defers. ir.CurFunc.SetOpenCodedDeferDisallowed(true) if n.Esc() != ir.EscNever // If n.Esc is not EscNever, then this defer occurs in a loop, // so open-coded defers cannot be used in this function. ir.CurFunc.SetOpenCodedDeferDisallowed(true) fallthrough ... ... 第一点是：当前函数中 defer 个数超过 8 的话，则禁用开放编码。 第二点是当 n.Esc() != ir.EscNever 使，就禁用开放编码。这个要求跟前面分析的“栈上分配”要求是一样的。 这里再补充一点：什么时候 n.Esc() 会被设置为 ir.EscNever 呢？ n.SetEsc(ir.EscNever) 这里面核心点是第一个，它对应的代码如下： func (e *escape) goDeferStmt(n *ir.GoDeferStmt) k := e.heapHole()\tif n.Op() == ir.ODEFER e.loopDepth == 1 ... n.SetEsc(ir.EscNever) ... e.loopDepth == 1 时就设置，换言之，defer 不在循环中的时候，才允许开放编码。 总而言之，第 ① 个条件约束了要采用 open-coded 开放编码策略的 3 个条件： 函数中 defer 个数不能超过 8； defer 不能在循环中； defer 不能发生逃逸。 ② base.Debug.NoOpenDefer != 0 如果base.Debug.NoOpenDefer不为 0，那么禁用开放式延迟。 NoOpenDefer int `help:disable open-coded defers concurrent:ok` ③ (base.Ctxt.Flag_shared || base.Ctxt.Flag_dynlink) base.Ctxt.Arch.Name == \"386\" 如果当前架构是386，并且使用共享库或动态链接，那么不支持开放式延迟，因为存在一些额外的代码（由rewriteToUseGot()添加）可能无法正确追踪。 ④ len(s.curfn.Exit) 如果存在任何额外的退出代码（比如可能是竞态检测相关的代码），则跳过开放式延迟。 ⑤ !f.Nname.(*ir.Name).OnStack() 如果有任何堆分配的结果参数需要复制回它们的栈槽，也跳过开放式延迟。 ⑥ s.curfn.NumReturns*s.curfn.NumDefers 15 如果函数的返回数乘以延迟调用数大于 15，考虑到每个退出点都要生成延迟调用，并且开放式延迟对于小函数（没有多个返回）的性能提升最为重要，所以在这种情况下也不使用开放式延迟。 堆上分配 当不满足开放编码和栈上分配的时候，默认就是堆上分配（heap-allocated），性能最差，这里不做分析。 以上就是本文关于 Go 语言中 defer 关键字的具体分析，Happy Coding! Peace~ 参考 深入理解 Go 语言 Go 语言底层原理剖析 Go 语言设计与实现 ChatGPT4","tags":["go"],"categories":["go"]},{"title":"深入 Go 语言核心：结构体的全方位解析","path":"/2024/03/09/go-struct/","content":"Go 语言，作为一种高效、静态类型的编程语言，自其问世以来便以其并发处理能力和简洁的语法结构广受开发者欢迎。虽然 Go 不是传统意义上的面向对象语言，它却以独特的方式支持面向对象编程的核心概念，其中结构体扮演了非常关键的角色。 结构体在 Go 语言中是一种复合数据类型，允许我们将不同类型的数据聚合到一起。它不仅提高了数据管理的效率和逻辑清晰度，还是 Go 语言中实现面向对象编程思想如封装、组合等概念的基石。了解和掌握结构体的使用，对于深入理解 Go 语言的特性和编写高效、可维护的 Go 代码至关重要。 本文将带您全面深入地探索 Go 语言中结构体的各个方面，从基本定义、初始化和使用，到高级特性如结构体的组合、方法定义、内存对齐等，每一个细节都将一一展开。无论您是 Go 语言的新手，还是有一定经验的开发者，相信本文都能为您提供有价值的见解和帮助。让我们一起探索 Go 结构体的奥秘，揭开其背后的原理，优化我们的代码结构，提升编程效率。 版本声明 Go 1.22.1 gopkg.in/yaml.v3 v3.0.1 os: m2max 全文概览 Go 语言结构体 1. 结构体的基本使用 1.1 定义结构体 结构体类型的定义形式如下： type T struct Field T1, Field T2, .... FieldN Tn, 比如： type Person struct Name string Age int ExtraInfo map[string]interface 结构体内部，也可以内嵌匿名结构体，如： type Person struct Name string Age int School struct Name string Address string Phone string 但是！注意，如果 Person 中包含了 Person 呢？ type Person struct person Person 这里会报错：不允许引用自身。 ./main.go:5:6: invalid recursive type: Person refers to itself 这是因为 Go 语言在编译时需要知道每个类型的确切大小，以便正确地分配内存。但在这个定义中，因为 Person 包含自身，编译器无法确定 Person 的大小，因此会报错。 如果你需要在一个结构体中引用相同类型的数据，你应该使用指针。指针的大小是固定的，因此编译器可以确定结构体的大小。 type Person struct person *Person 1.2 初始化结构体 假设我们有以下结构体： type Person struct Name string\tAge int\tExtraInfo map[string]interface 可以有以下几种初始化： // 逐个字段赋值，顺序不重要，也可以只赋值部分字段person1 := Person Age: 18, Name: hedon, ExtraInfo: make(map[string]interface),fmt.Println(person1) // hedon 18 map[]// 可以不指定字段，严格按照顺序person2 := Personhedon2, 19, make(map[string]interface)fmt.Println(person2) // hedon2 19 map[]// 默认初始化，则结构体中的每个字段都会被默认赋予其对应类型的“零值”var person3 Personfmt.Println(person3) // 0 map[]fmt.Println(person3.ExtraInfo == nil) // true// 也可以使用 new() 或 来初始化并返回指针person3 := new(Person)fmt.Println(person3) // 0 map[] 1.3 空结构体 有一种特殊的结构体，它一个字段都没有，我们称之为“空结构体”： type Empty struct 空结构体非常特殊，它不占据任何空间！你可以自己验证一下： type Empty structfunc main() fmt.Println(the size of empty:, unsafe.Sizeof(Empty)) // the size of empty: 0 而且，所有空结构体的地址都一样： type Empty structtype Empty1 structfunc main() e := Empty\te1 := Empty1\tfmt.Printf(the address of empty: %p , e) // the address of empty: 0x10460f520\tfmt.Printf(the address of empty1: %p , e1) // the address of empty1: 0x10460f520 这是因为 Go 语言为所有大小为 0 的变量都指向了同一个值： // base address for all 0-byte allocationsvar zerobase uintptr 好处就是减少了内存的浪费。典型的用法就是我们可以使用 map 来实现 Set，这样就只花费了存储键的空间，而值不占用任何空间。 type Set = map[string]struct 1.4 访问和修改结构体 结构体属性的可见性跟 Go 包的可见性规则一样：大写对包外可见，小写仅包内可见。 使用 . 访问和修改结构体中的属性。 Go 语言中只有“值传递”，所以如果你要将结构体示例传入一个 func 进行修改，则需要传入其引用。 type Person struct Name string\tAge intfunc main() p := PersonName: hedon, Age: 18\tUpdatePersonName(p)\tfmt.Println(1:, p)\tUpdatePersonNameWithRef(p)\tfmt.Println(2:, p)func UpdatePersonName(p Person) p.Name = hedon-1func UpdatePersonNameWithRef(p *Person) p.Name = hedon-2 输出： 1: hedon 182: hedon-2 18 2. 结构体的高级特性 2.1 结构体组合 在 Go 语言中，倡导的是“组合优于继承”的哲学，即倡导使用组合而不是继承来实现代码的复用。该理念鼓励开发者通过组合和接口来构建灵活、可维护的代码，而不是依赖于更严格、更易出错的继承关系。这种方式促进了代码的解耦，增强了代码的灵活性和可重用性，同时也使得代码更加清晰和易于理解。 在 Go 中，组合是通过将一个或多个类型（通常是结构体）嵌入到另一个结构体中来实现的。这使得嵌入的类型的方法被“提升”到包含它的结构体中，允许你调用这些方法就像它们是外部结构体的一部分一样。 type Engine struct Power intfunc (e *Engine) Start() // 启动引擎的逻辑type Car struct Engine // 通过组合的方式嵌入 Engine// 现在 Car 可以直接调用 Start 方法car := CarEnginePower: 100car.Start() // 调用的是 Engine 的 Start 方法 2.2 结构体的方法 假设我们定义了一个结构体 Person： type Person struct Name string 在 Go 中，你可以为结构体的值或指针实现特定的方法： func(p Person) SetName(name string) string p.Name = name func(p *Person) SetName(name string) string p.Name = name 这两者最核心的区别是：当你为结构体的指针类型定义方法时，该方法会在原始结构体实例上操作。这意味着方法内部对结构体的任何修改都会影响到原始结构体。 所以这两段代码的输出是不一样的： func main() p := PersonName: hedon\tp.SetName(new_name)\tfmt.Println(p.Name) // name_namefunc (p *Person) SetName(name string) p.Name = name func main() p := PersonName: hedon\tp.SetName(new_name)\tfmt.Println(p.Name) // hedonfunc (p Person) SetName(name string) p.Name = name 但是这里我想再补充两个小点。请先思考一下下面这两段代码是否可以编译通过？如果可以输出是什么？ func main() p := PersonName: hedon\tp.SetName(new_name)\tfmt.Println(person name after set:, p.Name)\tpt := reflect.TypeOf(p)\tfmt.Println(the number of persons method: , pt.NumMethod())\tp2 := Person\tpt = reflect.TypeOf(p2)\tfmt.Println(the number of persons method: , pt.NumMethod())func (p *Person) SetName(name string) p.Name = name func main() p := PersonName: hedon\tp.SetName(new_name)\tfmt.Println(person name after set:, p.Name)\tpt := reflect.TypeOf(p)\tfmt.Println(the number of persons method: , pt.NumMethod())\tp2 := Person\tpt = reflect.TypeOf(p2)\tfmt.Println(the number of persons method: , pt.NumMethod())func (p Person) SetName(name string) p.Name = name 很明显这两段代码的唯一区别就是，第一段代码我们是为 *Person 实现了 SetName 方法，而第二段代码我们是为 Person 实现了 SetName 方法。两段代码我们都打印了调用 SetName 后 p.name 的值，以及利用方式分别获取 Person 和 *Person 实现的方法个数。 第一段代码的输出如下： person name after set: new_namethe number of persons method: 0the number of persons method: 1 第二段代码的输出如下： person name after set: hedonthe number of persons method: 1the number of persons method: 1 这里我们可以得出 2 个结论： ① 结构体的修改依赖于方法接收器的类型： 当方法的接收器为值类型（Person）时，对结构体的修改不会影响原始结构体实例，因为方法作用于结构体的副本上。 当方法的接收器为指针类型（*Person）时，对结构体的修改会影响原始结构体实例，因为方法作用于结构体的引用上。 ② 方法集依赖于接收器的类型： 为值类型（Person）实现的方法，既属于值类型也属于指针类型（*Person）的方法集。 为指针类型（*Person）实现的方法，只属于指针类型的方法集。 对于 ②，我们可以通过 Plan9 汇编代码一探究竟。 我们为第一段代码执行以下命令： go build -gcflags -S main.go 在输出的最上面，可以看到只有 main.(*Person).GetName。 # command-line-argumentsmain.main STEXT size=128 args=0x0 locals=0x48 funcid=0x0 align=0x0 ...main.(*Person).GetName STEXT size=16 args=0x8 locals=0x0 funcid=0x0 align=0x0 leaf ... 我们再来为第二段代码执行相同的命令。可以在输出的最上面，看到不仅有 main.Person.GetName，还可以发现编译器自动帮我们生成了 main.(*Person).GetName。 # command-line-argumentsmain.main STEXT size=480 args=0x0 locals=0xe8 funcid=0x0 align=0x0\t...main.Person.SetName STEXT size=16 args=0x28 locals=0x0 funcid=0x0 align=0x0 leaf ...main.(*Person).SetName STEXT dupok size=128 args=0x18 locals=0x8 funcid=0x16 align=0x0\t... 对于 ②，笔者其实有一个不太理解的地方，比如下面这段代码： func main() p := PersonName: hedon\tp.SetName(new_name)\tfmt.Println(person name after set:, p.Name) // hedonfunc (p Person) SetName(name string) p.Name = name 这里 p 是引用类型，下面实现的是 Person.SetName，按照我们上面的结论，编译器会自动帮我们实现 (*Person).SetName。按照这种思路，输出 new_name 也是解释得通的。因为既然我们声明的是一个引用类型，那么 p 完全可以去调用自动生成的 (*Person).SetName。但是最终的结果还是输出 hedon，所以这里编译器自动帮我们将 p 进行解引用，然后调用了 Person.SetName。 这是比较困扰笔者的一个地方，欢迎评论区讨论~ 可能编译器还是更希望对于开发者来说“所见即所得”，既然开发者实现的是 Person.SetName，那么对于开发者来说，应该就是希望不影响原始结构体的值，所以编译器还是选择遵循这种“意愿”，不乱操作。 2.3 结构体比较 Go 允许直接比较两个结构体实例，但有一定的限制： 可比较性：只有当结构体中的所有字段都是可比较的时，结构体才是可比较的。基本数据类型（如 int、string 等）是可比较的，但切片、映射、函数等类型不可比较。 相等性检测：当两个结构体的对应字段都相等时，这两个结构体被认为是相等的。可以使用 == 和 != 操作符来进行比较。 下面这段示例，p3==p4 返回了 true，这符合我们上面总结的结论。p1==p2 返回了 false，因为这其实不是结构体之间的比较了，这是指针的比较了。 p1 := PersonName: hedon, Age: 18p2 := PersonName: hedon, Age: 18fmt.Println(p1 == p2) // falsep3 := PersonName: hedon, Age: 18p4 := PersonName: hedon, Age: 18fmt.Println(p3 == p4) // true 结构体的比较只支持 == 和 !=，不支持 和 等其他运算符的比较。而 Go 语言又不支持比较符重载。所以如果你要比较两个结构体的大小，那么只能自行封装类型 compare 的函数。在这我们排序结构体数组或切片的时候，经常使用到，比如我们希望按 Age 字段从小到大排序： sort.Slice(persons, func(i, j int) bool return persons[i].Age persons[j].Age) 2.4 结构体复制 在 Go 中，结构体也是值类型，这意味着当它们被赋值给新的变量或作为函数参数传递时，实际上是进行了一次深拷贝： 值复制：当将一个结构体赋值给一个新变量时，新变量会获得原始结构体的一个副本，它们在内存中占有不同的位置。 独立性：因为是深拷贝，所以原始结构体和副本结构体是完全独立的；修改其中一个不会影响另一个。 type Point struct X, Y intoriginal := Point1, 2copy := originalcopy.X = 3fmt.Println(original) // 1, 2fmt.Println(copy) // 3, 2 3. 结构体与接口 在 Go 语言中，如果一个类型实现了接口中所有的方法，则这个类型就实现了该接口。关于接口部分的知识点，比如接口定义、多态和断言等，本文就不赘述了。 在这里我主要想从另外一个角度继续来验证前面我们总结的：为值类型（Person）实现的方法，既属于值类型也属于指针类型（*Person）的方法集。 请看这段代码： package mainimport fmttype Person interface GetName() stringtype Man struct Name stringfunc (m Man) GetName() string return m.Namefunc PrintPersonName(p Person) fmt.Println(p.GetName())func main() m1 := ManName: hedon1\tPrintPersonName(m1)\tm2 := ManName: hedon2\tPrintPersonName(m2) 这段代码我们定义了 Person 接口，它只有一个方法 GetName。然后我们定义了一个结构体 Man，并为它的值类型实现了 Person 接口。通过我们上面的结论，这里 Man 和 *Man 其实都实现了 Person 接口，所以上面的代码是可以编译通过的。 如果改成为指针类型实现接口呢？你可以试一下~ func (m *Man) GetName() string return m.Name 4. 泛型结构体 Go 语言在其 1.18 版本中引入了泛型支持，这包括了对泛型结构体的支持。通过使用泛型，你可以创建更灵活和可重用的数据结构和函数。 type Container[T any] struct items []T 可以看到 Go 语言用 [] 来实现泛型，而不像其他语言一样用 ，真是喜欢搞特殊啊 🤡，又丑又容易跟 map 和 slice 混淆。 5. 结构体的标签（Tag） 在结构体字段后面，我们可以用 `` 来指定标签，这允许我们对结构体定制化一些常用操作，最经典的就是序列化与反序列化。 5.1 序列化与反序列化 对于常见的数据结构，如 json、yaml、xml 或 toml，我们都可以通过在结构体中指定标签，然后使用对应解析库进行序列化和反序列化。比如： package mainimport (\tencoding/json\tfmt)type Person struct Name string `json:name`\tAge int `json:age`func main() p := PersonName: hedon, Age: 18\tbs, _ := json.Marshal(p) // 序列化\tfmt.Println(string(bs)) // name:hedon,age:18\tnewP := Person\t_ = json.Unmarshal(bs, newP) // 反序列化\tfmt.Println(newP) // hedon 18 在笔者的实践过程中，在结构体组合的场景下，不同数据格式的解析会有一些小差别，这在实战过程中你需要重点关注和验证。比如 json 和 yaml 就会有一些不同。 比如说我这里定义了下面 2 个结构体，其中 Person 组合了 School： type Person struct Name string `json:name yaml:name`\tAge int `json:age yaml:age`\tSchooltype School struct SchoolName string `json:school_name yaml:school_name`\tSchoolAddress string `json:school_address json:school_address` 它们都加上了 json 和 yaml 标签，对于 json 类型，你可以用标准库的 encoding/json 来进行序列化和反序列化，而 yaml 你可以使用第三方库：go-yaml。 先来看系列化结果： func main() p := PersonName: hedon, Age: 18, School: SchoolSchoolName: nb_school, SchoolAddress: a_good_school_place\tbs, _ := json.Marshal(p)\tfmt.Println(json: , string(bs))\tbs, _ = yaml.Marshal(p)\tfmt.Println(yaml: , string(bs)) 输出如下： json: name:hedon,age:18,school_name:nb_school,school_address:a_good_school_placeyaml: name: hedon age: 18 school: school_name: nb_school school_address: a_good_school_place 通过观察你可以发现哈，在 json 中，组合的时候（没有给 School 加标签）直接将 School 平铺在 Person 中，所以在序列化的结果中，找不到 \"school\": 。而在 yaml 中，并不是直接平铺的。 这个区别在你解析配置文件的时候尤其重要，如果不注意，那么可能会导致配置解析失败。 我准备了 4 个配置文件，分别是： // person1.json name: hedon_json, age: 18, school: school_name: nb_json_school, school_address: a_good_place_in_json # person1.yamlname: hedon_yamlage: 18school: school_name: nb_yaml_school school_address: a_good_price_in_yaml // person2.json name: hedon_json, age: 18, school_name: nb_json_school, school_address: a_good_place_in_json # person2.yamlname: hedon_yamlage: 18school_name: nb_yaml_schoolschool_address: a_good_price_in_yaml 解析代码如下： func main() filenames := []stringperson1.json, person1.yaml, person2.json, person2.yaml\tfor i, fn := range filenames bs := readFileIntoBytes(fn) p := Person if i%2 == 0 _ = json.Unmarshal(bs, p) else _ = yaml.Unmarshal(bs, p) fmt.Printf(%s - %v , fn, p)\tfunc readFileIntoBytes(filename string) []byte f, err := os.Open(filename)\tif err != nil panic(err) bs, _ := io.ReadAll(f)\treturn bs 输出： person1.json - hedon_json 18 person1.yaml - hedon_yaml 18 nb_yaml_school a_good_price_in_yamlperson2.json - hedon_json 18 nb_json_school a_good_place_in_jsonperson2.yaml - hedon_yaml 18 如果给 School 字段加上 json tag 的话，结果又是不同： type Person struct Name string `json:name yaml:name`\tAge int `json:age yaml:age`\tSchool `json:school yaml:school` 输出： person1.json - hedon_json 18 nb_json_school a_good_place_in_jsonperson1.yaml - hedon_yaml 18 nb_yaml_school a_good_price_in_yamlperson2.json - hedon_json 18 person2.yaml - hedon_yaml 18 可以看到受影响的只有 json。 到这里我们可以总结：在组合场景下，如果不明确指定 tag，yaml 解析期望字段是嵌套的，而 json 解析期望字段是平铺的。 5.2 自定义 Tag 在 Go 中，你可以为结构体字段定义任意的标签。这些标签在编译时会被存储，并且可以在运行时通过反射（reflection）来访问。 假设我们定义一个名为 check 的标签，它用于我们对结构体字段的检查，假设我们这个标签支持以下功能： check:\"strnoempty\": 字符串不可以为空。 假如加入 check 标签的 Person 结构体如下： type Person struct Name string `check:strnoempty` 我们来为 check 实现解析函数： func CheckPerson(p Person) error pt := reflect.TypeOf(p)\tpv := reflect.ValueOf(p)\tfor i := 0; i pt.NumField(); i++ field := pt.Field(i) tagValue := field.Tag.Get(check) if tagValue == continue if field.Type.Kind() == reflect.String tagValue == strnoempty if err := checkStrNoEmpty(field.Name, pv.Field(i).Interface()); err != nil return err return nilfunc checkStrNoEmpty(fieldName string, v any) error s, ok := v.(string)\tif !ok return fmt.Errorf(%v is not string, v) if s == return fmt.Errorf([check] %s should not be empty, fieldName) return nil 测试如下： func main() p1 := Person\tp2 := PersonName: hedon\tfmt.Println(CheckPerson(p1)) // [check] Name should not be empty\tfmt.Println(CheckPerson(p2)) // nil 6. 结构体内存对齐 在本小节中，我们将探讨 Go 语言结构体的内存结构和对齐策略。 6.1 问题引出 思考下面这段代码的输出： type S1 struct num2 int8\tnum1 int16\tflag booltype S2 struct num1 int8\tflag bool\tnum2 int16func main() fmt.Println(unsafe.Sizeof(S1))\tfmt.Println(unsafe.Sizeof(S2)) 为什么仅是字段顺序不同，S1 和 S2 的大小就不一样了？ 我们可以写个简单的程序来输出 S1 和 S2 的内存结构： func main() s1 := S1\ts2 := S2\tfmt.Print(s1: )\tprintMemory(s1)\tfmt.Print(s2: )\tprintMemory(s2)func printMemory(a any) t := reflect.TypeOf(a)\tmem := make([]int, int(t.Size()))\tfor i := 0; i t.NumField(); i++ field := t.Field(i) offset := int(field.Offset) size := int(field.Type.Size()) for j := 0; j size; j++ mem[j+offset] = i + 1 fmt.Println(mem) 输出： s1: [1 0 2 2 3 0]s2: [1 2 3 3] 其中 1、2、3 分别替代结构体中的第 1/2/3 个字段所占用的内存。这里可以看到 s1 的长度是 6 字节，而 s2 是 4 字节。这里 s1 比 s2 多出的 2 个字节就是这两个填充的 0。这而 2 个字节的填充，就是为了内存对齐。 6.2 内存对齐 如上分析，s1 的内存结构如下： s1 内存结构 如果没有内存对齐呢？s1 的结构可能如下： 没有内存对齐的 s1 内存结构 如果是 16 位系统的话，那么没有内存对齐的情况下，要访问 s1.num2 字段，就需要跨过 2 个系统字长的内存，效率就低了。具体来说，内存对齐是计算机内存分配的一种优化方式，用于确保数据结构的存储按照特定的字节边界对齐。这种对齐是为了提高计算机处理数据的效率。 6.3 对齐系数 对齐系数：变量的内存地址必须被对齐系数整除。 unsafe.Alignof(): 可以查看值在内存中的对齐系数。 6.4 基本类型对齐 fmt.Printf(bool size: %d, align: %d , unsafe.Sizeof(bool(true)), unsafe.Alignof(bool(true)))fmt.Printf(byte size: %d, align: %d , unsafe.Sizeof(byte(0)), unsafe.Alignof(byte(0)))fmt.Printf(int8 size: %d, align: %d , unsafe.Sizeof(int8(0)), unsafe.Alignof(int8(0)))fmt.Printf(int16 size: %d, align: %d , unsafe.Sizeof(int16(0)), unsafe.Alignof(int16(0)))fmt.Printf(int32 size: %d, align: %d , unsafe.Sizeof(int32(0)), unsafe.Alignof(int32(0)))fmt.Printf(int64 size: %d, align: %d , unsafe.Sizeof(int64(0)), unsafe.Alignof(int64(0))) 输出： bool size: 1, align: 1byte size: 1, align: 1int8 size: 1, align: 1int16 size: 2, align: 2int32 size: 4, align: 4int64 size: 8, align: 8 结论：基本类型的对齐系数跟它的长度一致。 基本类型内存对齐 6.5 结构体内部对齐 结构体内存对齐分为内部对齐和结构体之间对齐。 我们先来看结构体内部对齐： 指的是结构体内部成员的相对位置（偏移量）； 每个成员的偏移量是 自身大小 和 对齐系数 的较小值的倍数 type Demo struct a bool b string c int16 假如我们定义了上面的结构体 Demo，如果在 64 位系统上（字长为 8 字节）通过上面的规则，可以判断出：（单位为字节） a: size=1, align=1 b: size=16, align=8 c: size=2, align=2 Demo 内存结构 当然我们也可以通过程序输出来验证： type Demo struct a bool // size=1, align=1\tb string // size=16, align=8\tc int16 // size=2, align=2func main() d := Demo\tfmt.Printf(a: size=%d, align=%d , unsafe.Sizeof(d.a), unsafe.Alignof(d.a))\tfmt.Printf(b: size=%d, align=%d , unsafe.Sizeof(d.b), unsafe.Alignof(d.b))\tfmt.Printf(c: size=%d, align=%d , unsafe.Sizeof(d.c), unsafe.Alignof(d.c))\tprintMemory(d)func printMemory(a any) t := reflect.TypeOf(a)\tmem := make([]int, int(t.Size()))\tfor i := 0; i t.NumField(); i++ field := t.Field(i) offset := int(field.Offset) size := int(field.Type.Size()) for j := 0; j size; j++ mem[j+offset] = i + 1 fmt.Println(mem) 输出： a: size=1, align=1b: size=16, align=8c: size=2, align=2[1 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 0 0 0 0 0 0] 6.6 结构体长度填充 上面 Demo 结构体最后还填了 6 个字节的 0，这就是结构体长度填充： 结构体通过填充长度，来对齐系统字长。 结构体长度是 最大成员长度 和 系统字长 较小值的整数倍。 我的系统环境是 m2max，系统字长是 8 字节，Demo 最大成员长度是 b string，即 16 个字节，所以 Demo 的长度应该是 8 的倍数，所以最后填充了 6 个字节的 0。 6.7 结构体之间对齐 结构体之间对齐，是为了确定结构体的第一个成员变量的内存地址，以让后面的成员地址都合法。 结构体的对齐系数是 其成员的最大对齐系数； 6.8 空结构体对齐 前面我们专门讨论了空结构体 struct，它们的内存地址统一指向 zerobase，而且内存长度为 0。这也导致了它的内存对齐规则，有一些不同。具体可以分为以下 4 个情况。 6.8.1 空结构体单独存在 空结构体单独存在时，其内存地址为 zerobase，不额外分配内存。 6.8.2 空结构体在结构体最前 空结构体是结构体第一个字段时，它的地址跟结构体本身及结构体第 2 个字段一样，不占据内存空间。 type TestEmpty struct empty struct\ta bool\tb stringfunc main() te := TestEmpty\tfmt.Printf(address of te: %p , te)\tfmt.Printf(address of te.empty: %p , (te.empty))\tfmt.Printf(address of te.a: %p , (te.a))\tfmt.Printf(empty: size=%d, align=%d , unsafe.Sizeof(te.empty), unsafe.Alignof(te.empty))\tfmt.Printf(a: size=%d, align=%d , unsafe.Sizeof(te.a), unsafe.Alignof(te.a))\tfmt.Printf(b: size=%d, align=%d , unsafe.Sizeof(te.b), unsafe.Alignof(te.b))\tprintMemory(te) 输出： address of te: 0x140000ba000address of te.empty: 0x140000ba000address of te.a: 0x140000ba000empty: size=0, align=1a: size=1, align=1b: size=16, align=8[2 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3] 6.8.3 空结构体在结构体中间 空结构体出现在结构体中时，地址跟随前一个变量。 空结构体在结构体中间内存对齐 type TestEmpty struct a bool\tempty struct\tb stringfunc main() te := TestEmpty\tfmt.Printf(address of te: %p , te)\tfmt.Printf(address of te.a: %p , (te.a))\tfmt.Printf(address of te.empty: %p , (te.empty))\tfmt.Printf(a: size=%d, align=%d , unsafe.Sizeof(te.a), unsafe.Alignof(te.a))\tfmt.Printf(empty: size=%d, align=%d , unsafe.Sizeof(te.empty), unsafe.Alignof(te.empty))\tfmt.Printf(b: size=%d, align=%d , unsafe.Sizeof(te.b), unsafe.Alignof(te.b))\tprintMemory(te) 输出： address of te: 0x14000128000address of te.a: 0x14000128000address of te.empty: 0x14000128001a: size=1, align=1empty: size=0, align=1b: size=16, align=8[1 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3] 6.8.4 空结构体在结构体最后 空结构体出现在结构体最后，如果开启了一个新的系统字长，则需要补零，防止与其他结构体混用地址。 空结构体在结构体最后内存对齐 type TestEmpty struct a bool\tb string\tempty structfunc main() te := TestEmpty\tfmt.Printf(address of te: %p , te)\tfmt.Printf(address of te.a: %p , (te.a))\tfmt.Printf(address of te.empty: %p , (te.empty))\tfmt.Printf(a: size=%d, align=%d , unsafe.Sizeof(te.a), unsafe.Alignof(te.a))\tfmt.Printf(b: size=%d, align=%d , unsafe.Sizeof(te.b), unsafe.Alignof(te.b))\tfmt.Printf(empty: size=%d, align=%d , unsafe.Sizeof(te.empty), unsafe.Alignof(te.empty))\tprintMemory(te) 输出： address of te: 0x1400006a020address of te.a: 0x1400006a020address of te.empty: 0x1400006a038a: size=1, align=1b: size=16, align=8empty: size=0, align=1[1 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0] 6.9 使用 fieldalignment -fix 工具优化结构体内存对齐 还记得我们最开始提出的问题吗？ type S1 struct num2 int8\tnum1 int16\tflag booltype S2 struct num1 int8\tflag bool\tnum2 int16func main() fmt.Println(unsafe.Sizeof(S1))\tfmt.Println(unsafe.Sizeof(S2)) S1 和 S2 提供的程序功能是一样的，但是 S1 却比 S2 花费了更多的内存空间。所以有时候我们可以通过仅仅调整结构体内部字段的顺序就减少不少的内存空间消耗。在这个时候 fieldalignment 可以帮助我们自动检测并优化。 你可以运行下面命令安装 fieldalignment 命令： go install golang.org/x/tools/go/analysis/passes/fieldalignment/cmd/fieldalignment@latest 然后在项目根目录下运行下面命令，对我们的代码进行检查： go vet -vettool=$(which fieldalignment) ./... 这里会输出： ./main.go:9:9: struct of size 6 could be 4 这个时候可以执行 fieldalignment -fix 目录|文件 ，它会自动帮我们的代码进行修复，但是强烈建议你在运行之前，备份你的代码，因为注释会被删除！ fieldalignment -fix ./... 输出： /Users/hedon/GolandProjects/learn-go-struct/main.go:9:9: struct of size 6 could be 4 这个时候 S1 已经被优化好了： type S1 struct num1 int16\tnum2 int8\tflag bool","tags":["go"],"categories":["go"]},{"title":"Rust 实战丨HTTPie","path":"/2024/03/06/rust-action-httpie/","content":"概述 之前学习过《陈天·Rust 编程第一课 - 04 ｜ get hands dirty：来写个实用的 CLI 小工具》，学的时候迷迷糊糊。后来在系统学习完 Rust 后，重新回过头来看这个实战小案例，基本上都能掌握，并且有了一些新的理解。所以我决定以一个 Rust 初学者的角度，并以最新版本的 Rust（1.7.6）和 clap（4.5.1）来重新实现这个案例，期望能对 Rust 感兴趣的初学者提供一些帮助。 本文将实现的应用叫 HTTPie，HTTPie 是一个用 Python 编写的命令行 HTTP 客户端，其目标是使 CLI 与 web 服务的交互尽可能愉快。它被设计为一个 curl 和 wget 的替代品，提供易于使用的界面和一些用户友好的功能，如 JSON 支持、语法高亮和插件。它对于测试、调试和通常与 HTTP 服务器或 RESTful API 进行交云的开发人员来说非常有用。 HTTPie 的一些关键特性包括： JSON 支持：默认情况下，HTTPie 会自动发送 JSON，并且可以轻松地通过命令行发送 JSON 请求体。 语法高亮：它会为 HTTP 响应输出提供语法高亮显示，使得结果更加易于阅读。 插件：HTTPie 支持插件，允许扩展其核心功能。 表单和文件上传：可以很容易地通过表单上传文件。 自定义 HTTP 方法和头部：可以发送任何 HTTP 方法的请求，自定义请求头部。 HTTPS、代理和身份验证支持：支持 HTTPS 请求、使用代理以及多种 HTTP 身份验证机制。 流式上传和下载：支持大文件的流式上传和下载。 会话支持：可以保存和重用常用的请求和集合。 本文我们将实现其中的 1、2 和 5。我们会支持发送 GET 和 POST 请求，其中 POST 支持设置请求头和 JSON 数据。 在本文中，你可以学习到： 如何用 clap 解析命令行参数。 如何用 tokio 进行异步编程。 如何用 reqwest 发送 HTTP 请求。 如何用 colored 在终端输出带颜色的内容。 如何用 jsonxf 美化 json 字符串。 如何用 anyhow 配合 ? 进行错误传播。 如何使用 HTTPie 来进行 HTTP 接口测试。 在进行实际开发之前，推荐你先了解一下： https://hedon.top/2024/03/02/rust-crate-reqwest/https://hedon.top/2024/03/02/rust-crate-reqwest/ https://hedon.top/2024/03/05/rust-crate-anyhow/https://hedon.top/2024/03/05/rust-crate-anyhow/ https://hedon.top/2024/03/02/rust-crate-clap/https://hedon.top/2024/03/02/rust-crate-clap/ 本文完整代码： https://github.com/hedon-rust-road/httpiehttps://github.com/hedon-rust-road/httpie 开发思路 HTTP 协议 回顾一下 HTTP 协议的请求体和响应体结构。 请求结构： http request structure 响应结构： http response structure 命令分析 在本文中，我们就实现 HTTPie cli 官方的这个示例：即允许指定请求方法、携带 headers 和 json 数据发送请求。 HTTPie 官方示例 我们来拆解一下，这个命令可以分为以下几个部分： httpie METHOD URL [headers | params]... METHOD: 请求方法，本案例中，我们仅支持 GET 和 POST。 URL: 请求地址。 HEADERS: 请求头，格式为 h1:v1。 PARAMS: 请求参数，格式为 k1=v1，最终以 json 结构发送。 效果展示 ➜ httpie git:(master) ✗ ./Httpie --helpUsage: Httpie COMMANDCommands: get post help Print this message or the help of the given subcommand(s)Options: -h, --help Print help -V, --version Print version 其中 post 子命令： Usage: Httpie post URL BODY...Arguments: URL Specify the url you wanna request to BODY... Set the request body. Examples: headers: header1:value1 params: key1=value1Options: -h, --help Print help 请求示例： httpie response demo 思路梳理 httpie 开发思路梳理 第 1 步：解析命令行参数 本案例中 httpie 支持 2 个子命令： get 支持 url 参数 post 支持 url、body 参数，因为其中 headers 和 params 是变长的，我们统一用 VecString 类型的 body 来接收，然后用 : 和 = 来区分它们。 第 2 步：发送请求 使用 reqwest 创建 http client； 设置 url； 设置 method； 设置 headers； 设置 params； 发送请求； 获取响应体。 第 3 步：打印响应 打印 http version 和 status，并使用 colored 赋予蓝色； 打印 response headers，并使用 colored 赋予绿色； 确定 content-type，如果是 json，我们就用 jsonxf 美化 json 串并使用 colored 赋予蓝绿色输出，如果是其他类型，这里我们就输出原文即可。 实战过程 1. 创建项目 cargo new httpie 2. 添加依赖 [package]name = httpieversion = 0.1.0edition = 2021[dependencies]anyhow = 1.0.80clap = version = 4.5.1, features = [derive] colored = 2.1.0jsonxf = 1.1.1mime = 0.3.17reqwest = version = 0.11.24, features = [json] tokio = version = 1.36.0, features = [rt, rt-multi-thread, macros] anyhow: 用于简化异常处理。 clap: 解析命令行参数。 colored: 为终端输出内容赋予颜色。 jsonxf: 美化 json 串。 mime: 提供了各种 Media Type 的类型封装。 reqwest: http 客户端。 tokio: 异步库，本案例种我们使用 reqwest 的异步功能。 3. 完整源码 // src/main.rs 为减小篇幅，省略了单元测试，读者可自行补充。use std::collections::HashMap;use reqwest::Client, header, Response;use std::str::FromStr;use anyhow::anyhow;use clap::Args, Parser, Subcommand;use colored::Colorize;use mime::Mime;use reqwest::header::HeaderMap, HeaderName, HeaderValue;use reqwest::Url;#[derive(Parser)]#[command(version, author, about, long_about = None)]struct Httpie #[command(subcommand)] methods: Method,#[derive(Subcommand)]enum Method Get(Get), Post(Post)#[derive(Args)]struct Get #[arg(value_parser = parse_url)] url: String,#[derive(Args)]struct Post /// Specify the url you wanna request to. #[arg(value_parser = parse_url)] url: String, /// Set the request body. /// Examples: /// headers: /// header1:value1 /// params: /// key1=value1 #[arg(required = true, value_parser = parse_kv_pairs)] body: VecKvPair#[derive(Debug, Clone)]struct KvPair k: String, v: String, t: KvPairType,#[derive(Debug,Clone)]enum KvPairType Header, Param,impl FromStr for KvPair type Err = anyhow::Error; fn from_str(s: str) - ResultSelf, Self::Err let pair_type: KvPairType; let split_char = if s.contains(:) pair_type = KvPairType::Header; : else pair_type = KvPairType::Param; = ; let mut split = s.split(split_char); let err = || anyhow!(format!(failed to parse pairs ,s)); Ok(Self k: (split.next().ok_or_else(err)?).to_string(), v: (split.next().ok_or_else(err)?).to_string(), t: pair_type, ) fn parse_url(s: str) - anyhow::ResultString let _url: Url = s.parse()?; Ok(s.into())fn parse_kv_pairs(s: str) - anyhow::ResultKvPair Ok(s.parse()?)async fn get(client: Client, args: Get) - anyhow::Result() let resp = client.get(args.url).send().await?; Ok(print_resp(resp).await?)async fn post(client: Client, args: Post) - anyhow::Result() let mut body = HashMap::new(); let mut header_map = HeaderMap::new(); for pair in args.body.iter() match pair.t KvPairType::Param = body.insert(pair.k, pair.v); KvPairType::Header = if let Ok(name) = HeaderName::from_str(pair.k.as_str()) if let Ok(value) = HeaderValue::from_str(pair.v.as_str()) header_map.insert(name,value); else println!(Invalid header value for key: , pair.v); else println!(Invalid header key: , pair.k); let resp = client.post(args.url) .headers(header_map) .json(body).send().await?; Ok(print_resp(resp).await?)async fn print_resp(resp: Response) - anyhow::Result() print_status(resp); print_headers(resp); let mime = get_content_type(resp); let body = resp.text().await?; print_body(mime, body); Ok(())fn print_status(resp: Response) let status = format!(:? , resp.version(), resp.status()).blue(); println!( , status);fn print_headers(resp: Response) for (k,v) in resp.headers() println!(: :?, k.to_string().green(), v); print!( );fn print_body(mime: OptionMime, resp: String) match mime Some(v) = if v == mime::APPLICATION_JSON println!(, jsonxf::pretty_print(resp).unwrap().cyan()) _ = print!(, resp), fn get_content_type(resp: Response) - OptionMime resp.headers() .get(header::CONTENT_TYPE) .map(|v|v.to_str().unwrap().parse().unwrap())#[tokio::main]async fn main() - anyhow::Result() let httpie = Httpie::parse(); let client = Client::new(); let result = match httpie.methods Method::Get(ref args) = get(client, args).await?, Method::Post(ref args) = post(client, args).await?, ; Ok(result) 可以看到，即使算上 use 部分，总代码也不过 160 行左右，Rust 的 clap 库在 CLI 开发上确实 yyds！ 接下来我们来一一拆解这部分的代码，其中关于 clap 的部分我不会过多展开，刚兴趣的读者可以参阅：深入探索 Rust 的 clap 库：命令行解析的艺术。 3.1 命令行解析 我们先从 main() 开始： #[tokio::main]async fn main() - anyhow::Result() let httpie = Httpie::parse(); let client = Client::new(); let result = match httpie.methods Method::Get(ref args) = get(client, args).await?, Method::Post(ref args) = post(client, args).await?, ; Ok(result) 我们希望使用 clap 的异步功能，所以使用了 async 关键字，同时加上了 tokio 提供的属性宏 #[tokio::main]，用于设置异步环境。为了能够使用 ? 快速传播错误，我们设置返回值为 anyhow::Result()，本项目中我们不对错误进行过多处理，所以这种方式可以大大简化我们的错误处理过程。 main() 中我们使用 Httpie::parse() 解析命令行中的参数，使用 Client::new() 创建一个 http client，根据解析到的命令行参数，我们匹配子命令 methods，分别调用 get() 和 post() 来发送 GET 和 POST 请求。 Httpie 的定义如下： #[derive(Parser)]#[command(version, author, about, long_about = None)]struct Httpie #[command(subcommand)] methods: Method, #[derive(Parser)] 是一个过程宏（procedural macro），用于自动为结构体实现 clap::Parser trait。这使得该结构体可以用来解析命令行参数。 在 Httpie 中我们定义了子命令 Method： #[derive(Subcommand)]enum Method Get(Get), Post(Post) #[derive(Subcommand)] 属性宏会自动为枚举派生一些代码，以便它可以作为子命令来解析命令行参数。目前支持 Get 和 Post 两个子命令，它们分别接收 Get 和 Post 参数： #[derive(Args)]struct Get #[arg(value_parser = parse_url)] url: String,#[derive(Args)]struct Post #[arg(value_parser = parse_url)] url: String, #[arg(value_parser = parse_kv_pairs)] body: VecKvPair #[derive(Args)] 属性宏表明当前 struct 是命令的参数，其中 Get 仅支持 url 参数，Post 支持 url 和 body 参数。 url 参数我们使用 parse_url 函数来进行解析： use reqwest::Url;fn parse_url(s: str) - anyhow::ResultString let _url: Url = s.parse()?; Ok(s.into()) 这里 reqwest::Url 已经实现了 FromStr trait，所以这里我们可以直接调用 s.parse() 来解析 url。 而 body，因为我们期望 CLI 使用起来像： httpie url header1:value1 param1=v1 body 就是 header1:value1 param1=v1，一对 kv 就代表着一个 header 或者 param，用 : 和 = 来区分。因为 kv 对的个数的变长的，所以我们使用 VecKvPair 来接收 body 这个参数，并使用 parse_kv_pairs 来解析 kv 对。 KvPair 是我们自定义的类型： #[derive(Debug, Clone)]struct KvPair k: String, v: String, t: KvPairType,#[derive(Debug,Clone)]enum KvPairType Header, Param, parse_kv_pairs 的实现如下： fn parse_kv_pairs(s: str) - anyhow::ResultKvPair Ok(s.parse()?) 在这里，你可以在 parse_kv_pairs() 函数中，对 s 进行解析并返回 anyhow::ResultKvPair。不过，更优雅，更统一的方式是什么呢？就是像 reqwest::Url 一样，为 KvPair 实现 FromStr trait，这样就可以直接调用 s.parse() 来进行解析了。 impl FromStr for KvPair type Err = anyhow::Error; fn from_str(s: str) - ResultSelf, Self::Err ... 3.2 发送请求 参数解析完，就到了发送请求的地方了，这里使用 reqwest crate 就非常方便了，这里就不赘述了，具体可以参考：Rust reqwest 简明教程。 async fn get(client: Client, args: Get) - anyhow::Result() ... async fn post(client: Client, args: Post) - anyhow::Result() ... 3.3 打印响应 httpie response demo 响应分为 3 个部分： print_status() print_headers() print_body() async fn print_resp(resp: Response) - anyhow::Result() print_status(resp); print_headers(resp); let mime = get_content_type(resp); let body = resp.text().await?; print_body(mime, body); Ok(()) print_status() 比较简单，就是打印 HTTP 版本和响应状态码，然后我们使用 colored crate 的 blue() 使其在终端以蓝色输出。 fn print_status(resp: Response) let status = format!(:? , resp.version(), resp.status()).blue(); println!( , status); print_headers() 中，我们使用 green() 使 header_name 在终端以绿色输出。 fn print_headers(resp: Response) for (k,v) in resp.headers() println!(: :?, k.to_string().green(), v); print!( ); 响应体的格式（Media Type）有很多，本案例中我们仅支持 application/json，所以在 print_body() 之前，我们需要先读取 response header 中的 content-type： fn get_content_type(resp: Response) - OptionMime resp.headers() .get(header::CONTENT_TYPE) .map(|v|v.to_str().unwrap().parse().unwrap()) 在 print_resp() 中，对于 application/json，我们使用 jsonxf crate 对进行美化，并使用 cyan() 使其在终端以蓝绿色输出。对于其他类型，我们姑且照原文输出。 fn print_body(mime: OptionMime, resp: String) match mime Some(v) = if v == mime::APPLICATION_JSON println!(, jsonxf::pretty_print(resp).unwrap().cyan()) _ = print!(, resp), 总结 在本文中，我们深入探讨了如何使用 Rust 语言来实现一个类似于 HTTPie 的命令行工具。这个过程包括了对 HTTP 协议的理解、命令行参数的解析、HTTP 客户端的创建和请求发送，以及对响应的处理和展示。通过本文，读者不仅能够获得一个实用的命令行工具，还能够学习到如何使用 Rust 的库来构建实际的应用程序，包括 clap、reqwest、tokio 和 colored 等。此外，文章也说明了在 Rust 中进行异步编程和错误处理的一些常见模式。尽管示例代码的错误处理较为简单，但它提供了一个良好的起点，开发者可以在此基础上进行扩展和改进，以适应更复杂的应用场景。","tags":["rust"],"categories":["rust","rust 实战"]},{"title":"Rust anyhow 简明教程","path":"/2024/03/05/rust-crate-anyhow/","content":"anyhow 是 Rust 中的一个库，旨在提供灵活的、具体的错误处理能力，建立在 std::error::Error 基础上。它主要用于那些需要简单错误处理的应用程序和原型开发中，尤其是在错误类型不需要被严格区分的场景下。 以下是 anyhow 的几个关键特性： 易用性: anyhow 提供了一个 Error 类型，这个类型可以包含任何实现了 std::error::Error 的错误。这意味着你可以使用 anyhow::Error 来包装几乎所有类型的错误，无需担心具体的错误类型。 简洁的错误链: anyhow 支持通过 ? 操作符来传播错误，同时保留错误发生的上下文。这让错误处理更加直观，同时还能保留错误链，便于调试。 便于调试: anyhow 支持通过 :# 格式化指示符来打印错误及其所有相关的上下文和原因，这使得调试复杂的错误链变得更加简单。 无需关心错误类型: 在很多情况下，特别是在应用程序的顶层，你可能不需要关心错误的具体类型，只需要知道出错了并且能够将错误信息传递给用户或日志。anyhow 让这一过程变得简单，因为它可以包装任何错误，而不需要显式地指定错误类型。 使用 anyhow 的典型场景包括快速原型开发、应用程序顶层的错误处理，或者在库中作为返回错误类型的一个简便选择，尤其是在库的使用者不需要关心具体错误类型的时候。 anyhow::Error anyhow::Error 是 anyhow 库定义的一个错误类型。它是一个包装器（wrapper）类型，可以包含任何实现了 std::error::Error trait 的错误类型。这意味着你可以将几乎所有的错误转换为 anyhow::Error 类型，从而在函数之间传递，而不需要在意具体的错误类型。这在快速原型开发或应用程序顶层错误处理中特别有用，因为它简化了错误处理的逻辑。 它的定义如下： #[cfg_attr(not(doc), repr(transparent))]pub struct Error inner: OwnErrorImpl, 其中核心是 ErrorImpl： #[repr(C)]pub(crate) struct ErrorImplE = () vtable: static ErrorVTable, backtrace: OptionBacktrace, // NOTE: Dont use directly. Use only through vtable. Erased type may have // different alignment. _object: E, ErrorImpl 是一个内部结构体，用于实现 anyhow::Error 类型的具体功能。它包含了三个主要字段： vtable 是一个指向静态虚拟表的指针，用于动态派发错误相关的方法。 backtrace 是一个可选的回溯（Backtrace）类型，用于存储错误发生时的调用栈信息。 _object 字段用于存储具体的错误对象，其类型在编译时被擦除以提供类型安全的动态错误处理。 这种设计允许 anyhow 错误封装并表示各种不同的错误类型，同时提供了方法动态派发和回溯功能，以便于错误调试。 anyhow::Error 可以包含任何实现了 std::error::Error trait 的错误类型，这里因为下面的 impl： implE StdError for ErrorImplEwhere E: StdError, fn source(self) - Option(dyn StdError + static) unsafe ErrorImpl::error(self.erase()).source() #[cfg(error_generic_member_access)] fn providea(a self, request: mut Requesta) unsafe ErrorImpl::provide(self.erase(), request) anyhow::Result anyhow::Result 是一个别名（type alias），它是 std::result::ResultT, anyhow::Error 的简写。在使用 anyhow 库进行错误处理时，你会频繁地看到这个类型。它基本上是标准的 Result 类型，但错误类型被固定为 anyhow::Error。这使得你可以很容易地在函数之间传递错误，而不需要声明具体的错误类型。 pub type ResultT, E = Error = core::result::ResultT, E; 使用 anyhow::Result 的好处在于它提供了一种统一的方式来处理错误。你可以使用 ? 操作符来传播错误，同时保留错误的上下文信息和回溯。这极大地简化了错误处理代码，尤其是在多个可能产生不同错误类型的操作链中。 3 个核心使用技巧 使用 ResultT, anyhow::Error 或者 anyhow::ResultT 作为返回值，然后利用 ? 语法糖无脑传播报错。 使用 with_context(f) 来附加错误信息。 使用 downcast 反解具体的错误类型。 实战案例 下面我们用一个案例来体会 anyhow 的使用方式： 我们的需求是：打开一个文件，解析文件中的数据并进行大写化，然后输出处理后的数据。 use anyhow::Result, Context;use std::fs, io;// 1. 读取文件、解析数据和执行数据操作都可能出现错误，// 所以我们需要返回 Result 来兼容异常情况。// 这里我们使用 anyhow::Result 来简化和传播错误。fn read_and_process_file(file_path: str) - Result() // 尝试读取文件 let data = fs::read_to_string(file_path) // 2. 使用 with_context 来附加错误信息，然后利用 ? 语法糖传播错误。 .with_context(||format!(failed to read file ``, file_path))?; // 解析数据 let processed_data = parse_data(data) .with_context(||format!(failed to parse data from file ``, file_path))?; // 执行数据操作 perform_some_operation(processed_data) .with_context(|| failed to perform operation based on file data)?; Ok(())fn parse_data(data: str) - ResultString Ok(data.to_uppercase())fn perform_some_operation(data: String) - Result() println!(processed data: , data); Ok(())fn main() let file_path = ./anyhow.txt; // 执行处理逻辑 let res = read_and_process_file(file_path); // 处理结果 match res Ok(_) = println!(successfully!), Err(e) = // 3. 使用 downcast 来反解出实际的错误实例，本案例中可能出现的异常是 io::Error。 if let Some(my_error) = e.downcast_ref::io::Error() println!(has io error: :#, my_error); else println!(unknown error: :?, e);","tags":["rust"],"categories":["rust","rust 常用库"]},{"title":"深入探索 Rust 的 clap 库：命令行解析的艺术","path":"/2024/03/02/rust-crate-clap/","content":"版本声明 Rust: 1.76 clap: 4.5.1 clap_complete 4.5.1 rpassword: 7.3.1 结论先行 本文将从 CLI（Command Line Interface）命令行工具的概述讲起，介绍一个优秀的命令行工具应该具备的功能和特性。然后介绍 Rust 中一个非常优秀的命令行解析工具 clap 经典使用方法，并利用 clap 实现一个类似于 curl 的工具 httpie。文章最后还将 clap 于 Go 语言中同样优秀的命令行解析工具 cobra 进行一个简单对比，便于读者进一步体会 clap 的简洁和优秀。 本文将包含以下几个部分： CLI 概述：从 CLI 的基本概念出发，介绍优秀命令行工具应该具备的功能特性，并以 curl 作为经典范例进行说明。 详细介绍 clap：基于 clap 官方文档，分别详细介绍 clap 以 derive 和 builder 两个方式构建 cli 的常用方法。 实战 httpie：参考陈天老师的《Rust 编程第一课》，用最新的 clap 版本（1.7.6）实现 httpie 工具。 对比 cobra：从设计理念和目标、功能特点、使用场景等方面简要对比 clap 和 Go 流行的命令行解析库 cobra。 特此声明，本文包含 AI 辅助生成内容，如有错误遗漏之处，敬请指出。 CLI 概述 CLI（Command Line Interface，命令行界面）是一种允许用户通过文本命令与计算机程序或操作系统进行交互的接口。与图形用户界面（GUI，Graphical User Interface）相比，CLI 不提供图形元素，如按钮或图标，而是依赖于文本输入。用户通过键盘输入特定的命令行指令，命令行界面解释这些指令并执行相应的操作。 一款优秀的 CLI 工具应该具备以下的功能和特性，以提升用户体验和效率： 一个优秀的命令行工具（CLI, Command Line Interface）应该具备以下功能和特性，以提升用户体验和效率： 直观易用： 简洁的命令语法：命令和参数的设计应直观易懂，方便用户记忆和使用。 自动补全：支持命令和参数的自动补全功能，提高用户输入效率。 命令别名：提供常用命令的简短别名，减少输入的工作量。 强大的帮助系统： 详细的帮助文档：每个命令和参数都应有清晰的说明文档。 示例使用方式：提供常见的使用示例，帮助用户快速理解和应用。 内置帮助命令：通过如--help或-h参数轻松访问帮助信息。 错误处理与反馈： 清晰的错误信息：出现错误时，提供明确、具体的错误信息，帮助用户快速定位问题。 建议和解决方案：在可能的情况下，给出错误解决的建议或自动修复选项。 高效的执行和输出： 快速响应：命令执行应迅速，减少用户等待时间。 格式化的输出：提供易于阅读和解析的输出格式，如表格、JSON 或 XML 等。 输出过滤和排序：允许用户根据需要过滤和排序输出结果，提高信息的查找效率。 跨平台兼容： 多平台支持：能够在不同的操作系统上运行，如 Windows、macOS、Linux 等。 环境适应性：自动适应不同的终端环境和字符编码，确保输出显示正确。 安全性： 安全的默认设置：默认配置应强调安全，避免暴露敏感信息。 数据加密：在处理敏感信息（如密码）时，应使用加密手段保护数据安全。 版本管理： 版本控制：提供命令查看工具版本，支持多版本共存或升级。 向后兼容：新版本应尽量保持与旧版本的兼容性，避免破坏用户现有的工作流程。 这些特性不仅能够提高用户的工作效率，还能增强用户体验，使命令行工具更加强大和易用。 下面我们以 curl 为例，看看优秀的 CLI 工具大概长什么样子。 curl 是一种命令行工具和库，用于传输数据。它支持多种协议，包括 HTTP、HTTPS、FTP、FTPS、SCP、SFTP、TFTP、TELNET、DICT、LDAP、LDAPS、IMAP、POP3、SMTP 和 RTSP 等。curl 是一个非常强大和灵活的工具，广泛应用于自动化脚本、系统测试、数据收集和许多其他用途。 进入终端，我们可以用下面命令查看 curl 的说明文档： ➜ ~ curl --helpUsage: curl [options...] url -d, --data data HTTP POST data -f, --fail Fail fast with no output on HTTP errors -h, --help category Get help for commands -i, --include Include protocol response headers in the output -o, --output file Write to file instead of stdout -O, --remote-name Write output to a file named as the remote file -s, --silent Silent mode -T, --upload-file file Transfer local FILE to destination -u, --user user:password Server user and password -A, --user-agent name Send User-Agent name to server -v, --verbose Make the operation more talkative -V, --version Show version number and quitThis is not the full help, this menu is stripped into categories.Use --help category to get an overview of all categories.For all options use the manual or --help all. 使用示例： 下载文件： curl -O http://example.com/file.txt 发送 POST 请求： curl -d param1=value1param2=value2 http://example.com/resource 使用 HTTPS 并忽略证书验证： curl -k https://example.com 使用基本认证： curl -u username:password http://example.com curl 的这些特性使其成为开发者、系统管理员和自动化脚本中广泛使用的工具之一。 clap 概述 clap，代表 Command Line Argument Parser，是一个旨在创建直观、易用且功能强大的命令行界面的 Rust 库。截至目前（2024.2），clap 已经发展到了 4.5.1 版本，它通过简化命令行参数的处理，让开发者能更专注于应用逻辑的构建。 clap 之所以在 Rust 社区如此流行，得益于以下几个优点： 1. 易于使用 clap 的设计理念是让命令行参数的解析变得简单而直观。即使是没有经验的开发者也能快速上手，通过几行代码就能实现复杂的命令行参数解析。 2. 功能丰富 clap 提供了广泛的功能来满足各种命令行解析需求，包括但不限于： 自动生成的帮助信息：clap 能根据定义的参数自动生成帮助信息，包括参数的说明、类型、默认值等。 强大的错误提示：当用户输入无效的命令行参数时，clap 会提供清晰且有用的错误提示，帮助用户快速定位问题。 参数验证：开发者可以为参数设定验证规则，确保输入的参数符合预期。 复杂的命令结构：支持子命令的嵌套，允许构建复杂的命令行应用结构。 自定义派生：通过 clap 的派生宏，可以简化命令行解析器的定义，使代码更加清晰。 3. 高度可定制 clap 允许开发者高度定制命令行解析的行为和外观，包括自定义帮助信息的格式、控制错误消息的显示方式等。这种灵活性意味着你可以根据应用程序的需求调整 clap 的行为。 4. 性能优异 尽管 clap 功能强大，但它仍然非常注重性能。clap 经过优化，以尽可能少的性能开销处理命令行参数。 5. 活跃的社区支持 clap 有一个非常活跃的社区，在 GitHub 上不断有新的贡献者加入。这意味着 clap 不断地得到改进和更新，同时也有大量的社区资源可供参考。 Derive vs Builder (1) 初探 clap 提供了 2 种构建命令行的方式，分别为 Derive 和 Builder。顾名思义，Derive 就是利用宏强大的功能来构建命令行，而 Builder 则采用构建者模式链式构建命令行工具。 在这里我们先给出示例来直观感受这 2 种构建方式的不同： Derive: #[derive(Parser)]#[command(version, author, about, long_about = None)]struct Cli /// Specify your name name: String, /// Specify your age optionally #[arg(short, long)] age: Optioni8,fn main() let cli = Cli::parse(); println!(name: , cli.name); println!(age: :?, cli.age); Builder: fn main() let matches = Command::new(myapp) .version(1.0.0) .author(hedon) .about(this is the short about) .long_about(this is the long about) .arg(arg!([NAME]).required(true).help(Specify your name)) .arg(arg!(-a --age AGE) .value_parser(clap::value_parser!(u8)) .help(Specify your age optionally)) .get_matches(); println!(name: :?, matches.get_one::String(NAME)); println!(age: :?, matches.get_one::u8(age)); 这 2 个程序都实现了相同的功能，使用 --help ，输出的内容大致都如下： Usage: derive [OPTIONS] NAMEArguments: NAME Specify your nameOptions: -a, --age AGE Specify your age optionally -h, --help Print help 通过观察，可以发现 Derive 模式下，宏中的每一个属性，如 version、author 等，都对应到 Builder 模式下一个同名的函数。 下面我们将从「应用配置」、「参数类型」和「参数校验」三个方面，分别介绍 clap 中 Derive 和 Builder 两种模式构建 CLI 的常用方法。 特别说明：后续的例子均在 examples 目录下实现，故编译和执行命令都包含 example。 目录结构大概如下： ➜ learn-clap git:(master) ✗ tree.├── Cargo.lock├── Cargo.toml├── examples│ ├── optional.rs├── src│ └── main.rs└── target └── release └── examples └── optional Derive 要使用 clap 的 Derive 模式，需要： cargo add clap --features derive 1. 应用配置 我们需要定义一个 strut 来表示我们的 application，利用它来承载应用的参数： /// The example of clap derive#[derive(Parser)]#[command(version, author, about, long_about = None)]struct Cli /// Specify your name name: String, /// Specify your age optionally #[arg(short, long)] age: Optioni8,fn main() let cli = Cli::parse(); println!(name: , cli.name); println!(age: :?, cli.age); #[derive(Parser)] 是一个过程宏（procedural macro），用于自动为结构体实现 clap::Parser trait。这使得该结构体可以用来解析命令行参数。 使用 #[derive(Parser)]，你可以简化命令行解析的代码，因为 clap 会根据结构体的字段自动生成命令行解析的逻辑。 每个字段都对应一个命令行参数，字段的类型和属性用来决定参数的解析方式和验证规则。 #[command(version, about, long_about = None)] 属性用于为整个命令行程序提供元信息，它支持以下几个元素： #[command] 支持的元素 #[arg(short, long)] 属性用于配置命令参数的元信息，它支持以下几个属性： 属性 方法 默认值/行为 备注 id Arg::id field’s name 当属性不存在时，使用字段名 value_parser Arg::value_parser auto-select based on field type 当属性不存在时，会基于字段类型自动选择实现 action Arg::action auto-select based on field type 当属性不存在时，会基于字段类型自动选择动作 help Arg::help Doc comment summary 当属性不存在时，使用文档注释摘要 long_help Arg::long_help Doc comment with blank line, else nothing 当属性不存在时，使用文档注释，如果有空行 verbatim_doc_comment Minimizes preprocessing - 将文档注释转换为 help/long_help 时最小化预处理 short Arg::short no short set 当属性不存在时，没有短名称设置 long Arg::long no long set 当属性不存在时，没有长名称设置 env Arg::env no env set 当属性不存在时，没有环境变量设置 from_global Read Arg::global - 无论在哪个子命令中，都读取 Arg::global 参数 value_enum Parse with ValueEnum - 使用 ValueEnum 解析值 skip Ignore this field fills the field with Default::default() 忽略此字段，用 expr 或 Default::default() 填充 default_value Arg::default_value Arg::required(false) 设置默认值，并将 Arg 设置为非必须 default_value_t Arg::default_value Arg::required(false) 要求 std::fmt::Display 与 Arg::value_parser 相匹配 default_values_t Arg::default_values Arg::required(false) 要求字段类型为 VecT，T 实现 std::fmt::Display default_value_os_t Arg::default_value_os Arg::required(false) 要求 std::convert::IntoOsString default_values_os_t Arg::default_values_os Arg::required(false) 要求字段类型为 VecT，T 实现 std::convert::IntoOsString 2. 参数类型 2.1 Arguments Options 从上面这个输出样例中： the example of clap deriveUsage: derive [OPTIONS] NAMEArguments: NAME Specify your nameOptions: -a, --age AGE Specify your age optionally -h, --help Print help 可以看到跟在命令后面有 2 中参数类型： Arguments: 直接在命令后面指定值，如 cmd hedon，有严格的顺序要求。 Options: 需要用 -short 或 --long 来指定是哪个参数，无严格的顺序要求。 它们的定义区别就是是否使用了 #[arg]： Options: 指定了 short 或 long。 Arguments: 没有 short 和 long。 #[derive(Parser)]struct Cli /// 会被解析成 [NAME] name: String, /// 会被解析成 -a AGE #[arg(short, long)] age: u8, 2.2 可选参数 可以使用 Option 来实现可选参数： use clap::Parser;#[derive(Parser)]#[command(version, author, about, long_about = None)]struct Cli name: OptionString, #[arg(short, long)] age: Optionu8,fn main() let cli = Cli::parse(); println!(name: :?, cli.name); println!(age: :?, cli.age); 编译： cargo build --example optional --release 执行： /target/release/examples/optional --help 输出： this is the about from Cargo.tomlUsage: optional [OPTIONS] [NAME]Arguments: [NAME]Options: -a, --age AGE -h, --help Print help -V, --version Print version 测试： ➜ learn-clap git:(master) ✗ ./target/release/examples/optionalname: Noneage: None➜ learn-clap git:(master) ✗ ./target/release/examples/optional -a 1name: Noneage: Some(1)➜ learn-clap git:(master) ✗ ./target/release/examples/optional hedonname: Some(hedon)age: None➜ learn-clap git:(master) ✗ ./target/release/examples/optional hedon -a 18name: Some(hedon)age: Some(18) 2.3 枚举参数 可以使用 enum 搭配 value_enum 来实现多选一参数，并限制可选参数的取值。 use clap::Parser, ValueEnum;#[derive(Parser)]#[command(version, author, about, long_about = None)]struct Cli /// Choose the program mode run in #[arg(value_enum)] mode: Mode,#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, ValueEnum)]enum Mode /// run in fast mode Fast, /// run in slow mode Slow,fn main() let cli = Cli::parse(); match cli.mode Mode::Fast = println!(fast!!!!!), Mode::Slow = println!(slow......), 输出： Usage: enum MODEArguments: MODE Choose the program mode run in Possible values: - fast: run in fast mode - slow: run in slow modeOptions: -h, --help Print help (see a summary with -h) -V, --version Print version 2.4 累计参数 累积参数允许用户通过重复指定同一个标志（例如 -d）来累加值或效果，通常用于控制命令行应用的详细级别（verbosity level）或其他需要根据次数变化的行为。 在很多命令行工具中，累积参数常见于控制日志输出的详细程度。例如，一个 -v（verbose）标志可能每被指定一次，就增加一层详细级别。所以，-vvv（等价于 -v -v -v） 会比单个 -v 提供更多的详细信息。 在 clap 中可以通过 clap::ArgAction::Count 来实现这种累积参数。 use clap::Parser;#[derive(Parser)]#[command(version, author, about, long_about = None)]struct Cli #[arg(short, long, action = clap::ArgAction::Count)] verbose: u8,fn main() let cli = Cli::parse(); println!(verbose: , cli.verbose); 输出： ➜ learn-clap git:(master) ✗ ./target/release/examples/accurate --helpthis is the about from Cargo.tomlUsage: accurate [OPTIONS]Options: -v, --verbose... -h, --help Print help -V, --version Print version➜ learn-clap git:(master) ✗ ./target/release/examples/accurate -vverbose: 1➜ learn-clap git:(master) ✗ ./target/release/examples/accurate -vvvvverbose: 4 2.5 变长参数 有时候我们希望接收变长参数，比如说： del file1 file2 file3 这个时候可以使用 Vec 来实现。 use clap::Parser;#[derive(Parser)]#[command(version, author, about, long_about = None)]struct Cli files: VecString,fn main() let cli = Cli::parse(); println!(files: :?, cli.files); 输出： ➜ learn-clap git:(master) ✗ ./target/release/examples/var_length --helpthis is the about from Cargo.tomlUsage: var_length [FILES]...Arguments: [FILES]...Options: -h, --help Print help -V, --version Print version➜ learn-clap git:(master) ✗ ./target/release/examples/var_lengthfiles: []➜ learn-clap git:(master) ✗ ./target/release/examples/var_length file1files: [file1]➜ learn-clap git:(master) ✗ ./target/release/examples/var_length file1 file2files: [file1, file2]➜ learn-clap git:(master) ✗ ./target/release/examples/var_length file1 file2 file3files: [file1, file2, file3] 2.6 标志参数 对于标志参数，只要指定类型为 bool，就可以自动实现了。 use clap::Parser;#[derive(Parser)]#[command(version, author, about, long_about = None)]struct Cli #[arg(short, long)] verbose: bool,fn main() let cli = Cli::parse(); println!(verbose: , cli.verbose); 输出： ➜ learn-clap git:(master) ✗ ./target/release/examples/flag --helpUsage: flag [OPTIONS]Options: -v, --verbose -h, --help Print help -V, --version Print version➜ learn-clap git:(master) ✗ ./target/release/examples/flagverbose: false➜ learn-clap git:(master) ✗ ./target/release/examples/flag -vverbose: true 2.7 子命令 在更复杂的命令行工具中，除了主命令，还有子命令，甚至子命令下面还有子命令，其实就是一颗命令树。 command tree 在 clap 中可以使用 #[command(subcommand)] 搭配 #[derive(Subcommand)] 实现子命令功能。 use clap::Parser, Subcommand;#[derive(Parser)]#[command(version, author, about, long_about = None)]struct Cli #[command(subcommand)] test: OptionTest,#[derive(Subcommand)]enum Test /// Add a number Add #[arg(short, long)] num: u16, , /// Sub a number Sub #[arg(short, long)] num: u16, fn main() let cli = Cli::parse(); if let Some(test) = cli.test match test Test::Add num = println!(test add num: :?, num), Test::Sub num = println!(test sub num: :?, num), 输出： ➜ learn-clap git:(master) ✗ ./target/release/examples/subcommand --helpthis is the about from Cargo.tomlUsage: subcommand [COMMAND]Commands: add Add a number sub Sub a number help Print this message or the help of the given subcommand(s)Options: -h, --help Print help -V, --version Print version➜ learn-clap git:(master) ✗ ./target/release/examples/subcommand add --helpAdd a numberUsage: subcommand add --num NUMOptions: -n, --num NUM -h, --help Print help➜ learn-clap git:(master) ✗ ./target/release/examples/subcommand add -n 1test add num: 1➜ learn-clap git:(master) ✗ ./target/release/examples/subcommand sub -n 2test sub num: 2 3. 参数校验 3.1 类型校验 可以发现，使用 Derive 模式的时候，我们在参数后面指定参数类型的时候，clap 就会对我们输入参数进行类型检查，不匹配的时候会输出丰富的报错信息和指导建议。 error: invalid value xxxx for --num NUM: invalid digit found in stringFor more information, try --help. 默认支持： 原生类型：bool, String, OsString, PathBuf、usize、isize 范围数据：u8, i8, u16, i16, u32, i32, u64, i64 实现了 ValueEnum 的 enum 类型 实现了 FromOsString、FromOsStr 、FromStr 的类型 这是因为他们都实现了 TypedValueParser trait，你自定义的类型也可以实现这个 triat，这样就可以自动进行类型校验了。 clap 还提供了一些更加严格的参数校验功能。👇🏻 3.2 枚举校验 对于实现 ValueEnum 的枚举类型，如果输入的值不是枚举中定义的，则 clap 会报错并提示可选值。 我们复用上面介绍「多选一参数」的代码： use clap::Parser, ValueEnum;#[derive(Parser)]#[command(version, author, about, long_about = None)]struct Cli /// Choose the program mode run in #[arg(value_enum)] mode: Mode,#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, ValueEnum)]enum Mode /// run in fast mode Fast, /// run in slow mode Slow,fn main() let cli = Cli::parse(); match cli.mode Mode::Fast = println!(fast!!!!!), Mode::Slow = println!(slow......), 使用错误的值进行尝试： ➜ learn-clap git:(master) ✗ ./target/release/examples/enum xxxxerror: invalid value xxxx for MODE [possible values: fast, slow]For more information, try --help. 3.3 范围校验 如果你想要实现数字类型范围限制的话，比如端口号参数的范围应该是 [1, 65535]，那可以使用 value_parser! = clap::value_parser!(u16).range(1..) 来实现这个功能： use clap::Parser;#[derive(Parser)]#[command(version, author, about, long_about = None)]struct Cli #[arg(short, long, value_parser = clap::value_parser!(u16).range(1..))] port: u16,fn main() let cli = Cli::parse(); println!(port: :?, cli.port); 输出： ➜ learn-clap git:(master) ✗ ./target/release/examples/range --helpthis is the about from Cargo.tomlUsage: range --port PORTOptions: -p, --port PORT -h, --help Print help -V, --version Print version➜ learn-clap git:(master) ✗ ./target/release/examples/range -p 0error: invalid value 0 for --port PORT: 0 is not in 1..=65535For more information, try --help.➜ learn-clap git:(master) ✗ ./target/release/examples/range -p 11111111error: invalid value 11111111 for --port PORT: 11111111 is not in 1..=65535For more information, try --help.➜ learn-clap git:(master) ✗ ./target/release/examples/range -p 1111port: 1111 在这个例子中，value_parser = clap::value_parser!(u16).range(1..) 的含义可以分为两部分解释： 1. clap::value_parser!(u16) 这部分使用 value_parser! 宏为命令行参数指定了 u16 类型的解析器。这意味着输入的参数值会被尝试解析为无符号 16 位整数（u16）。如果输入不能被成功解析为 u16 类型（例如，输入是非数字字符串或者数字过大/过小而不符合 u16 的范围），clap 会报错并提示用户输入有效的参数值。 2. .range(1..) 这部分进一步限制了参数值的有效范围。.range(1..) 指定了参数值必须大于或等于 1（包含 1），但没有上限。换句话说，任何小于 1 的值都将被认为是无效的，clap 会因此报错并要求用户输入一个符合范围要求的值。这在需要限定参数值为正数时非常有用。 结合起来，value_parser = clap::value_parser!(u16).range(1..) 创建了一个规则，要求命令行参数必须是一个大于或等于 1 的 u16 类型的数值。这在很多场景下都非常有用，比如当你需要用户指定一个正数端口号时。 在 RustRover 中，你可以在 Builder 模式，通过在 clap::value_parser!() 中指定其他的类型，然后输入 . 获得其他类型的内置校验规则。 3.4 自定义校验 对于更复杂的规则，clap 还支持自定义校验规则。比如上面对 port 的校验，也可以自己实现。 use std::ops::RangeInclusive;use clap::Parser;#[derive(Parser)]#[command(version, author, about, long_about = None)]struct Cli #[arg(short, long, value_parser = parse_port)] port: u16,const PORT_RANGE: RangeInclusiveusize = 1..=65535;fn parse_port(s: str) - Resultu16, String let port: usize = s .parse() .map_err(|_| format!(`s` isnt a port number`))?; if PORT_RANGE.contains(port) Ok(port as u16) else Err(format!( port not in range -, PORT_RANGE.start(), PORT_RANGE.end(), )) fn main() let cli = Cli::parse(); println!(port: :?, cli.port); 在代码中，我们直接使用 value_parser = parse_port 来指定自定义的校验规则。 我们自定义的校验规则为： fn parse_port(s: str) - Resultu16, String 它需要满足： 入参是 str 出参是 Result参数类型, String 可以测试输出： ➜ learn-clap git:(master) ✗ ./target/release/examples/custom --helpthis is the about from Cargo.tomlUsage: custom --port PORTOptions: -p, --port PORT -h, --help Print help -V, --version Print version➜ learn-clap git:(master) ✗ ./target/release/examples/custom -p xxxerror: invalid value xxx for --port PORT: `xxx` isnt a port number`For more information, try --help.➜ learn-clap git:(master) ✗ ./target/release/examples/custom -p 0error: invalid value 0 for --port PORT: port not in range 1-65535For more information, try --help.➜ learn-clap git:(master) ✗ ./target/release/examples/custom -p 9527port: 9527 3.5 关联参数 有时候参数直接还有关联关系，比如说： 依赖：必须存在 -a 参数，-b 参数才有意义，即要使用 -b 参数时，必须指定 -a 参数。 互斥：-a 和 -b 只能同时存在一个。 可以使用 requires 实现依赖关系： use clap::Parser;#[derive(Parser)]#[command(version, author, about, long_about = None)]struct Cli #[arg(short, long)] a: OptionString, #[arg(short, long,requires = a)] b: OptionString,fn main() let cli = Cli::parse(); println!(a: :?, cli.a); println!(b: :?, cli.b); 上述代码中，我们在参数 b 中加入了 requires = \"a\"，表示要使用 b 参数必须要有 a 参数。 输出： ➜ learn-clap git:(master) ✗ ./target/release/examples/relationa: Noneb: None➜ learn-clap git:(master) ✗ ./target/release/examples/relation -b 1error: the following required arguments were not provided: --a AUsage: relation --a A --b BFor more information, try --help.➜ learn-clap git:(master) ✗ ./target/release/examples/relation -a 1a: Some(1)b: None➜ learn-clap git:(master) ✗ ./target/release/examples/relation -a 1 -b 2a: Some(1)b: Some(2) 可以使用 #[group(required = true, mutiple = false)] 来实现互斥关系： use clap::Args, Parser;#[derive(Parser)]#[command(version, author, about, long_about = None)]struct Cli #[command(flatten)] args: Only,#[derive(Args, Debug)]#[group(required = true, multiple = false)]struct Only #[arg(long)] a: OptionString, #[arg(long)] b: OptionString, #[arg(long)] c: OptionString, #[arg(long)] d: OptionString,fn main() let cli = Cli::parse(); println!(only: :?, cli.args) #[command(flattern)] 直接将结构体里面的参数平铺。 #[group] 用于将一组参数归为一个组，required = true 表示必须提供该 group 中的参数，multiple = false 表示只能有一个参数被提供。 测试输出如下： ➜ learn-clap git:(master) ✗ ./target/release/examples/only_one --helpthis is the about from Cargo.tomlUsage: only_one --a A|--b B|--c C|--d DOptions: --a A --b B --c C --d D -h, --help Print help -V, --version Print version➜ learn-clap git:(master) ✗ ./target/release/examples/only_oneerror: the following required arguments were not provided: --a A|--b B|--c C|--d DUsage: only_one --a A|--b B|--c C|--d DFor more information, try --help.➜ learn-clap git:(master) ✗ ./target/release/examples/only_one --a 1only: Only a: Some(1), b: None, c: None, d: None ➜ learn-clap git:(master) ✗ ./target/release/examples/only_one --a 1 --b 2error: the argument --a A cannot be used with --b BUsage: only_one --a A|--b B|--c C|--d DFor more information, try --help.➜ learn-clap git:(master) ✗ ./target/release/examples/only_one --b 2only: Only a: None, b: Some(2), c: None, d: None Builder 使用 clap 的 Builder 模式，一般情况下不需要额外引入其他的 features： cargo add clap 但是如果要使用 command! 来构建应用的话，需要引入 cargo 这个 features： cargo add clap --features cargo 1. 应用配置 在 Builder 模式下，你可以使用 command!() 或 Command::new(\"appname\") 来构建一个命令行应用，其中 command!() 默认将 appname 设置应用名称，而 Command::new() 必须显示指定 appname。 use clap::arg, Arg, Command, command, value_parser;fn main() // let matches = command!() let matches = Command::new(MyApp) // Application configuration .version(1.0.0) .author(hedon) .about(This the intro of the cli application) // Application args .arg(arg!([NAME]).help(Specify your name)) .arg( Arg::new(age).short(a).long(age).value_parser(value_parser!(u8)) ) .get_matches(); // Read and parse command args if let Some(name) = matches.get_one::String(NAME) println!(Value for name: name); if let Some(age) = matches.get_one::u8(age) println!(Value for age: age); 这段代码分为以下几个部分： 1. 创建命令行应用实例： let matches = Command::new(MyApp) 这里使用 Command::new 方法创建了一个新的命令行应用实例，命名为 \"MyApp\"。 2. 配置应用： .version(1.0.0).author(hedon).about(This the intro of the cli application) 接下来，使用链式调用方法配置应用的版本号为 \"1.0.0\"，作者为 \"hedon\"，并添加了一个简短的描述。 这里等价于 Builder 模式下的： #[command(version, author, about)] 3. 添加命令行参数： .arg(arg!([NAME]).help(Specify your name)).arg( Arg::new(age).short(a).long(age).value_parser(value_parser!(u8))) 这部分代码添加了两个命令行参数： .arg(arg!([NAME]).required(true).help(\"Specify your name\")) 使用 arg! 宏添加了一个名为 NAME 的必需参数，并提供了一些帮助信息。 .arg(Arg::new(\"age\").short('a').long(\"age\").value_parser(value_parser!(u8))) 创建了另一个参数 age，可以通过 -a 或 --age 来指定。这个参数使用了 value_parser 宏来指明它的值应被解析为 u8 类型的数字。 4. 解析命令行参数： .get_matches(); 使用 .get_matches() 方法来解析命令行参数并将结果存储在 matches 变量中。 5. 读取并打印参数值： if let Some(name) = matches.get_one::String(NAME) println!(Value for name: name);if let Some(age) = matches.get_one::u8(age) println!(Value for age: age); 最后，使用 matches.get_one::T(\"arg_name\") 方法尝试获取指定名称的参数值。如果成功找到，则将其打印出来。这里分别尝试获取 \"NAME\" 和 \"age\" 参数的值，并使用 println! 宏将它们打印到控制台。 使用 -- help 测试输出如下： This the intro of the cli applicationUsage: app2 [OPTIONS] [NAME]Arguments: [NAME] Specify your nameOptions: -a, --age AGE -h, --help Print help -V, --version Print version 你可以将其与「Derive - 应用配置」进行比较，应该很容易找到它们之间的对应关系。 在 Derive 中 #[command] 和 #[arg] 支持的属性，都可以在 Builder 中找到对应的同名的函数，这里就不赘述了。 2. 参数类型 在 Builder 模式中，配置参数有两种方式： arg!([-short] [--long] id) Args::new(\"id\").short('s').long(\"long\") 2.1 Arguments Options Arguments: [NAME] Specify your nameOptions: -a, --age AGE Argument: 不包含 -short 和 --long。 Options: 包含 -short 或 --long。 .arg(arg!([NAME]).help(Specify your name)).arg(arg!(-a --age AGE).value_parser(value_parser!(u16))) 2.2 可选参数 根据约定， 表示必须，而 [] 表示可选： .arg(arg!(NAME) // 必须.arg(arg!([ADDRESS])) // 可选 你也可以使用 .required(bool) 函数明确指出是否必须： .arg(arg!(NAME).required(true)) .required() 的优先级高于 和 []，但是建议你在构建的时候还是遵循约定。 2.3 枚举参数 第 1 种：在 value_parser() 中直接指定可选的枚举参数 .arg(arg!(MODE).value_parser([fast, slow])) 第 2 种：使用枚举，但是枚举需要实现 ValueEnum trait 这里又有 2 种方式，你可以向 Derive 一样引入 derive features，然后直接 #[derive(ValueElem)] 使用默认实现，也可以手动实现。我更倾向于前者。 use clap::arg, command, value_parser, ValueEnum;fn main() let matches = command!() // .arg(arg!(MODE).value_parser([fast, slow])) .arg( arg!(MODE).value_parser(value_parser!(Mode)).required(true) ) .get_matches(); match matches.get_one::Mode(MODE) .expect(Mode is required and parsing will fail if its missing) Mode::Fast = println!(fast), Mode::Slow = println!(slow), #[derive(Copy, Clone, Ord, PartialOrd, Eq, PartialEq, ValueEnum)]enum Mode /// Run in fast mode Fast, /// Run in slow mode Slow, 2.4 累计参数 使用 clap::ArgAction::Count 设置参数为累计参数，然后使用 get_count(id) 获取参数的值： use clap::arg, command;fn main() let matches = command!() .arg(arg!(-v --verbose...).action(clap::ArgAction::Count)) .get_matches(); println!(v count: :?, matches.get_count(verbose)); 这里要注意，arg!() 中参数的定义，也要符合累计参数的格式 -short --long...。 2.5 变长参数 使用 clap::ArgAction::Append 设置参数为变长参数，然后使用 get_many::类型(\"id\") 获取参数的值： use clap::arg, Command;fn main() let matches = Command::new(append-application) .arg(arg!([FILES]...).action(clap::ArgAction::Append)) .get_matches(); let files = matches .get_many::String(FILES) .unwrap_or_default() .map(|v|v.as_str()) .collect::Vec_(); println!(files: :?, files); 这里要注意，arg!() 中参数的定义，也要符合变长参数的格式 [arg]|arg...。 2.6 标志参数 使用 clap::ArgAction::SetTrue 或 clap::ArgAction::SetFalse 设置参数为标志参数，然后使用 get_flag() 获取参数的值： use clap::arg, command;fn main() let matches = command!() .arg(arg!(-d --debug).action(clap::ArgAction::SetTrue)) .arg(arg!(-v --verbose).action(clap::ArgAction::SetFalse)) .get_matches(); println!(debug: :?, matches.get_flag(debug)); println!(verbose: :?, matches.get_flag(verbose)) 其中： clap::ArgAction::SetTrue : 设置参数的话，则为 true，否则 false（默认）。 clap::ArgAction::SetFalse : 设置参数的话，则为 false，否则 true（默认）。 测试： ➜ learn-clap-builder git:(master) ✗ ./target/release/examples/flagdebug: falseverbose: true➜ learn-clap-builder git:(master) ✗ ./target/release/examples/flag -ddebug: trueverbose: true➜ learn-clap-builder git:(master) ✗ ./target/release/examples/flag -vdebug: falseverbose: false 2.7 子命令 可以使用 subcommand(sub_cmd) 和 subcommand([sub_cmd1, sub_cmd2]) 来添加子命令，解析的时候使用 matches.subcommand() 匹配子命令，再按照之前的规则解析子命令中对应的参数即可。 use clap::arg, Command, value_parser;fn main() let matches = Command::new(myapp) .subcommands([ Command::new(add) .arg(arg!(NUM).value_parser(value_parser!(i16))), Command::new(sub) .arg(arg!(NUM).value_parser(value_parser!(i16))), ]) .get_matches(); match matches.subcommand() Some((add, add_cmd)) = println!( myapp add was used, num is: :?, add_cmd.get_one::i16(NUM), ), Some((sub, sub_cmd)) = println!( myapp sub was used, num is: :?, sub_cmd.get_one::i16(NUM), ), _ = unreachable!() 3. 参数校验 3.1 类型校验 使用 value_parser!() 在括号中指定类型，clap 就会自动帮我们对参数进行类型校验，当然你在获取参数值 get_one::类型() 的时候，类型要对上，否则会 panic。 默认支持： 原生类型：bool, String, OsString, PathBuf、usize、isize 范围数据：u8, i8, u16, i16, u32, i32, u64, i64 实现了 ValueEnum 的 enum 类型 实现了 FromOsString、FromOsStr 、FromStr 的类型 3.2 枚举校验 2.3 中枚举参数的说明中，已经体现了枚举校验的功能了，这里不赘述。 3.3 范围校验 对于上述提到的「范围数据」，可以使用 value_parser!(类型).range() 进行范围校验。 arg!(PORT) .value_parser(value_parser!(u16).range(1..)) 3.4 自定义校验 value_parser() 中也可以传自定义的校验函数，该函数的签名需要满足的条件跟我们在介绍 Derive 时一样。 use std::ops::RangeInclusive;use clap::arg, command;fn main() let matches = command!() .arg(arg!(PORT).value_parser(port_in_range)) .get_matches(); println!(port: :?, matches.get_one::u16(PORT))const PORT_RANGE: RangeInclusiveusize = RangeInclusive::new(1, 65535);fn port_in_range(s: str) - Resultu16, String let port: usize = s .parse() .map_err(|_|format!(`s` is not a port number))?; if PORT_RANGE.contains(port) Ok(port as u16) else Err(format!( port not in range -, PORT_RANGE.start(), PORT_RANGE.end(), )) 3.5 关联参数 依赖关系：使用 requires(id | group) 排斥关系：使用 group().multiple(false).required(true) .group( ArgGroup::new(vers) // 表示 set-ver, major, minor, patch 必须有一个且只能有一个存在 .multiple(false) .required(true) .args([set-ver, major, minor, patch]),).arg( arg!([INPUT_FILE] some regular input) .value_parser(value_parser!(PathBuf)) .group(input),).arg( arg!(config: -c CONFIG) .value_parser(value_parser!(PathBuf)) // 表示 -c 需要有 group 为 input 的命令存在才可以使用 .requires(input),) Derive vs Builder (2) 对比 Derive vs Builder clap + rpassword 实现加密输入 对于密码、密钥等关键信息的输入，为了信息安全，我们一般会使用加密输出，clap 本身不支持加密输入功能。若你有这方面的需求，可以使用 rpassword crate 辅助完成。 示例： use clap::Parser;use rpassword::read_password;#[derive(Parser)]#[command(version, author, about, long_about = None)]struct Cli #[arg(short, long)] username: String, #[arg(short, long, required = true)] password: bool,fn main() let cli = Cli::parse(); let password = if cli.password // Prompt user to enter password read_password().expect(Failed to read password) else .to_string() ; // Use username and password to do something println!(username: , password: , cli.username, password); clap_complete 实现自动补全 要实现自动补全，需要在 .zshrc 或 .bashrc 等 SHELL 文件中加入命令自动补全脚本。这时候可以使用 clap_complete 来实现这个功能。 下面的示例目录结构如下： ├── Cargo.lock├── Cargo.toml├── build.rs└── src ├── cli.rs └── main.rs 首先我们需要引入 clap 和 clap_complete crate，其中 clap_complete 只需在 build 环境下即可，所以我们的 Cargo.tmol 如下： [package]name = myappversion = 0.1.0edition = 2021build = build.rs[dependencies]clap = version = 4.5.1 dirs = 5.0.1[build-dependencies]clap = version = 4.5.1clap_complete = 4.5.1 我们先在 src/cli.rs 中实现一个简单的命令行程序 myapp： use clap::Arg, ArgAction, Command;pub fn build_cli() - Command Command::new(myapp) .about(Tests completions) .arg(Arg::new(file) .help(some input file)) .subcommand(Command::new(test) .about(tests things) .arg(Arg::new(case) .long(case) .action(ArgAction::Set) .help(the case to test))) 我们主要是演示这个自动补全功能，为了省事，src/main.rs 中就不实现具体逻辑了： mod cli;fn main() let _m = cli::build_cli().get_matches(); 接着，我们在项目根目录下实现 build.rs，它将为我们指定的命令生成自动补全脚本： touch build.rs use clap_complete::generate_to, shells::Bash;use std::env;use std::io::Error;include!(src/cli.rs);fn main() - Result(), Error let outdir = match env::var_os(OUT_DIR) None = return Ok(()), Some(outdir) = outdir, ; let mut cmd = build_cli(); let path = generate_to( Bash, mut cmd, // We need to specify what generator to use myapp, // We need to specify the bin name manually outdir, // We need to specify where to write to )?; println!(cargo:warning=completion file is generated: path:?); Ok(()) 你需要把其中的 myapp 替换为你的命令。 执行构建命令： cargo build 可以看到输出： warning: myapp@0.1.0: completion file is generated: /Users/hedon/RustroverProjects/learn-clap-complete/target/debug/build/myapp-42e401d08c044ca3/out/myapp.bash Finished dev [unoptimized + debuginfo] target(s) in 1.90s 这里会输出生成脚本所在的位置，我这里是 /Users/hedon/RustroverProjects/learn-clap-complete/target/debug/build/myapp-42e401d08c044ca3/out/myapp.bash。 我的终端使用的是 zsh： ➜ echo $SHELL/bin/zsh 所以我需要将这个文件的内容加到 ~/.zshrc 文件的末尾： cat /Users/hedon/RustroverProjects/learn-clap-complete/target/debug/build/myapp-42e401d08c044ca3/out/myapp.bash ~/.zshrc 重新加载配置文件： source ~/.zshrc 这个时候你使用 myapp 命令的时候，按 tap 键，就有自动补全了： ➜ ./target/debug/myapp--help -h \\[file\\] help test HTTPie 由于篇幅原因，实战 HTTPie 部分请看：Rust 实战丨 HTTPie 与 Go 语言 cobra 比较 Go 的 cobra 也是用于构建命令行应用程序的库，它在 Go 语言生态中非常受欢迎。 为了直观展示这 2 个库构建命令行应用程序的区别，我们来设计一个简单的命令行程序，用 clap 和 cobra 分别实现，以展示如何用这两个库实现相同的功能。 让我们创建一个 CLI 程序，它有一个 greet 子命令，接受一个 -n 或 --name 参数，并打印出一条欢迎信息。 Rust clap 实现 use clap:: Parser, Subcommand;#[derive(Parser)]#[command(bin_name = greet_app)]struct Cli #[command(subcommand)] sub: OptionSub,#[derive(Subcommand)]enum Sub Greet #[arg(short, long)] name: String, fn main() let cli = Cli::parse(); if let Some(sub) = cli.sub match sub Sub::Greetname = println!(greeting: :?, name), Go cobra 实现 package mainimport (\tfmt\tos\tgithub.com/spf13/cobra)var rootCmd = cobra.Command\tUse: greet_app,\tShort: A simple greeting application,\tLong: `This is a simple greeting application with a greet command.`,var greetCmd = cobra.Command\tUse: greet,\tShort: Greets a user,\tLong: `Prints a greeting message for the specified user.`,\tRun: func(cmd *cobra.Command, args []string) name, _ := cmd.Flags().GetString(name) fmt.Printf(Hello, %s! , name)\t,func init() rootCmd.AddCommand(greetCmd)\tgreetCmd.Flags().StringP(name, n, , Sets the name to greet)\tgreetCmd.MarkFlagRequired(name)func main() if err := rootCmd.Execute(); err != nil fmt.Println(err) os.Exit(1) 输出： This is a simple greeting application with a greet command.Usage: greet_app [command]Available Commands: completion Generate the autocompletion script for the specified shell greet Greets a user help Help about any commandFlags: -h, --help help for greet_appUse greet_app [command] --help for more information about a command. 对比 设计哲学和易用性 clap: 使用 Rust 的宏来提供强大的编译时功能，如参数解析、验证等。 利用 Rust 的类型安全特性，减少运行时错误。 支持通过派生宏自动从结构体生成命令行解析代码，简化开发流程。 cobra: 采用更传统的命令式编程模型，直观且易于上手。 通过组合命令对象来构建复杂的命令行应用。 提供了一套完整的生成工具来创建命令和配置，促进了开发速度。 功能和特性 clap: 自动生成帮助信息、版本信息等。 支持多级子命令。 支持自定义验证器和复杂的参数关系（如互斥、依赖等）。 cobra: 支持自动生成帮助文档。 内置命令自动补全脚本生成功能。 支持持久化命令行标志到配置文件。 通过插件支持增加额外的子命令。 能够轻松地与其他 Go 库集成，如 Viper 用于配置管理。 性能 clap: 由于 Rust 的编译时优化，clap 在解析命令行参数时通常会有更好的性能。 更少的运行时开销，尤其是在处理大量复杂命令行参数时。 cobra: 性能对于大多数命令行应用来说已经足够，但可能不如 clap 优化。 Go 的运行时可能会引入额外的开销，尤其是在并发处理时。","tags":["rust","clap"],"categories":["rust","rust 常用库"]},{"title":"Rust reqwest 简明教程","path":"/2024/03/02/rust-crate-reqwest/","content":"概述 reqwest 是 Rust 中一个非常流行和强大的 HTTP 客户端库，它提供了一种简单的方式来发送 HTTP 请求并处理响应。reqwest 支持阻塞和非阻塞（异步）请求，使其适合于各种不同的应用场景。在这篇博文中，我们将详细介绍如何使用 reqwest 发送各种 HTTP 请求，并处理返回的响应。 开始之前 在开始编写代码之前，你需要在你的 Rust 项目中添加 reqwest 依赖。打开你的 Cargo.toml 文件，并添加以下内容： [dependencies]reqwest = version = 0.12.4, features = [json] tokio = version = 1, features = [full] serde = version = 1.0.197, features = [derive] serde_json = 1.0.114 这里我们还添加了其他几个依赖： tokio: 在后面的示例中，我们将使用 reqwest 的异步功能。 serde: 用于数据解析，在示例中，我们会演示 json 数据的解析。 serde_json: 便于使用 json! 宏快速构建 json 数据。 发送 GET 请求 发送一个 GET 请求是最基本的 HTTP 操作。以下是如何使用 reqwest 发送 GET 请求并设置请求头的示例： use reqwest::header;#[tokio::main]async fn main() - Result(), reqwest::Error let params = [(key1, value1), (key2, values)]; let client = reqwest::Client::new(); let body = client .get(http://httpbin.org/get) // set query params .form(params) // set request headers .header(header::USER_AGENT, My Rust Program) .header(header::CONTENT_TYPE, application/json) .send() .await? .text() .await?; println!(body = :?, body); Ok(()) 在这个例子中，我们使用 reqwest::get 函数发送一个 GET 请求到 \"https://httpbin.org/get\"，并通过 text 方法获取响应的文本内容。 发送 POST - text 请求 use reqwest::Client;#[tokio::main]async fn main() - Result(), reqwest::Error let client = Client::new(); let res = client.post(http://httpbin.org/post) .body(the exact body that is sent) .send() .await? .text() .await?; println!(body: :?, res); Ok(()) 发送 POST - form 请求 #[tokio::main]async fn main() - Result(), reqwest::Error let params = [(key1, value1), (key2, values)]; let client = reqwest::Client::new(); let res = client.post(http://httpbin.org/post) .form(params) .send() .await? .text() .await?; println!(body: :?, res); Ok(()) 发送 POST - json 请求 发送 POST 请求通常用于向服务器提交数据。以下是如何使用 reqwest 发送包含 JSON 数据的 POST 请求的示例： use reqwest;use serde_json::json;#[tokio::main]async fn main() - Result(), reqwest::Error let client = reqwest::Client::new(); let res = client.post(https://httpbin.org/post) .json(json!(key: value)) .send() .await?; let body = res.text().await?; println!(Body: , body); Ok(()) 这里我们使用 Client::post 方法创建一个 POST 请求，并通过 json 方法设置 JSON 负载。然后，我们调用 send 方法发送请求。 处理 JSON 响应 use reqwest;use serde::Deserialize;#[derive(Deserialize)]struct Ip origin: String,#[tokio::main]async fn main() - Result(), reqwest::Error let ip: Ip = reqwest::get(https://httpbin.org/ip) .await? .json() .await?; println!(IP: , ip.origin); Ok(()) 在这个示例中，我们定义了一个 Ip 结构体来表示 JSON 响应，然后使用 json 方法将响应反序列化为 Ip 类型。 总结 reqwest 库为 Rust 提供了一个功能丰富而灵活的 HTTP 客户端，适用于各种网络编程任务。无论是简单的数据获取还是复杂的 API 交互，reqwest 都能帮助你以简洁的 Rust 代码完成任务。希望这篇博文能帮助你开始使用 reqwest 来开发网络相关的 Rust 应用！","tags":["rust"],"categories":["rust","rust 常用库"]},{"title":"深入浅出 Go 语言的 GPM 模型（Go1.21）","path":"/2024/01/20/go-gpm/","content":"引言 在现代软件开发中，有效地利用并发是提高应用性能和响应速度的关键。随着多核处理器的普及，编程语言和框架如何高效、简便地支持并发编程，成为了软件工程师们评估和选择工具时的一个重要考量。在这方面，Go 语言凭借其创新的并发模型—GPM（Goroutine, P, M）—在众多编程语言中脱颖而出，为开发者提供了强大的工具，以简单、高效的方式实现并发。 自从 2009 年首次发布以来，Go 语言就以其出色的性能、简洁的语法和对并发的原生支持赢得了广泛的关注。尤其是其并发模型，被设计为能够充分利用现代多核处理器的能力，同时隐藏底层的线程管理和同步复杂性，让开发者能够以更直观、更高级的抽象来构建并发程序。GPM 模型，作为 Go 语言并发编程的核心，通过 Goroutine、P（processor）、M（machine）三者的协同工作，实现了一种高效且易于管理的并发机制。 本文将基于 Go1.21 深入浅出地探讨 Go 语言的 GPM 模型，主要分为几个部分： 首先从其设计理念出发，详细解析 Goroutine、P 和 M 三者的角色、工作原理及其相互之间的交互方式。 然后引入几个关键问题，我们会从结论上先总结 GPM 的核心要点，内容包括协程调度循环、调度策略和调度时机。 接着我们会深入源码，去一步步洞察 Go 语言设计者是如何实现 GPM 模型中的各个要点的，这个过程会比较繁琐，但其实也比较有趣，感兴趣的读者可以阅读这一块，若只是想对 GPM 模型有个大概了解，那么停留在上一步也足矣了。 最后我们基于前面的分析，总结 G、P、M 三大组件在 Go 程序运行过程中的状态流转图。 通过对 GPM 模型的探讨，我们不仅能够理解 Go 语言如何在众多现代编程语言中以其并发编程能力脱颖而出，还能够洞察其设计背后的智慧，以及这一模型如何随着 Go 语言版本的迭代而不断进化和优化。无论你是对 Go 语言充满好奇的新手，还是希望深化理解其并发模型的经验开发者，本文都将为你提供宝贵的视角和深刻的洞见。 结论先行 GPM 调度原理图 Goroutine 调度原理图 Goroutine 底层结构 Goroutine 底层结构示例 调度器 P 底层结构 P 底层结构 GPM 调度循环 GPM 调度循环图 GPM 协程调度优先级与顺序 Go 协程调度优先级与顺序 寻找可执行 G 过程 findRunnable() 协程切换时机 Go 协程切换时机 GPM 模型 1. 概览 这里有一张很流行的 Goroutine 调度原理图： Goroutine 调度原理图 代号 名称 定义位置 作用 Sched 调度器 proc.c 维护有存储 M 和 G 的队列以及调度器的一些状态信息等。 M Machine 系统线程 runtime.h 它由操作系统管理的，Goroutine 就是跑在 M 之上的；M 是一个很大的结构，里面维护小对象内存 cache（mcache）、当前执行的 Goroutine、随机数发生器等等非常多的信息。 P Processor 处理器 runtime.h 它的主要用途就是用来执行 Goroutine 的，它维护了一个 Goroutine 队列，即 runqueue。Processor 是让我们从 N:1 调度到 M:N 调度的重要部分。所有的 P 都在程序启动时创建，并保存在数组中，最多有 GOMAXPROCS（可配置）个。 G Goroutine 实现的核心结构 runtime.h 它包含了栈，指令指针，以及其他对调度 Goroutine 很重要的信息，例如其阻塞的 channel。 Global Queue 全局队列 proc.h 存放等待运行的 G。全局队列可能被任意的 P 加锁去获取里面的 G。 P Local Queue P 的本地队列 proc.h 同全局队列类似，存放的也是等待运行的 G，但存放的数据有限，不会超过 256 个。新建 G 时，G 会优先加入本地队列。如果队列满了，则会把本地队列中一半的 G 以及新 G 一起移动到全局队列。 通过这个原理图我们知道 Go 语言的 GPM 模型的作用非常简单，它就是一个“精打细算”的工具。以前单进程无法充分利用 CPU 资源，所以引入了多进程。又因为进程拥有的资源太多，其创建、切换和销毁都会占用很长时间，所以引入了更小粒度的线程。随着计算机科学的进步，现在看来，线程拥有的资源也是“比较多”的，所以线程的创建、切换和销毁代价也是“相对大”的。所以很多编程语言就引入了协程这个概念，其核心目的就是应用层自己抽象一个比线程更小粒度的调度单元，应用层结合操作系统的多线程能力，自己来管理“调度单元”的创建、切换和销毁，从而尽可能减少由线程切换带来的开销，以做到更轻量级的并发。 不同的编程语言可能有不同的实现，而关键就在于如何让调度更快、开销更小。这便是我们本文要探讨的主要内容。 Go 语言的实现 线程想运行任务就得获取 P，从 P 的本地队列获取 G，当 P的本地队列为空时，M 会尝试从全局队列获得一批 G 放到 P的本地队列，或者从其他 P 的本地队列中“偷”一半 G 放到自己的本地队列。然后M 运行 G，G 执行之后，再从 P 获取下一个 G，如此不断重复下去。 在进入更加具体深入的讨论之前，我们需要重点思考以下几个问题： G 我们可以随便创建，可能有成千上万个，那 P 和 M 有多少个呢？ P 和 M 什么时候被创建呢？ 操作系统只知道线程，所以实际上还是线程在执行任务，那么 G 是如何调度到线程上并执行的呢？ 如何防止协程饥饿？ 如何减少频繁地创建和销毁线程？ 多个线程从全局队列拿 G 如何解决并发问题？又如何减少这种数据竞争呢？ 在整个 Go 调度协程的过程中，G、P、M 有哪些状态？它们又是如何轮转的呢？ 如果你对这几个问题有兴趣，请继续阅读下文。 2. P 和 M 的个数问题 P 的数量由启动时环境变量 $GOMAXPROCS 或者程序中 runtime.GOMAXPROCS() 决定。这意味着在程序执行的任意时刻都只有 GOMAXPROCS 个 Goroutine 在同时运行。 M 的数量由 Go 语言本身的限制决定，Go 程序启动时会设置 M 的最大数量为 10000 个，但是内核很难支持这么多的线程数，所以这个限制可以忽略。可以使用 runtime.SetMaxThreads() 设置 M 的最大数量。 3. P 和 M 何时被创建 P 的创建时机在确定了 P 的最大数量 n 后，runtime 会根据这个数量创建 n 个 P。 M 创建的时机是在当没有足够的 M 来关联 P 并运行其中可运行的 G 的时候，如所有的 M 此时都阻塞住了，而 P 中还有很多就绪任务，就会去寻找空闲的 M，如果没有空闲的 M，就会去创建新的 M。 4. 调度循环 在讨论 G 是如何被调度到 M 去执行的时候，我们需要先介绍 GPM 模型中两个比较特殊的角色：m0 和 g0。 4.1 m0 定义：m0 是 Go 程序启动时创建的第一个 M。它是由 Go 运行时系统直接从操作系统线程创建的，不是从线程池中获取的。 作用：m0 负责初始化和启动 Go 运行时环境，包括创建调度器、分配第一个 P（p0），并创建其他系统级别的资源。在程序的整个生命周期中，m0 会继续存在，即使它可能不执行任何 Go 代码。 特点：m0 不同于其他 M，因为它不是从线程池中获取的。它可能没有绑定任何 P，除非程序中只有一个 P（即 GOMAXPROCS 设置为 1）。 4.2 g0 定义：g0 是每个 M 的特殊 Goroutine，它不执行任何实际的 Go 代码。每个 M 在创建时都会分配一个 g0。 作用：g0 主要用于执行调度器代码和进行系统调用。当 M 需要执行这些非用户代码时，会切换到 g0 的栈上运行。 特点：g0 拥有自己的栈，这个栈用于存放调度器函数和系统调用的数据。这意味着当执行这些操作时，不会影响当前运行的用户 Goroutine 的栈。 4.3 协程栈切换 g0 是 M 中负责调度其他 g 的协程，所以一个 M 中的协程调度其实就是在 g 和 g0 之间不断切换： 协程 g 与 协程 g0 的对应关系 大致过程如下： 当 M 执行一个 G（用户 Goroutine）时，它使用 G 的栈来运行用户代码。 当需要执行系统调用或调度器相关的代码时，M 会切换到 g0。g0 拥有自己的栈，专门用于执行系统调用和调度器代码，这样可以避免污染用户 Goroutine 的栈空间。在 g0 上，M 可以执行如内存分配、调度决策、处理 Goroutine 的创建和销毁等操作。 完成系统调用或调度器任务后，M 会切换回之前的 G，继续执行用户代码。这个过程会从 g0 的栈切换回 G 的栈。 详细细节我们留到后面的源码分析揭晓。 5. 调度策略 Go 协程调度优先级与顺序 5.1 获取本地运行队列 P 会优先尝试从自己的本地队列中寻找就绪的 G，它一般会优先调度最近加入的。 因为这个时候可能由其他 P 来窃取 G，所以这里是需要同步机制的，Go 采用原子操作来降低同步开销。 本地队列 G 的个数不超过 256 个，如果在创建 G 的时候本地队列满了，会将本地队列中 1/2 的 G 连同新创建的 G 一起放入全局队列中。 5.2 获取全局运行队列 P 会优先本地队列，然后才全局队列，这有个好处： 如果只有全局队列，那么所有的 P 都需要去竞争全局队列中的 G，这个时候需要上锁，且数据竞争会比较激烈，性能较差。 通过每个 P 维护一个自己的本地队列可以减少并发冲突，如果实在需要去全局队列拿 G，也可以一次性拿多个，大大减少了并发冲突的情况发生。 但这又带来了一个问题，全局队列中协程的饥饿问题，因为 P 会优先调度最近加入到自己本地队列中的 G，那可能会一直有新的 G 被创建，导致全局队列中的 G 没有机会被调度到。Go 的解决思路是： P 每调度 61 次后，就会从全局队列中获取一个 G 来运行。 5.3 获取准备就绪的网络协程 如果本地队列和全局队列都找不到就绪的 G 可以执行的话。调度会通过 runtime.netpoll 获取可以运行的网络协程。 Go 语言的网络模型是对不同操作系统平台上 I/O 多路复用技术的封装。 当 Goroutine 在进行网络 I/O 时，它会被挂起，线程会去执行其他 Goroutine。一旦 I/O 操作完成，该 Goroutine 会被唤醒并重新排队等待执行。 5.4 系统调用 当一个 Goroutine 执行系统调用时，它可能会被阻塞，这时它的执行线程（M）可能会释放当前绑定的处理器（P），以便其他 Goroutine 可以在该 P 上运行。 5.5 协程窃取 空闲的 M 如果绑定了 P，那么它的 P 会一直尝试从其他 P 的队列中窃取 Goroutine，以平衡负载和避免空闲。这个时候为了让每个 P 都有可能被窃取，Go 没有直接顺序遍历 P 列表，而是采用了一种相对随机的方式去遍历 P 列表，直到找到可以运行的协程就返回。M 不断寻找可执行 G 的这段期间，它被称为自旋线程。 所以为减少创建、切换和销毁线程的开销，Go 做了至少两点努力： 偷取（Work Stealing）机制 当本线程无可运行的 G 时，它所绑定的 P 会尝试从其他线程绑定的 P 窃取 G，而不是销毁线程。 移交（Hand Off）机制 当本线程因为 G 进行系统调用而阻塞时，线程会释放绑定的 P，把 P 移交给其他空闲的线程执行。 6. 调度时机 Go 语言的调度器结合了抢占式调度和协作式调度，以下是 Go 中这两种调度方式的具体实现和特性： 6.1 协作式调度（Cooperative Scheduling） 阻塞操作： 当 Goroutine 执行阻塞操作（如通道操作、等待锁、系统调用等）时，它会主动放弃 CPU 控制权，允许调度器切换到其他 Goroutine。 显式调度： Goroutine 显式请求 runtime.Gosched() 调用，调度器进行调度。 这个时候回从当前协程切换到 g0 协程，取消 G 与 M 之间的绑定关系，把 G 放入全局队列中。 6.2 抢占式调度（Preemptive Scheduling） 基于时间的抢占： 从 Go 1.14 开始，调度器引入了基于时间的抢占机制。如果一个 Goroutine 运行时间超过 10 毫秒，或者在系统调用中超过了 20 微妙，调度器会在安全点（如函数调用、循环迭代、阻塞操作等）尝试暂停该 Goroutine。 这种抢占不依赖于 Goroutine 的显式放弃控制，而是由调度器主动触发。 安全点的选择旨在减少对 Goroutine 执行的干扰，同时确保调度的公平性和响应性。 基于信号的抢占： 当程序在执行过程中既无法主动挂起，也不能进行系统调用，且无法进行函数调用时，就可以使用信号来调度。 信号其实就是线程信号，在操作系统中有很多基于信号的底层通信方式（SIGPIPE / SIGURG / SIGHUP），而我们的线程可以注册对应信号的处理函数。 当线程接收到抢占信号时，会进入一个专门的信号处理器。这个处理器会检查是否处于安全点，如果是，则暂停当前 Goroutine 并进行上下文切换。 源码分析 前面我们对 Go 语言的 GPM 模型在基本概念、调度循环、调度策略和调度时机各个方面都进行了详细的阐述。如果读者只是想简单了解一下 GPM 模型的一些概念和设计思想，那么阅读到这里就基本足够了。如果对其源码实现有兴趣的话，那么请继续往下阅读~ 接下来我们会从以下几个方面来对 Go 语言的 GPM 模型进行源码分析： G、P、M 在 Go 语言中的表示。 G 的创建过程。 g 和 g0 的切换过程。 GPM 的调度机制。 1. G 的底层结构 G 在 Go 里面就是 runtime2.go 里面定义的 g 结构体： type g struct // 栈参数。\t// stack 描述实际的栈内存：[stack.lo, stack.hi)。\t// stackguard0 是在 Go 栈增长序言中比较的栈指针。\t// 通常是 stack.lo+StackGuard，但可以是 StackPreempt 来触发抢占。\t// stackguard1 是在 C 栈增长序言中比较的栈指针。\t// 在 g0 和 gsignal 栈上是 stack.lo+StackGuard。\t// 在其他 goroutine 栈上是 ~0，以触发对 morestackc 的调用（并崩溃）。\tstack stack // 运行时/CGO 已知的偏移\tstackguard0 uintptr // liblink 已知的偏移\tstackguard1 uintptr // liblink 已知的偏移\t_panic *_panic // 最内层的 panic - liblink 已知的偏移\t_defer *_defer // 最内层的 defer\tm *m // 当前 m；arm liblink 已知的偏移\tsched gobuf // 当前协程的运行现场\tsyscallsp uintptr // 如果 status==Gsyscall, syscallsp = sched.sp 在 gc 期间使用\tsyscallpc uintptr // 如果 status==Gsyscall, syscallpc = sched.pc 在 gc 期间使用\tstktopsp uintptr // 栈顶的预期 sp，用于回溯检查\t// param 是一个通用的指针参数字段，用于在特定上下文中传递值，\t// 其他存储参数的方式难以找到。目前有三种用途：\t// 1. 当通道操作唤醒一个阻塞的 goroutine 时，它将 param 设置为\t// 指向已完成阻塞操作的 sudog。\t// 2. 由 gcAssistAlloc1 使用，以向其调用者信号，表明 goroutine 完成了 GC 周期。\t// 以任何其他方式这样做是不安全的，因为此时 goroutine 的栈可能已经移动。\t// 3. 由 debugCallWrap 使用，以将参数传递给新的 goroutine，因为在运行时分配闭包是被禁止的。\tparam unsafe.Pointer\tatomicstatus atomic.Uint32\tstackLock uint32 // sigprof/scang 锁；TODO: 合并到 atomicstatus\tgoid uint64\tschedlink guintptr\twaitsince int64 // g 变为阻塞的大致时间\twaitreason waitReason // 如果 status==Gwaiting\tpreempt bool // 抢占信号，复制 stackguard0 = stackpreempt\tpreemptStop bool // 在抢占时转换为 _Gpreempted；否则，只是取消调度\tpreemptShrink bool // 在同步安全点缩小栈\t// asyncSafePoint 设置为 true 表示 g 在异步安全点停止。\t// 这意味着栈上有没有精确指针信息的帧。\tasyncSafePoint bool\tpaniconfault bool // 在意外的故障地址上触发 panic（而不是崩溃）\tgcscandone bool // g 已扫描栈；由 _Gscan 位在状态中保护\tthrowsplit bool // 必须不分割栈\t// activeStackChans 表示有未锁定的通道指向这个 goroutine 的栈。\t// 如果为 true，栈复制需要获取通道锁来保护这些栈区域。\tactiveStackChans bool\t// parkingOnChan 表示 goroutine 即将在 chansend 或 chanrecv 上停车。\t// 用于标记栈缩小的不安全点。\tparkingOnChan atomic.Bool\traceignore int8 // 忽略竞态检测事件\ttracking bool // 是否跟踪此 G 以获取调度延迟统计\ttrackingSeq uint8 // 用于决定是否跟踪此 G\ttrackingStamp int64 // G 最后开始被跟踪的时间戳\trunnableTime int64 // 可运行时间，运行时清除，仅在跟踪时使用\tlockedm muintptr\tsig uint32\twritebuf []byte\tsigcode0 uintptr\tsigcode1 uintptr\tsigpc uintptr\tparentGoid uint64 // 创建此 goroutine 的 goroutine 的 goid\tgopc uintptr // 创建此 goroutine 的 go 语句的 pc\tancestors *[]ancestorInfo // 创建此 goroutine 的祖先 goroutine 的信息（仅在 debug.tracebackancestors 使用）\tstartpc uintptr // goroutine 函数的 pc\tracectx uintptr\twaiting *sudog // 此 g 正在等待的 sudog 结构（具有有效的 elem 指针）；按锁顺序\tcgoCtxt []uintptr // cgo 回溯上下文\tlabels unsafe.Pointer // 分析器标签\ttimer *timer // 缓存的计时器，用于 time.Sleep\tselectDone atomic.Uint32 // 我们是否参与 select 并且有人赢得了竞赛？\t// goroutineProfiled 指示当前 goroutine 的栈状态 // 是否已经被记录在进行中的 goroutine 性能分析中。\tgoroutineProfiled goroutineProfileStateHolder\t// 每个 G 的追踪状态。\ttrace gTraceState\t// 每个 G 的 GC 状态\t// gcAssistBytes 是此 G 的 GC 协助信用，以分配的字节为单位。\t// 如果为正，则 G 有信用分配 gcAssistBytes 字节而不协助。\t// 如果为负，则 G 必须通过执行扫描工作来纠正这一点。\t// 我们以字节为单位跟踪这一点，以便在 malloc 热路径中快速更新和检查债务。\t// 协助比率决定了这如何对应于扫描工作债务。\tgcAssistBytes int64 可以看到 g 结构字段非常多，这个结构体的设计反映了 Go 语言对并发和协程管理的底层机制，包括栈管理、调度、垃圾回收、异常处理等多个方面。通过这种抽象，Go 语言能够有效地管理成千上万的 goroutine，使得并发编程变得更加简单和高效。 这里我们只关注 GPM 模型相关的内容，需要重点关心以下几个字段： type g struct stack stack // 当前协程的协程栈\tm *m // 当前线程\tsched gobuf // 保存协程的运行现场\tatomicstatus atomic.Uint32 // 协程状态\tgoid uint64 // 协程ID 1.1 协程栈 stack 其中 stack 结构如下，它存储了协程栈的低地址和高地址。 type stack struct lo uintptr // 栈的低地址\thi uintptr // 栈的高地址 1.2 线程抽象 m 而 m 就是 Go 语言对操作系统线程的抽象，这不是实际的线程，这只是 Go 语言对线程相关信息的抽象，以方便更好地调度协程。 type m struct g0 *g // g0 协程，Go 中的主协程\tcurg *g // 现在正在运行的协程\tid int64 // 线程ID\tmOS // 当前操作系统对线程的额外描述信息\t... m 结构体包含了许多字段，这些字段涉及到线程管理、调度、信号处理、系统调用、锁管理等多个方面。这个结构体是 Go 并发模型的核心部分之一，它与 g（goroutine）和 p（processor）结构体一起，构成了 Go 的调度系统的基础。通过这种设计，Go 能够有效地在多个操作系统线程之间调度成千上万的 goroutine，实现高效的并发执行。 1.3 协程上下文 gobuf gobuf 结构体在 Go 语言的运行时系统中用于保存 Goroutine 的执行上下文，特别是在调度和系统调用中。这个结构体保存了足够的信息以便在 Goroutine 被暂停后能够恢复执行。 下面是对 gobuf 结构体中各个字段的解释： type gobuf struct // sp, pc 和 g 的偏移量是已知的（在 libmach 中硬编码）。\t//\t// ctxt 在 GC 方面比较特殊：它可能是一个堆分配的 funcval，\t// 因此 GC 需要跟踪它，但它需要在汇编中设置和清除，\t// 在那里实现写屏障比较困难。然而，ctxt 实际上是一个保存的、活跃的寄存器，\t// 我们只在真实寄存器和 gobuf 之间交换它。因此，我们在栈扫描期间将其视为根，\t// 这意味着保存和恢复它的汇编不需要写屏障。它仍然被类型化为指针，\t// 以便任何其他从 Go 进行的写操作都会获得写屏障。\tsp uintptr // 栈指针\tpc uintptr // 程序计数器\tg guintptr // 指向当前 goroutine 的指针\tctxt unsafe.Pointer // 上下文，用于保存额外的状态或信息\tret uintptr // 用于保存函数返回值\tlr uintptr // 链接寄存器（在某些架构中用于函数调用）\tbp uintptr // 基指针（在启用帧指针的架构中使用） 我们重点需要关注 2 个字段： sp：栈指针，表示当前协程运行到栈中的哪个位置了。 pc：程序计数器，表示当前协程运行到哪一行代码了。 1.4 协程状态 atomicstatus 我记得在 Go1.16 版本中，这个字段的类型还是 uint32： atomicstatus uint32 现在 Go1.21 版本中，已经用了原子操作来减少并发冲突了： atomicstatus atomic.Uint32 可以看到 Go 的底层也是随着版本更新不断优化中的。 runtime2.go 定义了 G 的各种状态，如： _Gidle (0): 表示 G 刚刚被分配，尚未初始化。 _Grunnable (1): 表示 G 在运行队列上。它当前没有执行用户代码。栈不被该 goroutine 拥有。 ... 后面我们会给出 G 状态的流转图。 1.5 举个例子 假设我们现在有以下 Go 代码：main() 调用 do1()，do1() 调用 do2()，do2() 调用 do3()。 func do3() fmt.Println(here is do3)func do2() do3() //\t---------------func do1() do2()func main() do1() 那么当这段程序运行到第 6 行的时候，它的底层结构大概如下图所示： Goroutine 底层结构示例 至于为什么这里有个 goexit()，其实就是为了可以跳回到 g0 协程，后面我们会具体分析到。 2. P 的底层结构 P 的本质是 runtime2.go 里面定义的 p 结构体： type p struct id int32 // P 的唯一标识符 status uint32 // P 的状态，如 pidle/prunning/... link puintptr // P 链接 schedtick uint32 // 每次调度器调用时递增 syscalltick uint32 // 每次系统调用时递增 sysmontick sysmontick // sysmon 观察到的最后一个 tick m muintptr // 关联的 M 的反向链接（如果空闲则为 nil） mcache *mcache // M 缓存 pcache pageCache // 页面缓存 raceprocctx uintptr // 用于竞态检测的上下文 // 延迟结构体池 deferpool []*_defer deferpoolbuf [32]*_defer // Goroutine ID 缓存，减少对 runtime·sched.goidgen 的访问 goidcache uint64 goidcacheend uint64 // 可运行 goroutine 队列，无锁访问 runqhead uint32 runqtail uint32 runq [256]guintptr runnext guintptr // 下一个要运行的 G // 空闲 G 的列表（状态 == Gdead） gFree struct gList n int32 // sudog 缓存 sudogcache []*sudog sudogbuf [128]*sudog // 堆上 mspan 对象的缓存 mspancache struct len int buf [128]*mspan // pinner 对象的缓存 pinnerCache *pinner // P 状态跟踪 trace pTraceState // 每个 P 的持久分配，避免互斥 palloc persistentAlloc // 定时器相关字段 timer0When atomic.Int64 timerModifiedEarliest atomic.Int64 timersLock mutex timers []*timer numTimers atomic.Uint32 deletedTimers atomic.Uint32 timerRaceCtx uintptr // GC 相关字段 gcAssistTime int64 gcFractionalMarkTime int64 gcw gcWork wbBuf wbBuf // 指示是否在下一个安全点运行特定的函数 runSafePointFn uint32 // 指示当前 P 是否正在写入任何统计数据。 // 偶数时表示没有写入，奇数时表示正在写入。 statsSeq atomic.Uint32 // 指示当前的 P 应该尽快进入调度器，无论其上运行的是哪个 G。 // 这是实现抢占式调度的一部分，允许调度器在必要时中断长时间运行的 goroutine， // 以便其他 goroutine 有机会运行。 preempt bool // 记录页面分配、释放和清理跟踪信息的缓冲区。 pageTraceBuf pageTraceBuf p 结构体在 Go 语言的运行时系统中代表了一个处理器（processor），它是调度器的核心组成部分。每个 p 负责管理一组 goroutine 的运行。这个结构体包含了许多字段，涉及到 goroutine 的调度、内存分配、垃圾回收和其他系统级别的操作。 我们重点关注以下几个字段： type p struct m muintptr // 当前负责的线程\t// 本地可运行的协程的队列，可无锁访问\trunqhead uint32 // 队头\trunqtail uint32 // 队尾\trunq [256]guintptr // 长度为 256\trunnext guintptr // 下一个可用的协程的指针 // 抢占标识，指示当前的 P 应该尽快进入调度器，无论其上运行的是哪个 G。 preempt bool P 底层结构 3. Goroutine 的创建 Go 并发能力的优秀之处，就在于它启动一个新的协程实在是太方便了： go func() ... () 那么底层究竟做了什么呢？ 3.1 newproc() Goroutine 通过 proc.go 中的 newproc() 创建： func newproc(fn *funcval) gp := getg() pc := getcallerpc() systemstack(func() newg := newproc1(fn, gp, pc) pp := getg().m.p.ptr() runqput(pp, newg, true) if mainStarted wakep() ) 获取当前 goroutine 和调用者 PC: getg() 获取当前正在执行的 goroutine，getcallerpc() 获取调用者的程序计数器地址。 在系统栈上执行 newproc1: systemstack 确保 newproc1 在系统栈上执行，而不是当前 goroutine 的栈。这是因为新的 goroutine 可能需要更多的栈空间。 创建新的 goroutine: newproc1 被调用来实际创建新的 goroutine。 将新的 goroutine 放入运行队列: runqput 将新创建的 goroutine 放入运行队列。 唤醒处理器: 如果主函数已经开始执行，wakep 用于唤醒一个空闲的 P 来运行新的 goroutine。 3.2 newproc1() func newproc1(fn *funcval, callergp *g, callerpc uintptr) *g // ... (省略了错误检查和获取 M 的代码) // 尝试从 P 的空闲列表获取一个 G，如果没有则创建一个新的 newg := gfget(pp) if newg == nil newg = malg(stackMin) casgstatus(newg, _Gidle, _Gdead) allgadd(newg) // 设置新 G 的栈 totalSize := uintptr(4*goarch.PtrSize + sys.MinFrameSize) totalSize = alignUp(totalSize, sys.StackAlign) sp := newg.stack.hi - totalSize spArg := sp // 清空并设置新 G 的调度器相关字段 memclrNoHeapPointers(unsafe.Pointer(newg.sched), unsafe.Sizeof(newg.sched)) newg.sched.sp = sp newg.stktopsp = sp newg.sched.pc = abi.FuncPCABI0(goexit) + sys.PCQuantum newg.sched.g = guintptr(unsafe.Pointer(newg)) // 设置新 G 的其他字段 gostartcallfn(newg.sched, fn) newg.parentGoid = callergp.goid newg.gopc = callerpc newg.startpc = fn.fn // ... (省略了跟踪和调试相关的代码) casgstatus(newg, _Gdead, _Grunnable) return newg 创建或获取一个新的 goroutine: gfget 尝试从 P 的空闲列表中获取一个 goroutine，如果没有可用的，则通过 malg 分配一个新的。 初始化 goroutine 的栈和调度器: 设置新 goroutine 的栈、程序计数器、调用函数等。这里有个非常核心的点 newg.sched.pc = abi.FuncPCABI0(goexit) + sys.PCQuantum，我们前面留了个疑问，协程栈顶的 goexit 是哪里来的，就是这里来的。这里设置新 goroutine 的程序计数器（pc）指向 goexit 函数。goexit 是每个 goroutine 在退出时必须调用的函数，用于执行清理工作并切换到 g0 栈。 设置父 goroutine ID 和创建点: 记录创建这个新 goroutine 的父 goroutine 的 ID 和 go 语句的位置。 更改 goroutine 状态: 将新 goroutine 的状态从 _Gdead 改为 _Grunnable，使其准备好被调度。 返回新的 goroutine: 函数返回新创建的 goroutine。 3.3 runqput() // runqput tries to put g on the local runnable queue.// If next is false, runqput adds g to the tail of the runnable queue.// If next is true, runqput puts g in the pp.runnext slot.// If the run queue is full, runnext puts g on the global queue.// Executed only by the owner P.func runqput(pp *p, gp *g, next bool) if randomizeScheduler next fastrandn(2) == 0 next = false if next retryNext: oldnext := pp.runnext if !pp.runnext.cas(oldnext, guintptr(unsafe.Pointer(gp))) goto retryNext if oldnext == 0 return gp = oldnext.ptr()\tretry:\th := atomic.LoadAcq(pp.runqhead)\tt := pp.runqtail\tif t-h uint32(len(pp.runq)) pp.runq[t%uint32(len(pp.runq))].set(gp) atomic.StoreRel(pp.runqtail, t+1) return if runqputslow(pp, gp, h, t) return goto retry 随机调度器：如果启用了调度器的随机化（randomizeScheduler），并且 next 为 true（newproc 调用的时候永远都是传的 true），则有一半的概率将 next 设置为 false。这有助于防止调度器的行为过于可预测。 处理 runnext 槽：如果 next 为 true，函数尝试将 gp 放入 pp.runnext 槽。如果该槽已被占用，则将原有的 goroutine 移动到常规运行队列，并重试将新的 gp 放入 runnext。 放入本地运行队列：如果 next 为 false 或 runnext 槽已满，函数尝试将 gp 放入本地运行队列的尾部。如果队列未满，gp 将被成功添加。 处理队列满的情况：如果本地运行队列已满，runqputslow 被调用，尝试将 gp 连同自己队列中一半的 g 放入全局运行队列。如果这也失败了，函数会重试将 gp 放入本地队列。 原子操作：函数使用原子操作来加载和存储队列头（runqhead）和尾（runqtail）指针，以确保多线程环境下的数据一致性和线程安全。 3.4 runqputslow() runqputslow 函数处理本地运行队列满的情况，将 goroutine 批量转移到全局队列。这个函数通过原子操作和锁来确保操作的原子性和线程安全。随机化调度器的使用增加了调度的随机性和公平性。 // Put g and a batch of work from local runnable queue on global queue.// Executed only by the owner P.func runqputslow(pp *p, gp *g, h, t uint32) bool // 从 pp 的本地队列中获取一半的 goroutine var batch [len(pp.runq)/2 + 1]*g\tn := t - h\tn = n / 2\tif n != uint32(len(pp.runq)/2) throw(runqputslow: queue is not full) for i := uint32(0); i n; i++ batch[i] = pp.runq[(h+i)%uint32(len(pp.runq))].ptr() if !atomic.CasRel(pp.runqhead, h, h+n) return false batch[n] = gp // 随机打乱 goroutine 的顺序，以增加调度的随机性\tif randomizeScheduler for i := uint32(1); i = n; i++ j := fastrandn(i + 1) batch[i], batch[j] = batch[j], batch[i] // 串成队列\tfor i := uint32(0); i n; i++ batch[i].schedlink.set(batch[i+1]) var q gQueue\tq.head.set(batch[0])\tq.tail.set(batch[n])\t// 放入全局队列中\tlock(sched.lock)\tglobrunqputbatch(q, int32(n+1))\tunlock(sched.lock)\treturn true 创建批处理数组：函数首先创建一个 batch 数组，用于存储从本地队列中取出的 goroutine。 从本地队列中获取一批 goroutine：函数计算出要从本地队列中取出多少个 goroutine（通常是队列长度的一半），并将它们添加到 batch 数组中。 原子操作更新队列头部：使用原子操作 atomic.CasRel 更新本地运行队列的头部索引，这是一个释放（release）操作，确保之前的读取操作完成。 将当前 goroutine 添加到批处理中：将传入的 gp 添加到 batch 数组的末尾。 随机化调度器：如果启用了随机调度器，函数会随机打乱 batch 数组中的 goroutine 顺序，以增加调度的随机性。 链接 goroutine：将 batch 数组中的 goroutine 链接起来，形成一个队列。 准备全局队列：创建一个 gQueue 结构，并设置其头部和尾部指向 batch 数组中的第一个和最后一个 goroutine。 将批处理放入全局队列：加锁访问全局调度器的锁，然后将整个 batch 队列放入全局运行队列。 4. 调度过程 schedule() Go 的调度器核心执行逻辑都在 proc.go 的 schedule() 函数中。我们先不探讨过多的细节，我们先把整个大体脉络理清楚再说。 简化后的 schedule() 如下： func schedule() // 获取当前正在执行的 M\tmp := getg().m ... // 查找一个可运行的 G，会阻塞住直到返回。\tgp, inheritTime, tryWakeP := findRunnable() ...\t// 执行 g\texecute(gp, inheritTime) execute() 执行 g，它简化后如下： func execute(gp *g, inheritTime bool) mp := getg().m\t..\tgogo(gp.sched) 它调用了 gogo()： func gogo(buf *gobuf) 一般这种格式说明函数是用汇编实现的，我们在 Goland 上可以双击 shift 然后搜索 runtime·gogo： runtime·gogo 不同的平台有不同的实现，但是核心逻辑都是一样的， 它直接操作处理器的寄存器和栈，以实现从一个 goroutine 切换到另一个 goroutine 的功能。 我们后面的发内心会发现 goexit() 最终会调用 schedule()。 这就串起来了，Go 程序启动后会创建 m0 和 g0，所以第一个schedule() 是 g0 调用的，最后通过 gogo 切换到用户协程 g 上面执行业务方法，完事后 g 通过 goexit 回到 schedule()，以此循环反复下去。 现在我们可以来总结一下 GPM 调度循环的过程，大概如下图表示： GPM 调度循环图 下面我们再对这个过程中的关键函数进行细致分析： schedule()：调度入口。 findRunnable()：寻找可执行的 G。 execute()：执行 G。 gogo()：切换协程栈 g0 到 g。 goexit()：退出 g 协程，切换回 g0 栈。 4.1 schedule() func schedule() // 获取当前正在执行的 M\tmp := getg().m // 有锁的话抛出异常，避免该情况下调度出现死锁或其他问题\tif mp.locks != 0 throw(schedule: holding locks) // M 被锁定了特定的 G，这个时候直接执行这个锁定的 G。\tif mp.lockedg != 0 stoplockedm() execute(mp.lockedg.ptr(), false) // Never returns. // CGO 调用需要 g0 栈，所以这个时候不继续调度了，抛出异常。\tif mp.incgo throw(schedule: in cgo)\ttop:\tpp := mp.p.ptr()\tpp.preempt = false\t// 安全点检查：如果当前 M 在自旋的话，应该是没有可执行 G 的。\tif mp.spinning (pp.runnext != 0 || pp.runqhead != pp.runqtail) throw(schedule: spinning with local work) // 查找一个可运行的 G，会阻塞住直到返回。\tgp, inheritTime, tryWakeP := findRunnable() // 调试的时候系统会处于“冻结”状态， // 这里故意通过两次 lock 引入死锁使当前 M 陷入无限等待， // 以在调试时保持当前的调度器和运行时状态不变。\tif debug.dontfreezetheworld 0 freezing.Load() lock(deadlock) lock(deadlock) // 如果当前 M 之前是自旋的，但是现在要准备执行 G 了，那就不是自旋了。\tif mp.spinning resetspinning() // 当用户级调度被禁用时，采用双重检查后如果确实被禁用了， // 那么就把当前 g 放在 sched.disable.runnable 列表中， // 等待调度重启启用时再处理。 // 在 gc 的时候会出现这种情况： // gcStart() - schedEnableUser(false) // gcMarkDone() - schedEnableUser(true)\tif sched.disable.user !schedEnabled(gp) lock(sched.lock) if schedEnabled(gp) unlock(sched.lock) else sched.disable.runnable.pushBack(gp) sched.disable.n++ unlock(sched.lock) goto top // 检查是否需要唤醒一个 P。 // 如果返回的 g 比较特殊，比如要负责 gc，那么这个值会是 true。\tif tryWakeP wakep() // 如果 g 已经绑定了 M，则直接启动该 M 去执行 g。\tif gp.lockedm != 0 startlockedm(gp) goto top // 执行 g\texecute(gp, inheritTime) schedule() 函数是 Go 调度器的核心，负责管理 goroutine 的执行。它包括多个步骤，如检查当前 M 的状态，处理特殊情况（如 goroutine 被锁定到特定的 M，或者 M 正在执行 CGO 调用），以及选择和执行可运行的 goroutine。 4.2 findRunnable() // Finds a runnable goroutine to execute.// Tries to steal from other Ps, get g from local or global queue, poll network.// tryWakeP indicates that the returned goroutine is not normal (GC worker, trace reader) so the caller should try to wake a P.func findRunnable() (gp *g, inheritTime, tryWakeP bool) 通过注释就可以知道这个函数的作用：寻找一个可执行的 goroutine： 尝试从其他 P 窃取 g、从本地获取 g、从全局队列获取 g、从网络轮询器获取 g； 如果是一个特殊的 g，如要负责 gc 或 trace，那么会将 tryWakeP 置为 true，表示调度器需要尝试唤醒或启动一个新的 P 来运行这个 g，以确保了即使在系统负载较低时，这些特殊的 g 也能得到及时处理。 我们只关心它的核心部分： func findRunnable() (gp *g, inheritTime, tryWakeP bool) // 获取当前 M\tmp := getg().mtop: // 获取 M 绑定的 P\tpp := mp.p.ptr()\t// 1. 每 61 次循环调度，就会去全局队列中获取一个 g 来执行\tif pp.schedtick%61 == 0 sched.runqsize 0 lock(sched.lock) gp := globrunqget(pp, 1) unlock(sched.lock) if gp != nil return gp, false, false // 2. 从本地队列中获取 g\tif gp, inheritTime := runqget(pp); gp != nil return gp, inheritTime, false // 3. 从全局队列中获取 g\tif sched.runqsize != 0 lock(sched.lock) gp := globrunqget(pp, 0) unlock(sched.lock) if gp != nil return gp, false, false // 4. 从网络轮询器中获取 g\tif netpollinited() netpollWaiters.Load() 0 sched.lastpoll.Load() != 0 if list := netpoll(0); !list.empty() // non-blocking gp := list.pop() injectglist(list) casgstatus(gp, _Gwaiting, _Grunnable) if traceEnabled() traceGoUnpark(gp, 0) return gp, false, false // 5. 自旋，从其他 P 窃取 g // mp.spinning 这个条件检查当前 M（操作系统线程）是否应该进入自旋状态。 // 自旋状态意味着 M 会积极地寻找工作，而不是休眠。 // 2*sched.nmspinning.Load() gomaxprocs-sched.npidle.Load() // 这个条件确保系统中自旋的 M 的数量不会超过一定比例。 // 这是为了防止在低并发情况下过多的 CPU 使用。\tif mp.spinning || 2*sched.nmspinning.Load() gomaxprocs-sched.npidle.Load() if !mp.spinning mp.becomeSpinning() gp, inheritTime, tnow, w, newWork := stealWork(now) if gp != nil return gp, inheritTime, false if newWork goto top now = tnow if w != 0 (pollUntil == 0 || w pollUntil) pollUntil = w ...\tgoto top 这个过程涉及到几个重要的函数： globrunqget()：从全局队列中寻找可运行的 G。 runqget()：从本地队列中寻找可运行的 G。 netpoll()：寻找可以运行的网络协程。 stealWork()：从其他 P 窃取可运行的 G。 4.3 globrunqget() func globrunqget(pp *p, max int32) *g // 抢全局列表的锁 assertLockHeld(sched.lock) // 如果为空则直接返回 if sched.runqsize == 0 return nil // 确定 n 的大小，即要从全局队列中获取的 g 的个数。 // 这里会结合入参 max 对边界值进行判断，以获得一个合理的 n。 // 一次性最多拿 len(pp.runq)/2 个 g。 n := sched.runqsize/gomaxprocs + 1 if n sched.runqsize n = sched.runqsize if max 0 n max n = max if n int32(len(pp.runq))/2 n = int32(len(pp.runq)) / 2 sched.runqsize -= n // 通过 pop() 从全局队列中弹出 g gp := sched.runq.pop() n-- for ; n 0; n-- gp1 := sched.runq.pop() // 将 g 放入 pp 的本地队列中 // runqput 在前面创建协程的地方已经介绍过了，这里不赘述。 runqput(pp, gp1, false) return gp 4.4 runqget() func runqget(pp *p) (gp *g, inheritTime bool) // runnext 的 g 会优先执行\tnext := pp.runnext\tif next != 0 pp.runnext.cas(next, 0) return next.ptr(), true for // 原子操作获取队头指针 h := atomic.LoadAcq(pp.runqhead) t := pp.runqtail if t == h return nil, false // 从队头获取 g，并通过原子操作更新队头（即抢这个 g） gp := pp.runq[h%uint32(len(pp.runq))].ptr() if atomic.CasRel(pp.runqhead, h, h+1) return gp, false runqget() 函数用于从本地运行队列中获取一个可运行的 goroutine。这个函数只能由拥有该队列的处理器（P）执行。下面是对这个函数的详细解释： 1. 检查 runnext： runnext 是一个特殊的字段，用于存储下一个要运行的 goroutine。如果 runnext 非零，并且能成功通过原子操作（CAS）将其设置为零，则直接返回这个 goroutine。 如果成功获取 runnext 指向的 goroutine，inheritTime 被设置为 true，表示这个 goroutine 应该继承当前时间片的剩余时间。 如果没成功，意味着 runnext 的这个 g 已经被其他 P 给抢了，因为我们可以发现本 P 只可能将其设置为 0，只有其他 P 才会将其设置以为非 0。 2. 从本地队列中获取 goroutine： 使用原子操作加载 runqhead（队列头指针），runqtail（队列尾指针）。 如果 runqhead 等于 runqtail，表示队列为空，返回 nil。 否则，从队列中获取 runqhead 指向的 goroutine，并尝试通过原子操作（CAS）更新 runqhead。 如果更新成功，返回获取到的 goroutine，inheritTime 被设置为 false，表示这个 goroutine 应该开始一个新的时间片。 两个问题： 1. 为什么获取 runqhead 需要上锁，获取 runqtail 就不需要？ 单一生产者：每个本地运行队列只有一个生产者，即与之关联的当前 P。只有这个 P 可以向队列尾部添加新的 goroutine。由于不存在多个生产者的并发写入问题，因此不需要锁来保护队尾。 2. inheritTime 有什么用？ inheritTime 的主要作用是决定新调度的 goroutine 是否应该立即开始一个新的时间片，或者继续使用当前时间片的剩余部分。这在以下两种情况下尤为重要： 继承时间片 (inheritTime == true)：当 runqget 从 runnext 字段获取 goroutine 时，这个 goroutine 被认为是特别优先的，因此它继承了当前时间片的剩余时间。这通常发生在 goroutine 通过特定的同步机制（如通道操作）被明确唤醒时。 开始新的时间片 (inheritTime == false)：当 runqget 从本地运行队列中正常获取 goroutine 时，这个 goroutine 将开始一个全新的时间片。这确保了调度的公平性，使得每个 goroutine 都有机会在给定的时间片内运行。 4.5 netpoll() netpoll() 函数是 Go 语言运行时网络轮询机制的一部分，用于检查网络连接是否准备好进行非阻塞 I/O 操作。这个函数返回一组已经变为可运行状态的 goroutine，这些 goroutine 之前可能因等待网络 I/O 而被挂起。 这里涉及到 Go 语言网络编程原理，在本文中不细究，就简单带过了。 func netpoll(delay int64) gList // 检查轮询器是否初始化。\tif kq == -1 return gList // 设置轮询超时。\tvar tp *timespec\tvar ts timespec\tif delay 0 tp = nil else if delay == 0 tp = ts else ts.setNsec(delay) if ts.tv_sec 1e6 ts.tv_sec = 1e6 tp = ts // 使用 kevent 进行轮询操作，结果放在 events 中。\tvar events [64]keventtretry:\tn := kevent(kq, nil, 0, events[0], int32(len(events)), tp)\tif n 0 if n != -_EINTR println(runtime: kevent on fd, kq, failed with, -n) throw(runtime: netpoll failed) if delay 0 return gList goto retry // 遍历 events 处理轮询事件。\tvar toRun gList\tfor i := 0; i int(n); i++ ev := events[i] // netpollBreakRd 用于唤醒轮询，即唤醒等待中的 goroutine。 if uintptr(ev.ident) == netpollBreakRd if ev.filter != _EVFILT_READ println(runtime: netpoll: break fd ready for, ev.filter) throw(runtime: netpoll: break fd ready for something unexpected) if delay != 0 var tmp [16]byte read(int32(netpollBreakRd), noescape(unsafe.Pointer(tmp[0])), int32(len(tmp))) netpollWakeSig.Store(0) continue // 根据轮询事件的类型（读或写），唤醒相应等待网络 I/O 的 groutine。 var mode int32 switch ev.filter case _EVFILT_READ: mode += r if ev.flags_EV_EOF != 0 mode += w case _EVFILT_WRITE: mode += w if mode != 0 var pd *pollDesc var tag uintptr if goarch.PtrSize == 4 pd = (*pollDesc)(unsafe.Pointer(ev.udata)) tag = 0 else tp := taggedPointer(uintptr(unsafe.Pointer(ev.udata))) pd = (*pollDesc)(tp.pointer()) tag = tp.tag() if pd.fdseq.Load() != tag continue pd.setEventErr(ev.flags == _EV_ERROR, tag) // 标记 goroutine 可执行。 netpollready(toRun, pd, mode) // 返回可运行的 goroutine 列表。\treturn toRun 4.6 stealWork() stealWork() 函数用于尝试从其他处理器（P）窃取可运行的 goroutine 或定时器。 func stealWork(now int64) (gp *g, inheritTime bool, rnow, pollUntil int64, newWork bool) // 获取当前 M 绑定的 P。\tpp := getg().m.p.ptr()\tranTimer := false // 尝试 4 次。\tconst stealTries = 4\tfor i := 0; i stealTries; i++ // 前 3 次尝试窃取 g。 // 第 4 次尝试窃取 timer，并且尝试获取其他 P 的 runnext 中的 g。 stealTimersOrRunNextG := i == stealTries-1 // 随机选一个 P。 for enum := stealOrder.start(fastrand()); !enum.done(); enum.next() // 如果系统正在 GC，则可以直接返回 true，因为可能要负责 gc 了，有事干了。 if sched.gcwaiting.Load() return nil, false, now, pollUntil, true // 获取选中的 P，如果是当前 P 则直接 continue，重试。 p2 := allp[enum.position()] if pp == p2 continue // 第 4 次尝试去窃取 p2 的 timer。 if stealTimersOrRunNextG timerpMask.read(enum.position()) // 检查并可能执行 timer。 tnow, w, ran := checkTimers(p2, now) now = tnow if w != 0 (pollUntil == 0 || w pollUntil) pollUntil = w // 如果执行了 timer，则检查本地队列是否有 g 可以运行， // 因为 timer 会唤醒被挂起的 g。 if ran if gp, inheritTime := runqget(pp); gp != nil return gp, inheritTime, now, pollUntil, ranTimer ranTimer = true // 前 3 次尝试或者第 4 次尝试没有窃取到 timer 的时候， // 就从其他非空闲 P 的本地队列中尝试窃取 g。 if !idlepMask.read(enum.position()) // 如果 stealTimersOrRunNextG 为 true， // 那么会在窃取的时候，尝试窃取 p2 的 runnext。 if gp := runqsteal(pp, p2, stealTimersOrRunNextG); gp != nil return gp, false, now, pollUntil, ranTimer return nil, false, now, pollUntil, ranTimer 阅读源码的好处这就体现了，所有人都告诉我们 P 找不到可运行 G 的时候就会去窃取其他 P 的 G，但没人告诉我们，在这个过程还可能会去窃取其他 P 的 timer 和 runnext。 所谓 timer，即定时器，用于在指定的时间后执行某些操作。这些操作通常包括唤醒等待特定时间的 goroutine，或执行与时间相关的任务。定时器在 Go 的并发模型中扮演着重要的角色，特别是在涉及到时间延迟或周期性任务的场景中。在调度器层面，定时器的管理对于确保及时响应时间相关的事件和维持高效的调度至关重要。通过合理地处理定时器事件，Go 能够在保持高并发性的同时，有效地管理时间延迟和周期性任务。 在 Go 语言的调度器中，跨 P 的定时器窃取是一种优化机制，它有 2 个好处： 保持处理器活跃：当一个 P 没有足够的本地工作时，它可以尝试从其他 P 窃取定时器任务。这样做可以保持该 P 活跃，避免它进入休眠状态，从而提高整体系统的效率。 平衡系统负载：在多核系统中，不同的 P 可能会有不同的负载。跨 P 的定时器窃取有助于在 P 之间平衡负载，特别是在一些 P 非常忙碌而其他 P 相对空闲的情况下。 好的，回过头来，为什么我们会说窃取的时候会从队头窃取呢？为什么是窃取 p2 一半的 g 呢？这个过程就在 runqsteal() 中： func runqsteal(pp, p2 *p, stealRunNextG bool) *g t := pp.runqtail // 从 p2 中获取 n 个 g。\tn := runqgrab(p2, pp.runq, t, stealRunNextG)\tif n == 0 return nil // 返回第 1 个 g，因为它可以直接执行了。\tn--\tgp := pp.runq[(t+n)%uint32(len(pp.runq))].ptr()\tif n == 0 return gp // 如果还有剩下的 g，那么就加入到本地队列中。 // 这里可以看到是从队头加入的，所以需要使用原子操作获取队头。\th := atomic.LoadAcq(pp.runqhead)\tif t-h+n = uint32(len(pp.runq)) throw(runqsteal: runq overflow) atomic.StoreRel(pp.runqtail, t+n)\treturn gp runqgrab() 是窃取 n 个 g 的过程： func runqgrab(pp *p, batch *[256]guintptr, batchHead uint32, stealRunNextG bool) uint32 // 使用无限循环来尝试窃取工作，直到成功或确定没有可窃取的工作。\tfor h := atomic.LoadAcq(pp.runqhead) t := atomic.LoadAcq(pp.runqtail) n := t - h // 这里可以看到，要窃取的个数，就是 pp 本地队列中 g 个数的一半 n = n - n/2 // 如果 n 为 0，且 stealRunNextG == true， // 那么就尝试窃取 pp 的 runnext 中的 g。 if n == 0 if stealRunNextG if next := pp.runnext; next != 0 if !pp.runnext.cas(next, 0) continue batch[batchHead%uint32(len(batch))] = next return 1 return 0 // 如果 n 不为队列长度的一半，则说明队列发生了变化， // 这个时候重新尝试窃取。 if n uint32(len(pp.runq)/2) continue // 将要窃取的 g 从 pp.runq 中转移到 batch 中。 for i := uint32(0); i n; i++ g := pp.runq[(h+i)%uint32(len(pp.runq))] batch[(batchHead+i)%uint32(len(batch))] = g // 使用原子操作尝试更新 pp 的队列头部，即将 g 从 pp.runq 中移除。 if atomic.CasRel(pp.runqhead, h, h+n) return n findRunnable() 的全部过程我们总算是梳理完了，这个过程确实非常精彩，Go 调度器在提高调度性能、确保调度的公平性、平衡系统负载、降低同步开销、减少资源再分配等方面都做了很多的努力，这才让 Go 语言的并发又强大又易用。 下面是对 findRunnable() 一个简单的总结： findRunnable() 4.7 execute() findRunnable() 之后就是 execute()，它的核心过程如下（有删减）： func execute(gp *g, inheritTime bool) mp := getg().m\t// 将 g0 d 线程信息复制到即将要调用的协程 gp 中。\tmp.curg = gp\tgp.m = mp // 修改 gp 的状态为 _Grunning，即运行中。\tcasgstatus(gp, _Grunnable, _Grunning)\tgp.waitsince = 0 // 标记为非抢占\tgp.preempt = false // 用于栈保护，检测栈溢出\tgp.stackguard0 = gp.stack.lo + stackGuard // gogo 会完成 g0 到 g 的协程栈的切换，并从 gp.sched 开始执行。 // sched 字段我们前面介绍过，它是 gobuf 结构体，存储了 sp 和 pc。\tgogo(gp.sched) 所以 execute() 的工作非常简单，其实就是将 g0 的线程信息复制到 gp 上，并修改状态和一些元数据，核心部分其实在 gogo() 中。 4.8 gogo() 前面我们说过，gogo() 会完成 g0 栈到 g 栈的切换，且在不同平台下有不同的视线，这里我们以 asm_arm64.s 为代表来看一下 gogo() 的汇编实现： TEXT runtime·gogo(SB), NOSPLIT|NOFRAME, $0-8\tMOVD\tbuf+0(FP), R5\tMOVD\tgobuf_g(R5), R6\tMOVD\t0(R6), R4\t// make sure g != nil\tB\tgogo(SB)TEXT gogo(SB), NOSPLIT|NOFRAME, $0\tMOVD\tR6, g\tBL\truntime·save_g(SB)\tMOVD\tgobuf_sp(R5), R0\tMOVD\tR0, RSP\tMOVD\tgobuf_bp(R5), R29\tMOVD\tgobuf_lr(R5), LR\tMOVD\tgobuf_ret(R5), R0\tMOVD\tgobuf_ctxt(R5), R26\tMOVD\t$0, gobuf_sp(R5)\tMOVD\t$0, gobuf_bp(R5)\tMOVD\t$0, gobuf_ret(R5)\tMOVD\t$0, gobuf_lr(R5)\tMOVD\t$0, gobuf_ctxt(R5)\tCMP\tZR, ZR // set condition codes for == test, needed by stack split\tMOVD\tgobuf_pc(R5), R6\tB\t(R6) 具体过程如下： runtime·gogo 函数：这个函数用于设置新的 goroutine 上下文。它接收一个指向 gobuf 结构的指针（buf+0(FP)），该结构包含了 goroutine 的上下文信息。 加载 gobuf 并检查 g：加载 gobuf 结构，并检查 g 是否为 nil。 跳转到 gogo：执行无条件跳转到 gogo 函数。 gogo 函数：这个函数实际上完成了上下文切换。 设置当前 goroutine：将 R6 寄存器中的值（新的 goroutine）设置为当前 goroutine。 保存当前 goroutine：调用 runtime·save_g 保存当前 goroutine 的状态。 恢复栈指针和其他寄存器：从 gobuf 结构中恢复栈指针（RSP）、基指针（R29）、链接寄存器（LR）、返回值（R0）和上下文（R26）。 清空 gobuf 结构：将 gobuf 结构中的字段清零。 准备跳转到新的程序计数器位置：从 gobuf 中加载新的程序计数器地址（gobuf_pc(R5)）到 R6。 跳转执行：通过 B (R6) 跳转到新的程序计数器地址，继续执行新 goroutine 的代码。 这段汇编代码是 Go 运行时中处理 goroutine 上下文切换的关键部分。它直接操作处理器的寄存器和栈，以实现从一个 goroutine 切换到另一个 goroutine 的功能。 在 execute() 中是这么调用 gogo() 的： gogo(gp.sched) 所以完成栈的切换后会从 gp.sched 开始，执行代码，前面我们介绍过 sched 是一个 gobuf 结构体： type gobuf struct sp uintptr\tpc uintptr\tg guintptr\tctxt unsafe.Pointer\tret uintptr\tlr uintptr\tbp uintptr 所以会从 pc 处开始执行业务代码，前面在 newproc() 的时候，我们提过一行代码： newg.sched.pc = abi.FuncPCABI0(goexit) + sys.PCQuantum 这行代码的作用，是在协程创建的时候插入一个 goexit 函数的地址，因为这个时候 g 刚创建，所以其实就是往协程栈顶插入了 goexit 的地址。所以当 g 执行完业务代码后，当栈中元素不断弹出后，最终就会弹出 goexit 的地址，然后执行 goexit() 函数，退出当前 g，切换回 g0。 4.9 goexit() goexit 定义在 runtime/stubs.go 中： // goexit is the return stub at the top of every goroutine call stack.// Each goroutine stack is constructed as if goexit called the// goroutines entry point function, so that when the entry point// function returns, it will return to goexit, which will call goexit1// to perform the actual exit.//// This function must never be called directly. Call goexit1 instead.// gentraceback assumes that goexit terminates the stack. A direct// call on the stack will cause gentraceback to stop walking the stack// prematurely and if there is leftover state it may panic.func goexit(neverCallThisFunction) 通过注释我们可以得到 2 个信息： goexit 的位于每个 goroutine 调用栈的顶部。每个 goroutine 的栈被构造得好像 goexit 调用了 goroutine 的入口函数。这意味着当入口函数返回时，它实际上返回到 goexit。 不要直接调用 goexit，应该调用 goexit1。 goexit1 位于 runtime/proc.go 中： // Finishes execution of the current goroutine.func goexit1() if raceenabled racegoend() if traceEnabled() traceGoEnd() mcall(goexit0) 好吧，它调用了 goexit0，原来这才是真正的退出入口，它也位于 runtime/proc.go 中： func goexit0(gp *g) // 获取当前的 M 和 P\tmp := getg().m\tpp := mp.p.ptr() // 修改 gp 的状态为 _Gdead，标志它的终止\tcasgstatus(gp, _Grunning, _Gdead) // 标记 gc 的栈内存是可以进行 gc 扫描的\tgcController.addScannableStack(pp, -int64(gp.stack.hi-gp.stack.lo))\t// 如果 gp 是系统 goroutine，则将系统 goroutine 的计数减少 if isSystemGoroutine(gp, false) sched.ngsys.Add(-1) // 清理 gp 的状态\tgp.m = nil\tlocked := gp.lockedm != 0\tgp.lockedm = 0\tmp.lockedg = 0\tgp.preemptStop = false\tgp.paniconfault = false\tgp._defer = nil\tgp._panic = nil\tgp.writebuf = nil\tgp.waitreason = waitReasonZero\tgp.param = nil\tgp.labels = nil\tgp.timer = nil // 如果启用了垃圾回收（GC）并且 gp.gcAssistBytes 大于 0， // 则将辅助信用归还给全局池。这有助于更好地控制垃圾回收进程。\tif gcBlackenEnabled != 0 gp.gcAssistBytes 0 assistWorkPerByte := gcController.assistWorkPerByte.Load() scanCredit := int64(assistWorkPerByte * float64(gp.gcAssistBytes)) gcController.bgScanCredit.Add(scanCredit) gp.gcAssistBytes = 0 // 将当前 g 从 P 的运行队列中移除\tdropg() // WebAssembly 平台特殊处理\tif GOARCH == wasm gfput(pp, gp) schedule() // 将 gp 放回处理器的可用队列中，这样可以复用 g\tgfput(pp, gp) // 如果 goroutine 被绑定到当前线程上， // 那可能是在做系统调用，cgo 调用或其他特殊任务， // 那么就需要切到 g0，让 g0 来完成后面的调度。 if locked // 如果 goroutine 在终止前曾锁定当前线程， // 则根据不同的操作系统执行不同的处理。 // 在大多数操作系统上，会跳转到 mstart 函数，释放 P 并退出线程。 // 但在 Plan 9 操作系统上，会清除 lockedExt。 if GOOS != plan9 // See golang.org/issue/22227. gogo(mp.g0.sched) else mp.lockedExt = 0 // 继续调度 // 如果执行了 gogo，那就是 g0 在调度。 // 如果没有执行 gogo，那就是 gp 在调度。\tschedule() 到这里我们就完成了对 GPM 调度循环的全过程源码分析了，你可以回到 4. 调度过程 schedule() 看一下我总结的那张图，这回你应该会有更加深入的理解了。 5. 协程切换 如果要一个协程要一直到执行完毕才退出的话，那很可能会造成其他协程饥饿的问题。所以 Go 其实会在一些特殊的时机对协程进行切换，这个过程有抢占式调度，也有协作式的调度。 协程切换的时候，最核心的就是要保存当前协程的现场，以方便回到该协程的时候继续执行剩下的内容。大致过程如下： Go 协程切换 有哪些时机会触发切换呢，这里我直接给出结论： 基于协作的抢占式调度 主动挂起：runtime.gopark() 系统调用结束时：exitsyscall() 函数跳转时：morestack() 基于信号的抢占式调度 信号调度：doSigPreempt() 5.1 主动挂起 runtime.gopack() gopack 协程切换 这个函数位于 runtime/proc.go 中： func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceReason traceBlockReason, traceskip int) if reason != waitReasonSleep checkTimeouts() mp := acquirem()\tgp := mp.curg\tstatus := readgstatus(gp)\tif status != _Grunning status != _Gscanrunning throw(gopark: bad g status) mp.waitlock = lock\tmp.waitunlockf = unlockf\tgp.waitreason = reason\tmp.waitTraceBlockReason = traceReason\tmp.waitTraceSkip = traceskip\treleasem(mp)\tmcall(park_m) gopark 函数的主要目的是使 G 进入休眠状态，等待被唤醒。 最后它调用了 mcall()： // mcall switches from the g to the g0 stack and invokes fn(g)// mcall 切换到 g0，并执行 fn。func mcall(fn func(*g)) 所以这里切换回 g0，并执行了 pack_m： func park_m(gp *g) ...\tschedule() pack_m() 其实就是调用了 schedule() 去进行下一轮调度，这就完成了协程的切换。 当协程被阻塞的时候，就会去调用 runtime.gopark() 主动让出 CPU，切回 g0，等待被唤醒，以此保证最大化利用 CPU 资源。比如以下几种情况： 休眠 channel 通道阻塞 网络 I/O 阻塞 因为执行垃圾回收而暂停 5.2 系统调用结束时 exitsyscall() exitsyscall 协程切换 Go 通过 entersyscall() 进行系统调用，完事后会执行 exitsyscall()，它也位于 runtime/proc.go 中： func exitsyscall() ...\tmcall(exitsyscall0) 其实它最终也是调用 mcall() 切换到 g0， 我们不难猜出，它这里让 g0 去执行 exitsyscall0 函数，做完系统调用的善后后，肯定还是会执行 schedule() 函数进行协程调度。 func exitsyscall0(gp *g) ...\tschedule() 5.3 函数跳转时 morestack() morestack 协程切换 因为函数跳转意味着“压栈”，函数跳转时都会调用这个方法，它的本意在于检查当前协程栈空间是否有足够内存，如果不够就要扩大该栈空间。 为了让每个协程都有执行的机会，并且最大化利用 CPU 资源，Go 语言在初始化时会启动一个特殊的线程来执行系统监控任务，系统监控在一个独立的 M 上运行，不用绑定逻辑处理器 P。当系统监控到协程运行超过 10ms，就将 g.stackguard0 置为 stackPreempt（该值是一个抢占标志）。 const forcePreemptNS = 10 * 1000 * 1000 // 10msfunc retake(now int64) uint32 // 遍历所有的 P\tfor i := 0; i len(allp); i++ pp := allp[i] pd := pp.sysmontick s := pp.status sysretake := false if s == _Prunning || s == _Psyscall t := int64(pp.schedtick) if int64(pd.schedtick) != t pd.schedtick = uint32(t) pd.schedwhen = now else if pd.schedwhen+forcePreemptNS = now // 如果 G 运行时间过长，超过了 forcePreemptNS(10ms)， // 则标记抢占 preemptone(pp) sysretake = true if s == _Psyscall // 如果是系统调用，且已经超过了一个系统监控的 tick(20us)， // 则从系统调用中抢占 p。 t := int64(pp.syscalltick) if !sysretake int64(pd.syscalltick) != t pd.syscalltick = uint32(t) pd.syscallwhen = now continue ... // 标记抢占func preemptone(pp *p) bool mp := pp.m.ptr()\tgp := mp.curg\tgp.preempt = true\tgp.stackguard0 = stackPreempt\treturn true 巧的就是，Go 设计者，让程序在执行 morestack() 函数时顺便判断一下 g 中的 stackguard 是否已经被置为抢占 stackPreempt，如果的确被标记抢占，就回到 schedule() 方法，并将当前协程放回队列中。 morestack 是汇编实现： TEXT runtime·morestack(SB),NOSPLIT|NOFRAME,$0-0\t...\tBL\truntime·newstack(SB) 它最终会调用 newstack()： func newstack() thisg := getg() gp := thisg.m.curg // 1. 判断 gp.stackguard0 是否被标记为抢占 stackguard0 := atomic.Loaduintptr(gp.stackguard0)\tpreempt := stackguard0 == stackPreempt // 2. 如果被标记位抢占，调用 gopreempt_m() if preempt // 3. 最终会去调用 schedule() 去调新的协程执行 gopreempt_m(gp) // never return\tfunc gopreempt_m(gp *g) if traceEnabled() traceGoPreempt() goschedImpl(gp)func goschedImpl(gp *g) ...\tschedule() 5.4 信号调度 doSigPreempt() 当程序在执行过程中既无法主动挂起，也不能进行系统调用，且无法进行函数调用时，就可以使用信号来调度。 信号其实就是线程信号，在操作系统中有很多基于信号的底层通信方式（SIGPIPE / SIGURG / SIGHUP），而我们的线程可以注册对应信号的处理函数。 Go 中是注册了 SIGURG 信号的处理函数 doSigPreempt()，在 GC 工作时，向目标线程发送信号。线程收到信号后，会触发调度。 doSigPreempt 协程切换 doSigPreempt 位于 runtime.signal_unix.go 中： func doSigPreempt(gp *g, ctxt *sigctxt) // 检查此 g 是否要被抢占并且安全抢占\tif wantAsyncPreempt(gp) if ok, newpc := isAsyncSafePoint(gp, ctxt.sigpc(), ctxt.sigsp(), ctxt.siglr()); ok // 2. 调整程序计数器 PC 并异步调用 asyncPreempt ctxt.pushCall(abi.FuncPCABI0(asyncPreempt), newpc) 其中 asyncPreempt 的实现如下： func asyncPreempt()//func asyncPreempt2() gp := getg()\tgp.asyncSafePoint = true //\tif gp.preemptStop mcall(preemptPark) else mcall(gopreempt_m) gp.asyncSafePoint = false asyncPreempt 是汇编实现，最终是调的 asyncPreempt2，它会调用 mcall 切回 g0，并执行 preemptPark 或 gopreempt_m， gopreempt_m 就是前面 morestack 最后调的！不出意外，preemptPack 最后肯定还是调的 schedule()。 func preemptPark(gp *g) ...\tschedule() 5.5 runtime.Gosched() 在我们实际编程中，你可以通过显式调用 runtime.Gosched() 来主动让出 CPU，促进 Go 的下一轮调度，我们来看它的具体实现，肯定还是调的 schedule()，没有意外！ func Gosched() checkTimeouts()\tmcall(gosched_m)func gosched_m(gp *g) if traceEnabled() traceGoSched() goschedImpl(gp)func goschedImpl(gp *g) status := readgstatus(gp)\tif status^_Gscan != _Grunning dumpgstatus(gp) throw(bad g status) casgstatus(gp, _Grunning, _Grunnable)\tdropg()\tlock(sched.lock)\tglobrunqput(gp)\tunlock(sched.lock)\tschedule() 5.6 总结 Go 语言为了确保 P 不会因为 G 运行时间过长或系统调用阻塞时间过长而导致性能下降。它会尝试进行协程切换，以确保任务可以适时地被分配和执行。这有助于保持 Go 程序的并发性能和响应性。而协程切换的方式有基于协作的抢占式调度（主动挂起 runtime.gopark()，系统调用结束时exitsyscall()，函数跳转时 morestack()），也有基于信号的抢占式调度 doSigPreempt()，他们都无一例外的最终调用了 schedule()。 所以总结下来其实还是这张图： Go 协程切换 G、P、M 状态流转 经过我们前面的分析，你可以自行整理 G、P、M 状态的流转，这里我给出几张图供你参考： Go 协程（G）状态转换图 Go 处理器（P）状态转换图 Go M（操作系统线程）状态转换 总结 以上便是对 Go 语言 GPM 模型的全部分享啦！GPM 模型使得 Go 语言能够并发执行成千上万个协程。 为了减少线程“相对昂贵”的切换代价，Go 引入了 GPM，将大量的 Goroutine 分配到少量的系统线程上去执行，并利用多核并行，实现更强大的并发。 为了减小并发冲突，Go 在全局队列的基础上引入了本地队列。 为了避免协程饥饿，Go 又引入了多种协程调度的策略。 为了避免协程阻塞浪费 CPU，Go 引入了多种协程切换的方式。 Go 语言设计者进行了如此复杂的调度器实现，最终交付给 Gopher 的，仅仅是一个 go 关键字这么简单，真的是大道至简，这也是充分印证了那句话：“Go 为并发而生”。 希望本文能对你有所帮助，enjoy~ happy coding~ 参考 深入理解 Go 语言 Go 语言底层原理剖析 深入 Go 底层原理 ChatGPT4 作图工具 excalidraw whimsical","tags":["go"],"categories":["go"]},{"title":"Rust 实战丨绘制曼德博集","path":"/2024/01/17/rust-action-mandelbrot/","content":"曼德博集 曼德博集 曼德博集其实是一个“没什么用”的发现。 曼德博集（Mandelbrot Set）是一种在复平面上形成独特且复杂图案的点的集合。这个集合是以数学家本华·曼德博（Benoit Mandelbrot）的名字命名的，他在研究复杂结构和混沌理论时发现了这个集合。曼德博集是分形几何的一个经典例子，显示了一个简单的数学公式如何能产生无限复杂和美丽的图案。 曼德博集的定义相对简单。对于每一个复数 \\(c\\)，我们考虑以下迭代序列： \\[ Z_{n+1} = z_n^2 + c \\;\\;\\\\ 其中 \\;\\; (z_0 = 0) \\] 曼德博集合由那些使得上述序列不趋于无限大的复数 \\(c\\) 组成。在复平面上，这些点形成了一种独特的图案，通常以一种美丽且艺术的方式呈现。这个图案的边界非常复杂，包含了无限的细节和自相似的结构。这意味着无论你放大图案的哪一部分，你都会发现越来越精细的结构，这些结构在形式上与整体图案相似。 曼德博集合不仅在数学上有意义，也在艺术和科学中有广泛的应用，尤其是在研究混沌理论和复杂系统时。 具体可以看 维基百科-曼德博集 bilibili - 2000 亿倍放大曼德博集 目标功能 最终我们将实现一个命令行工具，它会根据我们输入的参数生成曼德博集图，使用如下： ./mandelbrot FILE PIXELS UPPERLEFT LOWERRIGHT FILE: 曼德博集图生成的图片路经。 PIXELS: 图片分辨率，如 4x3。 UPPERLEFT: 指定在复平面中图片覆盖的左上角，如 4.0,3.0。 LOWERRIGHT: 制定在复平面中图片覆盖的右下角。 所以我们最终会根据指定的图片范围，截取 PIXELS 分辨率大小的曼德博集图： 截取曼德博集示意图 基于以上目标，我们拆分成几个问题： 如何表示复数？ 如何解析分辨率和坐标？ 如何将图上像素映射到复数？ 如何生成曼德博集图？即如何找到那些符合曼德博集的点，并将其进行着色标注？ 如何写入图片文件？ 如何渲染曼德博集？ 如何解析命令行参数？ 如何并发写入图片文件？ 能学到什么 曼德博集是什么？ Rust 中的复数的原理与应用。 Rust 泛型初探。 Rust 中的 Option 和 Result 初探。 Rust 并发初探。 Rust 中如何解析命令行参数？ Rust 如何写入图像文件？ Rust 如何写测试用例？ Rust 实用 crate num、image、crossbeam。 版本 [package]name = mandelbrotversion = 0.1.0edition = 2021[dependencies]image = version = 0.13.0, features = [default, png]num = 0.4.1crossbeam = 0.8rayon = 1.10.0 完整代码：https://github.com/hedon954/mandelbrot/blob/master/src/main.rs 编码实现 0. 创建项目 cargo new mandelbrot cd mandelbrot 1. 复数表示 使用复数，我们需要引入一个 crete：num： cargo add num 其中定义了一个复数类型 Complex： pub struct ComplexT /// 复数的实部 pub re: T, /// 复数的虚部 pub im: T, 这里 T 是 Rust 中的泛型功能，表示任意类型 T，确定好这个结构体的 T 的类型后，其中的属性 re 和 im 的类型也就随之确定了。 2. 解析分辨率和坐标 分辨率格式为：4000x3000 坐标格式为：-1.0,2.0 2.1 解析数对 我们要做的就是，将分辨率拆成 (4000,3000)，将坐标拆为 (-1.0, 2.0)。这里： 带解析的元素 s 是一个字符串 str。 分隔符 separator 是一个字符 char。 返回值是一个元组 (T, T)，其中 T 这里可以是 u64/f32 等数字，它们都需要能从字符串转化而来，即 T:FromStr。 因为解析可能出错，所以我们使用 Option 来承载。 /// 把字符串 `s`（形如 `400×600` 或 ``1.0,0.5）解析成一个坐标对////// 具体来说，`s` 应该具有leftsepright的格式，其中sep是由`separator`/// 参数给出的字符，而left和right是可以被 `T:from_str` 解析的字符串。/// `separator` 必须是 ASCII 字符////// 如果 `s` 具有正确的格式，就返回 `Some(x,y)`，否则返回 `None`fn parse_pairT: FromStr(s: str, separator: char) - Option(T, T) match s.find(separator) None = None, Some(index) = match (T::from_str(s[..index]), T::from_str(s[index + 1..])) (Ok(l), Ok(r)) = Some((l, r)), _ = None, , 我们可以写几个测试用例来验证一下这个函数的正确性，这里我们用到 #[test] 和 assert_eq!： #[test]fn test_parse_pair() assert_eq!(parse_pair::i32(, ,), None); assert_eq!(parse_pair::i32(10,, ,), None); assert_eq!(parse_pair::i32(,10, ,), None); assert_eq!(parse_pair::i32(10,20, ,), Some((10, 20))); assert_eq!(parse_pair::i32(10,20xy, ,), None); assert_eq!(parse_pair::f64(0.5x, x), None); assert_eq!(parse_pair::f64(0.5x1.5, x), Some((0.5, 1.5))); 2.2 转为复数 我们需要的参数 upper_left 和 lower_right 都是复平面中的一个点，所以从字符串中将数对解析完毕后，我们将其赋值到复数的实部和虚部，转为复数实例。 // 把一对用逗号隔开的浮点数解析为复数fn parse_complex(s: str) - OptionComplexf64 match parse_pair(s, ,) Some((re, im)) = Some(Complex re, im ), None = None, 3. 将像素点映射成复数 第 2 步我们其实确定了两件事： 确定截取曼德博集的哪一部分。 要在这个部分中画多少个点。 目标区域中的像素点 这一步我们需要把 x 点转为复数，即确定它的横坐标和纵坐标。这部分可能需要发挥一下你的几何数学能力了（🤡🤡🤡）。 /// 给定输出图像重像素的行和列，返回复平面中对应的坐标////// `pixed` 是表示给图片中特定像素的 (column, row) 二元组。/// `upper_left` 参数和 `lower_right` 参数是在复平面中表示指定图像覆盖范围的点。fn pixed_to_point( /* ·-------------------- bounds.0 re 丨 丨 丨 bounds.1 im */ bounds: (usize, usize), pixed: (usize, usize), upper_left: Complexf64, lower_right: Complexf64,) - Complexf64 let (width, height) = ( lower_right.re - upper_left.re, // 右-左 upper_left.im - lower_right.im, // 上-下 ); Complex re: upper_left.re + pixed.0 as f64 * width / bounds.0 as f64, im: upper_left.im - pixed.1 as f64 * height / bounds.1 as f64, #[test]fn test_pixed_to_point() assert_eq!( pixed_to_point( (100, 200), (25, 175), Complex re: -1.0, im: 1.0 , Complex re: 1.0, im: -1.0 ), Complex re: -0.5, im: -0.75, ); 4. 寻找曼德博集点 什么是曼德博集点？看看上面的定义：曼德博集合由那些使得上述序列不趋于无限大的复数 \\(c\\) 组成。 现在我们可以来表示上述的公式 \\(Z_{n+1} = z_n^2 + c\\) 了： fn complex_square_add_loop(c: Complexf64) let mut z = Complex re: 0.0, im: 0.0 ; loop z = z * z + c 其中我们将泛型结构体 Complex 的 T 确定为 f64，并使用 loop 关键字进行无限循环。 所以我们的目标是什么？找到令 z 不会“飞到”无穷远的 c。 由于复数 \\(c\\) 具有实部 re 和虚部 im，因此可以把它们视为笛卡尔平面上某个点的 x 坐标和 y 坐标，如果 \\(c\\) 在曼德博集中，就在其中用黑色着色，否则就用浅色。因此，对于图像中的每个像素，必须在复平面上相应点位运行前面的循环，看看它是否逃逸到无穷远还是永远绕着原点运行，并相应将其着色。 无限循环肯定是不现实的，我们总要找到退出循环的机会，有 2 个思路： 进行有限次数的迭代，这样可以获得该集合的一个不错的近似值，迭代的次数取决了精度的需要； 业界已证明，一旦 z 离开了以原点为中心的半径 2 的圆，它最终一定会“飞到”无穷远。 所以我们最终确定的函数如下，其中 norm_sqr() 会返回 z 跟复平面原点的距离的平方： /// 尝试测试 `c` 是否位于曼德博集中，使用最多 `limit` 次迭代来判定////// 如果 `c` 不是集合成员之一，则返回 `Some(i)`，其中 `i` 是 `c` 离开以原点/// 为中心的半径为 2 的圆时所需的迭代次数。如果 `c` 似乎是集群成员之一（确/// 切而言是达到了迭代次数限制但仍然无法证明 `c` 不是成员），则返回 `None`fn escape_time(c: Complexf64, limit: usize) - Optionusize let mut z = Complex re: 0.0, im: 0.0 ; for i in 0..limit if z.norm_sqr() 4.0 return Some(i); z = z * z + c None 5. 写入图片文件 我们可以使用 image 这个 crate 来写入图片文件，它支持多种格式图片的读写，并内置了多种颜色色值。 这里我们准备生成 png 图片，且需要对图片进行不同颜色的着色，所以我们引入 default 和 png 这两个 feature。 cargo add image --features default,png 5.1 创建文件 File::create() 我们可以用标准库中的 File::create(filename) 来创建一个文件，成功的话会返回一个文件句柄： let output = File::create(filename)?; 5.2 写入图片 PNGEncoder image 中提供了 PNGEncoder 用于写入 png 图片，它有两个核心方法： implW: Write PNGEncoderW /// Create a new encoder that writes its output to ```w``` pub fn new(w: W) - PNGEncoderW PNGEncoder w: w /// Encodes the image ```image``` /// that has dimensions ```width``` and ```height``` /// and ```ColorType``` ```c``` pub fn encode(self, data: [u8], width: u32, height: u32, color: ColorType) - io::Result() let (ct, bits) = color.into(); let mut encoder = png::Encoder::new(self.w, width, height); encoder.set(ct).set(bits); let mut writer = try!(encoder.write_header()); writer.write_image_data(data).map_err(|e| e.into()) new(w): 传进目标 writer，即我们上面创建的 output。 encode(): 写入图片信息，这里有几个参数： width: u32: 图片宽度。 height: u32: 图片高度。 color: ColorType: 颜色类型，可以是 RGB, Gray(8) 等。 data: [u8]: 像素色值列表，它的长度应该由上面 3 个字段共同决定，如果选取的颜色是 RGB，意味着需要 3 个 u8 才能表示一个像素点的颜色，所以长度为 width _ height _ 3，如果选取的颜色是 Gray(8)，那么我们用 1 个 u8 就可以表示一个像素点的灰度值，所以长度为 width _ height _ 1。本文中我们会采用 Gray(8) 来汇总曼德博集的黑白图。 6. 渲染曼德博集 这一步我们需要来确定上述 PNGEncoder::encode() 的 4 个参数： width: u32: 图片宽度由命令行参数中指定即可。 height: u32: 图片高度由命令行参数中指定即可。 color: ColorType: 本文我们只绘制黑白图，这里使用 ColorType::Gray(8)，它表示图像是一个灰度（单色）图像，每个像素用 8 位（即 1 个字节）来表示。在这种格式中，每个像素的灰度值范围是 0 到 255，其中 0 通常表示黑色，255 表示白色，中间值表示不同的灰度。 data: [u8]: 像素色值列表，我们需要确定 width * height 个像素的灰度值。 首先我们根据第 3 步将像素点映射成复数 \\(c\\)，然后使用第 4 步中的 escape_time() 函数来判断复数 \\(c\\) 是否位于曼德博集中，如果是，则着黑色，即赋值 0，如果不是，则看它迭代了多少次才失败，次数越多，则越接近曼德博集，颜色越深，即越靠近 0，所以赋值 255-time。 最终我们实现的函数如下： /// 将曼德博集对应的矩形渲染到像素缓冲区中////// `bounds` 参数会给缓冲区 `pixels` 的宽度和高度，此缓冲区的每个字节都/// 包含一个灰度像素。`upper_left` 和 `lower_right` 参数分别指定了/// 复平面中对应于像素缓冲区左上角和右上角的点。fn render( pixels: mut [u8], bounds: (usize, usize), upper_left: Complexf64, lower_right: Complexf64,) assert_eq!(pixels.len(), bounds.0 * bounds.1); for raw in 0..bounds.1 for column in 0..bounds.0 let point = pixed_to_point(bounds, (column, raw), upper_left, lower_right); pixels[raw * bounds.0 + column] = match escape_time(point, 255) None = 0, Some(count) = 255 - count as u8, 7. 解析命令行参数 核心逻辑部分到这里其实就完成了，现在我们要做最后一步，就是解析命令行参数，让程序可以根据我们的要求绘制曼德博集图。 7.1 解析 std::env::args() 在 Rust 中解析命令行参数的一个常用方法是使用std::env::args函数，这个函数返回一个迭代器，它包含了命令行上传递给程序的所有参数。对于更复杂的命令行参数解析，可以使用像clap或structopt这样的第三方库，这些库提供了更高级的功能和更好的错误处理。 下面是一个使用std::env::args的基本例子： use std::env;fn main() let args: VecString = env::args().collect(); for arg in args.iter() println!(, arg); 7.2 基础版程序 到这里，我们就可以实现完整的基础版程序了。 fn main() // 读取参数 let args: VecString = env::args().collect(); // 参数个数 = 1 + 4，其中第 1 个是应用程序名 if args.len() != 5 eprintln!(Usage: FILE PIXELS UPPERLEFT LOWERRIGHT, args[0]); eprintln!( Example: mandel.png 1000x700 -1.20,0.35 -1,0.20, args[0] ); std::process::exit(1); // 解析参数 let bounds = parse_pair(args[2], x).expect(error parsing image dimensions); let upper_left = parse_complex(args[3]).expect(error parsing upper left corner point); let lower_right = parse_complex(args[4]).expect(error parsing lower right corner point); let mut pixels = vec![0; bounds.0 * bounds.1]; // 渲染曼德博集 render(mut pixels, bounds, upper_left, lower_right); // 输出图片 write_image(args[1], pixels, bounds).expect(error writing PNG file); 我们在项目根目录下编译一下程序： cargo build --release 会在 target/release 下生成可执行文件，执行： ./target/release/mandelbrot mandel.png 4000x3000 -1.20,0.35 -1,0.20 执行后你应该可以看到我们生成的曼德博集图如下： 程序生成的曼德博集 大概是处于这个位置： 程序截取的局部曼德博集处于整个曼德博集中的位置 8. 并发渲染 在 macOS 或 linux 系统下，我们可以使用 time 来输出程序的执行时间： time ./target/release/mandelbrot mandel.png 4000x3000 -1.20,0.35 -1,0.20./target/release/mandelbrot mandel.png 4000x3000 -1.20,0.35 -1,0.20 3.30s user 0.01s system 98% cpu 3.341 total 笔者使用的电脑为 macbook Pro m2 max 芯片 32 G 内存 12 核，可以看到在单核模式下，差不多需要 3~4s 的时间。 几乎所有的现代机器都有多个处理器核心，而当前这个程序只使用了一个。如果可以把此工作分派个机器提供的多个处理器核心，则应该可以更快地画完图像。 为此，我们可以将图像划分成多个部分，每个处理器负责其中的一个部分，并让每个处理器为分派给它的像素着色。为简单起见，可以将其分成一些水平条带，如下图所示： 将像素缓冲区划分为一些条带以进行并发渲染 crossbeam 是 Rust 中的一个并发编程工具箱，它广泛用于提供各种并发和多线程编程的组件。 crossbeam::scope 是 crossbeam 提供的一个非常有用的功能，它允许你安全地创建临时的线程，并确保这些线程在离开作用域之前结束。 这里我们引入 crossbeam： cargo add crossbeam 我们将 fn main() 中的： render(mut pixels, bounds, upper_left, lower_right); 替换成： // 使用 8 个线程来并发执行let threads = 8;// 计算每个线程负责渲染的高度，向上取整let rows_per_band = bounds.1 / threads + 1; // chunks_mut() 会返回一个迭代器，该迭代器会生成此缓冲区的可变且不可迭代的切片 let bands: Vecmut [u8] = pixels.chunks_mut(rows_per_band * bounds.0).collect(); // crossbeam::scope 确保所有子线程在作用域结束之前完成， // 这防止了悬垂指针和其他数据竞争问题。 crossbeam::scope(|spawner| // 遍历像素缓冲区的各个条带， // 这里 into_iter() 迭代器会为循环体的每次迭代赋予独占一个条带的所有权， // 确保一次只有一个线程可以写入它。 for (i, band) in bands.into_iter().enumerate() // 确定每个条带的参数 let top = rows_per_band * i; let height = band.len() / bounds.0; let band_bounds = (bounds.0, height); let band_upper_left = pixed_to_point(bounds, (0, top), upper_left, lower_right); let band_lower_right = pixed_to_point(bounds, (bounds.0, top + height), upper_left, lower_right); // 创建一个线程，渲染图像 // move 表示这个闭包会接手它所用遍历的所有权， // 所以只有此闭关，即只有此线程可以使用可变切片 band。 spawner.spawn(move |_| render(band, band_bounds, band_upper_left, band_lower_right); ); ) .unwrap(); 再次执行： time ./target/release/mandelbrot mandel.png 4000x3000 -1.20,0.35 -1,0.20./target/release/mandelbrot mandel.png 4000x3000 -1.20,0.35 -1,0.20 3.57s user 0.01s system 335% cpu 1.067 total 可以看到虽然总共使用的 CPU 时间还是 3~4s，但是整个程序的执行时间只缩短到 1s 左右了。 9. rayon 工作窃取 前面我们使用 8 个工作线程优化了曼德博集的绘制速度，大概是 4 倍的速度提升。其实这还不够快。 问题的根源在于我们没有平均分配工作量。计算图像的一个像素相当于运行一个循环。事实上，图像的浅灰色部分（循环会快速退出的地方）比黑色部分（循环会运行整整 255 次迭代的地方）渲染速度要快得多。因此，虽然我们将整个区域划分成了大小相等的水平条带，但创建了不均等的工作负载， 曼德博集程序中的工作分配不均等 使用 rayon 很容易解决这个问题。我们可以为输出中的每一行像素启动一个并行任务。这会创建数百个任务，而 rayon 可以在其线程中分配这些任务。有了工作窃取机制，任务的规模是无关紧要的。rayon 会对这些工作进行平衡。 我们先引入 rayon： cargo add rayon 在 main.rs 中引入 rayon: use rayon::prelude::*; 然后 main 中并发绘制的部分替换为下面的代码： let bands: Vec(usize, mut [u8]) = pixels.chunks_mut(bounds.0).enumerate().collect();bands.into_par_iter().for_each(|(i, band)| let top = i; let band_bounds = (bounds.0, 1); let band_upper_left = pixed_to_point(bounds, (0, top), upper_left, lower_right); let band_lower_right = pixed_to_point(bounds, (bounds.0, top + 1), upper_left, lower_right); render(band, band_bounds, band_upper_left, band_lower_right);); 首先，创建 bands，也就是要传给 rayon 的任务集合。每个任务只是一个元组类型 (usize, mut [u8])：第一个是计算所需的行号，第二个是要填充的 pixels 切片。我们使用 chunks_mut 方法将图像缓冲区分成一些行，enumerate 则会给每一行添加行号，然后 collect 会将所有数值切片对放入一个向量中。（这里需要一个向量，因为 rayon 只能从数组和向量中创建并行迭代器。） 编译： cargo build --release 再次执行： time ./target/release/mandelbrot mandel.png 4000x3000 -1.20,0.35 -1,0.20./target/release/mandelbrot mandel.png 4000x3000 -1.20,0.35 -1,0.20 3.96s user 0.01s system 973% cpu 0.408 total 可以看到，这次速度提升更加明显，总共只用了 0.4s 左右的时间。 以上就是实用 Rust 绘制曼德博集实战的全部内容，enjoy，happy coding~","tags":["rust"],"categories":["rust","rust 实战"]},{"title":"一文彻底掌握浮点数","path":"/2023/12/23/floating-point-number/","content":"经典问题 0.1 + 0.2 = ？ 我们写个 Go 程序来测试一下： func main() var f1 float64 = 0.1\tvar f2 float64 = 0.2\tfmt.Println(f1+f2 == 0.3)\tfmt.Println(f1 + f2) 输出： false0.30000000000000004 如此违背 “常识” 的结果，其实是因为当下计算机体系中小数的表示方式是浮点数，而计算机中对浮点数的表示并非百分百精确的，在表示和计算过程中都有可能会丢失精度。 这迫使必须深入理解浮点数在计算机中的存储方式及性质，才能正确处理关于数字的计算问题。 结论先行 IEEE-754 浮点数 定点数 要理解浮点数的第一步是考虑含有小数值的二进制数字。在这之前，我们来看看更加熟悉的十进制表示法： \\[ d_md_{m-1} ··· d_1d_0 . d_{-1}d_{-2}··· d_{-n} \\] 小数点 . 左边是整数部分，右边是小数部分。其中每个十进制数 di 的取值范围是 0~9。 如十进制的 12.34 即可以表示为： \\[ 1×10^1+2×10^0+3×10^{-1}+4×10^{-2} \\] 那其实二进制也是一样的道理，只不过把其中的 10 换成 2，而 di 的取值范围为 0~1。 如二进制的 101.11 可以表示为： \\[ 1×2^2+0×2^1+1×2^0+1×2^{-1}+1×2^{-2} \\] 如果我们仅考虑有限长度的编码，那么十进制表示法不能准确表达像 1/3 和 5/7 这样的数。类似的，小数的二进制表示法只能表示那些能够被写成以下形式的数： \\[ x × 2^y \\] 其他的值就只能近似地表示。 定点数的整数部分是小数部分的位数是固定不变的，在位数有限的情况下，定点数的取值范围和精度都比较差。于是就有了 IEEE-754 提出的浮点数表示法。 浮点数 所谓“浮点数”（Floating-point numbers），即小数点可以“浮动”，即小数点的位置不是固定的，而是可以根据数值的大小和精度需求移动的。这种表示法允许在广泛的范围内表示数值，同时保持相对恒定的精度。 在计算机中，浮点数通常遵循 IEEE-754 标准。这个标准定义了浮点数的存储和运算方式，确保了不同计算机系统之间的一致性。IEEE-754 用以下形式来表示一个数： \\[ V = (-1)^s×M×2^E \\] 其中： s 符号位（Sign bit）：表示数值的正负。 M 尾数（Mantissa）：表示数值的有效数字。 E 指数（Exponent）：决定小数点的位置。 IEEE-754 将浮点数的位表示划分成三个部分，分别对各个部分进行编码，对应上面公式右边的 3 个字母： 一个单独的符号位 s 直接编码符号 s。 \\(k\\) 位的阶码字段 \\(exp=e_{k-1}\\cdots e_1e_0\\) 编码阶码 E。 \\(n\\) 位小数字段 \\(frac=f_{n-1}\\cdots f_1f_0\\) 编码尾数 M，但是编码出来的值也依赖于阶码字段的值是否等于 0。 在 IEEE-754 标准中，定义了两种精度的浮点数，分别是单精度浮点数（32 位）和双精度浮点数（64 位）。 单精度： 1 位符号位 s 8 位指数 exp 23 位尾数 frac 单精度浮点数 双精度： 1 位符号位 s 11 位指数 exp 52 位尾数 frac 双精度浮点数 根据 exp 的值，浮点数又可以分成三类： 规格化的 非规格化的 特殊的 其中第三类“特殊的”又可以根据 frac 分成两类： 无穷大 不是一个数 NaN（Not a Number） 具体如下表所示： exp frac 规格化的 ≠0 ≠ 255 f 非规格化的 0 f 特殊的 1 f - 无穷大 1 0 - NaN 1 ≠0 对于不同类型的浮点数，在计算公式 \\(V=(-1)^s×M×2^E\\) 中，exp - E 和 frac - M 的方式有所不同。 下面我们来对这几种不同类型进行详细讨论，其中不乏有一些很有趣且充满智慧的设计理念。 特殊值 Special Values 指数部分：全为 0。 尾数部分：全为 0 则表示无穷大，不全为 0 则表示 NaN。 作用：特殊值用于表示那些无法用常规数值表示的情况，如无穷大、非数（NaN）等。这些值通常用于操作的错误或特殊情况的结果，如除以 0、无效操作等。 规格化的值 Normalize Values 指数部分：不全为 0 且不全为 1。 尾数部分：可以是任意值。 作用：用于表示大多数非零数值 在规格化值中： \\(E=e-bias\\) \\(M=1+f\\) 其中 e 即为 exp，，bias 是偏置量，它的值为 \\(2^{k-1} -1\\)，其中 k 为 exp 的位数，故： 在单精度中，\\(bias=2^{8-1}-1=2^7-1=128-1=127\\) 在双精度中，\\(bias=2^{11-1}-1=2^{10}-1=1024-1=1023\\) 其中 f 为 frac 表示的数，范围 \\(0≤f1\\)。 所以一个规格化数，具体可以表示为： \\[ V=(-1)^{sign}×1.frac×2^{(exp-bias)} \\] 这里有 4 个问题： 这个 bias 是什么？ 为什么 E 要 e 去减掉一个 bias？ bias 的值是怎么定下的，如单精度为什么是 127，不是 126 或 128？ M 为什么需要 f 去加上一个 1？ 下面我们来对这 4 个问题进行一一解答。 第 1 个问题，这个 bias 是什么？ bias 是一个预设的偏移量，用于将指数部分的值偏移到全正数，从而简化处理。 第 2 个问题：为什么 E 要 e 去减去一个 bias？ 先说结论：使用 bias（偏置指数，biased exponent）可以允许浮点数以统一的方式表示，同时也使得浮点数的排序和比较变得简单。 首先指数肯定得支持正负形式的出现，那么直接使用无符号整型来表示指数肯定是不行的，因为它无法表示负指数。暂时先抛开 IEEE-754 定下的标准，我们可以尝试用补码来表示指数。 假设我们有两个 32 位的浮点数 A 和 B，并且我们假设它们的指数部分使用 8 位二进制补码表示（这与 IEEE-754 标准不同）。 A 的二进制表示：0 0000010 00000000000000000000000 B 的二进制表示：0 1111110 00000000000000000000000 在这里，第一位是符号位（0 表示正数），接下来的 8 位是以补码形式表示的指数，剩下的 23 位是尾数。 我们想要比较这两个数的大小，需要怎么做呢？ 我们先解析这 2 个数： 符号位：对于 A 和 B，符号位都是 0，表示这是两个正数。 指数部分（使用补码表示） A 的指数为 0000010，解读为正数 +2。 B 的指数为 1111110，在补码表示中，这是一个负数。先加取反后加 1 转换为正数 00000010，它表示 -2。 要比较这 2 个数： 当我们比较 A 和 B 时，首先需要考虑它们的指数。 指数 A 为 +2，而 B 为 -2。即使它们的尾数部分相同（在这个例子中都是 0），A 的实际值要大于 B，因为正指数表示的数值范围远大于负指数。 可以看出：使用补码表示指数增加了比较过程的复杂性，因为我们需要解读补码并考虑其正负。特别是在涉及到负指数的情况下，我们不能仅仅比较二进制表示的大小，而必须将补码转换为实际的数值，然后再进行比较。 现在回过头来看看 IEEE-754 的设计，假设我们有两个单精度（32 位）浮点数 A 和 B： A 的二进制表示为：0 10000010 00000000000000000000000 B 的二进制表示为：0 01111110 00000000000000000000000 解析这两个数： A：符号位为 0（正数），指数部分为 10000010（二进制，对应十进制的 130），尾数部分为全 0。 B：符号位为 0（正数），指数部分为 01111110（二进制，对应十进制的 126），尾数部分为全 0。 计算实际指数值：单精度浮点数的偏置值 bias 为 127，故： A 的实际指数 E = 130 - 127 = 3。 B 的实际指数 E = 126 - 127 = -1。 比较这两个数： 在减去 bias 后，我们可以直接比较指数部分的二进制表示来确定数值的大小。 由于 10000010（130）大于 01111110（126），因此我们可以直接得出 A 大于 B，而无需考虑负指数的复杂表示问题。 这个例子说明了通过减去偏置值，IEEE-754 标准能够简化浮点数的比较和排序操作。偏置后的指数表示方法允许计算机以统一和高效的方式处理浮点数，无论它们的实际数值大小如何。 第 3 个问题：bias 的值是怎么定下的，如单精度为什么是 127，而不是 126 或 128？ bias 值的选择，是为了平衡正负指数的表示范围，并且充分利用指数部分的存储空间。 以单精度为例，exp 占了 8 位，8 位二进制可以表示的值的范围是 \\([0,255]\\)。如果我们选择 127 作为 bias，则存储的指数范围就是 \\([-127,128]\\)。这样可以使得指数部分可以均匀地表示从负大数到正大数的范围（对称）。 在 IEEE-754 标准中，全 0 的指数表示为非规格化数或 0，而全 1 的指数用于表示无穷大或 NaN）。选择 127 作为 bias 可以在保留这些特殊值的同时，提供最大的有效指数范围。 第 4 个问题：M 为什么需要 f 去加上一个 1？ 在规格化数中隐含最高位 1 是为了提高尾数部分的表示效率，从而增加精度。 其实这跟科学计数法的很像的，为了确保浮点数表示的唯一性，IEEE-754 规定规格化浮点数最高位一定是非零的。如果不规定最高位非零，同一个数可以有多种不同的浮点表示，例如，在二进制中 0.5 可以表示为 \\(1.0×2^{-1}\\)，也可以表示位 \\(0.1×2^0\\) 或 \\(0.01×2^1\\) 等等。这种多重表示会使浮点运算变得复杂且低效。 那既然最高位总是 1，那就没必要显示存储了，还可以使尾数部分中多 1 位的存储空间，从而允许存储更多的有效数字，以提高精度。 非规格化的值 Denormalized Values 指数部分：全为 0。 尾数部分：可以是任意值。 作用： 提供表示数值 0 的方法。因为规格化中 \\(M≥1\\)，所以无法表示 0。 用于表示非常接近于 0 的数值，这些数值太小，无法用规格化格式表示。它们填补了 0 和最小规格化正数之间的间隙，提供了渐近于 0 的连续表示，防止了所谓的“下溢”。 在非规格化值中： \\(E=1-bias\\) \\(M=f\\) 所以一个规格化数，具体可以表示为： \\[ V=(-1)^{sign}×0.frac×2^{(1-bias)} \\] 那这里又有 2 个问题了： 为什么指数部分不是 \\(0-bias\\) 而是 \\(1-bias\\)？ 为什么 M 不需要隐含的 1 了？ 第 1 个问题：为什么指数部分不是 0-bias 而是 1-bias？ 这是一个特殊的设计，旨在使非规格化数能够平滑地连接到规格化数的最小正值。 最小的规格化数的指数为 1 - bias。为了在数值上平滑地过渡到非规格化数，非规格化数的实际指数也被设定为 1 - bias。这样，非规格化数就可以代表那些小于最小规格化正数的数值，而不会出现一个数值的“间隙”。 第 2 个问题：为什么 M 不需要隐含的 1 了？ 不包含隐含的 1 使得非规格化数能够在浮点数表示中填补 0 和最小规格化数之间的空隙，提供对极小数值的连续表示。 避免下溢：非规格化数通过允许尾数部分不以隐含的 1 开始（而是以显式的 0 开始），使得它们可以表示比最小规格化数还要小的数值。这对于避免数值下溢至 0 非常重要，尤其是在累积了多次运算后的场合。 精度牺牲：使用非规格化数的代价是牺牲了一些精度。由于没有隐含的最高位 1，非规格化数的精度较低。但这是为了在非常小的数值范围内提供数值的连续性所做的必要妥协。 总结 规格化值、非规格化值和特殊值三种类型共同构成了 IEEE-754 浮点数标准的完整表示体系，使得浮点数能够在计算机中有效低处理从非常小到非常大的数值范围，同时还能应对特殊的计算情况。 举例 参考《深入理解计算机系统》，我们以 8 位浮点数为例，其中： 1 位符号 s 4 位指数 exp 3 位尾数 frac 可以算出 \\(bias=2^{4-1}-1=2^3-1=8-1=7\\)。 8 位浮点数（≥0部分） 其中靠近 0 的是非规格化值： 8 位浮点数 - 非规格化值 以 0 0000 001 为例： \\[ V = (-1)^s×M×2^E \\\\ = (-1)^s×0.frac×2^{1-bias} \\\\ = (-1)^0×(0+(1/8))×2^{1-7} \\\\ = 1×1/8×2^{-6} \\\\ =2^{(-9)} \\\\ = 1/512 \\] 再往下，就是规格化值： 8 位浮点数 - 规格化值 以 0 0110 110 为例： \\[ V = (-1)^s×M×2^E \\\\ = (-1)^s×1.frac×2^{e-bias} \\\\ = (-1)^0×(1+6/8)×2^{6-7} \\\\ =1×14/8×2^{-1} \\\\ = 14/16 \\\\ =7/8 \\] 整型转为浮点型 下面以一个例子来直观感受一下一个整型是如何转为浮点型的。 现在我们有一个 int32 的整型 123，我们希望将其转为单精度浮点型 123.0。 1. 将整型用二进制表示出来 \\[ 12345_{(10)} = 1111011_{(2)} \\] 2. 规范化表示 \\[ 1111011= 1.111011×2^6 \\] 3. 计算指数 \\[ exp = 6 + 127 = 133_{(10)} = 10000101_{(2)} \\] 4. 确定尾数** 这是个规范化值，所以 1.frac 的 1 省略，又因为单精度浮点数 frac 占 23 位，所以我们需要在 111011 后面再填 17 个 0，即： \\[ frac = 111011 0000 0000 0000 0000 0 \\] 5. 确定符号位 \\[ s = 0_{(2)} \\] 6. 组合起来 12345.0 = 0 10000101 11101100000000000000000 浮点数舍入 由于浮点数的表示具有固定的精度，在进行运算或表示时，经常会遇到无法精确表示的数值，这就需要采用舍入方法来近似表示这些数值。IEEE-754 标准定义了几种不同的舍入模式，以适应不同的计算需求。 舍入模式 最近舍入（Round to Nearest）: 这是最常用的舍入模式，也是默认的模式。 规则是向最接近的可表示值舍入。如果精确结果位于两个可表示值的中点，通常舍入到最近的偶数（即尾数的最后一位为 0）。 这种方法减少了累积误差，确保了在多次运算后的总体精度。 向零舍入（Round Toward Zero）: 这种模式总是舍入到零的方向，即舍去小数部分。 对于正数，这相当于取下限，对于负数，相当于取上限。 向上舍入（Round Up）: 无论正负，都向远离零的方向舍入。 对于正数，舍入后的值不小于原值；对于负数，舍入后的值不大于原值。 向下舍入（Round Down）: 无论正负，都向接近零的方向舍入。 对于正数，舍入后的值不大于原值；对于负数，舍入后的值不小于原值。 舍入的影响 精度损失：由于固定的尾数位数，舍入可能导致精度的损失。 舍入误差：舍入操作本身可能引入误差，这些误差在连续运算中可能会累积。 选择合适的舍入模式：不同的舍入模式适合不同的应用场景。例如，金融计算可能更倾向于使用向零舍入，而科学计算通常使用最近舍入以减少累积误差。 实例 Mode 1.40 1.60 1.50 2.50 -1.50 最近舍入 1 2 2 2 -2 向零舍入 1 1 1 2 -1 向上舍入 2 2 2 3 -1 向下舍入 1 1 1 2 -2 浮点数运算 因为浮点数本身就存在精度问题，所以浮点数运算在计算机中是一个近似过程，涉及到精确度的权衡、特殊值的处理、错误的传播，以及舍入规则的应用。 浮点数加减 浮点数加法和减法首先需要对操作数进行对齐，使得它们的指数相同。这可能涉及将尾数的二进制表示向右移位，可能导致精度损失。 然后执行加法或减法操作。 对结果进行规范化和舍入。 注意，浮点数的加减法不满足结合律、交换律和分配律，这你简单分析下应该就可以理解了，这里不赘述了。 假设我们要在单精度浮点数格式下计算： \\[ 12.375 + 0.1 = ? \\] 第 1 步：转为二进制表示 其中 12.375 我们可以用二进制精确表示： \\[ 12.375_{(10)} = 1100.011_{(2)} \\] 而 0.1 就比较特殊了，用二进制表示的话它会无限循环。 将十进制小数转换为二进制表示涉及到重复乘以 2 的过程，并提取每次乘法后整数部分作为二进制位。这个过程是一个不断重复的过程，直到小数部分变为 0 或开始循环。 取 0.1 的小数部分乘以 2（即 0.1 × 2 = 0.2），整数部分是 0，小数部分是 0.2。 再次取小数部分乘以 2（即 0.2 × 2 = 0.4），整数部分是 0，小数部分是 0.4。 继续这个过程，我们得到以下序列： 0.4 × 2 = 0.8 → 整数部分 0 0.8 × 2 = 1.6 → 整数部分 1 0.6 × 2 = 1.2 → 整数部分 1 0.2 × 2 = 0.4 → 整数部分 0 …（循环开始） 所以，0.1 的二进制表示开始为 0.0001100110011…，并且这个模式会无限循环下去。 第 2 步：规格化 回顾一下这张图： 单精度浮点数 所以 12.375 规格化表示为： 先规范化为 1.xxxx 形式： \\(1100.011_{(2)} = 1.100011 × 2^3\\) 指数为：\\(3 + 127 = 130 = 10000010_{(2)}\\) 尾数为：\\(10001100000000000000000（23\\;位，右边补\\;0）\\) 汇总：\\(0\\;10000010\\;10001100000000000000000\\) 而 0.1 由于无限循环，我们在单精度下只能保留 23 位，并采用最近舍入，所以 0.1 规格化表示为： 先规范为 1.xxxx 形式：\\(0.00011001100110011001100(循环) = 1.10011001100110011001100 × 2^-4\\) 指数为：\\(-4 + 127 = 123 = 01111011_{(2)}\\) 尾数为：\\(10011001100110011001100\\) 汇总：\\(0\\;01111011\\;10011001100110011001100\\) 第 3 步：对齐指数 先把 2 个浮点表示放在一起，好对比： \\(0\\;10000010\\;10001100000000000000000\\) \\(0\\;01111011\\;10011001100110011001100\\) 将两个数的指数对齐，较小的指数增加，同时相应地调整尾数。 这里需要调整将 0.1 的指数从 01111011 调整到 10000010，这里加了 7，所以 0.1 的尾数 1.10011001100110011001100需要右移 7 位，即：0.00000011001100110011001。 第 4 步：相加 现在两个数的指数相同了，我们可以直接把它们的尾数相加： \\[ \\;\\;\\;1.10001100000000000000000 \\\\ +\\;0.00000011001100110011001 \\\\ =\\;1.10001111001100110011001 \\] 第 5 步：规范化结果 这里无需规范化。 第 6 步：舍入 这里没有进位，不需要舍入。 第 7 步：浮点化表示 \\[ 0\\;10000010\\;10001111001100110011001 \\] 第 8 步：转为十进制 \\[ V =(-1)^s×M×2^E \\\\ = (-1)^s×1.frac×2^{e-bias} \\\\ = 1.10001111001100110011001 × 2^3 \\\\ = 1100.01111001100110011001_{(2)} \\\\ = 12.47499942779541015625_{(10)} \\\\ ≈ 12.475_{(10)} \\] 浮点数乘法 符号位计算：结果的符号由两个操作数的符号位决定。如果符号位相同（都是正数或都是负数），结果为正；如果符号位不同，结果为负。 指数相加：两个数的指数相加，并减去偏置值（单精度浮点数中为 127，双精度为 1023）。 尾数相乘：两个数的尾数相乘。这里的尾数包括隐含的最高位 1。 结果规范化：如果乘法的结果需要规范化（即调整为 1.xxxx 的形式），则相应调整指数。 舍入处理：如果需要，对结果进行舍入以适应目标格式。 检查溢出或下溢：如果指数超出了表示范围，则发生溢出（结果可能为无穷大或特殊值）；如果指数太小，发生下溢（结果可能为 0 或非规格化数）。 假设我们要在单精度浮点数格式下计算： \\[ 2.0 × 3.0 = ? \\] 第 1 步：转为二进制表示 \\[ 2.0_{(10)} = 1_{(2)} \\\\ 3.0_{(10)} = 11_{(2)} \\] 第 2 步：规范化 \\[ 1 = 1.0 × 2^0 \\\\ 11 = 1.1 × 2^1 \\] 第 3 步：浮点化 \\[ 2.0 = 0\\;00000001\\;00000000000000000000000 \\\\ 3.0 = 0\\;00000001\\;10000000000000000000000 \\] 第 4 步：乘法操作 符号位：正正得正：\\(0_{(2)} × 0_{(2)} = 0_{(2)}\\) 指数相加并减去偏置值：\\((127+1)+(127+1)-127=129\\) 尾数相乘：\\(1.0_{(2)}×1.1_{(2)} = 1.1_{(2)}\\) 第 5 步：规范化 这里无需规范化。 第 6 步：舍入 这里无需舍入。 第 7 步：浮点化结果 \\[ 0\\;00000010\\;10000000000000000000000 \\] 第 8 步：转为十进制 \\[ V = (-1)^s×1.frac×2^{(e-127)} \\\\ = 0 × 1.1 × 2^2 \\\\ = 110_{(2)} \\\\ = 6.0_{(10)} \\] 浮点数除法 浮点数除法类似于乘法，但有一些不同： 符号位计算：与乘法类似，结果的符号由两个操作数的符号位决定。 指数相减：被除数的指数减去除数的指数，再加上偏置值。 尾数相除：被除数的尾数除以除数的尾数。 结果规范化：如果必要，调整结果使其规范化。 舍入处理：如果需要，对结果进行舍入。 检查溢出或下溢：与乘法类似的检查。 假设我们要在单精度浮点数格式下计算： \\[ 6.0 ÷ 3.0 =? \\] 第 1 步：转为二进制表示 \\[ 6.0_{(10)} = 110_{(2)} \\\\ 3.0_{(10)} = 11_{(2)} \\] 第 2 步：规范化 \\[ 6.0 = 110 = 1.10 × 2^2 \\\\ 3.0 = 11 = 1.1 × 2^1 \\] 第 3 步：浮点化 \\[ 6.0 = 0\\;00000020\\;10000000000000000000000 \\\\ 3.0 = 0\\;00000001\\;10000000000000000000000 \\] 第 4 步：除法操作 符号位：正正得正：\\(0_{(2)} × 0_{(2)} = 0_{(2)}\\) 指数减并加上偏置值：\\((127+2)-(127+1)+127=128\\) 尾数相除：\\(1.1_{(2)}×1.1_{(2)} = 1.0_{(2)}\\) 第 5 步：规范化 这里无需规范化。 第 6 步：舍入 这里无需舍入。 第 7 步：浮点化结果 \\[ 0\\;00000001\\;00000000000000000000000 \\] 第 8 步：转为十进制 \\[ V = (-1)^s×1.frac×2^{(e-127)} \\\\ = 0 × 1.0 × 2^1 \\\\ = 10_{(2)} \\\\ = 2.0_{(10)} \\] Go 语言输出浮点数 func main() var number float32 = 12.375\tfmt.Printf(浮点数：%f , number)\tfmt.Printf(科学计数法：%e , number)\tfmt.Printf(保留 2 位小数：%.2f , number)\tbits := math.Float32bits(number)\tbitsStr := fmt.Sprintf(%.32b, bits)\tfmt.Printf(输出32位浮点表示：%s %s %s , bitsStr[:1], bitsStr[1:9], bitsStr[9:]) math/big Go 语言的 math/big 包提供了对大数的精确计算支持，这些大数的大小超出了标准整数类型（如 int64）或浮点类型（如 float64）的范围。这个包主要用于需要高精度计算的领域，如加密、科学计算等。 主要功能： 算术运算：支持基本的加、减、乘、除等算术运算。 比较操作：可以比较两个大数的大小。 位操作：对大整数进行位操作，如位移、与、或、异或等。 解析和格式化：可以从字符串解析大数，也可以将大数格式化为字符串。 示例： func main() a := big.NewFloat(math.MaxFloat64)\tb := big.NewFloat(math.MaxFloat64)\tsum := big.NewFloat(0)\tsum.Add(a, b)\tfmt.Println(a:, a)\tfmt.Println(sum:, sum)\tsum2 := big.NewFloat(0). SetPrec(15). // 设置精度，prec 越大，精度越高，计算越复杂 SetMode(big.ToZero) // 设置舍入策略\tsum2.Add(a, b)\tfmt.Println(sum2:, sum2)// a: 1.7976931348623157e+308// sum: 3.5953862697246314e+308// sum2: 3.5953e+308 注意事项： 性能考虑：由于 math/big 提供的是任意精度计算，其性能通常低于原生的固定大小数值类型。 内存使用：大数运算可能会消耗更多的内存。 方法链式调用：math/big 的许多方法返回接收者本身，支持链式调用。 参考资料 IEEE-754 深入理解计算机系统 Go 语言底层原理剖析 https://www.bilibili.com/video/BV1zK4y1j7Cn ChatGPT-4","tags":["计算机原理","浮点数"],"categories":["计算机基础"]},{"title":"Go1.21.0 程序启动过程","path":"/2023/12/07/go-start/","content":"版本说明 Go 1.21.0 操作系统：Windows11 Intel64 结论先行 开发关注版 在 Go 语言中，启动顺序通常如下： 导入包：首先，Go 编译器按照源文件中的 import 语句导入所有需要的包。 初始化常量和变量：接着，编译器会初始化包级别（全局）的常量和变量。它们的初始化顺序按照它们在源文件中出现的顺序进行。 执行 init 函数：然后，编译器会执行包级别的 init 函数。如果一个包有多个 init 函数，它们的执行顺序和它们在源文件中出现的顺序一致。 执行 main.main 函数：最后，编译器会执行 main 函数。 Go 程序启动流程 - 开发关注版 深入原理版 命令行参数复制：读取命令行参数，复制到 argc 和 argv。 初始化 g0 栈：g0 是运行时系统的一个特殊的 goroutine，它在程序启动时被创建，用于执行系统调用和协程调度。 runtime.check 运行时检查： 类型长度 指针操作 结构体字段偏移量 CAS atomic 操作 栈大小是否为 2 的幂次。 runtime.args 参数初始化：将 argc 和 argv 的参数赋值到 Go 的变量中。 runtime.osinit 初始化操作系统特点的设置：主要是判断系统字长和 CPU 核数。 runtime.schedinit 初始化调度器： 锁初始化 竞态检测器初始化 调度器设置，设置调度器可以管理的最大线程（M）数目 系统初始化，初始化内存管理、CPU 设置、算法等，这些都是调度器正常工作的基础 设置当前 M 的信号掩码 解析程序参数和环境变量 垃圾收集器初始化 设置 process 的数量 runtime.newproc 创建主协程 g0 并将其放入队列中等待执行。 runtime. mstart 启动调度器：初始化 m0，并调度 g0 去执行 runtime.main。 runtime.main 程序真正入口： runtime.init 启动 gc 执行用户包 init 执行用户函数 main.main Go 程序启动流程 - 深入原理版 如果只是想对 Go 语言程序的启动过程有一个简单的了解，那么阅读到这里就可以结束了。 Runtime 在分析 Go 程序的启动过程之前，我们需要先了解一下 Go 中的 Runtime。所谓 Runtime，即 Go 的运行时环境，可以理解为 Java 的 JVM、JavaScript 依赖的浏览器内核。 Go 的 Runtime 是一份代码，它会随着用户程序一起打包成二进制文件，随着程序一起运行。 Runtime 具有内存管理、GC、协程、屏蔽不同操作系统调用等能力。 综上，Go 程序的运行都依赖于 Runtime 运行，所以我们在分析 Go 语言程序的启动过程的时候，首先要确定程序的入口，即 Runtime。 这部分代码位于 go 源码中 src/runtime 目录下，当你在本机安装 go 后，你可以进入相应的代码目录下，在 Windows 上，你可以在该目录下运行下面命令： dir | findstr rt0 | findstr amd 这里我们输出 go 官方为多种 amd 处理器架构的操作系统所实现的 runtime，如： -a---- 2023/8/25 23:44 754 rt0_android_amd64.s-a---- 2023/8/25 23:44 399 rt0_darwin_amd64.s-a---- 2023/8/25 23:44 448 rt0_dragonfly_amd64.s-a---- 2023/8/25 23:44 442 rt0_freebsd_amd64.s-a---- 2023/8/25 23:44 311 rt0_illumos_amd64.s-a---- 2023/8/25 23:44 425 rt0_ios_amd64.s-a---- 2023/8/25 23:44 307 rt0_linux_amd64.s-a---- 2023/8/25 23:44 309 rt0_netbsd_amd64.s-a---- 2023/8/25 23:44 311 rt0_openbsd_amd64.s-a---- 2023/8/25 23:44 481 rt0_plan9_amd64.s-a---- 2023/8/25 23:44 311 rt0_solaris_amd64.s-a---- 2023/8/25 23:44 1166 rt0_windows_amd64.s 到这里也就明白了，前面所说的 Go Runtime 能力之 “屏蔽不同操作系统调用能力” 的方式便是针对每一种操作系统单独做实现，最后在编译的时候根据操作系统选择对应的实现即可。 这里我们以 rt0_windows_amd64.s 为例，看看这个文件写了些什么： TEXT _rt0_amd64_windows(SB),NOSPLIT|NOFRAME,$-8\tJMP\t_rt0_amd64(SB) 这里我们可以看到它会直接跳到 _rt0_amd64(SB) 这里，在 Goland IDE 中，你可以双击 Shift 键打开搜索，搜索 TEXT _rt0_amd64，就可以发现这个函数位于 asm_amd64.s 文件中，查看该文件： // _rt0_amd64 is common startup code for most amd64 systems when using// internal linking. This is the entry point for the program from the// kernel for an ordinary -buildmode=exe program. The stack holds the// number of arguments and the C-style argv.TEXT _rt0_amd64(SB),NOSPLIT,$-8\tMOVQ\t0(SP), DI\t// argc\tLEAQ\t8(SP), SI\t// argv\tJMP\truntime·rt0_go(SB) 翻译一下上面的注释：_rt0_amd64 是大多数 amd64 系统在使用内部链接时的通用启动代码。这是 exe 程序从内核进入程序的入口点。堆栈保存了参数的数量和 C 语言风格的 argv。 到这里我们就可以非常确定地找到了对应操作系统的 Go 语言程序启动入口了，接下来只需要沿着该入口继续分析即可。 runtime·rt0_go 上面我们分析到 _rt0_adm64 会 JMP 到 runtime·rt0_go 执行，这个函数也位于 asm_amd64.s 文件中，通过分析这个函数，我们可以了解到 Go 语言程序的整个启动过程。 下面将对这整个函数进行一个概览，后面会对重点过程逐个详述。 TEXT runtime·rt0_go(SB),NOSPLIT|NOFRAME|TOPFRAME,$0\t// 读取命令行参数，复制参数变量 argc 和 argv 到栈上\tMOVQ\tDI, AX // argc\tMOVQ\tSI, BX // argv\tSUBQ\t$(5*8), SP\tANDQ\t$~15, SP\tMOVQ\tAX, 24(SP)\tMOVQ\tBX, 32(SP)\t// 从给定的（操作系统）堆栈创建istack。\t// 这是在设置 g0 的堆栈，g0 是运行时系统的一个特殊的 goroutine。\t// 它在程序启动时被创建，用于执行系统调用和协程调度。\t// 这里只是初始化 g0 的堆栈，还没有启动 g0。\tMOVQ\t$runtime·g0(SB), DI\tLEAQ\t(-64*1024)(SP), BX\tMOVQ\tBX, g_stackguard0(DI)\tMOVQ\tBX, g_stackguard1(DI)\tMOVQ\tBX, (g_stack+stack_lo)(DI)\tMOVQ\tSP, (g_stack+stack_hi)(DI)\t// 检查 CPU 的厂商 ID：\t//\t如果没有 CPU 信息，则跳转到 nocpuinfo；\t//\t如果是 Intel 的 CPU，就设置 runtime·isIntel=1，否则跳到 notintel。\tMOVL\t$0, AX\tCPUID\tCMPL\tAX, $0\tJE\tnocpuinfo\tCMPL\tBX, $0x756E6547 // Genu\tJNE\tnotintel\tCMPL\tDX, $0x49656E69 // ineI\tJNE\tnotintel\tCMPL\tCX, $0x6C65746E // ntel\tJNE\tnotintel\tMOVB\t$1, runtime·isIntel(SB)notintel: // 加载 EXA=1 的 cpuid 标志和版本信息\tMOVL\t$1, AX\tCPUID\tMOVL\tAX, runtime·processorVersionInfo(SB)nocpuinfo:\t// 如果有 _cgo_init 就调用它\tMOVQ\t_cgo_init(SB), AX\tTESTQ\tAX, AX\t// 如果 _cgo_init 不存在，那么跳过后面的代码，\t// 直接进入到 needtls 进行 TLS 的初始化。\t// TLS，全称为Thread-Local Storage（线程局部存储），\t// 是操作系统提供的一种机制，允许每个线程拥有一份自己的数据副本。\t// 这些数据在同一线程的所有函数中都是可见的，但对其他线程是不可见的。\t// 这样，每个线程可以访问和修改自己的数据，而不会影响其他线程。\tJZ\tneedtls\t// 将 setg_gcc 函数的地址加载到 SI 寄存器中。\t// 这是 _cgo_init 函数的第二个参数。\tMOVQ\t$setg_gcc(SB), SI // arg 2: setg_gcc\t// 在使用平台的TLS时不使用这第3和第4个参数。\tMOVQ\t$0, DX\tMOVQ\t$0, CX#ifdef GOOS_android\tMOVQ\t$runtime·tls_g(SB), DX // arg 3: tls_g\t// arg 4: TLS base, stored in slot 0 (Androids TLS_SLOT_SELF).\t// Compensate for tls_g (+16).\tMOVQ\t-16(TLS), CX#endif#ifdef GOOS_windows\tMOVQ\t$runtime·tls_g(SB), DX // arg 3: tls_g\t// 调整 Win64 的调用约定。\tMOVQ\tCX, R9 // arg 4\tMOVQ\tDX, R8 // arg 3\tMOVQ\tSI, DX // arg 2\tMOVQ\tDI, CX // arg 1#endif\t// 前面 MOVQ\t_cgo_init(SB), AX，这里就是调用 _cgo_init\tCALL\tAX\t// 在 _cgo_init 之后更新 stackguard\tMOVQ\t$runtime·g0(SB), CX\tMOVQ\t(g_stack+stack_lo)(CX), AX\tADDQ\t$const_stackGuard, AX\tMOVQ\tAX, g_stackguard0(CX)\tMOVQ\tAX, g_stackguard1(CX)#ifndef GOOS_windows\tJMP ok#endif// 针对不同操作系统对 TLS 进行设置needtls:#ifdef GOOS_plan9\t// skip TLS setup on Plan 9\tJMP ok#endif#ifdef GOOS_solaris\t// skip TLS setup on Solaris\tJMP ok#endif#ifdef GOOS_illumos\t// skip TLS setup on illumos\tJMP ok#endif#ifdef GOOS_darwin\t// skip TLS setup on Darwin\tJMP ok#endif#ifdef GOOS_openbsd\t// skip TLS setup on OpenBSD\tJMP ok#endif#ifdef GOOS_windows\tCALL\truntime·wintls(SB)#endif\tLEAQ\truntime·m0+m_tls(SB), DI\tCALL\truntime·settls(SB)\t// 检查 TLS 是否正常工作\tget_tls(BX)\tMOVQ\t$0x123, g(BX)\tMOVQ\truntime·m0+m_tls(SB), AX\tCMPQ\tAX, $0x123\tJEQ 2(PC)\tCALL\truntime·abort(SB)ok:\t//设置 g0 和 m0 和 TLS\tget_tls(BX)\tLEAQ\truntime·g0(SB), CX\tMOVQ\tCX, g(BX)\tLEAQ\truntime·m0(SB), AX\tMOVQ\tCX, m_g0(AX)\tMOVQ\tAX, g_m(CX)\tCLD// 下面的 ifdef NEED_xxx 主要是在检查 CPU 是否支持 Go 运行时系统需要的特性。// 我们需要在设置了 TLS 之后做这个，// 如果失败就跳转到 bad_cpu 报告错误。#ifdef NEED_FEATURES_CX\tMOVL\t$0, AX\tCPUID\tCMPL\tAX, $0\tJE\tbad_cpu\tMOVL\t$1, AX\tCPUID\tANDL\t$NEED_FEATURES_CX, CX\tCMPL\tCX, $NEED_FEATURES_CX\tJNE\tbad_cpu#endif#ifdef NEED_MAX_CPUID\tMOVL\t$0x80000000, AX\tCPUID\tCMPL\tAX, $NEED_MAX_CPUID\tJL\tbad_cpu#endif#ifdef NEED_EXT_FEATURES_BX\tMOVL\t$7, AX\tMOVL\t$0, CX\tCPUID\tANDL\t$NEED_EXT_FEATURES_BX, BX\tCMPL\tBX, $NEED_EXT_FEATURES_BX\tJNE\tbad_cpu#endif#ifdef NEED_EXT_FEATURES_CX\tMOVL\t$0x80000001, AX\tCPUID\tANDL\t$NEED_EXT_FEATURES_CX, CX\tCMPL\tCX, $NEED_EXT_FEATURES_CX\tJNE\tbad_cpu#endif#ifdef NEED_OS_SUPPORT_AX\tXORL CX, CX\tXGETBV\tANDL\t$NEED_OS_SUPPORT_AX, AX\tCMPL\tAX, $NEED_OS_SUPPORT_AX\tJNE\tbad_cpu#endif#ifdef NEED_DARWIN_SUPPORT\tMOVQ\t$commpage64_version, BX\tCMPW\t(BX), $13 // cpu_capabilities64 undefined in versions 13\tJL\tbad_cpu\tMOVQ\t$commpage64_cpu_capabilities64, BX\tMOVQ\t(BX), BX\tMOVQ\t$NEED_DARWIN_SUPPORT, CX\tANDQ\tCX, BX\tCMPQ\tBX, CX\tJNE\tbad_cpu#endif\t// 检查完 AMD64 不同操作系统是否支持 Go 运行时系统需要的特性后，\t// 这里执行 runtime·check 对代码做一下运行时检查。\tCALL\truntime·check(SB)\t// 复制 argc（命令行参数的数量）到 AX 寄存器，\t// 然后把 AX 寄存器的值存到栈上。\tMOVL\t24(SP), AX\tMOVL\tAX, 0(SP)\t// 复制 argv（命令行参数的数组）到 AX 寄存器，\t// 然后把 AX 寄存器的值存到栈上。\tMOVQ\t32(SP), AX\tMOVQ\tAX, 8(SP)\t// 调用 runtime·args 函数处理命令行参数。\tCALL\truntime·args(SB)\t// 调用 runtime·osinit 函数初始化操作系统特定的设置。\tCALL\truntime·osinit(SB)\t// 调用 runtime·schedinit 函数初始化调度器。\tCALL\truntime·schedinit(SB) /** 补充：这是该文件下面对 runtime·mainPC 的声明 // mainPC is a function value for runtime.main, to be passed to newproc. // The reference to runtime.main is made via ABIInternal, since the // actual function (not the ABI0 wrapper) is needed by newproc. DATA\truntime·mainPC+0(SB)/8,$runtime·mainABIInternal(SB) */ // 取 runtime·mainPC 的地址，这其实就是 runtime 包下的 main() 方法。 // 它是 Go 语言程序的真正入口，而不是 main.main()。\tMOVQ\t$runtime·mainPC(SB), AX\tPUSHQ\tAX // 创建一个新的 goroutine 来运行程序的主函数。 // 这里还没有正在的运行，因为调度器还没有启动， // 只是将 runtime.main 放进 goroutine 的 queue 中等待执行。\tCALL\truntime·newproc(SB)\tPOPQ\tAX\t// 调用 runtime·mstart 函数启动 M（machine，代表一个操作系统线程），\t// 开始执行 goroutines。\tCALL\truntime·mstart(SB)\t// 如果 runtime·mstart 函数返回，那么就调用 runtime·abort 函数终止程序。\t// 因为 runtime·mstart 函数在正常情况下是不应该返回的，如果返回了，说明有错误发生。\tCALL\truntime·abort(SB)\t// mstart should never return\tRETbad_cpu: // 当前 CPU 不支持 Go 运行时系统需要的时候的错误报告。\tMOVQ\t$2, 0(SP)\tMOVQ\t$bad_cpu_msg(SB), AX\tMOVQ\tAX, 8(SP)\tMOVQ\t$84, 16(SP)\tCALL\truntime·write(SB)\tMOVQ\t$1, 0(SP)\tCALL\truntime·exit(SB)\tCALL\truntime·abort(SB)\tRET\t// Prevent dead-code elimination of debugCallV2, which is\t// intended to be called by debuggers.\tMOVQ\t$runtime·debugCallV2ABIInternal(SB), AX\tRET 整理一下，runtime·rt0_go 的大体过程如下： 读取命令行参数，复制参数变量 argc 和 argv 到栈上。 初始化 g0 栈，g0 是为了调度协程而产生的协程，是 g0 是运行时系统的一个特殊的 goroutine，它在程序启动时被创建，用于执行系统调用和协程调度。 获取 CPU 信息。 如果存在 _cgo_init，这调用它。 检查并设置线性局部存储（TLS）。 检查 CPU 是否支持 Go 运行时系统需要的特性。 完成运行时系统检查和初始化： 调用 runtime·check 对代码进行运行时检查。 调用 runtime·args 函数处理命令行参数。 调用 runtime·osinit 函数初始化操作系统特定的设置。 调用 runtime·schedinit 函数初始化调度器。 调用 runtime·newproc 创建一个新的 goroutine 来运行程序的主函数。 调用 runtime·mstart 启动当前的 machine，执行 goroutines，执行程序。 一句话：runtime·rt0_go 是 Go 语言运行时的入口点，它负责设置和初始化运行时环境，然后创建 g0 和 m0 来运行程序的主函数。 Go 运行时系统初始化流程 了解完 Go 程序的整体启动流程后，我们重点来分析一下其中的 runtime·check、runtime·args、runtime·osinit、runtime·schedinit、runtime·newproc 和 runtime·mstart。 对了，充分理解 Go 启动流程，可能需要你对 Go 的 GMP 模型有一定的了解。 // TODO runtime·check 在 Goland IDE 上，我们双击 Shift，全局搜索 runtime·check 会发现找不到函数的实现。 Go 语言的运行时系统大部分是用 Go 自己编写的，但是有一部分，特别是与平台相关的部分，是用汇编语言编写的。在汇编语言中，调用 Go 函数的一种方式是使用 CALL 指令和函数的全名，包括包名和函数名。在这种情况下，runtime·check 就是调用 runtime 包下的 check() 函数。 所以我们需要双击 Shift，搜索 runtine.check，即将 · 换成 .（后面所有函数均是这个道理）。我们会发现 check() 位于 runtime/runtime1.go 中。 func check() var ( a int8 b uint8 c int16 d uint16 e int32 f uint32 g int64 h uint64 i, i1 float32 j, j1 float64 k unsafe.Pointer l *uint16 m [4]byte\t)\ttype x1t struct x uint8 type y1t struct x1 x1t y uint8 var x1 x1t\tvar y1 y1t // 检查各种类型的变量的大小是否符合预期\tif unsafe.Sizeof(a) != 1 throw(bad a) ... // 检查指针操作\tif unsafe.Sizeof(k) != goarch.PtrSize throw(bad k) ...\t// 检查结构体中字段的偏移量是否符合预期\tif unsafe.Offsetof(y1.y) != 1 throw(bad offsetof y1.y) // timediv 函数的目的是在 32 位处理器上实现 64 位的除法运算。 // 由于在 32 位处理器上，64 位的除法运算会被转换为 _divv() 函数调用， // 这可能会超出 nosplit 函数的栈限制，所以需要这个特殊的函数来进行处理。 // //go:nosplit 是一个编译器指令，它告诉编译器不要在这个函数中插入栈分割检查。 // 这意味着这个函数必须在当前的栈帧中运行，不能增加栈的大小。 // 如果这个函数需要更多的栈空间，那么它将会导致栈溢出。\tif timediv(12345*1000000000+54321, 1000000000, e) != 12345 || e != 54321 throw(bad timediv) // CAS 操作检查\tvar z uint32\tz = 1\tif !atomic.Cas(z, 1, 2) throw(cas1) ... // 检查 atomic 原子操作\tm = [4]byte1, 1, 1, 1\tatomic.Or8(m[1], 0xf0)\tif m[0] != 1 || m[1] != 0xf1 || m[2] != 1 || m[3] != 1 throw(atomicor8) // 测试浮点数 NaN（Not a Number）的行为\t*(*uint64)(unsafe.Pointer(j)) = ^uint64(0)\tif j == j throw(float64nan) if !(j != j) throw(float64nan1) // 测试 64 位原子操作\ttestAtomic64() // 检查栈大小是否是 2 的 n 次幂\tif fixedStack != round2(fixedStack) throw(FixedStack is not power-of-2) // 上报编代码的运行时检查中是否有异常\tif !checkASM() throw(assembly checks failed) 综上：runtime·check 主要是做一些运行时的检查。 使用 unsafe.Sizeof 函数检查各种类型的变量的大小是否符合预期。 使用 unsafe.Offsetof 函数检查结构体中字段的偏移量是否符合预期。 测试 timediv 函数检查在 32 位机器上进行 64 位除法运算的结果是否符合预期。 使用 atomic.Cas 函数（Compare and Swap）进行原子比较和交换测试。 使用 atomic.Or8 和 atomic.And8 函数进行原子位操作测试。 测试浮点数 NaN（Not a Number）的行为。 调用 testAtomic64 函数测试 64 位的原子操作。 检查 fixedStack 栈大小是否是 2 的幂。 调用 checkASM 函数检查汇编代码检查运行时中是否有异常。 runtime·args package runtimevar (\targc int32\targv **byte)func args(c int32, v **byte) argc = c\targv = v\tsysargs(c, v) 这个函数比较简单，就是将命令行参数拷贝到 runtime 包下的全局变量 argc 和 argv 上。后面在 shcedinit() 函数中会调用 goargs() 来遍历 argv 将参数复制到 slice 上。 func goargs() if GOOS == windows return argslice = make([]string, argc)\tfor i := int32(0); i argc; i++ argslice[i] = gostringnocopy(argv_index(argv, i)) runtime·osinit 这里函数主要是初始化操作系统特点的设置，可以看到这里针对不同操作系统都做了实现： osinit 这里我们以 os_windows.go 为例： func osinit() // 获取 asmstdcall 函数的地址，并将其转换为一个不安全的指针。 // 这通常在需要直接操作内存或进行系统调用的时候使用。\tasmstdcallAddr = unsafe.Pointer(abi.FuncPCABI0(asmstdcall)) // 加载一些可选的系统调用。\tloadOptionalSyscalls() // 阻止显示错误对话框。这可能是为了防止在出现错误时打断用户。\tpreventErrorDialogs() // 初始化异常处理器，用于处理运行时发生的异常。\tinitExceptionHandler() // 初始化高分辨率计时器，用于精确的时间测量。\tinitHighResTimer()\ttimeBeginPeriodRetValue = osRelax(false) // 初始化系统目录\tinitSysDirectory() // 启用长路径支持。 // 在 Windows 中，路径的长度通常限制为 260 个字符。启用长路径支持可以突破这个限制。\tinitLongPathSupport() // 码获取处理器的数量并将其赋给 ncpu。 ncpu = getproccount() // 获取内存页的大小并将其赋给 physPageSize，为了后面进行内存管理。\tphysPageSize = getPageSize() // 调用 SetProcessPriorityBoost 函数，禁用动态优先级提升。 // 在 Windows 中，动态优先级提升是一种机制，可以根据线程的类型和行为自动调整其优先级。 // 但在 Go 的环境中，所有的线程都是等价的，都可能进行 GUI、IO、计算等各种操作， // 所以动态优先级提升可能会带来问题，因此这里选择禁用它。\tstdcall2(_SetProcessPriorityBoost, currentProcess, 1) runtime·schedinit ★ 这个函数就非常重要了，从名字就可以看出来，这是 Go 语言调度器的初始化过程。这个函数位于：runtime/proc.go。 我们可以先来看看 schedinit() 的函数注释，这里也透露了 Go 语言程序的启动流程的核心顺序。 // The bootstrap sequence is: 启动流程顺序：////\tcall osinit 1. 调用 osinit//\tcall schedinit 2. 调用 schedinit//\tmake queue new G 3. 创建一个协程 G//\tcall runtime·mstart 4. 调用 mstart//// The new G calls runtime·main. 5. G 执行 runtime.mainfunc schedinit() 接下来我们详细来看看 schedinit() 都做了些什么： func schedinit() // 初始化各种锁，其中 lockRankXXX 指定锁的级别。\tlockInit(sched.lock, lockRankSched)\tlockInit(sched.sysmonlock, lockRankSysmon)\tlockInit(sched.deferlock, lockRankDefer)\tlockInit(sched.sudoglock, lockRankSudog)\tlockInit(deadlock, lockRankDeadlock)\tlockInit(paniclk, lockRankPanic)\tlockInit(allglock, lockRankAllg)\tlockInit(allpLock, lockRankAllp)\tlockInit(reflectOffs.lock, lockRankReflectOffs)\tlockInit(finlock, lockRankFin)\tlockInit(cpuprof.lock, lockRankCpuprof)\ttraceLockInit()\tlockInit(memstats.heapStats.noPLock, lockRankLeafRank)\t// 如果启用了竞态检测，则初始化竞态检测器， // 即我们使用 -race 的时候会执行这里。\tgp := getg()\tif raceenabled gp.racectx, raceprocctx0 = raceinit() // 限制 M 的数量，即线程的数量。 // maxmcount int32 // maximum number of ms allowed (or die)\tsched.maxmcount = 10000\t// 将调度器设置为初始暂停状态，在必要的初始化完成之前不调度任何协程。\tworldStopped() // 进行一系列的系统初始化（内存管理、CPU 设置、栈、算法等）\tmoduledataverify()\tstackinit()\tmallocinit()\tgodebug := getGodebugEarly()\tinitPageTrace(godebug) // must run after mallocinit but before anything allocates\tcpuinit(godebug) // must run before alginit\talginit() // maps, hash, fastrand must not be used before this call\tfastrandinit() // must run before mcommoninit\tmcommoninit(gp.m, -1)\tmodulesinit() // provides activeModules\ttypelinksinit() // uses maps, activeModules\titabsinit() // uses activeModules\tstkobjinit() // must run before GC starts // 设置和保存当前 M 的信号掩码\tsigsave(gp.m.sigmask)\tinitSigmask = gp.m.sigmask // 解析程序参数和环境变量\tgoargs()\tgoenvs()\tsecure()\tparsedebugvars() // 初始化垃圾回收器\tgcinit()\t// 如果设置了 disableMemoryProfiling，即禁用内存分析， // 则将 MemProfileRate 置为 0，关闭内存分析。\tif disableMemoryProfiling MemProfileRate = 0 // 锁定调度器，处理环境变量 GOMAXPROCS，这是开发者可以设置的允许的最多的 P 的数量。\tlock(sched.lock)\tsched.lastpoll.Store(nanotime())\tprocs := ncpu\tif n, ok := atoi32(gogetenv(GOMAXPROCS)); ok n 0 procs = n if procresize(procs) != nil throw(unknown runnable goroutine during bootstrap) unlock(sched.lock)\t// 将调度器设置为开始状态。\tworldStarted() // 确保构建版本和模块信息被保留在最终的二进制文件中。\tif buildVersion == buildVersion = unknown if len(modinfo) == 1 modinfo = 总结：schedinit 是 Go 语言运行时中的一个函数，负责初始化调度器及其相关组件，如锁、信号掩码、内存分配、以及其他系统级别的设置，确保并发执行环境的正确配置和高效运作。 具体过程如下： 锁初始化: 函数开始时，通过 lockInit 调用初始化了多个锁。在 Go 的调度器中，锁用于保护共享资源和调度数据结构，确保在多个线程或协程中的安全访问。每个锁都有一个特定的级别，这有助于防止死锁。 竞态检测器初始化: 如果启用了竞态检测 (raceenabled)，则初始化竞态上下文。这对于在开发阶段检测和避免竞态条件非常重要。 调度器设置: sched.maxmcount = 10000 设置调度器可以管理的最大线程（M）数目，这对于控制资源使用和性能调优很重要。 worldStopped() 将调度器设置为初始暂停状态，在必要的初始化完成之前不调度任何协程。 系统初始化: 接下来调用一系列函数（如 moduledataverify, mallocinit, cpuinit, alginit 等）来初始化内存管理、CPU 设置、算法等，这些都是调度器正常工作的基础。 环境和调试变量设置: 解析程序参数、环境变量、安全设置和调试变量。 垃圾收集器初始化: gcinit() 初始化垃圾收集器，这是 Go 运行时的关键组成部分，负责自动内存管理。 内存分析设置: 根据 disableMemoryProfiling 标志决定是否关闭内存分析功能。 处理器数量设置和调度器锁: 锁定调度器来安全地基于环境变量 GOMAXPROCS 设置处理器（procs）数量。 使用 procresize 函数根据处理器数量调整调度器的内部结构。 最终步骤和错误检查: 调用 worldStarted() 表示调度器已准备好开始调度协程。 检查和设置构建版本和模块信息，保证这些信息在最终的二进制文件中。 这里有几个地方比较有趣，我们来做一下简单的了解。（可跳过） 初始化锁 lockInit(mutex, rank) 我们知道 lockInit(mutex,rank) 是用来初始化锁的，第 2 个参数 rank 便是锁的等级。如果这个时候你链接到 lcokInit 实现的地方，你会发现默认会跳到 lockrank_off.go，而且你会发现，它的实现是空的： //go:build !goexperiment.staticlockrankingpackage runtimefunc lockInit(l *mutex, rank lockRank) 其实 lockInit 还有另外一个实现，在 lockrank_on.go 文件中： //go:build goexperiment.staticlockrankingpackage runtimeconst staticLockRanking = truefunc getLockRank(l *mutex) lockRank return l.rank 这什么意思呢？通过文件名称我们其实就可以猜到了，lockrank_off.go 是提供了无锁级别的锁，而 lockrank_on.go 是提供了有锁级别的锁。至于应该采用哪一个，是通过 go build 中的 goexperiment.staticlockranking 参数来控制的。 这里涉及一个概念，叫做锁排序（Lock Ranking）： 锁排序是一种用于避免死锁的技术。在这种机制中，每个锁都被赋予一个等级（或称为 “rank”），并且有规则确保锁的获取遵循这些等级的顺序。 通常，这意味着一个线程在获取等级较低的锁之前，必须先释放所有等级较高的锁。这样可以防止死锁，因为它避免了循环等待条件的发生。 Go 语言中锁的等级和顺序定义在 lockrank.go 文件中。 锁排序的作用： 在 Go 的并发模型中，锁是同步共享资源访问的重要机制。lockInit 函数在运行时初始化锁，为其分配等级，有助于维护程序的稳定性和性能。 锁排序功能的开启或关闭取决于是否需要额外的死锁检测。在开发和调试阶段，开启锁排序可以帮助发现死锁问题。然而，它可能引入额外的性能开销，因此在生产环境中可能会被关闭。 最后我们来看一下 schedinit() 都初始化了哪些锁： Lock Description sched.lock 初始化调度器的主锁。这个锁用于控制对调度器的访问，保证调度过程的正确性。 sched.sysmonlock 系统监控锁，用于保护系统监控相关的数据结构。 sched.deferlock 用于控制延迟执行函数列表的锁。 sched.sudoglock sudog 是 Go 中表示等待通信的 goroutine 的结构。这个锁保护与 sudog 相关的操作。 deadlock 可能用于检测或防止死锁的锁。 paniclk 在处理 panic 时使用的锁。 allglock 用于控制对所有 goroutine 列表的访问。 allpLock 控制对所有处理器（P）的访问。 reflectOffs.lock 用于反射操作的锁。 finlock 管理终结器列表的锁。 cpuprof.lock 用于 CPU 分析数据的锁。 traceLockInit() 专门用于追踪系统的锁初始化函数。 memstats.heapStats.noPLock 这是一个特殊的锁，被标记为 lockRankLeafRank，意味着它应该是锁层级中的最末端（leaf）。这样的锁应该只在非常短的关键部分中使用，以避免成为死锁的源头。 信号掩码 initSigmask 这两行代码是在搞啥呢？ sigsave(gp.m.sigmask)initSigmask = gp.m.sigmask sigmask 的中文意思是 信号掩码。 先看一下源码中 initSigmast 的注释： // Value to use for signal mask for newly created Ms.var initSigmask sigset initSigmask 是一个变量，存储着用于新创建的 M（Machine，即操作系统线程）的初始信号掩码。 什么是信号掩码： 信号掩码是操作系统中用于控制信号传递给进程或线程的一种机制。它允许进程或线程指定哪些信号可以被阻塞（暂时忽略）或允许。在多线程环境中，这个机制尤其重要，因为它帮助确保线程安全地处理信号。 信号掩码的作用： 信号掩码定义了一组信号，这些信号在特定时间内不会传递给进程或线程，即使这些信号发生了也会被系统挂起。这允许进程或线程在一个稳定的状态下运行，不被特定信号中断。 这种机制对于处理那些可能在关键操作期间导致不稳定状态的信号特别重要。 信号掩码的重要性： 在多线程程序中，不同的线程可能需要响应不同的信号或以不同方式处理相同的信号。通过为每个线程设置适当的信号掩码，可以确保线程只处理对它们来说重要的信号。 这有助于防止线程在执行关键代码时被不相关的信号打断。 sigsave(gp.m.sigmask)： sigsave(gp.m.sigmask) 这个调用是在保存当前 M 的信号掩码。gp 指的是当前的 goroutine，gp.m 是该 goroutine 正在运行的 M（操作系统线程）。 sigsave 函数的作用是将 gp.m 的当前信号掩码保存到提供的地址（在这里是 gp.m.sigmask）。这对于恢复线程的信号掩码到一个已知状态是非常有用的。 initSigmask = gp.m.sigmask： 这一行将 gp.m 的信号掩码赋值给 initSigmask。这意味着 initSigmask 现在保存了当前 M 的信号掩码，这个掩码将被用作新创建的 M 的初始信号掩码。 这是一个重要的步骤，因为它确保了所有新创建的 M 都将具有与当前 M 相同的信号处理行为。 这意味着所有新线程都会以一致的信号掩码启动，这有助于避免由于不同线程处理信号的不一致性导致的问题。 总体来说，Go 语言在其运行时中这样处理信号掩码，是为了确保在并发执行和线程调度中能够安全、一致地处理信号，这对于维护高效和稳定的运行时环境至关重要。 初始化垃圾回收器 gcinit() func gcinit() // 检查 workbuf 结构体的大小是否等于预期的 _WorkbufSize。 // 如果不是，抛出异常。这是为了确保 workbuf 的大小是最优的， // workbuf 用于垃圾回收过程中的内部工作。 if unsafe.Sizeof(workbuf) != _WorkbufSize throw(size of Workbuf is suboptimal) // 第一个垃圾回收周期不进行扫描操作。 // 在 Go 的垃圾回收过程中，扫描是回收前清理内存的重要步骤。 sweep.active.state.Store(sweepDrainedMask) // 使用环境变量 GOGC 和 GOMEMLIMIT 来设置初始的垃圾回收百分比和内存限制。 gcController.init(readGOGC(), readGOMEMLIMIT()) // 初始化用于控制垃圾回收工作流程的信号量。 // 这些信号量用于同步垃圾回收过程中的不同阶段。 work.startSema = 1 work.markDoneSema = 1 // 初始化了用于垃圾回收过程中的各种锁。 // 这些锁用于保护垃圾回收相关数据结构的并发访问，确保垃圾回收过程的线程安全。 lockInit(work.sweepWaiters.lock, lockRankSweepWaiters) lockInit(work.assistQueue.lock, lockRankAssistQueue) lockInit(work.wbufSpans.lock, lockRankWbufSpans)func (c *gcControllerState) init(gcPercent int32, memoryLimit int64) // 设置 heapMinimum 为默认的最小堆大小。 // 这是垃圾回收器考虑启动新回收周期前的最小堆内存大小。\tc.heapMinimum = defaultHeapMinimum // 将 triggered 设置为 uint64 的最大值。 // 这个字段用于表示触发垃圾回收的内存阈值， // 这里的设置意味着在初始状态下不会自动触发垃圾回收\tc.triggered = ^uint64(0) // 设置垃圾回收的百分比阈值。 // gcPercent 参数表示触发垃圾回收的内存增长百分比。 // 这个设置控制了堆内存增长到多少百分比时会触发垃圾回收。\tc.setGCPercent(gcPercent) // 设置内存限制。 // memoryLimit 参数可能表示堆内存的最大限制， // 用于控制垃圾回收器在内存使用方面的行为。\tc.setMemoryLimit(memoryLimit) // 提交垃圾回收控制器的当前设置，并指示第一次垃圾回收周期没有扫描（sweep）阶段。 // 在 Go 的垃圾回收中，扫描是回收周期的一部分，这里指明在第一次垃圾回收时跳过扫描阶段。\tc.commit(true) runtime·newproc ★ 初始化完调度器后，就进入到创建 g0 的阶段了，我们需要一个协程来运行程序的入口：runtime.main。 newproc() 的作用如注释所说：创建一个新的 goroutine 来执行 fn，并将它放入等待运行的 g 队列中。 // Create a new g running fn.// Put it on the queue of gs waiting to run.// The compiler turns a go statement into a call to this.func newproc(fn *funcval) gp := getg() // 获取当前协程\tpc := getcallerpc() // 获取当前程序计数器\tsystemstack(func() // 在系统栈上执行新 goroutine 的创建 newg := newproc1(fn, gp, pc)\t// 创建新 goroutine pp := getg().m.p.ptr()\t// 获取当前 M 绑定的 P runqput(pp, newg, true)\t// 将新创建的 goroutine 放入 P 的本地队列中 if mainStarted wakep()\t// 如果主程序已经启动，则唤醒或启动一个 M，以确保新的 goroutine 有机会被执行 ) 重点来看一下 newproc1() 和 runqput()。 创建协程 newproc1() 这段代码的主要作用是创建一个新的 goroutine 并设置其初始状态，以便它可以被调度器安排运行。它处理了从分配 goroutine 的内存到设置其栈空间和调度信息等一系列步骤。 // 创建一个新的 goroutine，状态为 _Grunnable，从函数 fn 开始执行。// callerpc 是创建此 goroutine 的 go 语句的地址。// 调用者负责将新的 g 添加到调度器中。func newproc1(fn *funcval, callergp *g, callerpc uintptr) *g if fn == nil fatal(go of nil func value) // 如果 fn 是 nil，抛出致命错误 mp := acquirem() // 禁用抢占，因为我们在局部变量中持有 M 和 P\tpp := mp.p.ptr()\tnewg := gfget(pp) // 尝试从 P 的空闲列表中获取一个 g\tif newg == nil newg = malg(stackMin) // 如果没有空闲的 g，创建一个新的 casgstatus(newg, _Gidle, _Gdead) // 将 g 的状态从 _Gidle 改为 _Gdead allgadd(newg) // 将新的 g 添加到所有 goroutine 的列表中 if newg.stack.hi == 0 throw(newproc1: newg missing stack) // 如果新的 g 没有栈，抛出异常 if readgstatus(newg) != _Gdead throw(newproc1: new g is not Gdead) // 确保新的 g 的状态为 _Gdead totalSize := uintptr(4*goarch.PtrSize + sys.MinFrameSize) // 计算额外的栈空间大小\ttotalSize = alignUp(totalSize, sys.StackAlign) // 栈空间对齐\tsp := newg.stack.hi - totalSize // 设置栈指针\tspArg := sp\tif usesLR *(*uintptr)(unsafe.Pointer(sp)) = 0 // 针对 LR 架构，设置调用者的 LR prepGoExitFrame(sp) spArg += sys.MinFrameSize memclrNoHeapPointers(unsafe.Pointer(newg.sched), unsafe.Sizeof(newg.sched)) // 清除调度器的内存\tnewg.sched.sp = sp // 设置调度器的栈指针\tnewg.stktopsp = sp // 设置栈顶指针\t// 设置调度器的程序计数器，+PCQuantum 使得前一个指令在同一函数中\tnewg.sched.pc = abi.FuncPCABI0(goexit) + sys.PCQuantum\tnewg.sched.g = guintptr(unsafe.Pointer(newg)) // 设置调度器的 g 指针\tgostartcallfn(newg.sched, fn) // 启动新的 g 执行函数 fn\tnewg.parentGoid = callergp.goid // 设置新 g 的父 goroutine ID\tnewg.gopc = callerpc // 设置新 g 的创建位置\tnewg.ancestors = saveAncestors(callergp) // 保存祖先信息\tnewg.startpc = fn.fn // 设置新 g 的起始函数地址\tif isSystemGoroutine(newg, false) sched.ngsys.Add(1) // 如果是系统 goroutine，增加计数 else if mp.curg != nil newg.labels = mp.curg.labels // 只有用户 goroutines 继承 pprof 标签 if goroutineProfile.active newg.goroutineProfiled.Store(goroutineProfileSatisfied) // 标记不需要纳入 goroutine 分析 newg.trackingSeq = uint8(fastrand()) // 设置追踪序列号\tif newg.trackingSeq%gTrackingPeriod == 0 newg.tracking = true // 是否启用追踪 casgstatus(newg, _Gdead, _Grunnable) // 将新 g 的状态从 _Gdead 改为 _Grunnable\tgcController.addScannableStack(pp, int64(newg.stack.hi-newg.stack.lo)) // 将新 g 的栈添加到可扫描栈列表\tif pp.goidcache == pp.goidcacheend pp.goidcache = sched.goidgen.Add(_GoidCacheBatch) // 分配新的 goroutine ID pp.goidcache -= _GoidCacheBatch - 1 pp.goidcacheend = pp.goidcache + _GoidCacheBatch newg.goid = pp.goidcache // 设置新 g 的 ID\tpp.goidcache++\tif raceenabled newg.racectx = racegostart(callerpc) // 设置竞态检测上下文 newg.raceignore = 0 if newg.labels != nil racereleasemergeg(newg, unsafe.Pointer(labelSync)) // 同步竞态检测和信号处理 if traceEnabled() traceGoCreate(newg, newg.startpc) // 记录追踪信息 releasem(mp) // 释放当前 M\treturn newg // 返回新创建的 goroutine newproc1() 函数概述 有几个地方比较有趣，我们可以来研究一下。 获取 g // The minimum size of stack used by Go codestackMin = 2048func newproc1() ... newg := gfget(pp) // 尝试从 P 的空闲列表中获取一个 g if newg == nil newg = malg(stackMin) // 如果没有空闲的 g，创建一个新的 casgstatus(newg, _Gidle, _Gdead) // 将 g 的状态从 _Gidle 改为 _Gdead allgadd(newg) // 将新的 g 添加到所有 goroutine 的列表中 ... 这里可以看出，Go 语言每个新创建的协程分配的默认大小就是 stackMin，即 2KB。 其实 statck.go 还定义了另外一个字段，即栈最大为 2GB，所以我们可以知道，Go 协程栈大小 [2KB, 2GB]。 var maxstacksize uintptr = 1 20 // enough until runtime.main sets it for real 另外我们可以看出来，Go 语言会尽可能重用现有的空闲 goroutine，以减少内存分配的开销，提供创建新 goroutine 的效率。 重用的具体逻辑在 gfget(pp) 中，这个函数的作用是从与当前 M 绑定的 P 的空闲列表中获取一个空闲的 g，如果没有，则尝试从全局空闲列表中获取 g。 // Get from gfree list.// If local list is empty, grab a batch from global list.func gfget(pp *p) *g retry: // 如果当前 P 的空闲队列为空，并且全局空闲队列中有可用的 goroutine，则进行下列操作。\tif pp.gFree.empty() (!sched.gFree.stack.empty() || !sched.gFree.noStack.empty()) // 枷锁全局空闲列表 lock(sched.gFree.lock) // 将最多 32 个空闲的 g 从全局列表中移动到当前 P 的空闲列表 for pp.gFree.n 32 // 优先选择有栈的 g gp := sched.gFree.stack.pop() if gp == nil // 实在没有栈，也可以接受 gp = sched.gFree.noStack.pop() if gp == nil break sched.gFree.n-- pp.gFree.push(gp) pp.gFree.n++ unlock(sched.gFree.lock) // 一直尝试，知道 P 有空闲 g，或者全局列表也没有空闲 g 了，就退出 for 循环，进行下面的操作。 goto retry // 从 P 的空闲列表中取出一个 g\tgp := pp.gFree.pop()\tif gp == nil return nil pp.gFree.n-- // 检查获取到的 g 是否有一个有效的栈，如果栈不符合预期的大小，则释放旧栈\tif gp.stack.lo != 0 gp.stack.hi-gp.stack.lo != uintptr(startingStackSize) systemstack(func() stackfree(gp.stack) gp.stack.lo = 0 gp.stack.hi = 0 gp.stackguard0 = 0 ) // 如果 g 没有有效的栈，或者刚刚被释放了，则分配新栈给 g\tif gp.stack.lo == 0 systemstack(func() gp.stack = stackalloc(startingStackSize) ) gp.stackguard0 = gp.stack.lo + stackGuard else if raceenabled racemalloc(unsafe.Pointer(gp.stack.lo), gp.stack.hi-gp.stack.lo) if msanenabled msanmalloc(unsafe.Pointer(gp.stack.lo), gp.stack.hi-gp.stack.lo) if asanenabled asanunpoison(unsafe.Pointer(gp.stack.lo), gp.stack.hi-gp.stack.lo) return gp 其中 startingStackSize 表示新创建的 goroutine 开始时的栈大小。它被初始化为 fixedStack 的值。startingStackSize 在每次垃圾回收（GC）后可能会更新，以反映在 GC 过程中扫描的栈的平均大小。 var startingStackSize uint32 = fixedStackfunc gcComputeStartingStackSize() ... // 求出栈平均大小\tavg := scannedStackSize/scannedStacks + stackGuard\tif avg uint64(maxstacksize) avg = uint64(maxstacksize) if avg fixedStack avg = fixedStack // 更新 startingStackSize\tstartingStackSize = uint32(round2(int32(avg))) 初始化协程栈 totalSize := uintptr(4*goarch.PtrSize + sys.MinFrameSize) // 计算额外的栈空间大小totalSize = alignUp(totalSize, sys.StackAlign) // 栈空间对齐sp := newg.stack.hi - totalSize // 设置栈指针spArg := spif usesLR *(*uintptr)(unsafe.Pointer(sp)) = 0 // 针对 LR 架构，设置调用者的 LR prepGoExitFrame(sp) spArg += sys.MinFrameSize 1. 计算额外的栈空间大小: totalSize := uintptr(4*goarch.PtrSize + sys.MinFrameSize) 这行代码计算新 goroutine 需要的额外栈空间大小。 4*goarch.PtrSize 是为了留出足够的空间来存储函数调用过程中的一些额外信息（例如返回地址、寄存器保存等）。 sys.MinFrameSize ：是系统为每个栈帧保留的最小空间，用于存储一些特定于架构的信息。 // MinFrameSize is the size of the system-reserved words at the bottom// of a frame (just above the architectural stack pointer).// It is zero on x86 and PtrSize on most non-x86 (LR-based) systems.// On PowerPC it is larger, to cover three more reserved words:// the compiler word, the link editor word, and the TOC save word.const MinFrameSize = goarch.MinFrameSize goarch.PtrSize：指针大小。 // PtrSize is the size of a pointer in bytes - unsafe.Sizeof(uintptr(0)) but as an ideal constant.// It is also the size of the machines native word size (that is, 4 on 32-bit systems, 8 on 64-bit).const PtrSize = 4 (^uintptr(0) 63) 2. 栈空间对齐: totalSize = alignUp(totalSize, sys.StackAlign): 根据系统的栈对齐要求调整 totalSize 的大小。栈对齐是为了确保栈上的数据按照硬件要求的边界对齐，这通常是为了提高访问效率或满足特定的硬件要求。 3. 设置栈指针 (sp): sp := newg.stack.hi - totalSize: 计算新 goroutine 的栈顶指针。newg.stack.hi 是分配给这个 goroutine 的栈的高地址（栈顶），从这里向下分配空间。通过从栈顶地址减去计算出的 totalSize，设置新的栈顶位置。 4. 处理链接寄存器（LR）架构: 在某些架构上（如 ARM、PowerPC），函数调用的返回地址不是存储在栈上，而是存储在一个名为链接寄存器（LR）的特殊寄存器中。这几行代码检查是否在这种架构上运行 (usesLR)。 如果是，则在栈上的适当位置存储一个 0 值作为返回地址，并调用 prepGoExitFrame 来准备 goroutine 退出时的栈帧。这是为了模拟在非 LR 架构上的栈帧结构。 spArg 是一个辅助变量，用于记录参数传递时应该使用的栈地址。在 LR 架构上，它需要根据 sys.MinFrameSize 进行调整，以保证函数参数的正确位置。 放入队列 runqput() runqput() 负责将 goroutine (gp) 放入到本地可运行队列或全局队列中。 // runqput 尝试将 g 放入当前的执行队列中// 如果 next=false，则将 g 放在队列末尾，// 如果 next=true，则将 g 放在 pp.runnext，即下一个要执行的 goroutine。// 如果本地队列满了，则加入到全局队列。func runqput(pp *p, gp *g, next bool) // 如果启用了随机调度器 (randomizeScheduler)， // 并且调用者指示将 goroutine 放入 runnext 位置 (next 为 true)， // 则有 50% 的概率将 next 设置为 false，以随机地将 goroutine 放入队列尾部。\tif randomizeScheduler next fastrandn(2) == 0 next = false if next retryNext: // 如果为 next，则尝试将 p 放入 pp.runnext 插槽 oldnext := pp.runnext if !pp.runnext.cas(oldnext, guintptr(unsafe.Pointer(gp))) goto retryNext // 如果这个槽之前没有被占用，则直接返回 if oldnext == 0 return // 如果这个槽之前已经被占用了，则剔除旧 goroutine， // 然后进行下面的逻辑，即将其放入常规的运行队列中 gp = oldnext.ptr()\tretry:\th := atomic.LoadAcq(pp.runqhead) // 取出队列头部\tt := pp.runqtail // 取出队列尾部 // 如果还没满，则将 gp 放入本地队列中（可能是新 g，也可能是之前在 runnext 的 g\tif t-h uint32(len(pp.runq)) pp.runq[t%uint32(len(pp.runq))].set(gp) atomic.StoreRel(pp.runqtail, t+1) // store-release, makes the item available for consumption return // 如果本地队列满了，则尝试将其放入全局队列中\tif runqputslow(pp, gp, h, t) return goto retry 其中 runqputslow() 不仅会尝试将 gp 放入全局队列中，还会尝试将本地队列的部分 g 放入全局队列中，因为这个时候本地队列已经满了，放入全局队列中就有机会被其他 P 所调度，减少饥饿。 // Put g and a batch of work from local runnable queue on global queue.// Executed only by the owner P.func runqputslow(pp *p, gp *g, h, t uint32) bool // 初始化一个数组 batch，大小为本地队列的一半， // 它用来存储将要移动到全局队列的 goroutine。\tvar batch [len(pp.runq)/2 + 1]*g\tn := t - h\tn = n / 2 // 只有本地队列满才这么操作\tif n != uint32(len(pp.runq)/2) throw(runqputslow: queue is not full) // 将本地队列一半的 g 复制到 batch 中\tfor i := uint32(0); i n; i++ batch[i] = pp.runq[(h+i)%uint32(len(pp.runq))].ptr() // CAS 更新本地队列头指针，如果失败，则返回\tif !atomic.CasRel(pp.runqhead, h, h+n) // cas-release, commits consume return false // 将当前要调度的 gp 放入 batch 的末尾\tbatch[n] = gp // 如果启动了随机调度器，则随机化 batch 数组\tif randomizeScheduler for i := uint32(1); i = n; i++ j := fastrandn(i + 1) batch[i], batch[j] = batch[j], batch[i] // 链接 goroutine，以便它们可以作为一个连续的队列被处理\tfor i := uint32(0); i n; i++ batch[i].schedlink.set(batch[i+1]) var q gQueue\tq.head.set(batch[0])\tq.tail.set(batch[n])\t// 将 batch 放入全局队列中\tlock(sched.lock)\tglobrunqputbatch(q, int32(n+1))\tunlock(sched.lock)\treturn true runqput() 函数概述 总结 到这里我们可以总结，runtime·newproc 的核心功能就是初始化一个新的 goroutine，并将其放入队列中进行调度。其中 newproc1() 会新建或复用空闲的 goroutine，然后初始化其栈空间和调度信息； runqput() 会优先将 g 放入本地队列中调度，如果本地队列满了，会连带本地队列中一半的 goroutine 一起转移到全局队列中调度。 runtime·mstart 调度器 m0 和主协程 g0 都初始化完毕了，这个时候就可以启动调度器来调度协程工作了。 我们找到 mstart() 函数的声明，位于：proc.go // mstart is the entry-point for new Ms.// It is written in assembly, uses ABI0, is marked TOPFRAME, and calls mstart0.func mstart() 可以看到，这里 mstart() 是没用函数体的，通过注释我们可以知道这个函数的实现部分是用汇编实现的，Go 编译器会在编译的时候往这个函数里面插入相关指令。另外注释也告诉我们，这里最终其实就是调用 mstart0() 函数。我们找到相关的汇编代码，果然是如此： TEXT runtime·mstart(SB),NOSPLIT|TOPFRAME,$0\tCALL\truntime·mstart0(SB)\tRET // not reached mstart0() 就在 mstart() 的下面： func mstart0() gp := getg()\tosStack := gp.stack.lo == 0\tif osStack // Initialize stack bounds from system stack. // Cgo may have left stack size in stack.hi. // minit may update the stack bounds. size := gp.stack.hi if size == 0 size = 8192 * sys.StackGuardMultiplier gp.stack.hi = uintptr(noescape(unsafe.Pointer(size))) gp.stack.lo = gp.stack.hi - size + 1024 // Initialize stack guard so that we can start calling regular\t// Go code.\tgp.stackguard0 = gp.stack.lo + stackGuard\t// This is the g0, so we can also call go:systemstack\t// functions, which check stackguard1.\tgp.stackguard1 = gp.stackguard0\tmstart1()\t// Exit this thread.\tif mStackIsSystemAllocated() // Windows, Solaris, illumos, Darwin, AIX and Plan 9 always system-allocate // the stack, but put it in gp.stack before mstart, // so the logic above hasnt set osStack yet. osStack = true mexit(osStack) 简单过一下 mstart0() 后，我们会发现其实 mstart0() 也不是关键，关键是 mstart1()。 我们先对 mstart0() 做一个简单的总结：它是 Go 语言运行时中新 M（操作系统线程）的入口点。这个函数负责初始化新线程的栈和一些其他设置，然后调用 mstart1 来继续线程的初始化过程。 我们继续来看 mstart1()，它用于进一步设置新线程并最终将控制权交给调度器。 func mstart1() // 获取当前协程\tgp := getg() // 只有 g0 协程可以执行 mstart1()，即启动 m0。 // 每一个 M 都有一个特殊的 goroutine，其被称为 g0，它用于执行系统级任务。\tif gp != gp.m.g0 throw(bad runtime·mstart) // 设置 gp.sched 调度信息，以便 goroutine 能够在未来被正确调度。\tgp.sched.g = guintptr(unsafe.Pointer(gp))\t// goroutine 指针\tgp.sched.pc = getcallerpc()\t// 程序计数器\tgp.sched.sp = getcallersp()\t// 栈指针 // 初始化于汇编相关的设置。\tasminit() // 初始化当前 M 的线程局部存储和其他线程相关的数据。\tminit()\t// 如果是 m0，则安装信号处理器\tif gp.m == m0 mstartm0() // 如果 M 启动时配置了函数，则调用它\tif fn := gp.m.mstartfn; fn != nil fn() // 如果当前现场不是 m0（主线程），则获取一个 P，准备开始执行 goroutine\tif gp.m != m0 acquirep(gp.m.nextp.ptr()) gp.m.nextp = 0 // 调用 schedule() 将控制权交给调度器，开始执行 goroutine\tschedule()func mstartm0() if (iscgo || GOOS == windows) !cgoHasExtraM cgoHasExtraM = true newextram() // 安装信号处理器\tinitsig(false) 其中 schedule() 是调度器的具体调度过程，这部分会在 GMP 篇章进行展开（TODO 😁）。 注意这里： // 如果 M 启动时配置了函数，则调用它if fn := gp.m.mstartfn; fn != nil fn() 前面我们提到 runtime·newproc 的时候，获取并设置了 runtime.main 的函数地址： // 取 runtime·mainPC 的地址，这其实就是 runtime 包下的 main() 方法。// 它是 Go 语言程序的真正入口，而不是 main.main()。MOVQ\t$runtime·mainPC(SB), AXPUSHQ\tAX// 创建一个新的 goroutine 来运行程序的主函数。// 这里还没有正在的运行，因为调度器还没有启动，// 只是将 runtime.main 放进 goroutine 的 queue 中等待执行。CALL\truntime·newproc(SB)POPQ\tAX 所以这里其实就是调用 runtime.main()，到这里，我们终于开始执行程序的主函数了。 runtime.main ★ 终于我们到了 runtime.main 这个 Go 语言世界中 “真正” 的主函数了，它位于：proc.go。 // The main goroutine.func main() // 获取当前的 M\tmp := getg().m\tmp.g0.racectx = 0\t// 限制栈大小的上限，64 位系统为 1G，32 位系统为 250M\tif goarch.PtrSize == 8 maxstacksize = 1000000000 else maxstacksize = 250000000 // 这里就将栈的上限提升 2 倍，用于避免在分配过大的栈时崩溃。 // 所以其实 64 位系统最大栈 2G，32 位系统最大栈 500M。\tmaxstackceiling = 2 * maxstacksize\t// 将 mainStarted 置为 true，允许 newproc 启动新的 M。\tmainStarted = true // WebAssemby 上暂时没有线程和系统监视器，所以这里过滤掉。\tif GOARCH != wasm // 其他架构就启动系统监视器。 systemstack(func() newm(sysmon, nil, -1) ) // 在初始化期间将 g0 锁定在 m0 上。\tlockOSThread() // 只有 m0 可以运行 runtime.main\tif mp != m0 throw(runtime.main not on m0) // 记录 runtime 的开始时间，需要在 doInit() 之前，因为这样才把 init 也追踪上。\truntimeInitTime = nanotime()\tif runtimeInitTime == 0 throw(nanotime returning zero) // 初始化 trace\tif debug.inittrace != 0 inittrace.id = getg().goid inittrace.active = true // 执行 runtime 的 init() 方法\tdoInit(runtime_inittasks) // Must be before defer.\t// defer 解锁，以便在初始化期间调用 runtime.Goexit 时也能解锁。\tneedUnlock := true\tdefer func() if needUnlock unlockOSThread() () // 启动垃圾回收期\tgcenable() // 监听初始化完成的信号\tmain_init_done = make(chan bool) // 如果使用 cgo，则进行相关的初始化\tif iscgo ... cgocall(_cgo_notify_runtime_init_done, nil) // 执行所有模块的 init()\tfor _, m := range activeModules() doInit(m.inittasks) // 初始化任务都完成后，则禁用初始化 trace。\tinittrace.active = false // 关闭初始化完成的信号通道\tclose(main_init_done) // 解锁 m0 线程\tneedUnlock = false\tunlockOSThread() // 以 -buildmode=(c-archive|c-shared) 方式进行构建程序的话，则不执行 main.main\tif isarchive || islibrary // A program compiled with -buildmode=c-archive or c-shared // has a main, but it is not executed. return // 执行 main.main() 函数，也就是我们自己写的 main()\tfn := main_main\tfn() // 如果我们启动了一个 server 服务，这里就会被阻塞住，直到我们的 main 返回。 // 静态检测输出\tif raceenabled runExitHooks(0) // run hooks now, since racefini does not return racefini() // 处理在 main 返回时同时存在的其他 goroutine 的 panic\tif runningPanicDefers.Load() != 0 for c := 0; c 1000; c++ // 执行 defer if runningPanicDefers.Load() == 0 break Gosched() // 阻塞 g0 的执行，直到所有的 panic 都处理完毕\tif panicking.Load() != 0 gopark(nil, nil, waitReasonPanicWait, traceBlockForever, 1) // 执行 hook 退出函数\trunExitHooks(0) // 退出程序，0 表示正常退出\texit(0) // 理论上 exit(0) 应该退出程序的， // 如果还没退出，使用 nil 指针强行赋值，引发崩溃，强行退出程序。\tfor var x *int32 *x = 0 总的来说，runtime.main() 干这么几件事： 首先进行一些基本的设置和检查，包括设置栈大小限制和锁定主 goroutine 到主 OS 线程。 然后，函数执行一系列初始化操作，包括启动垃圾回收器、处理 CGo 交互、执行包的 init()。 在完成所有 init() 后，函数调用用户定义的 main.main 函数。 最后，函数处理程序退出，包括执行 defer、等待 panic 处理完成，并正式退出程序。 所以这里我们就知道了为什么 init() 会在 main.main() 之前被执行，而且如果一个 package 在整个程序路径都没有被 import 的时候，init() 是不会被执行的，就是因为 runtime.main() 只处理了 activeModules() 的 initTasks()。 补充说明：全局变量的初始化 到这里，还有个遗留问题，开发时我们需要关注的 init() 和main.main()可以讨论过了，那全局变量的初始化是在哪里做的呢？在 Go语言的编译过程中，全局变量的初始化主要发生在链接阶段。编译器首先编译每个包，生成对象文件。然后在链接阶段，编译器或链接器将这些对象文件合并成一个可执行文件。在这个过程中，编译器或链接器负责生成初始化全局变量的代码，并安排这些代码在程序启动时执行。这些初始化代码通常嵌入在程序的启动序列中，确保在执行任何包级init 函数或用户定义的 main函数之前，所有全局变量已经被初始化。由于这些操作是编译器在内部执行的，它们不会直接显示在源代码或运行时代码中。 runtime.main 函数概述 至此，我们就分析完 Go 语言程序的整个启动过程了。具体的启动流程总结，可以回到开头的 “结论先行” 查看。 参考 慕课网-深入 Go 底层原理 Go1.21.0 官方源码 ChatGPT-4","tags":["go"],"categories":["go"]},{"title":"Go1.21.0 程序编译过程","path":"/2023/11/29/go-compilation/","content":"版本说明 Go 1.21 官方文档 Go 语言官方文档详细阐述了 Go 语言编译器的具体执行过程，Go1.21 版本可以看这个：https://github.com/golang/go/tree/release-branch.go1.21/src/cmd/compile 大致过程如下： 解析 (cmd/compile/internal/syntax): 词法分析器和语法分析器：源代码被分词（词法分析）并解析（语法分析）。 语法树构建：为每个源文件构建一个语法树。 类型检查 (cmd/compile/internal/types2): 类型检查：types2 包是 go/types 的一个移植版本，它使用 syntax 包的 AST（抽象语法树）而不是 go/ast。 IR 构建（\"noding\"）: 编译器类型 (cmd/compile/internal/types) 编译器 AST (cmd/compile/internal/ir) AST 转换 (cmd/compile/internal/typecheck) 创建编译器 AST (cmd/compile/internal/noder) 这个阶段使用自己的 AST 定义和 Go 类型的表示，这些定义和表示形式是用 C 编写时遗留下来的。它的所有代码都是根据这些编写的，因此类型检查后的下一步是转换语法和 types2 表示形式到 ir 和 types。这个过程被称为“noding”。 中间阶段: 死代码消除 (cmd/compile/internal/deadcode) 函数调用内联 (cmd/compile/internal/inline) 已知接口方法调用的去虚拟化 (cmd/compile/internal/devirtualize) 逃逸分析 (cmd/compile/internal/escape) 在 IR 表示上执行几个优化过程：死代码消除、（早期的）去虚拟化、函数调用内联和逃逸分析。 Walk (cmd/compile/internal/walk): 求值顺序和语法糖：这是对 IR 表示的最后一次遍历，它有两个目的：将复杂的语句分解为简单的单个语句，引入临时变量并遵守求值顺序；将高级 Go 构造转换为更原始的构造。 通用 SSA (cmd/compile/internal/ssa 和 cmd/compile/internal/ssagen): 在这个阶段，IR 被转换为静态单赋值（SSA）形式，这是一种具有特定属性的低级中间表示，使得从中实现优化并最终生成机器代码变得更容易。 生成机器代码 (cmd/compile/internal/ssa 和 cmd/internal/obj): 这是编译器的机器依赖阶段，以“lower”过程开始，将通用值重写为它们的机器特定变体。然后进行最终的代码优化过程。最后，Go 函数被转换为一系列 obj.Prog 指令，这些指令被汇编器（cmd/internal/obj）转换为机器代码，并写出最终的目标文件。 Go编译器概览 编译过程 Go 程序的编译过程符合经典编译原理的过程拆解，即三阶段编译器，分别为编译器前端、中端和后端： 前端（Front End）： 前端的任务是进行语法分析和语义分析。这一阶段会将源代码转换为一个中间表示。在这个过程中，编译器会检查代码的语法和语义，比如语法错误、类型错误等。前端通常是依赖于具体语言的，比如 Go 的前端和 C++ 的前端就会有很大的不同。 中间端（Middle End）： 中间端的任务是对中间表示进行优化。这一阶段的优化是语言无关的，比如常量折叠、死代码消除、循环优化等。这些优化可以提高生成的代码的性能，但是不会改变程序的语义。 后端（Back End）： 后端的任务是将优化后的中间表示转换为目标机器代码。这一阶段会进行更多的优化，比如寄存器分配、指令选择、指令调度等。后端通常是依赖于具体机器的，比如 x86 的后端和 ARM 的后端就会有很大的不同。 参考《Go 语言底层原理剖析（郑建勋）》一书，本文将 Go 语言编译器执行流程拆分为以下几个阶段： 词法解析 语法解析 抽象语法树构建 类型检查 死代码消除 去虚拟化 函数内联 逃逸分析 变量捕获 闭包重写 遍历函数 SSA 生成 机器码生成 下面本文将以此书为参考并结合 Go1.21.0 版本，对每个过程进行阐述。 如果只想对 Go 程序的编译过程做一个简单的了解，那阅读到这里就已经足够了。 词法解析 词法解析过程主要负责将源代码中的字符序列转换成一系列的标记（tokens），这些标记是编译器更进一步处理的基本单位。在 Go 语言的编译器中，tokens.go 文件包含了与词法分析有关的标记定义。 词法解析的过程可以分为几个关键步骤： 扫描（Scanning）：编译器的扫描器会逐字符读取源代码，识别出基本的语法单位，如标识符、关键字、字面量、运算符等。 标记生成（Token Generation）：每当扫描器识别出一个有效的语法单位时，它会生成一个相应的标记。例如，对于一个变量名，扫描器会生成一个标识符标记。 去除空白字符和注释：在生成标记的过程中，扫描器还会忽略空白字符（如空格、换行符）和注释，因为它们对程序的逻辑没有影响。 错误处理：如果扫描器在源代码中遇到无法识别的字符或序列，它会生成一个错误消息。 我们来看以下 tokens.go 文件中的 token 定义，它们实质上是用 iota 声明的一系列整数： const (\t_ token = iota\t_EOF // EOF\t// names and literals\t_Name // name\t_Literal // literal\t// operators and operations\t// _Operator is excluding * (_Star)\t_IncOp // opop _Define // := ...\t// delimiters\t_Lparen // (\t_Rparen // )\t...\t// keywords\t_Break // break\t...\t// empty line comment to exclude it from .String\ttokenCount //) 举个例子，a := b + c(12) 这个表达式，被解析后，如下图所示： Go 语言编译器词法解析示例图 语法解析 语法解析发生在词法解析之后，其主要目的是分析源代码中标记（tokens）的排列和结构，以确定它们是否形成了有效的语句。核心算法位于两个文件中： syntax/nodes.go：定义了语法节点（Syntax Nodes），这些节点是构成抽象语法树（AST）的基本元素。每个节点代表了 Go 语法中的一个构造，比如变量声明、函数调用、表达式等。通过这些节点，编译器能够理解和表示程序代码的结构。 syntax/parser.go：包含了解析器的实现。解析器负责读取词法分析阶段生成的标记流，并根据这些标记构建 AST。它遵循 Go 语言的语法规则，确保代码符合语法结构，并在遇到语法错误时提供相应的反馈。 Go 语言采用了标准的自上而下的递归下降（Top-Down Recursive-Descent）算法，以简单高效的方式完成无须回溯的语法扫描。 下面我们来看下 nodes.go 文件中对各个节点的声明（以下都省略了 struct 中的具体属性）： 声明 Declarations type (\tDecl interface Node aDecl() ImportDecl struct // 导入声明\tConstDecl struct // 常量声明\tTypeDecl struct // 类型声明\tVarDecl struct // 变量声明\tFuncDecl struct // 函数声明) 表达式 Expressions type (\tExpr interface Node typeInfo aExpr() // 省略了结构体属性\tBadExpr struct // 无效表达式\tName struct // Value\tBasicLit struct // Value\tCompositeLit struct // Type ElemList[0], ElemList[1], ... KeyValueExpr struct // Key: Value\tFuncLit struct // func Type Body ParenExpr struct // (X)\tSelectorExpr struct // X.Sel\tIndexExpr struct // X[Index]\tSliceExpr struct // X[Index[0] : Index[1] : Index[2]]\tAssertExpr struct // X.(Type)\tTypeSwitchGuard struct // Lhs := X.(type)\tOperation struct // 操作 +-*\\\tCallExpr struct // Fun(ArgList[0], ArgList[1], ...)\tListExpr struct // ElemList[0], ElemList[1], ...\tArrayType struct // [Len]Elem\tSliceType struct // []Elem\tDotsType struct // ...Elem\tStructType struct // struct FieldList[0] TagList[0]; FieldList[1] TagList[1]; ... Field struct // Name Type\tInterfaceType struct // interface MethodList[0]; MethodList[1]; ... FuncType struct // type FuncName func (param1, param2) return1, return2\tMapType struct // map[Key]Value\tChanType struct // chan Elem, -chan Elem, chan- Elem) 语句 Statements type ( // 所有语句的通用接口 Stmt interface Node aStmt() // 更加简单语句的通用接口 SimpleStmt interface EmptyStmt struct // 空语句 LabeledStmt struct // 标签语句 BlockStmt struct // 代码块语句 ExprStmt struct // 表达式语句 SendStmt struct // 发送语句，用于 channel DeclStmt struct // 声明语句 AssignStmt struct // 赋值语句 BranchStmt struct // 分支语句，break, continue CallStmt struct // 调用语句 ReturnStmt struct // 返回语句 IfStmt struct // if 条件语句 ForStmt struct // for 循环语句 SwitchStmt struct // switch 语句 SelectStmt struct // select 语句) 我们可以重点来看一下最常用的赋值语句： type AssignStmt struct Op Operator // 操作符 0 means no operation Lhs, Rhs Expr // 左右两个表达式 Rhs == nil means Lhs++ (Op == Add) or Lhs-- (Op == Sub) simpleStmt 举例 package mainimport fmtconst name = hedontype String stringvar s String = hello + wordfunc main() fmt.Println(s) 上面的源代码会被解析成如下图所示： Go 编译器语法解析示例图 再来看一个赋值语句是如何解析的，就以之前的 a := b + c(12) 为例： 特定赋值语句的语法解析示例 抽象语法树构建 编译器前端必须构建程序的中间表示形式，以便在编译器中端及后端使用，抽象语法树（Abstract Syntax Tree，AST）是一种常见的树状结构的中间态。 抽象语法树 抽象语法树（AST，Abstract Syntax Tree）是源代码的树状结构表示，它用于表示编程语言的语法结构，但不包括所有语法细节。AST 是编译器设计中的关键概念，广泛应用于编译器的各个阶段。 基本概念： 结构：AST 是一种树形结构，其中每个节点代表程序中的一种构造（如表达式、语句等）。 抽象性：它抽象出了代码的语法结构，省略了某些语法细节（如括号、特定的语法格式等）。 节点类型： 根节点：代表整个程序或一段完整代码。 内部节点：通常代表控制结构（如循环、条件语句）和操作符（如加、减、乘、除）。 叶节点：代表程序中的基本元素，如常量、变量和标识符。 构建过程： 词法分析：源代码首先经过词法分析，分解为一系列标记（tokens）。 语法分析：然后，基于这些标记，语法分析器根据编程语言的语法规则构建 AST。 树的构建：在这个过程中，分析器会根据语言的语法创建不同类型的节点，并按照程序的结构将它们组织成树。 使用场景： 语义分析：编译器使用 AST 来进行类型检查和其他语义分析。 代码优化：在优化阶段，编译器会对 AST 进行变换，以提高代码的执行效率。 代码生成：编译器根据 AST 生成中间代码或目标代码。 优点： 简化处理：由于省略了不必要的语法细节，AST 使得编译器的设计更为简洁和高效。 灵活性：AST 可以轻松地进行修改和扩展，便于实现各种编译器功能。 可视化：AST 的树形结构使得代码的逻辑结构一目了然，有助于理解和调试。 Go 构建抽象语法树 在 Go 语言源文件中的任何一种 Declarations 都是一个根节点，如下 pkgInit(decls) 函数将源文件中的所有声明语句都转换为节点（Node），代码位于：syntax/syntax.go 和 syntax/parser.go 中。 Parse() func Parse(base *PosBase, src io.Reader, errh ErrorHandler, pragh PragmaHandler, mode Mode) (_ *File, first error) defer func() if p := recover(); p != nil if err, ok := p.(Error); ok first = err return panic(p) ()\tvar p parser\tp.init(base, src, errh, pragh, mode)\tp.next()\treturn p.fileOrNil(), p.first 下面是对 Parse() 函数的一个简单解释： 作用：解析单个 Go 源文件并返回相应的语法树。 参数 base: 位置基础信息。 src: 要解析的源文件。 errh: 错误处理函数。 pragh: 用于处理每个遇到的编译指令（pragma）。 mode: 解析模式。 返回值：返回一个 File 类型的指针，表示解析后的 AST，以及可能的错误。 错误处理 如果 errh 不为空：将会调用它处理每个遇到的错误，解析器尽可能多地处理源文件。此时，只有在没有找到正确的包声明时，返回的语法树才为 nil。 如果 errh 为空：解析器在遇到第一个错误时立即终止，返回的语法树为 nil。 其中 File 类型结构如下： type File struct Pragma Pragma // 编译指令\tPkgName *Name // 包名\tDeclList []Decl // 源文件中的各种声明\tEOF Pos // 解析位置\tGoVersion string // go 版本\tnode // 该源文件的 AST 根节点 parser.fileOrNil() 具体的解析过程在 parser.fileOrNil() 方法中： func (p *parser) fileOrNil() *File if trace defer p.trace(file)() // 1. 初始化文件节点\tf := new(File)\tf.pos = p.pos()\t// 2. 解析包声明\tf.GoVersion = p.goVersion\tp.top = false\tif !p.got(_Package) // 包声明必须放在第一位，这跟我们学 Go 语法对应上了 p.syntaxError(package statement must be first) return nil f.Pragma = p.takePragma() // 获取编译指令\tf.PkgName = p.name()\t// 获取包名 p.want(_Semi) // _Semi 在之前的 tokens.go 中可以发现是分号（;)，是的，包声明后面就是得带分号\t// 3. 处理包声明错误\tif p.first != nil return nil // 4. 循环解析顶层声明 // 循环处理文件中的所有声明，包括 import、const、type、var 和 func // 对每种类型的声明，调用其解析函数，如 importDecl、constDecl 进行解析\tprev := _Import\tfor p.tok != _EOF if p.tok == _Import prev != _Import p.syntaxError(imports must appear before other declarations) prev = p.tok switch p.tok case _Import: p.next() f.DeclList = p.appendGroup(f.DeclList, p.importDecl) case _Const: p.next() f.DeclList = p.appendGroup(f.DeclList, p.constDecl) case _Type: p.next() f.DeclList = p.appendGroup(f.DeclList, p.typeDecl) case _Var: p.next() f.DeclList = p.appendGroup(f.DeclList, p.varDecl) case _Func: p.next() if d := p.funcDeclOrNil(); d != nil f.DeclList = append(f.DeclList, d) default: // 5. 处理异常和错误 if p.tok == _Lbrace len(f.DeclList) 0 isEmptyFuncDecl(f.DeclList[len(f.DeclList)-1]) p.syntaxError(unexpected semicolon or newline before ) else p.syntaxError(non-declaration statement outside function body) p.advance(_Import, _Const, _Type, _Var, _Func) continue // Reset p.pragma BEFORE advancing to the next token (consuming ;) // since comments before may set pragmas for the next function decl. p.clearPragma() if p.tok != _EOF !p.got(_Semi) p.syntaxError(after top level declaration) p.advance(_Import, _Const, _Type, _Var, _Func) // 6. 完成解析，记录文件结束的位置\tp.clearPragma()\tf.EOF = p.pos()\treturn f 总结 parser.fileOrNil() 方法的处理过程大致如下： 初始化文件节点： f := new(File): 创建一个新的 File 节点。 f.pos = p.pos(): 设置节点的位置信息。 解析包声明（Package Clause）： f.GoVersion = p.goVersion: 记录 Go 版本。 p.top = false: 设置状态，表示不再处于文件顶层。 if !p.got(_Package) ...: 检查是否存在包声明，如果没有，则报错并返回 nil。 f.Pragma = p.takePragma(): 获取与包声明相关的编译指令。 f.PkgName = p.name(): 获取包名。 p.want(_Semi): 确认包声明后有分号。 处理包声明错误： if p.first != nil ...: 如果已有错误，停止解析并返回 nil。 解析顶层声明： 通过一个循环处理文件中的所有声明，包括导入（import）、常量（const）、类型（type）、变量（var）和函数（func）。 对每种类型的声明，调用相应的解析函数（如 p.importDecl、p.constDecl 等）。 将解析得到的声明添加到 f.DeclList 中。 处理异常和错误： 在解析过程中遇到的任何不符合语法的情况都会触发错误处理。 使用 p.syntaxError 报告语法错误。 使用 p.advance 在遇到错误时跳过一些标记，以尝试恢复到一个已知的稳定状态。 完成解析： 当遇到文件结束标记（EOF）时，完成解析。 f.EOF = p.pos(): 记录文件结束的位置。 返回构建的 File 节点。 Op 字段 AST 每个节点都包含了当前节点属性的 Op 字段，定义在 ir/node.go 中，以 O 开头。与词法解析阶段中的 token 相同的是，Op 字段也是一个整数。不同的是，每个 Op 字段都包含了语义信息。例如，当一个节点的 Op 操作为 OAS 时，该节点代表的语义为 Left := Right，而当节点的操作为 OAS2 时，代码的语义为 x,y,z = a,b,c。 这里仅展示部分 Op 字段的定义： type Op uint8// Node ops.const (\tOXXX Op = iota\t// names\tONAME // var or func name\t// Unnamed arg or return value: f(int, string) (int, error) etc // Also used for a qualified package identifier that hasnt been resolved yet.\tONONAME\tOTYPE // type name\tOLITERAL // literal\tONIL // nil\t// expressions\tOADD // X + Y\t...\t// X = Y or (if Def=true) X := Y\t// If Def, then Init includes a DCL node for X.\tOAS\t// Lhs = Rhs (x, y, z = a, b, c) or (if Def=true) Lhs := Rhs\t// If Def, then Init includes DCL nodes for Lhs\tOAS2\t... // statements OLABEL // Label: ...\tOEND) 以前面举例的赋值语句 a := b + c(12) 为例，该赋值语句最终会编程如下图所示的抽象语法树，节点之间具有从上到下的层次结构和依赖关系。 抽象语法树示例图 类型检查 完成 AST 的初步构建后，就进入类型检查阶段遍历节点树并决定节点的类型。具体的代码在 types2/check,go。 checker.CheckFiles() 其中最核心的方法就是 checker.CheckFiles()： func (check *Checker) checkFiles(files []*syntax.File) (err error) // 1. 不检查 unsafe 包\tif check.pkg == Unsafe return nil // 2. 检查 go 版本\tcheck.version, err = parseGoVersion(check.conf.GoVersion)\tif err != nil return err if check.version.after(version1, goversion.Version) return fmt.Errorf(package requires newer Go version %v, check.version) if check.conf.FakeImportC check.conf.go115UsesCgo return errBadCgo // 3. 错误处理\tdefer check.handleBailout(err) // 4. 详细检查每个地方\tprint := func(msg string) if check.conf.Trace fmt.Println() fmt.Println(msg) print(== initFiles ==)\tcheck.initFiles(files)\tprint(== collectObjects ==)\tcheck.collectObjects()\tprint(== packageObjects ==)\tcheck.packageObjects()\tprint(== processDelayed ==)\tcheck.processDelayed(0) // incl. all functions\tprint(== cleanup ==)\tcheck.cleanup()\tprint(== initOrder ==)\tcheck.initOrder()\tif !check.conf.DisableUnusedImportCheck print(== unusedImports ==) check.unusedImports() print(== recordUntyped ==)\tcheck.recordUntyped()\tif check.firstErr == nil check.monomorph() check.pkg.goVersion = check.conf.GoVersion\tcheck.pkg.complete = true\t// 5. 更新和清理\tcheck.imports = nil\tcheck.dotImportMap = nil\tcheck.pkgPathMap = nil\tcheck.seenPkgMap = nil\tcheck.recvTParamMap = nil\tcheck.brokenAliases = nil\tcheck.unionTypeSets = nil\tcheck.ctxt = nil\treturn 总结 checker.checkFiles() 的过程大致如下： 检查特殊包：如果是 Unsafe 包，则直接返回，因为它不能进行类型检查且不应被修改。 解析 Go 版本：根据配置解析 Go 版本，并进行兼容性检查。 错误处理：设置一个延迟函数来处理任何可能出现的错误。 类型检查的步骤： initFiles: 初始化文件。 collectObjects: 收集对象。 packageObjects: 打包对象。 processDelayed: 处理延迟的任务（包括所有函数）。 cleanup: 清理。 initOrder: 初始化顺序。 unusedImports: 检查未使用的导入。 recordUntyped: 记录未定类型。 monomorph: 如果没有错误，进行单态化处理。 更新和清理： 更新包的 Go 版本和完成状态。 清理不再需要的内部数据结构，释放内存。 返回：函数完成类型检查并返回。 可以看出具体的检查步骤都封装在第 4 点的各个函数中，其实检查的东西我们学习 Go 语言时所需要掌握的那些语法，我们以 initFiles 为例子来分析一下，对于其他检查函数，你有兴趣的话也可以了解一下，这里推荐将函数源代码拷贝发给 ChatGPT-4，相信对你会有很大的帮助。 checker.initFiles() // initFiles 初始化与文件相关的类型检查器// 参数中的 files 必须都属于同一个 packagefunc (check *Checker) initFiles(files []*syntax.File) // 1. 初始化\tcheck.files = nil\tcheck.imports = nil\tcheck.dotImportMap = nil\tcheck.firstErr = nil\tcheck.methods = nil\tcheck.untyped = nil\tcheck.delayed = nil\tcheck.objPath = nil\tcheck.cleaners = nil\t// 2. 确定包名和有效文件\tpkg := check.pkg\tfor _, file := range files switch name := file.PkgName.Value; pkg.name case : if name != _ pkg.name = name else check.error(file.PkgName, BlankPkgName, invalid package name _) fallthrough case name: check.files = append(check.files, file) default: check.errorf(file, MismatchedPkgName, package %s; expected %s, name, pkg.name) // ignore this file // 3. 对每个文件，解析其中指定的 Go 版本\tfor _, file := range check.files v, _ := parseGoVersion(file.GoVersion) if v.major 0 if v.equal(check.version) continue // Go 1.21 introduced the feature of setting the go.mod // go line to an early version of Go and allowing //go:build lines // to “upgrade” the Go version in a given file. // We can do that backwards compatibly. // Go 1.21 also introduced the feature of allowing //go:build lines // to “downgrade” the Go version in a given file. // That cant be done compatibly in general, since before the // build lines were ignored and code got the modules Go version. // To work around this, downgrades are only allowed when the // modules Go version is Go 1.21 or later. // If there is no check.version, then we dont really know what Go version to apply. // Legacy tools may do this, and they historically have accepted everything. // Preserve that behavior by ignoring //go:build constraints entirely in that case. if (v.before(check.version) check.version.before(version1, 21)) || check.version.equal(version0, 0) continue if check.posVers == nil check.posVers = make(map[*syntax.PosBase]version) check.posVers[base(file.Pos())] = v 总结 checker.initFiles() 方法的大致流程如下： 初始化状态：清空 Checker 结构体中与文件相关的多个字段，如 files, imports, dotImportMap 等，为新的检查过程做准备。 确定包名和有效文件： 遍历提供的文件，确定包名，并收集有效的文件。 如果文件的包名与 Checker 中的包名不匹配，则报错并忽略该文件。 处理 Go 版本： 对每个文件，解析其中指定的 Go 版本。 处理 Go 版本的兼容性和升级逻辑，尤其是在 Go 1.21 引入的一些特性，如 //go:build 行的处理。 可以看到 Go 语言开发团队在这里写了一大段关于 Go1.21 的注释，这段注释描述了 Go 1.21 版本引入的关于 Go 版本设置的两个新特性,这里简单解释一下： 升级 Go 版本的特性：在 Go 1.21 版本中，可以在 go.mod 文件里设置一个较旧的 Go 版本，同时允许在源文件中通过 //go:build 行来指定一个更高的 Go 版本。这样做可以向后兼容，即允许旧版本代码在新版本的 Go 环境中运行。 降级 Go 版本的限制：Go 1.21 也允许通过 //go:build 行来降低源文件中的 Go 版本。但这通常不是向后兼容的，因为在以前，//go:build 行被忽略，代码总是使用模块定义的 Go 版本。为了避免兼容性问题，仅当模块的 Go 版本为 1.21 或更高时，才允许这种降级。 未指定版本的情况：如果没有明确指定 check.version，编译器就不确定应该使用哪个 Go 版本。为了保持与旧工具的兼容，如果没有明确的版本约束，编译器将忽略 //go:build 行的限制。 死代码消除 类型检查阶段完成后，编译器前端工作基本完成，后面就进入中端了。这个阶段 Go 语言编译器将对 AST 进行分析和重构，从而完成一系列优化。 第一部分是死代码消除（dead code elimination），过程识别并移除不会在运行时执行的代码。这包括未使用的变量、函数、常量等。通过删除这些无用代码片段，可以减小最终程序的大小并提高运行效率。 这部分的代码在：deadcode/deadcode.go。打开代码文件，可以看到核心就是 Func() 和 stmt() 这 2 个函数。 Func() func Func(fn *ir.Func) // 1. 对函数体进行预处理\tstmts(fn.Body) // 2. 空函数体直接返回\tif len(fn.Body) == 0 return // 3. 遍历函数体，对其中每个节点进行处理\tfor _, n := range fn.Body // 节点有任何初始化操作，则不可消除，提前返回。 if len(n.Init()) 0 return switch n.Op() case ir.OIF: n := n.(*ir.IfStmt) // 如果 if 语句判断条件不是常量，或者 if else 中的 body 不为空，则不可消除，提前返回 if !ir.IsConst(n.Cond, constant.Bool) || len(n.Body) 0 || len(n.Else) 0 return case ir.OFOR: n := n.(*ir.ForStmt) // 如果 for 循环条件不是常量或一直为真，则不可消除，提前返回 if !ir.IsConst(n.Cond, constant.Bool) || ir.BoolVal(n.Cond) return default: return // 4. 标记隐藏闭包为死代码\tir.VisitList(fn.Body, markHiddenClosureDead) // 5. 重置函数体，替换为一个空语句，进行清理和优化\tfn.Body = []ir.Nodeir.NewBlockStmt(base.Pos, nil) 语句处理（stmts(fn.Body)）：对函数体中的语句进行预处理或转换，以便于后续的分析和优化。 空函数体直接返回：如果函数体为空，没有任何代码需要执行，因此函数直接返回。这是一种优化，避免对空函数体进行不必要的分析。 遍历函数体: 节点初始化检查：如果任何节点有初始化操作，意味着可能存在副作用或必要的代码执行，因此函数提前返回。 If 和 For 语句特殊处理 ir.OIF：如果 If 语句的条件不是常量布尔值，或者 If 语句有非空的 body 或 else 分支，则提前返回，因为这些分支可能包含重要的代码。 ir.OFOR：对于 For 循环，如果条件不是常量布尔值或者布尔值为真，意味着循环可能执行，因此提前返回。 标记隐藏闭包为死代码（markHiddenClosureDead）：如果所有节点都不触发提前返回，意味着整个函数体可能没有有效的代码执行。此时，将隐藏的闭包标记为死代码，可能是为了进一步的优化处理，如移除这些代码。 重置函数体：最后，将函数体替换为一个空的新块语句，这表明原始的函数体被认为是无效的或不会被执行，从而进行了代码的清理和优化。 stmt() 这个函数的目的是通过分析和简化控制流结构，来识别和移除那些在程序执行中永远不会到达的代码部分。这样的优化可以减少编译后的代码量，并提高程序运行时的效率。 func stmts(nn *ir.Nodes) // 1. 标记最后一个标签，其对应的 Op 字段就是 OLABEL\tvar lastLabel = -1\tfor i, n := range *nn if n != nil n.Op() == ir.OLABEL lastLabel = i // 2. 处理 if 和 switch 语句\tfor i, n := range *nn cut := false if n == nil continue if n.Op() == ir.OIF n := n.(*ir.IfStmt) n.Cond = expr(n.Cond) // if 语句根据条件是否为常量来保留和移除分支 if ir.IsConst(n.Cond, constant.Bool) var body ir.Nodes if ir.BoolVal(n.Cond) ir.VisitList(n.Else, markHiddenClosureDead) n.Else = ir.Nodes body = n.Body else ir.VisitList(n.Body, markHiddenClosureDead) n.Body = ir.Nodes body = n.Else // 如果 then 或 else 分支以 panic 或 return 语句结束，那么可以安全地移除该节点之后的所有语句。 // 这是因为 panic 或 return 会导致函数终止，后续的代码永远不会被执行。 // 同时，注释提到要避免移除标签（labels），因为它们可能是 goto 语句的目标， // 而且为了避免 goto 相关的复杂性，没有使用 isterminating 标记。 // might be the target of a goto. See issue 28616. if body := body; len(body) != 0 switch body[(len(body) - 1)].Op() case ir.ORETURN, ir.OTAILCALL, ir.OPANIC: if i lastLabel cut = true // 尝试简化 switch 语句，根据条件值决定哪个分支始终被执行 if n.Op() == ir.OSWITCH n := n.(*ir.SwitchStmt) func() if n.Tag != nil n.Tag.Op() == ir.OTYPESW return // no special type-switch case yet. var x constant.Value // value were switching on if n.Tag != nil if ir.ConstType(n.Tag) == constant.Unknown return x = n.Tag.Val() else x = constant.MakeBool(true) // switch ... = switch true ... var def *ir.CaseClause for _, cas := range n.Cases if len(cas.List) == 0 // default case def = cas continue for _, c := range cas.List if ir.ConstType(c) == constant.Unknown return // cant statically tell if it matches or not - give up. if constant.Compare(x, token.EQL, c.Val()) for _, n := range cas.Body if n.Op() == ir.OFALL return // fallthrough makes it complicated - abort. // This switch entry is the one that always triggers. for _, cas2 := range n.Cases for _, c2 := range cas2.List ir.Visit(c2, markHiddenClosureDead) if cas2 != cas ir.VisitList(cas2.Body, markHiddenClosureDead) // Rewrite to switch case true: ... n.Tag = nil cas.List[0] = ir.NewBool(c.Pos(), true) cas.List = cas.List[:1] n.Cases[0] = cas n.Cases = n.Cases[:1] return if def != nil for _, n := range def.Body if n.Op() == ir.OFALL return // fallthrough makes it complicated - abort. for _, cas := range n.Cases if cas != def ir.VisitList(cas.List, markHiddenClosureDead) ir.VisitList(cas.Body, markHiddenClosureDead) n.Cases[0] = def n.Cases = n.Cases[:1] return // TODO: handle case bodies ending with panic/return as we do in the IF case above. // entire switch is a nop - no case ever triggers for _, cas := range n.Cases ir.VisitList(cas.List, markHiddenClosureDead) ir.VisitList(cas.Body, markHiddenClosureDead) n.Cases = n.Cases[:0] () // 3. 对节点的初始化语句递归调用 stmt 函数进行处理 if len(n.Init()) != 0 stmts(n.(ir.InitNode).PtrInit()) // 4. 遍历其他控制结构，递归处理它们的内部语句 switch n.Op() case ir.OBLOCK: n := n.(*ir.BlockStmt) stmts(n.List) case ir.OFOR: n := n.(*ir.ForStmt) stmts(n.Body) case ir.OIF: n := n.(*ir.IfStmt) stmts(n.Body) stmts(n.Else) case ir.ORANGE: n := n.(*ir.RangeStmt) stmts(n.Body) case ir.OSELECT: n := n.(*ir.SelectStmt) for _, cas := range n.Cases stmts(cas.Body) case ir.OSWITCH: n := n.(*ir.SwitchStmt) for _, cas := range n.Cases stmts(cas.Body) // 5. 如果确定了是可以消除的代码，则对函数体进行阶段，且标记其中的闭包为死代码 if cut ir.VisitList((*nn)[i+1:len(*nn)], markHiddenClosureDead) *nn = (*nn)[:i+1] break 标记最后一个标签：遍历所有节点，记录最后一个标签（OLABEL）的位置。这对于后面判断是否可以安全地移除代码非常重要。 处理 if 和 switch 语句： 对于 if 语句，它根据条件是否为常量来决定保留哪个分支，移除另一个分支。 对于 switch 语句，它尝试简化 switch，根据条件值决定哪个分支将始终被执行。 节点初始化：如果节点有初始化语句，对这些初始化语句递归调用 stmts 函数。 遍历其他控制结构：对于 for、if、range、select 和 switch 等控制结构，递归地处理它们的内部语句。 消除死代码：如果判断一个节点之后的所有代码都是无效的，它会标记这些代码为死代码并截断函数体。 去虚拟化 去虚拟化（Devirtualization）是编译器优化的一种技术，用于提高面向对象程序的性能。在面向对象编程中，方法调用通常是通过虚拟函数表（vtable）动态解析的，这被称为虚拟调用。虚拟调用允许对象在运行时表现出多态行为，但这也带来了一定的性能开销。 去虚拟化的目的是在编译时静态确定方法调用的目标，从而避免运行时的动态查找。如果编译器能够确定一个特定的接口调用总是调用同一个方法，它可以将这个虚拟调用替换为直接调用，减少运行时开销。这种优化特别适用于那些调用目标不会因为程序执行的不同路径而改变的情况。 这部分的代码在 devirtuailze/devirtualize.go。 核心就 2 个函数： Static() ：遍历函数中的所有节点，尤其注意跳过在 go 或 defer 语句中的调用，并对其他接口方法调用尝试进行静态去虚拟化优化。 staticCall() ：针对一个具体的接口方法调用，如果可能，将其替换为直接的具体类型方法调用，以优化性能。 Static() func Static(fn *ir.Func) ir.CurFunc = fn\tgoDeferCall := make(map[*ir.CallExpr]bool) // 1. VisitList 对 fn.Body 中所有节点调用后面的 func\tir.VisitList(fn.Body, func(n ir.Node) switch n := n.(type) // 2. 跳过 go 和 defer 语句 case *ir.GoDeferStmt: if call, ok := n.Call.(*ir.CallExpr); ok goDeferCall[call] = true return // 3. 调用 staticCall 尝试进行去虚拟化 case *ir.CallExpr: if !goDeferCall[n] staticCall(n) ) 设定当前函数为 fn。 遍历函数体内的节点，特别注意 go 和 defer 语句。如果调用发生在这些语句中，它会被跳过，因为去虚拟化可能改变程序的语义。 对于不在 go 或 defer 语句中的接口方法调用，调用 staticCall 函数尝试进行去虚拟化。 staticCall() func staticCall(call *ir.CallExpr) // 1. 检查调用是否为接口方法调用，如果不是，直接返回\tif call.Op() != ir.OCALLINTER return // 2. 获取接收器和相关类型\tsel := call.X.(*ir.SelectorExpr)\tr := ir.StaticValue(sel.X) // 3. 检查接收器是否是接口转换，如果不是，直接返回\tif r.Op() != ir.OCONVIFACE return recv := r.(*ir.ConvExpr) // 4. 提取接收器类型\ttyp := recv.X.Type()\tif typ.IsInterface() return // 5. shape 类型直接返回，因为这一般涉及到泛型，需要通过字典进行间接调用\tif typ.IsShape() return if typ.HasShape() if base.Flag.LowerM != 0 base.WarnfAt(call.Pos(), cannot devirtualize %v: shaped receiver %v, call, typ) return if sel.X.Type().HasShape() if base.Flag.LowerM != 0 base.WarnfAt(call.Pos(), cannot devirtualize %v: shaped interface %v, call, sel.X.Type()) return // 6. 类型断言和方法选择，尝试确定调用的具体方法\tdt := ir.NewTypeAssertExpr(sel.Pos(), sel.X, nil)\tdt.SetType(typ)\tx := typecheck.Callee(ir.NewSelectorExpr(sel.Pos(), ir.OXDOT, dt, sel.Sel))\tswitch x.Op() case ir.ODOTMETH: x := x.(*ir.SelectorExpr) if base.Flag.LowerM != 0 base.WarnfAt(call.Pos(), devirtualizing %v to %v, sel, typ) call.SetOp(ir.OCALLMETH) call.X = x\tcase ir.ODOTINTER: x := x.(*ir.SelectorExpr) if base.Flag.LowerM != 0 base.WarnfAt(call.Pos(), partially devirtualizing %v to %v, sel, typ) call.SetOp(ir.OCALLINTER) call.X = x\tdefault: if base.Flag.LowerM != 0 base.WarnfAt(call.Pos(), failed to devirtualize %v (%v), x, x.Op()) return // 7. 根据类型断言的结果，尝试将接口方法调用转换为直接方法调用或保留为接口方法调用。\ttypes.CheckSize(x.Type())\tswitch ft := x.Type(); ft.NumResults() case 0:\tcase 1: call.SetType(ft.Results().Field(0).Type)\tdefault: call.SetType(ft.Results()) // 8. 对可能修改后的方法调用进行进一步的类型检查和调整。\ttypecheck.FixMethodCall(call) 检查是否为接口方法调用：函数首先判断传入的调用是否是接口方法调用（ir.OCALLINTER），这是去虚拟化的前提条件。 处理形状类型：代码中提到，如果接收器的类型是形状类型（用于泛型），则无法去虚拟化，因为这需要通过字典进行间接调用。 处理形状类型的接收器：如果接收器的类型具有形状类型，则当前无法进行去虚拟化。注释中还提到了一些待实现（TODO）的优化点，例如处理非泛型的提升方法。 处理形状类型的接口：如果调用的接口本身是一个形状类型，由于指针身份的不同，类型断言可能会失败，因此在这种情况下也无法去虚拟化。 转换方法调用：根据调用的具体情况，将接口方法调用转换为直接的方法调用（OCALLMETH）或保留为接口方法调用（OCALLINTER）。 更新调用类型：为了正确处理函数返回值，需要更新调用的类型，确保参数大小和栈偏移量正确。 反糖化方法调用：如果创建了直接方法调用，需要对其进行后续的类型检查和调整。 函数内联 函数内联是将一个函数的代码直接插入到每个调用点，而不是进行常规的函数调用。这意味着函数的整个体被复制到每个调用该函数的地方。 优点： 减少开销：内联消除了函数调用的开销，如参数传递、栈操作等。 提升性能：有助于其他优化，比如循环展开、常量传播，因为编译器可以看到函数体内的代码。 选择哪些函数内联： 小函数：通常是小函数，因为它们的内联带来的性能提升相对于代码膨胀的代价来说是值得的。 调用频率高的函数：这些函数如果内联，可以显著减少运行时的调用开销。 在 Go 语言中，可以通过 //go:noinline 来禁止函数内联。 这部分的主要实现在 inline.inl.go，核心函数是：CanInline() 和 InlineImpossible()。 CanInline() // Inlining budget parameters, gathered in one placeconst ( // budget 是内联复杂度的衡量， // 超过 80 表示编译器认为这个函数太复杂了，就不进行函数内联了\tinlineMaxBudget = 80)// CanInline 用于判断 fn 是否可内联。// 如果可以，会将 fn.Body 和 fn.Dcl 拷贝一份放到 fn.Inl，// 其中 fn 和 fn.Body 需要确保已经经过类型检查了。func CanInline(fn *ir.Func, profile *pgo.Profile) // 函数名必须有效\tif fn.Nname == nil base.Fatalf(CanInline no nname %+v, fn) // 如果不能内联，输出原因\tvar reason string\tif base.Flag.LowerM 1 || logopt.Enabled() defer func() if reason != if base.Flag.LowerM 1 fmt.Printf(%v: cannot inline %v: %s , ir.Line(fn), fn.Nname, reason) if logopt.Enabled() logopt.LogOpt(fn.Pos(), cannotInlineFunction, inline, ir.FuncName(fn), reason) () // 检查是否符合不可能内联的情况，如果返回的 reason 不为空，则表示有不可以内联的原因\treason = InlineImpossible(fn)\tif reason != return if fn.Typecheck() == 0 base.Fatalf(CanInline on non-typechecked function %v, fn) n := fn.Nname\tif n.Func.InlinabilityChecked() return defer n.Func.SetInlinabilityChecked(true)\tcc := int32(inlineExtraCallCost)\tif base.Flag.LowerL == 4 cc = 1 // this appears to yield better performance than 0. // 设置内联预算，后面如果检查函数的复杂度超过预算了，就不内联了\tbudget := int32(inlineMaxBudget)\tif profile != nil if n, ok := profile.WeightedCG.IRNodes[ir.LinkFuncName(fn)]; ok if _, ok := candHotCalleeMap[n]; ok budget = int32(inlineHotMaxBudget) if base.Debug.PGODebug 0 fmt.Printf(hot-node enabled increased budget=%v for func=%v , budget, ir.PkgFuncName(fn)) // 遍历函数体，计算复杂度，判断是否超过内联预算\tvisitor := hairyVisitor curFunc: fn, budget: budget, maxBudget: budget, extraCallCost: cc, profile: profile, if visitor.tooHairy(fn) reason = visitor.reason return // 前面检查都没问题，则标记为可以内联，并复制其函数体和声明到内联结构体中\tn.Func.Inl = ir.Inline Cost: budget - visitor.budget, Dcl: pruneUnusedAutos(n.Defn.(*ir.Func).Dcl, visitor), Body: inlcopylist(fn.Body), CanDelayResults: canDelayResults(fn), // 日志和调试\tif base.Flag.LowerM 1 fmt.Printf(%v: can inline %v with cost %d as: %v %v , ir.Line(fn), n, budget-visitor.budget, fn.Type(), ir.Nodes(n.Func.Inl.Body)) else if base.Flag.LowerM != 0 fmt.Printf(%v: can inline %v , ir.Line(fn), n) if logopt.Enabled() logopt.LogOpt(fn.Pos(), canInlineFunction, inline, ir.FuncName(fn), fmt.Sprintf(cost: %d, budget-visitor.budget)) 基本检查：验证函数是否已经进行了类型检查，以及函数名是否有效。 判断是否可以内联：调用 InlineImpossible 函数来检查是否有任何基本的限制条件阻止内联（例如函数太大、递归等）。 内联预算设置：根据函数的特征和可能的性能剖析信息来设定内联预算。这个预算是内联决策的关键参数之一。 详细分析：hairyVisitor 结构用于遍历函数体，判断是否超出了内联预算。这涉及对函数体的复杂度和大小的评估。 内联决策：如果函数通过了所有检查并且未超出预算，则标记为可以内联，并复制其函数体和声明（Dcl）到内联结构体中。 日志和调试：根据编译器的日志级别，输出关于内联决策的详细信息，例如为什么一个函数不能被内联或者它的内联成本是多少。 InlineImpossible() func InlineImpossible(fn *ir.Func) string var reason string // reason, if any, that the function can not be inlined.\tif fn.Nname == nil reason = no name return reason // If marked go:noinline, dont inline.\tif fn.Pragmair.Noinline != 0 reason = marked go:noinline return reason // If marked go:norace and -race compilation, dont inline.\tif base.Flag.Race fn.Pragmair.Norace != 0 reason = marked go:norace with -race compilation return reason // If marked go:nocheckptr and -d checkptr compilation, dont inline.\tif base.Debug.Checkptr != 0 fn.Pragmair.NoCheckPtr != 0 reason = marked go:nocheckptr return reason // If marked go:cgo_unsafe_args, dont inline, since the function\t// makes assumptions about its argument frame layout.\tif fn.Pragmair.CgoUnsafeArgs != 0 reason = marked go:cgo_unsafe_args return reason // If marked as go:uintptrkeepalive, dont inline, since the keep\t// alive information is lost during inlining.\t//\t// TODO(prattmic): This is handled on calls during escape analysis,\t// which is after inlining. Move prior to inlining so the keep-alive is\t// maintained after inlining.\tif fn.Pragmair.UintptrKeepAlive != 0 reason = marked as having a keep-alive uintptr argument return reason // If marked as go:uintptrescapes, dont inline, since the escape\t// information is lost during inlining.\tif fn.Pragmair.UintptrEscapes != 0 reason = marked as having an escaping uintptr argument return reason // The nowritebarrierrec checker currently works at function\t// granularity, so inlining yeswritebarrierrec functions can confuse it\t// (#22342). As a workaround, disallow inlining them for now.\tif fn.Pragmair.Yeswritebarrierrec != 0 reason = marked go:yeswritebarrierrec return reason // If a local function has no fn.Body (is defined outside of Go), cannot inline it.\t// Imported functions dont have fn.Body but might have inline body in fn.Inl.\tif len(fn.Body) == 0 !typecheck.HaveInlineBody(fn) reason = no function body return reason // If fn is synthetic hash or eq function, cannot inline it.\t// The function is not generated in Unified IR frontend at this moment.\tif ir.IsEqOrHashFunc(fn) reason = type eq/hash function return reason return 无函数名：如果函数没有名字，不能内联。 有 go:noinline 指令：显式标记为不内联。 有 go:norace 指令并在 -race 编译模式下：在竞态检测编译模式下不内联标记为 norace 的函数。 有 go:nocheckptr 指令并在 -d checkptr 编译模式下：在指针检查编译模式下不内联标记为 nocheckptr 的函数。 有 go:cgo_unsafe_args 指令：对于标记为 cgo_unsafe_args 的函数，由于参数布局的假设，不内联。 有 go:uintptrkeepalive 指令：不内联标记为 uintptrkeepalive 的函数。 有 go:uintptrescapes 指令：不内联标记为 uintptrescapes 的函数。 有 go:yeswritebarrierrec 指令：为了防止写屏障记录检查器的混淆，不内联标记为 yeswritebarrierrec 的函数。 无函数体：本地定义但没有函数体的函数（外部定义的 Go 函数）不可内联。 是合成的 hash 或 eq 函数：不能内联这些类型的函数。 举例 我们通过一段代码来看看编译器的函数内联情况。 func SayHello() string s := hello, + world\treturn sfunc Fib(index int) int if index 2 return index return Fib(index-1) + Fib(index-2)func ForSearch() int var s = []int1, 2, 3, 4, 5, 6, 7, 8, 9, 0\tres := 0\tfor i := 0; i len(s); i++ if s[i] == i res = i return resfunc main() SayHello()\tFib(65)\tForSearch() 在编译时我们可以加入 -m=2 标签，来打印函数的内联调试信息。在 main.go 目录下执行： go tool compile -m=2 main.go 输出： main.go:3:6: can inline SayHello with cost 7 as: func() string s := hello, + world; return s main.go:8:6: cannot inline Fib: recursivemain.go:15:6: can inline ForSearch with cost 45 as: func() int s := []int...; res := 0; for loop; return res main.go:26:6: cannot inline main: function too complex: cost 116 exceeds budget 80main.go:27:10: inlining call to SayHellomain.go:29:11: inlining call to ForSearchmain.go:16:15: []int... does not escapemain.go:29:11: []int... does not escape 可以看到 SayHello() 和 ForSearch 都被内联了，而 Fib() 因为有递归，所以不会被内联。 逃逸分析 逃逸分析是 Go 语言中非常重要的优化阶段，用于标识变量内存应该被分配在栈上还是堆上。 在传统的 C 或 C++ 开发中，开发者经常会犯的错误就是函数返回了一个栈上的对象指针，在函数执行完毕后，函数栈会被销毁，如果继续访问被销毁栈上的对象指针，那么就会出现问题。 Go 语言能够通过编译时的逃逸分析识别这种问题，自动将这类变量放置到堆区，并借助 Go 运行时的垃圾回收机制自动释放内存。编译器会尽可能地将变量放置在栈上，因为栈中的对象会随着函数调用结束被自动销毁，这可以减轻运行时分配和垃圾回收的负担。 在 Go 语言中，开发者模糊了栈区和堆区的区别，不管是字符串、数组字面量，还是通过 new、make 标识符创建的对象，都既可能被分配到栈上，也可能被分配到堆上。但是，整体上会遵循 2 个原则： 指向栈上对象的指针不能被存储到堆上； 指向栈上对象的指针不能超过该栈对象的生命周期。 这部分的代码主要在 escape。 分析过程 Go 语言通过对 AST 的静态数据流分析来实现逃逸分析（escape/graph.go），在这个过程，它会构建带权重的有向图，其中权重可以表面当前变量引用和解引用的数量。 引用（a） 减 1 解引用（*a）加 1 func (k hole) deref(where ir.Node, why string) hole return k.shift(1).note(where, why) // 解引用func (k hole) addr(where ir.Node, why string) hole return k.shift(-1).note(where, why) // 引用 具体来说，Go 逃逸分析会按照如下规则生成数据流图（带权重的有向图）： 每个变量作为一个节点（location）； 每个赋值动作是一个有向边（edge），赋值给谁则指向谁； 解引用（deref），即 *操作会给边的权重 +1； 引用（addr），即 操作会给边权重 -1。 其中：节点权重 = 指向的节点权重 + 边权重 逃逸分析的目标就是找到其中节点权重为 -1 的变量，并结合上述提到的 2 个原则，来判断要不要将变量分配到堆上。 分析实例 我们举一个例子来进行分析： package mainvar o *intfunc main() l := new(int)\t*l = 42\tm := l\tn := m\to = **n 再次回顾一下，* 是加 1， 是减一。按照常规思路，我们从上往下分析： 先画出节点的赋值顺序，赋值给谁，边就指向谁： 第1步：梳理节点顺序 然后根据引用和解引用给边赋权重，因为 new(int) 其实就是分配一个 int(0) 并取地址，相当于 ，所以指向 l 的边权重是 -1： 第2步：给边赋值 节点权重 = 边权重 + 指向节点权重，因为没有对 o 变量进行任何的操作，所以 o 权重为 0，从右往左推可以得到： 第3步：计算节点权重 经过分析，我们就找到了节点权重为 -1 的节点 new(int)，又由于它的节点变量地址最终会被传递到变量 o 上，结合之前的 2 个原则，o 是一个全局变量，声明周期是超过函数栈的，所以 new(int) 会被分配到堆上。 可以执行下面语句输出逃逸结果： go tool compile -m main.go 如： /escape/main.go:5:6: can inline main/escape/main.go:6:10: new(int) escapes to hea 也可以执行下面语句输出数据流图构建过程： go build -gcflags=-m -m -l main.go 如： # command-line-arguments./main.go:6:10: new(int) escapes to heap:./main.go:6:10: flow: l = storage for new(int):./main.go:6:10: from new(int) (spill) at ./main.go:6:10./main.go:6:10: from l := new(int) (assign) at ./main.go:6:4./main.go:6:10: flow: m = l:./main.go:6:10: from l (address-of) at ./main.go:8:7./main.go:6:10: from m := l (assign) at ./main.go:8:4./main.go:6:10: flow: n = m:./main.go:6:10: from m (address-of) at ./main.go:9:7./main.go:6:10: from n := m (assign) at ./main.go:9:4./main.go:6:10: flow: heap = **n:./main.go:6:10: from *n (indirection) at ./main.go:10:7./main.go:6:10: from *(*n) (indirection) at ./main.go:10:6./main.go:6:10: from o = *(*n) (assign) at ./main.go:10:4./main.go:6:10: new(int) escapes to heap 如果我们试一下，把 o 放在 main() 里面呢？ func main() var o *int\tl := new(int)\t*l = 42\tm := l\tn := m\to = **n\to = o // 让编译通过 执行下面语句： go tool compile -m main.go 输出： /escape/main.go:3:6: can inline main/escape/main.go:5:10: new(int) does not escape 如我们所想，虽然 new(int) 的权重为 -1，但是它的声明周期始终没有超过 main()，所以没必要逃逸到堆上。 变量捕获 变量捕获主要是针对闭包（closure）场景而言的，由于闭包函数中可能引用闭包外的变量，因此变量捕获需要明确在闭包中通过值引用或者地址引用的方式来捕获变量。 这一过程在前面提到的逃逸分析过程中进行，具体实现在 escape/escape.go 的 flowClosure() 函数中： func (b *batch) flowClosure(k hole, clo *ir.ClosureExpr) // 遍历闭包中的所有变量 for _, cv := range clo.Func.ClosureVars n := cv.Canonical() loc := b.oldLoc(cv) // 如果变量未被捕获，则触发错误 if !loc.captured base.FatalfAt(cv.Pos(), closure variable never captured: %v, cv) // 根据变量的特性决定是通过值还是引用捕获 // 如果变量未被重新赋值或取址，并且小于等于 128 字节，则通过值捕获 n.SetByval(!loc.addrtaken !loc.reassigned n.Type().Size() = 128) if !n.Byval() n.SetAddrtaken(true) // 特殊情况处理：字典变量不通过值捕获 if n.Sym().Name == typecheck.LocalDictName base.FatalfAt(n.Pos(), dictionary variable not captured by value) // 记录闭包捕获变量的方式（值或引用） if base.Flag.LowerM 1 how := ref if n.Byval() how = value base.WarnfAt(n.Pos(), %v capturing by %s: %v (addr=%v assign=%v width=%d), n.Curfn, how, n, loc.addrtaken, loc.reassigned, n.Type().Size()) // 建立闭包变量的数据流 k := k if !cv.Byval() k = k.addr(cv, reference) b.flow(k.note(cv, captured by a closure), loc) 举个例子： package mainfunc main() a := 1\tb := 2\tgo func() add(a, b)\t()\ta = 99func add(a, b int) a = a + b 执行下面语句看看变量的捕获方式： go tool compile -m=2 main.go | grep capturing 输出： main.go:4:2: main capturing by ref: a (addr=false assign=true width=8)main.go:5:2: main capturing by value: b (addr=false assign=false width=8) 可以看到 a 是通过 ref 地址引用 的方式进行引用的，而 b 是通过 value 值传递 的方式进行引用的。 简单分析一下：上述例子中，闭包引用了 a 和 b 这 2 个闭包外声明的变量，而变量 a 在闭包之前又做了一些其他的操作，而 b 没有，所以对于 a，因为闭包外有操作，所以闭包内的操作可能是有特殊意义的，需要反馈到闭包外，就需要用 ref 地址引用了，而 b 在闭包外并不关心，所以闭包内的操作不会影响到闭包外，故直接使用 value 值传递 即可。 闭包重写 逃逸分析后，现在我们进入 walk 阶段了。这里首先会进行闭包重写。其核心逻辑在 walk/closure.go 中。 闭包重写分为 2 种情况： 闭包定义后被立即调用 闭包定义后不立即调用 闭包定义后被立即调用 在闭包定义后被立即调用的情况下，闭包只会被调用一次，这时可以将闭包转换为普通函数的调用形式。 如： func main() a := 1\tb := 2\tgo func() add(a, b)\t()\ta = 99func add(a, b int) a = a + b 会被转换为普通函数的调用形式： func main() a := 1\tb := 2\tgo func1(a, b)\ta = 99// 注意这里 a 的类型的 *int，因为在变量捕获阶段，判断了 a 应该用地址引用func func1(a *int, b int) add(*a, b)func add(a, b int) a = a + b 编译器具体的处理逻辑在 directClosureCall() 中： // directClosureCall rewrites a direct call of a function literal into// a normal function call with closure variables passed as arguments.// This avoids allocation of a closure object.//// For illustration, the following call:////\tfunc(a int) // println(byval)// byref++//\t(42)//// becomes:////\tfunc(byval int, byref *int, a int) // println(byval)// (*byref)++//\t(byval, byref, 42)func directClosureCall(n *ir.CallExpr) clo := n.X.(*ir.ClosureExpr)\tclofn := clo.Func // 如果闭包足够简单，不进行处理，留给 walkClosure 处理。\tif ir.IsTrivialClosure(clo) return // leave for walkClosure to handle // 将闭包中的每个变量转换为函数的参数。对于引用捕获的变量，创建相应的指针参数。\tvar params []*types.Field\tvar decls []*ir.Name\tfor _, v := range clofn.ClosureVars if !v.Byval() // 对于引用捕获的变量，创建相应的指针参数。 addr := ir.NewNameAt(clofn.Pos(), typecheck.Lookup(+v.Sym().Name)) addr.Curfn = clofn addr.SetType(types.NewPtr(v.Type())) v.Heapaddr = addr v = addr v.Class = ir.PPARAM decls = append(decls, v) fld := types.NewField(src.NoXPos, v.Sym(), v.Type()) fld.Nname = v params = append(params, fld) // 创建一个新的函数类型，将捕获的变量作为前置参数，并更新函数的声明。\tf := clofn.Nname\ttyp := f.Type()\ttyp = types.NewSignature(nil, append(params, typ.Params().FieldSlice()...), typ.Results().FieldSlice())\tf.SetType(typ)\tclofn.Dcl = append(decls, clofn.Dcl...)\t// 将原始的闭包调用重写为对新函数的调用，并将捕获的变量作为实际参数传递。\tn.X = f\tn.Args.Prepend(closureArgs(clo)...)\t// 调整调用表达式的类型，以反映参数和返回值类型的变化。\tif typ.NumResults() == 1 n.SetType(typ.Results().Field(0).Type) else n.SetType(typ.Results()) // 虽然不再是传统意义上的闭包，但为了确保函数被编译，将其添加到待编译列表中。\tir.CurFunc.Closures = append(ir.CurFunc.Closures, clofn) 这段代码是 Go 编译器中的 directClosureCall 函数，用于将直接调用的函数字面量重写为正常的函数调用，同时将闭包变量作为参数传递。这避免了闭包对象的分配。 主要步骤如下： 检查闭包是否简单：如果闭包足够简单，不进行处理，留给 walkClosure 处理。 处理闭包变量：将闭包中的每个变量转换为函数的参数。对于引用捕获的变量，创建相应的指针参数。 更新函数类型和声明：创建一个新的函数类型，将捕获的变量作为前置参数，并更新函数的声明。 重写调用：将原始的闭包调用重写为对新函数的调用，并将捕获的变量作为实际参数传递。 更新调用表达式类型：调整调用表达式的类型，以反映参数和返回值类型的变化。 添加到待编译列表：虽然不再是传统意义上的闭包，但为了确保函数被编译，将其添加到待编译列表中。 这个函数的目的是优化闭包的调用，通过避免闭包对象的分配来提高性能。 闭包定义后不立即调用 如果闭包定义后不被立即调用，而是后续调用，那么同一个闭包可能会被调用多次，这个时候就必须创建闭包对象了。 编译器具体的处理逻辑在 walkClosure() 中： func walkClosure(clo *ir.ClosureExpr, init *ir.Nodes) ir.Node clofn := clo.Func\t// 如果没有闭包变量，闭包被视为全局函数，直接返回函数名。\tif ir.IsTrivialClosure(clo) if base.Debug.Closure 0 base.WarnfAt(clo.Pos(), closure converted to global) return clofn.Nname // 对于复杂闭包，设置需要上下文标记，并进行运行时检查。\tir.ClosureDebugRuntimeCheck(clo)\tclofn.SetNeedctxt(true)\t// 确保闭包函数不会被重复添加到编译队列。\tif !clofn.Walked() clofn.SetWalked(true) ir.CurFunc.Closures = append(ir.CurFunc.Closures, clofn) // 构造一个复合字面量表达式来表示闭包实例。\ttyp := typecheck.ClosureType(clo) // 将闭包函数和捕获的变量作为字段添加到闭包结构中。\tclos := ir.NewCompLitExpr(base.Pos, ir.OCOMPLIT, typ, nil)\tclos.SetEsc(clo.Esc())\tclos.List = append([]ir.Nodeir.NewUnaryExpr(base.Pos, ir.OCFUNC, clofn.Nname), closureArgs(clo)...)\tfor i, value := range clos.List clos.List[i] = ir.NewStructKeyExpr(base.Pos, typ.Field(i), value) // 创建闭包结构的地址，并进行类型转换以符合闭包类型。\taddr := typecheck.NodAddr(clos)\taddr.SetEsc(clo.Esc())\tcfn := typecheck.ConvNop(addr, clo.Type())\t// 如果存在预分配的闭包对象，进行相关处理。\tif x := clo.Prealloc; x != nil if !types.Identical(typ, x.Type()) panic(closure type does not match orders assigned type) addr.Prealloc = x clo.Prealloc = nil // 对最终构建的闭包表达式进行进一步处理。\treturn walkExpr(cfn, init) 检查是否为简单闭包：如果没有闭包变量，闭包被视为全局函数，直接返回函数名。 处理非简单闭包：对于复杂闭包，设置需要上下文标记，并进行运行时检查。 防止重复处理：确保闭包函数不会被重复添加到编译队列。 创建闭包结构：构造一个复合字面量表达式来表示闭包实例。 填充闭包参数：将闭包函数和捕获的变量作为字段添加到闭包结构中。 地址和类型转换：创建闭包结构的地址，并进行类型转换以符合闭包类型。 处理预分配的闭包：如果存在预分配的闭包对象，进行相关处理。 表达式处理：对最终构建的闭包表达式进行进一步处理。 遍历函数 闭包重写后，会进入 walk 阶段，如官方 文档所说：这是对 IR 表示的最后一次遍历，它有两个目的： 将复杂的语句分解为简单的单个语句，引入临时变量并遵守求值顺序； 将高级 Go 构造转换为更原始的构造。 举个例子，walkRange() 函数针对不同类型的 range 语句（数组、切片、映射、通道和字符串）进行处理，将其转换为更基本的循环结构，并应用必要的变换。 func walkRange(nrange *ir.RangeStmt) ir.Node // ... 省略代码 ... // 遍历 range 语句的不同情况 switch t.Kind() default: base.Fatalf(walkRange) // 处理数组、切片、指针（指向数组）的情况 case types.TARRAY, types.TSLICE, types.TPTR: // ... 省略代码 ... // 处理映射的情况 case types.TMAP: // ... 省略代码 ... // 处理通道的情况 case types.TCHAN: // ... 省略代码 ... // 处理字符串的情况 case types.TSTRING: // ... 省略代码 ... // ... 省略代码 ... // 构建并返回新的 for 语句 nfor.PtrInit().Append(init...) typecheck.Stmts(nfor.Cond.Init()) nfor.Cond = typecheck.Expr(nfor.Cond) nfor.Cond = typecheck.DefaultLit(nfor.Cond, nil) nfor.Post = typecheck.Stmt(nfor.Post) typecheck.Stmts(body) nfor.Body.Append(body...) nfor.Body.Append(nrange.Body...) var n ir.Node = nfor n = walkStmt(n) base.Pos = lno return n 这部分代码在 walk，对其他优化感兴趣的读者可以阅读这部分的代码。 SSA 生成 遍历函数（Walk）阶段后，编译器会将 AST 转换为下一个重要的中间表示形态，称为 SSA，其全称为 Static Single Assignment，静态单赋值。SSA 被大多数现代的编译器（包括 GCC 和 LLVM）使用，用于编译过程中的优化和代码生成。其核心特点和用途如下： 变量唯一赋值：在 SSA 形式中，每个变量只被赋值一次，使得变量的使用和修改更加清晰。 方便的数据流分析：SSA 使得数据流分析更加直接和高效，因为每个变量的赋值点只有一个。 优化算法的基础：许多编译器优化技术，如死代码消除、常量传播、强度削减等，在 SSA 形式下更易实现。 Phi 函数：SSA 引入了 Phi 函数来处理变量在不同控制流路径上的不同赋值。 代码生成：SSA 形式简化了目标代码生成的过程，因为它提供了更清晰的操作和变量使用视图。 官方对 SSA 生成阶段进行了详细的描述：Introduction to the Go compiler's SSA backend Go 提供了强有力的工具查看 SSA 初始及其后续优化阶段生成的代码片段，可以通过编译时指定 GOSSAFUNC=pkg.func 实现。 以下面代码为例： package mainvar d uint8func main() var a uint8 = 1\ta = 2\tif true a = 3 d = a 我们可以自行简单分析一下，这段代码前面 a 的所有操作其实都是无意义的，整段代码其实就在说 d = 3 这件事。 在 linux 或者 mac 上执行： GOSSAFUNC=main.main go build main.go 在 Windows 上执行： $env:GOSSAFUNC=maingo build .\\main.go 可以看到输出： dumped SSA to .\\ssa.html 通过浏览器打开生成的 ssa.html 文件，我们可以看到 SSA 的初始阶段、优化阶段和最终阶段的代码片段。 ssa.html 文件示例 我们直接看最终的结果，来看看我们前面的分析正确与否： ssa 最终结果 可以看到这一行：00003 (**+11**) MOVB $3, main.d(SB)，那其实就是直接 d = 3。 机器码生成 在 SSA 阶段，编译器先执行与特定指令集无关的优化，再执行与特定指令集有关的优化，并最终生成与特定指令集有关的指令和寄存器分配方式。如 ssa/_gen/genericOps.go 中包含了与特定指令集无关的 Op 操作，在 ssa/_gen/AMD64Ops.go 中包含了和 AMD64 指令集相关的 Op 操作。 机器码生成阶段是编译器的机器依赖阶段，主要过程如下： Lowering 过程：这个过程将通用的 SSA 形式转换为特定于目标机器的变体。这包括将通用操作符替换为针对特定硬件优化的操作。 代码优化：在机器特定的形式上执行最终优化，进一步提高代码效率。 生成机器指令：将 Go 函数转换为 obj.Prog 指令序列。 汇编和输出：这些指令由 cmd/internal/obj 模块的汇编器处理，转换为机器代码，并输出最终的目标文件。 Go 为我们了解 Go 语言程序的编译和链接过程提供了一个非常好用的命令： go build -n 其中 -n 表示只输出编译过程中将要执行的 shell 命令，但不执行。 以下面程序为例： package mainimport (\tfmt\tgithub.com/spf13/cast)func main() i := cast.ToInt(1)\tfmt.Println(i) 这个程序引入了标准库 fmt 以及第三方库 github.com/spf13/cast。 在工程目录下执行： go build -n -o main 可以看到输出： mkdir -p $WORK/b001/cat $WORK/b001/importcfg.link EOF # internalpackagefile go-compilation=/Users/wangjiahan/Library/Caches/go-build/48/48745ff5ef7f8945297b5894ec377f47e246d94739e0b8f00e86b6d58879e71d-dpackagefile fmt=/Users/wangjiahan/Library/Caches/go-build/10/10ab74ff0df27a2f4bdbe7651290f13ad466f3df63e11241e07ccd21c169b206-dpackagefile github.com/spf13/cast=/Users/wangjiahan/Library/Caches/go-build/77/77eed0b7028cfc4c90d78d6670325d982325399573dff9d7f82ffbf76e4559e8-d...packagefile net/url=/Users/wangjiahan/Library/Caches/go-build/72/72d0ef9b8f99a52bf1de760bb2f630998d6bb66a3d2a3fa66bd66f4efddfbc71-dmodinfo 0w\\xaf\\f\\x92t\\b\\x02A\\xe1\\xc1\\a\\xe6\\xd6\\x18\\xe6path\\tgo-compilation mod\\tgo-compilation\\t(devel)\\t dep\\tgithub.com/spf13/cast\\tv1.6.0\\th1:GEiTHELF+vaR5dhz3VqZfFSzZjYbgeKDpBxQVS4GYJ0= build\\t-buildmode=exe build\\t-compiler=gc build\\tCGO_ENABLED=1 build\\tCGO_CFLAGS= build\\tCGO_CPPFLAGS= build\\tCGO_CXXFLAGS= build\\tCGO_LDFLAGS= build\\tGOARCH=arm64 build\\tGOOS=darwin \\xf92C1\\x86\\x18 r\\x00\\x82B\\x10A\\x16\\xd8\\xf2EOFmkdir -p $WORK/b001/exe/cd ./opt/homebrew/opt/go/libexec/pkg/tool/darwin_arm64/link -o $WORK/b001/exe/a.out -importcfg $WORK/b001/importcfg.link -buildmode=pie -buildid=FDJiS-4glijTlqBbjVbe/UWsngURatTblImv3DE6-/OjO-hZGekrr-XpHFs_zA/FDJiS-4glijTlqBbjVbe -extld=cc /Users/wangjiahan/Library/Caches/go-build/48/48745ff5ef7f8945297b5894ec377f47e246d94739e0b8f00e86b6d58879e71d-d/opt/homebrew/opt/go/libexec/pkg/tool/darwin_arm64/buildid -w $WORK/b001/exe/a.out # internalmv $WORK/b001/exe/a.out main 这里建议你先尝试自行分析一下这个编译过程，再继续往下阅读。 经过分析，上述过程可以分为以下 8 个步骤： 创建工作目录：mkdir -p $WORK/b001/ 创建一个临时工作目录，用于存放编译过程中的临时文件。 生成导入配置文件：cat $WORK/b001/importcfg.link 'EOF' 命令开始创建一个名为 importcfg.link 的文件，这个文件包含了编译过程中需要的包文件路径。 写入包文件路径：接下来的多行内容是对 importcfg.link 文件的填充，指定了各个依赖包的存储位置。 结束文件写入：EOF 标志着 importcfg.link 文件内容的结束。 创建可执行文件目录：mkdir -p $WORK/b001/exe/ 创建一个目录，用于存放最终的可执行文件。 编译链接：/opt/homebrew/opt/go/libexec/pkg/tool/darwin_arm64/link -o $WORK/b001/exe/a.out ... 这一步是编译链接的核心，它使用 Go 的链接工具，根据之前生成的 importcfg.link 文件，将代码编译成可执行文件。 更新构建 ID：/opt/homebrew/opt/go/libexec/pkg/tool/darwin_arm64/buildid -w $WORK/b001/exe/a.out 这一步更新了可执行文件的构建 ID。 移动可执行文件：mv $WORK/b001/exe/a.out main 将编译好的可执行文件移动到当前目录，并重命名为 main。 如下图所示： Go语言编译和链接过程 参考资料 Go1.21 官方文档 《Go 语言底层原理剖析》 《Go 语言设计与实现》 Go: Overview of the Compiler 维基百科 - AST 维基百科 - SSA Go 机制：逃逸分析学习笔记 ChatGPT-4 以上便是 Go 语言在 1.21.0 这个版本下编译过程的整个过程，笔者会在阅读完《用 Go 语言自制解释器》和《用 Go 语言自制编译器》这两本书后，若有对编译原理有更深入的体会和感悟，再回过来对本文的内容进行勘误和进一步提炼。","tags":["go","编译原理"],"categories":["go"]},{"title":"Kafka 顺序消息实现","path":"/2023/11/23/kafka-ordered-msg/","content":"版本说明 本文所有的讨论均在如下版本进行，其他版本可能会有所不同。 Kafka: 3.6.0 Pulsar: 2.9.0 RabbitMQ 3.7.8 RocketMQ 5.0 Go1.21 github.com/segmentio/kafka-go v0.4.45 结论先行 Kafka 只能保证单一分区内的顺序消息，无法保证多分区间的顺序消息。具体来说，要在 Kafka 完全实现顺序消息，至少需要保证以下几个条件： 同一生产者生产消息； 同步发送消息到 Kafka broker； 所有消息发布到同一个分区； 同一消费者同步按照顺序消费消息。 而要满足第 3 点，常用的有 2 种思路： 固定消息的 key，生产端采用 key hash 的方式写入 broker； 自定义分区策略，要保证顺序的消息都写入到指定的分区。 消息队列中的顺序消息如何实现 顺序消息定义 生产端发送出来的消息的顺序和消费端接收到消息的顺序是一样的。 消息存储结构 一般来说，消息队列都是基于顺序存储结构来存储数据的，不需要 B 树、B+ 树等复杂数据结构，利用文件的顺序读写，性能也很高。所以理想情况下，生产者按顺序发送消息，broker 会按顺序存储消息，消费者再按顺序消费消息，那么天然就实现了我们要的顺序消息了，如下： 消息队列顺序存储结构 基本条件 但是一般情况下，消息队列为了支持更高的并发和吞吐，大多数都有分区（partition）和消费者组（consumer group）机制，而为了高可用，一般也会有副本（replica）机制，所以情况就复杂得多了，如下面几个例子，就会导致消息失序： 多个生产者同时发送消息，那么到达 broker 的时间也是不确定的，所以 broker 就无法保证落盘的顺序性了； 单个生产者，但是采用异步发送，因为异步线程是并发执行的，由 CPU 进行调度，且有可能会因为发送失败而重试，所以也无法保证消息可以按照顺序到达 broker，同理，消费者异步处理消息，也无法保证顺序性； 一个 topic 有多个分区，那么即使是同一个生产者，由于分区策略，消息可能会被分发到多个分区中，消费者也就无法保证顺序性了。 所以到这里，我们可以总结出实现顺序消息，至少需要满足以下 3 点： 单一生产者同步发送； 单一分区； 单一消费者同步消费； 第 1、3 点比较简单，Kafka 通过分区和 offset 的方式保证了消息的顺序。每个分区都是一个有序的、不可变的消息序列，每个消息在分区中都有一个唯一的序数标识，称为 offset。生产者在发送消息到分区时，Kafka 会自动为消息分配一个 offset。消费者在读取消息时，会按照 offset 的顺序来读取，从而保证了消息的顺序。 下面我们主要来谈一谈第 2 点。 Kafka 顺序消息的实现 写入消息的过程 配置生产者：首先，你需要配置 Kafka 生产者。这包括指定 Kafka 集群的地址和端口，以及其他相关配置项，如消息序列化器、分区策略等。 创建生产者实例：在应用程序中，你需要创建一个 Kafka 生产者的实例。这个实例将用于与 Kafka 集群进行通信。 序列化消息：在将消息发送到 Kafka 集群之前，你需要将消息进行序列化。Kafka 使用字节数组来表示消息的内容，因此你需要将消息对象序列化为字节数组。这通常涉及将消息对象转换为 JSON、Avro、Protobuf 等格式。 选择分区：Kafka 的主题（topic）被分为多个分区（partition），每个分区都是有序且持久化的消息日志。当你发送消息时，你可以选择将消息发送到特定的分区，或者让 Kafka 根据分区策略自动选择分区。 发送消息：一旦消息被序列化并选择了目标分区，你可以使用 Kafka 生产者的 send() 方法将消息发送到 Kafka 集群。发送消息时，生产者会将消息发送到对应分区的 leader 副本。 异步发送：Kafka 生产者通常使用异步方式发送消息，这样可以提高吞吐量。生产者将消息添加到一个发送缓冲区（send buffer）中，并在后台线程中批量发送消息到 Kafka 集群。 消息持久化：一旦消息被发送到 Kafka 集群的 leader 副本，它将被持久化并复制到其他副本，以确保数据的高可靠性和冗余性。只有当消息被成功写入到指定数量的副本后，生产者才会收到确认（acknowledgement）。 错误处理和重试：如果发送消息时发生错误，生产者可以根据配置进行错误处理和重试。你可以设置重试次数、重试间隔等参数来控制重试行为。 Kafka 生产者组件 -《Kafka权威指南第2版》 实现单一分区 再 Kafka 中，我们要实现将消息写入到同一个分区，有 3 种思路： 配置 num.partitions=1 或者创建 topic 的时候指定只有 1 个分区，但这会显著降低 Kafka 的吞吐量。 固定消息的 key，然后采用 key hash 的分区策略，这样就可以让所有消息都被分到同一个分区中。 实现并指定自定义分区策略，可以根据业务需求，将需要顺序消费的消息都分到固定一个分区中。 // 如下例子，所有使用same-key作为key的消息都会被发送到同一个PartitionProducerRecordString, String record = new ProducerRecordString, String(topic, same-key, message);producer.send(record); 重平衡带来的问题 如果采用上述的第 2 种思路：固定消息 key，依靠 key hash 分区策略，实现单一分区。在我们只有 1 个消费者的情况下是没有问题的，但是如果我们使用的是消费者组，那么，在发生重平衡操作的时候，就可能会有问题了。 Kafka 的重平衡（Rebalance）是指 Kafka 消费者组（Consumer Group）中的消费者实例对分区的重新分配。这个过程主要发生在以下几种情况： 消费者组中新的消费者加入。 消费者组中的消费者离开或者挂掉。 订阅的 Topic 的分区数发生变化。 消费者调用了 #unsubscribe() 或者 #subscribe() 方法。 重平衡的过程主要包括以下几个步骤： Revoke：首先，Kafka 会撤销消费者组中所有消费者当前持有的分区。 Assignment：然后，Kafka 会重新计算分区的分配情况，然后将分区分配给消费者。 Resume：最后，消费者会开始消费新分配到的分区。 重平衡的目的是为了保证消费者组中的消费者能够公平地消费 Topic 的分区。通过重平衡，Kafka 可以在消费者的数量发生变化时，动态地调整消费者对分区的分配，从而实现负载均衡。 然而，当发生重平衡时，分区可能会被重新分配给不同的消费者，这可能会影响消息的消费顺序。 举个例子： 假设消费者 A 正在消费分区 P 的消息，它已经消费了消息 1，消息 2，正在处理消息 3。 此时，发生了重平衡，分区 P 被重新分配给了消费者 B。 消费者 B 开始消费分区 P，它会从上一次提交的偏移量（offset）开始消费。假设消费者 A 在处理消息 3 时发生了故障，没有提交偏移量，那么消费者 B 会从消息 3 开始消费。 这样，消息 3 可能会被消费两次，而且如果消费者 B 处理消息 3 的速度快于消费者 A，那么消息 3 可能会在消息 2 之后被处理，这就打破了消息的顺序性。 重平衡导致消息失序 再举个例子： topic-A 本来只有 3 个分区，按照 key hash，key 为 same-key 的消息应该都发到 第 2 个分区； 但是后来 topic-A 变成了 4 个分区，按照 key hash，key 为 same-key 的消息可能就被发到第 3 个分区了； 这就无法做到单一分区，可能会导致消息失序。 当然这个例子不是由重平衡直接引起的，但是这种情况也是有可能导致消息失序的。 缓解重平衡的问题 避免动态改变分区数：在需要严格保持消息顺序的场景下，应避免动态地改变分区数。这意味着在设计 Kafka 主题时，应提前规划好所需的分区数，以避免日后需要进行更改。 使用单个分区：对于严格顺序要求的场景，可以考虑使用单分区主题。虽然这会限制吞吐量和并发性，但可以保证消息的全局顺序。 使用其他策略保持顺序：在某些情况下，可以通过在应用层实现逻辑来保持顺序，比如在消息中包含顺序号或时间戳，并在消费时根据这些信息重建正确的顺序。 使用静态成员功能：它允许消费者在断开和重新连接时保持其消费者组内的身份，这可以减少因短暂的网络问题或消费者重启导致的不必要的重平衡。 上面这些措施，只能减少重平衡带来的问题，并无法根除，如果非要实现严格意义上的顺序消息，要么在消息中加入时间戳等标记，在业务层保证顺序消费，要么就只能采用 单一生产者同步发送 + 单一分区 +单一消费者同步消费 这种模式了。 静态成员功能 Kafka 2.3.0 版本引入了一项新功能：静态成员（Static Membership）。这个功能主要是为了减少由于消费者重平衡（rebalance）引起的开销和延迟。在传统的 Kafka 消费者组中，当新的消费者加入或离开消费者组时，会触发重平衡。这个过程可能会导致消息的处理延迟，并且在高吞吐量的场景下可能会对性能造成影响。静态成员功能旨在缓解这些问题。以下是它的一些关键点： 静态成员的工作原理： 静态成员标识：消费者在加入消费者组时可以提供一个静态成员标识（Static Member ID）。这允许 Kafka Broker 识别特定的消费者实例，而不是仅仅依赖于消费者组内的动态分配。 重平衡优化：当使用静态成员功能时，如果一个已知的消费者由于某种原因（如网络问题）短暂断开后重新连接，Kafka 不会立即触发重平衡。相反，Kafka 会等待一个预设的超时期限（session.timeout.ms），在此期间如果消费者重新连接，它将保留原来的分区分配。 减少重平衡次数：这大大减少了由于消费者崩溃和恢复、网络问题或维护操作引起的不必要的重平衡次数。 使用静态成员的优点： 提高稳定性：减少重平衡可以提高消费者组的整体稳定性，尤其是在大型消费者组和高吞吐量的情况下。 减少延迟：由于减少了重平衡的次数，可以减少因重平衡导致的消息处理延迟。 持久的消费者分区分配：这使得消费者在分区分配上更加持久，有助于更好地管理和优化消息的消费。 如何使用： 要使用静态成员功能，需要在 Kafka 消费者的配置中设置 group.instance.id。这个 ID 应该是唯一的，并且在消费者重启或重新连接时保持不变。同时，还需要配置 session.timeout.ms，以决定在触发重平衡之前消费者可以离线多长时间。 注意事项： 虽然静态成员功能可以减少重平衡的发生，但它不会完全消除重平衡。在消费者组成员的长期变化（如新消费者的加入或永久离开）时，仍然会发生重平衡。 需要合理设置 session.timeout.ms，以避免消费者由于短暂的网络问题或其他原因的断开而过早触发重平衡。 静态成员功能在处理大规模 Kafka 应用时尤其有用，它提供了一种机制来优化消费者组的性能和稳定性。 幂等性 Kafka 0.11 版本后提供了幂等性生产者，这意味着即使生产者因为某些错误重试发送相同的消息，这些消息也只会被记录一次。这是通过给每一批发送到 Kafka 的消息分配一个序列号实现的，broker 使用这个序列号来删除重复发送的消息。使用幂等性生产者，可以减少重复消息的风险，这意味着即使在网络重试等情况下，消息的顺序也能得到更好的保证。因为重复消息不会被多次记录，所以不会破坏已有消息的顺序。 其他常见消息队列顺序消息的实现 Pulsar Pulsar 和 Kafka 一样，都是通过生产端按 Key Hash 的方案将数据写入到同一个分区。 RabbitMQ RabbitMQ 在生产时没有生产分区分配的过程。它是通过 Exchange 和 Route Key 机制来实现顺序消息的。Exchange 会根据设置好的 Route Key 将数据路由到不同的 Queue 中存储。此时 Route Key 的作用和 Kafka 的消息的 Key 是一样的。 RocketMQ RocektMQ 支持消息组（MessageGroup）的概念。在生产端指定消息组，则同一个消息组的消息就会被发送到同一个分区中。此时这个消息组起到的作用和 Kakfa 的消息的 Key 是一样的。 实战 Kafka 实现顺序消息 代码仓库：https://github.com/hedon954/kafka-go-examples/tree/master/orderedmsg 下面我们来写一写实战用例，更加直观地感受一下 Kafka 顺序消息的实现细节。 首先我们在集群上创建一个 topic ordered-msg-topic，分区为 3 个，运行以下命令： /opt/kafka-3.6.0/bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic ordered-msg-topic --partitions 3 --replication-factor 1 搭建 Kafka 集群可以看这两篇：Kafka 集群搭建(Zookeeper)、Kafka 集群搭建(KRaft)。 单生产者单消费者 正常情况下，使用单一生产者同步发送和单一消费者同步发送，只要我们保证 key 是固定的，则所有消息都会写到同一个分区，是可以实现顺序消息的。 代码目录如下： ├─config│ config.go # 常量定义├─consumer│ consumer.go # 消费者└─producer producer.go # 生产者 首先我们先定义一些常量： import github.com/segmentio/kafka-govar (\tTopic = ordered-msg-topic\tBrokers = []stringkafka1.com:9092, kafka2.com:9092, kafka3.com:9092\tAddr = kafka.TCP(Brokers...)\tGroupId = ordered-msg-group\tMessageKey = []byte(message-key)) 我们先实现生产者端，主要是不断往 ordered-msg-topic 中写入数据： package mainimport (\tcontext\tfmt\ttime\tkafka-go-examples/orderedmsg/config\tgithub.com/segmentio/kafka-go)func NewProducer() *kafka.Writer return kafka.Writer Addr: config.Addr, Topic: config.Topic, Balancer: kafka.Hash, // 哈希分区\tfunc NewMessages(count int) []kafka.Message res := make([]kafka.Message, count)\tfor i := 0; i count; i++ res[i] = kafka.Message Key: config.MessageKey, Value: []byte(fmt.Sprintf(msg-%d, i+1)), return resfunc main() producer := NewProducer()\tmessages := NewMessages(100)\tif err := producer.WriteMessages(context.Background(), messages...); err != nil panic(err) _ = producer.Close() 我们再来实现消费者，目前我们就启动 1 个消费者： package mainimport (\tcontext\tfmt\ttime\tkafka-go-examples/orderedmsg/config\tgithub.com/segmentio/kafka-go)type Consumer struct Id string\t*kafka.Reader// NewConsumer 创建一个消费者，它属于 config.GroupId 这个消费者组func NewConsumer(id string) *Consumer c := Consumer Id: id, Reader: kafka.NewReader(kafka.ReaderConfig Brokers: config.Brokers, GroupID: config.GroupId, Topic: config.Topic, Dialer: kafka.Dialer ClientID: id, , ), return c// Read 读取消息，intervalMs 用来控制消费者的消费速度func (c *Consumer) Read(intervalMs int) fmt.Printf(%s start read , c.Id)\tfor msg, err := c.ReadMessage(context.Background()) if err != nil fmt.Printf(%s read msg err: %v , c.Id, err) return // 模拟消费速度 time.Sleep(time.Millisecond * time.Duration(intervalMs)) fmt.Printf(%s read msg: %s, time: %s , c.Id, string(msg.Value), time.Now().Format(03-04-05))\tfunc main() c1 := NewConsumer(consumer-1)\tc1.Read(500) 启动生产者生产消息，然后启动消费者，观察控制台，不难看出这种情况下就是顺序消费： consumer-1 read msg: msg-10, time: 04:29:10consumer-1 read msg: msg-11, time: 04:29:11consumer-1 read msg: msg-12, time: 04:29:12consumer-1 read msg: msg-13, time: 04:29:13consumer-1 read msg: msg-14, time: 04:29:14consumer-1 read msg: msg-15, time: 04:29:15consumer-1 read msg: msg-16, time: 04:29:16 重平衡带来的问题 我们先重建 topic，清楚掉之前的数据： /opt/kafka-3.6.0/bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic ordered-msg-topic /opt/kafka-3.6.0/bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic ordered-msg-topic --partitions 3 --replication-factor 1 下面我们来采用消费者组的形式消费消息，在这期间，我们不断往消费者组中新增消费者，使其发生重平衡，我们来观察下消息的消费情况。 修改消费者端的 main()： func main() // 先启动 c1\tc1 := NewConsumer(consumer-1)\tgo func() c1.Read(500)\t()\t// 5 秒后启动 c2\ttime.Sleep(5 * time.Second)\tgo func() c2 := NewConsumer(consumer-2) c2.Read(300)\t()\t// 再 10 秒后启动 c3 和 c4\ttime.Sleep(10 * time.Second)\tgo func() c3 := NewConsumer(consumer-3) c3.Read(100)\t()\tgo func() c4 := NewConsumer(consumer-4) c4.Read(100)\t()\tselect 先启动生产者重新生产数据，然后再启动消费者消费数据，观察控制台： consumer-1 start readconsumer-1 read msg: msg-1, time: 04:44:28consumer-1 read msg: msg-2, time: 04:44:28consumer-1 read msg: msg-3, time: 04:44:29 # consumer-1 按顺序消费consumer-2 start read # consumer-2 进来consumer-1 read msg: msg-4, time: 04:44:30consumer-1 read msg: msg-5, time: 04:44:30consumer-1 read msg: msg-6, time: 04:44:31 # 这里相差了 6s，就是在进行重平衡consumer-2 read msg: msg-7, time: 04:44:37 # 重平衡后发现原来的分区给 consumer-2 消费了consumer-1 read msg: msg-7, time: 04:44:37 # 这里发生了重复消费consumer-2 read msg: msg-8, time: 04:44:37consumer-2 read msg: msg-9, time: 04:44:37consumer-2 read msg: msg-10, time: 04:44:38consumer-2 read msg: msg-11, time: 04:44:38consumer-2 read msg: msg-12, time: 04:44:38consumer-2 read msg: msg-13, time: 04:44:39consumer-2 read msg: msg-14, time: 04:44:39consumer-2 read msg: msg-15, time: 04:44:39 # consumer-2 按顺序消息consumer-4 start read # consumer-3 和 consumer-4 进来consumer-3 start readconsumer-2 read msg: msg-16, time: 04:44:40consumer-4 read msg: msg-17, time: 04:44:46 # 这里发生重平衡consumer-4 read msg: msg-18, time: 04:44:46 # 重平衡后由 consumer-4 负责该分区consumer-2 read msg: msg-17, time: 04:44:46 # 这里由于 2 的速度比 4 慢很多，所以就乱序了，还重复消费consumer-4 read msg: msg-19, time: 04:44:46consumer-4 read msg: msg-20, time: 04:44:46# ... 总结 当我们采用消费者组的时候，由于重平衡机制的存在，单纯从 Kafka 的角度来说是无法完全实现顺序消息的，只能通过静态成员功能、避免分区数量变化和减少消费者组成员数量变化等方式来尽可能减少重平衡的发生，进而尽可能维持消息的顺序性。 参考 极客时间 - 深入拆解消息队列 47 讲（许文强） 《Kafka 权威指南（第 2 版）》 Pulsar 官方文档-分区 topic-顺序保证 RocketMQ 官方文档-功能特性-顺序消息 RabbitMQ 官方文档","tags":["kafka","中间件","消息队列"],"categories":["kafka"]},{"title":"Kafka 集群部署（KRaft）","path":"/2023/11/22/kafka-kraft-deploy/","content":"版本说明 Ubuntu 18.04.6 Kafka 3.6.0 JDK8 集群配置 操作系统 ip 域名 Kafka Broker 端口 Kafka Controller 端口 Ubuntu 18.04.6 192.168.50.131 kafka1.com 9092 9093 Ubuntu 18.04.6 192.168.50.132 kafka2.com 9092 9093 Ubuntu 18.04.6 192.168.50.133 kafka3.com 9092 9093 安装 vim, curl sudo apt updatesudo apt install vimsudo apt install curl 配置静态 ip 和 hosts 为了使用域名，更加方便的进行配置，这里将虚拟机的 DHCP 改成了静态分配 IP，所以需要手动设置一下每台机器 IP 地址，这里以 192.168.50.131 为例。 找到网络接口名称，运行以下命令： ip addr 查找以 ens 或 eth 开头的接口名称。例如，ens33 或 eth0。 hedon@ubuntu:~$ ip addr1: lo: LOOPBACK,UP,LOWER_UP mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: BROADCAST,MULTICAST,UP,LOWER_UP mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:82:9e:69 brd ff:ff:ff:ff:ff:ff inet 192.168.50.133/24 brd 192.168.50.255 scope global dynamic noprefixroute ens33 valid_lft 1644sec preferred_lft 1644sec inet6 fe80::c367:c7cc:3ad4:23b3/64 scope link valid_lft forever preferred_lft forever 可以找到 ens33，其中 inet 192.168.50.133/24 表示 IP 地址为 192.168.50.133，子网掩码为 /24（等于 255.255.255.0）。 这个 IP 地址是 DHCP 动态分配的，说明宿主机分配给虚拟机的 IP 范围就在 192.168.50.xxx，所以我们会将静态 IP 配置在这个范围内。 获取网关地址 ip route | grep default 输出： hedon@ubuntu:~$ ip route | grep defaultdefault via 192.168.50.2 dev ens33 proto dhcp metric 100 说明默认网关是 192.168.50.2， 编辑 /etc/network/interfaces 文件，配置静态 IP 地址，内容如下： auto ens33iface ens33 inet static address 192.168.50.131 netmask 255.255.255.0 gateway 192.168.50.2 dns-nameservers 8.8.8.8 8.8.4.4 重启 su reboot 再次查看 ip 地址 ip addr 有以下输出便说明静态 IP 配置成功了。 1: lo: LOOPBACK,UP,LOWER_UP mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: BROADCAST,MULTICAST,UP,LOWER_UP mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:82:9e:69 brd ff:ff:ff:ff:ff:ff inet 192.168.50.131/24 brd 192.168.50.255 scope global ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe82:9e69/64 scope link valid_lft forever preferred_lft forever 配置域名 sudo vim /etc/hosts 追加内容如下： 192.168.50.131 kafka1.com192.168.50.132 kafka2.com192.168.50.133 kafka3.com ping 一下 hedon@ubuntu:~$ ping kafka1.comPING kafka1.com (192.168.50.131) 56(84) bytes of data.64 bytes from kafka1.com (192.168.50.131): icmp_seq=1 ttl=64 time=0.024 ms64 bytes from kafka1.com (192.168.50.131): icmp_seq=2 ttl=64 time=0.021 ms64 bytes from kafka1.com (192.168.50.131): icmp_seq=3 ttl=64 time=0.029 ms^C--- kafka1.com ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2029msrtt min/avg/max/mdev = 0.021/0.024/0.029/0.006 ms ping 一下百度，看看能不能访问外网 hedon@ubuntu:~$ ping baidu.comping: baidu.com: Name or service not known 如果这里可以访问，则直接跳过进入下一步，不可以的话，需要配置一下域名解析系统。 配置域名解析系统 Ubuntu 系统使用 systemd-resolved 服务来管理 DNS，你可以在 /etc/systemd/resolved.conf 文件中进行 DNS 配置。 sudo vim /etc/systemd/resolved.conf 取消或添加 DNS 的注释，并修改为： [Resolve]DNS=8.8.8.8 8.8.4.4 重启启动 systemd-resolved： sudo systemctl restart systemd-resolved 再尝试 ping 一下百度： hedon@ubuntu:~$ ping www.baidu.comPING www.a.shifen.com (153.3.238.110) 56(84) bytes of data.64 bytes from 153.3.238.110 (153.3.238.110): icmp_seq=1 ttl=128 time=15.9 ms64 bytes from 153.3.238.110 (153.3.238.110): icmp_seq=2 ttl=128 time=15.9 ms64 bytes from 153.3.238.110 (153.3.238.110): icmp_seq=3 ttl=128 time=16.1 ms64 bytes from 153.3.238.110 (153.3.238.110): icmp_seq=4 ttl=128 time=15.3 ms^C--- www.a.shifen.com ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 14104msrtt min/avg/max/mdev = 15.368/15.850/16.145/0.291 ms 补充说明：/etc/network/interfaces 文件的配置 这是一个用于配置 Linux 系统上网络接口的文件。在这个示例中，我们为名为ens33 的网络接口配置了静态 IP地址和相关的网络设置。下面是各行的解释：auto ens33: 这一行表示在系统启动时自动激活ens33 网络接口。auto关键字后面跟着接口名称。iface ens33 inet static: 这一行定义了ens33 网络接口的配置。iface关键字后面跟着接口名称，inet 表示我们正在配置 IPv4地址，static 表示我们要为接口分配一个静态 IP地址（而不是通过 DHCP 获得）。address 192.168.50.131: 这一行设置了网络接口的静态IP 地址。在这个例子中，我们为 ens33 接口分配了192.168.50.131 IP 地址。IP 地址是 Internet协议（IP）用于在网络中唯一标识设备的数字标签。每个连接到网络的设备都需要一个唯一的IP 地址，以便其他设备可以找到并与之通信。IP 地址通常分为两种版本：IPv4和 IPv6。在此示例中，我们使用了一个 IPv4 地址。netmask 255.255.255.0:这一行定义了子网掩码。在这个例子中，子网掩码是255.255.255.0，表示前三个字节（24位）是网络地址，最后一个字节（8 位）是主机地址。子网掩码用于划分 IP 地址的网络部分和主机部分。子网掩码与 IP地址进行按位与操作，从而得到网络地址。这有助于确定哪些 IP地址属于同一子网，以便正确地将数据包路由到目的地。子网划分有助于组织网络、提高安全性和管理性。gateway 192.168.50.2:这一行设置了默认网关。在这个例子中，我们将默认网关设置为192.168.50.2。默认网关是用于将数据包发送到其他网络的路由器或设备的IP 地址。网关是一个充当网络中数据包传输的中继点的设备，通常是一个路由器。当一个设备需要将数据包发送到不同子网的另一个设备时，它会将数据包发送到网关。网关负责将数据包路由到正确的目的地。默认网关是设备用于将数据包发送到其他网络的首选网关。dns-nameservers 8.8.8.8 8.8.4.4: 这一行指定了 DNS服务器的 IP 地址。在这个例子中，我们使用了谷歌的公共 DNS 服务器8.8.8.8 和 8.8.4.4。DNS服务器用于将主机名解析为 IP 地址。域名系统（DNS）是将人类可读的域名（例如 www.baidu.com）IP地址的系统。DNS服务器是负责执行此解析过程的服务器。当您在浏览器中输入一个网址时，计算机会向DNS 服务器查询该域名对应的 IP 地址，然后将请求发送到该 IP地址以获取网页内容。配置文件中的这些设置将在系统启动时生效。要立即应用更改，您可以使用以下命令重启网络服务：sudo systemctl restart networking 安装 jdk sudo apt updatesudo apt install openjdk-8-jdk 验证 java8 是否已经安装成功： java -version 有以下类似输出的话则表明安装成功： openjdk version 1.8.0_362OpenJDK Runtime Environment (build 1.8.0_362-8u372-ga~us1-0ubuntu1~18.04-b09)OpenJDK 64-Bit Server VM (build 25.362-b09, mixed mode) 安装 Kafka 下载并解压 Kafka wget https://archive.apache.org/dist/kafka/3.6.0/kafka_2.13-3.6.0.tgztar -zxvf kafka_2.13-3.6.0.tgz 将解压缩后的文件夹移动到 /opt 目录中： sudo mv kafka_2.13-3.6.0 /opt/kafka-3.6.0 使用 Kafka 提供的脚本生成一个 ClusterID export KAFKA_CLUSTER_ID=$(/opt/kafka-3.6.0/bin/kafka-storage.sh random-uuid) 输出 ClusterID hedon@ubuntu:/opt/kafka-3.6.0$ echo $KAFKA_CLUSTER_IDXiMRcbJ-QEO694L7sfDdBQ 在其他节点上将 KAFKA_CLUSTER_ID 设置为上面的值： export KAFKA_CLUSTER_ID=XiMRcbJ-QEO694L7sfDdBQ 备份配置文件，注意这里的配置文件是 config/kraft/server.properties，在 config 目录下的 kraft 目录中： cp /opt/kafka-3.6.0/config/kraft/server.properties /opt/kafka-3.6.0/config/kraft/server.properties.bak 修改配置 vim /opt/kafka-3.6.0/config/kraft/server.properties 主要修改内容如下： # 节点 ID，分别为 1，2，3node.id=1# 日志目录log.dirs=/opt/kafka-3.6.0/kafka-combined-logs# 可以成为控制器的节点和它们的端口controller.quorum.voters=1@kafka1.com:9093,2@kafka2.com:9093,3@kafka3.com:9093# 定义 Kafka Broker 如何向外部公布它的地址。# 这是 Kafka Broker 通知 Producer 和 Consumer 如何连接到自己的方式。# 例如，如果你设置 advertised.listeners=PLAINTEXT://my.public.ip:9092，# 那么 Kafka Broker 将告诉 Producer 和 Consumer 它的公共 IP 地址是 my.public.ip，并且它在 9092 端口上监听连接。# 这里我们需要在 3 个节点分别设置对应的地址advertised.listeners=PLAINTEXT://kafka1.com:9092 格式化日志目录 /opt/kafka-3.6.0/bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c /opt/kafka-3.6.0/config/kraft/server.properties 输出： Formatting /opt/kafka-3.6.0/kraft-combined-logs with metadata.version 3.6-IV2. 三个节点都启动 Kafka /opt/kafka-3.6.0/bin/kafka-server-start.sh -daemon /opt/kafka-3.6.0/config/kraft/server.properties 选择任意一个节点创建一个新 topic /opt/kafka-3.6.0/bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic test --replication-factor 1 --partitions=2 输出： Created topic test. 在其他节点获取 test 这个 topic 的信息 /opt/kafka-3.6.0/bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic test 可以看到关于 test 这个 topic 的信息是可以获取到的，说明集群之前信息是互通的，集群搭建完毕。 Topic: test\tTopicId: svJClTUpSFa9Z6FWDvkARg\tPartitionCount: 2\tReplicationFactor: 1\tConfigs: segment.bytes=1073741824\tTopic: test\tPartition: 0\tLeader: 2\tReplicas: 2\tIsr: 2\tTopic: test\tPartition: 1\tLeader: 3\tReplicas: 3\tIsr: 3 随便选择一个节点，往 test 里面写入数据： /opt/kafka-3.6.0/bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test 输入数据后按回车即发送一条数据，可以随时按 Ctrl + C 退出： hedon@ubuntu:~/Downloads$ /opt/kafka-3.6.0/bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic testmsg1msg2msg 3^ 随便选择一个节点，启动消费者消费 topic 中的数据： /opt/kafka-3.6.0/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning 输出： hedon@ubuntu:/opt/kafka-3.6.0$ /opt/kafka-3.6.0/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginningmsg1msg2msg 3^CProcessed a total of 3 messages 至此，Kafka 的 KRaft 版本集群就部署完毕了！ 补充说明 - KRaft 配置文件 下面是 Kafka KRaft 版本配置文件每个配置项的解释：配置项说明process.rolesKafka 服务器的角色，设置此项将 Kafka 置于 KRaft 模式。可能的值包括\"broker\" 和 \"controller\"。node.id与此实例关联的节点 ID。controller.quorum.voters控制器选举的投票节点，格式为 node-id@host:port。listeners服务器监听的地址，格式为listener_name://host_name:port。inter.broker.listener.name用于 broker 之间通信的监听器名称。advertised.listeners服务器向客户端宣告的监听器名称、主机名和端口。controller.listener.names控制器使用的监听器名称列表。listener.security.protocol.map监听器名称到安全协议的映射。默认情况下，它们是相同的。num.network.threads服务器用于从网络接收请求和向网络发送响应的线程数。num.io.threads服务器用于处理请求（可能包括磁盘 I/O）的线程数。socket.send.buffer.bytes服务器用于发送数据的缓冲区大小。socket.receive.buffer.bytes服务器用于接收数据的缓冲区大小。socket.request.max.bytes服务器接受的请求的最大大小（用于防止内存溢出）。log.dirs用于存储日志文件的目录列表。num.partitions每个主题的默认日志分区数。num.recovery.threads.per.data.dir每个数据目录在启动时用于日志恢复和关闭时用于刷新的线程数。offsets.topic.replication.factor内部主题 \"consumer_offsets\" 和 \"transaction_state\"的复制因子。transaction.state.log.replication.factor事务状态日志的复制因子。transaction.state.log.min.isr事务状态日志的最小同步副本数。log.flush.interval.messages强制将数据刷新到磁盘之前接受的消息数。log.flush.interval.ms消息在日志中停留的最大时间，超过这个时间就会强制刷新到磁盘。log.retention.hours由于年龄而使日志文件有资格被删除的最小年龄。log.retention.bytes基于大小的日志保留策略。log.segment.bytes日志段文件的最大大小。log.retention.check.interval.ms检查日志段是否可以根据保留策略被删除的间隔。请注意，这只是 Kafka 配置的一部分，Kafka 配置的完整列表可以在 Kafka的官方文档中找到。","tags":["kafka","中间件","消息队列","部署"],"categories":["kafka"]},{"title":"Kafka 集群部署","path":"/2023/11/22/kakfa-cluster-deploy/","content":"版本说明 Ubuntu 18.04.6 Zookeeper 3.5.9 Kafka 2.7.0 JDK8 集群配置 操作系统 ip 域名 Zookeeper 端口 Kafka 端口 Ubuntu 18.04.6 192.168.50.131 kafka1.com 2181 9092 Ubuntu 18.04.6 192.168.50.132 kafka2.com 2181 9092 Ubuntu 18.04.6 192.168.50.133 kafka3.com 2181 9092 安装 vim, curl sudo apt updatesudo apt install vimsudo apt install curl 配置静态 ip 和 hosts 为了使用域名，更加方便的进行配置，这里将虚拟机的 DHCP 改成了静态分配 IP，所以需要手动设置一下每台机器 IP 地址，这里以 192.168.50.131 为例。 找到网络接口名称，运行以下命令： ip addr 查找以 ens 或 eth 开头的接口名称。例如，ens33 或 eth0。 hedon@ubuntu:~$ ip addr1: lo: LOOPBACK,UP,LOWER_UP mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: BROADCAST,MULTICAST,UP,LOWER_UP mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:82:9e:69 brd ff:ff:ff:ff:ff:ff inet 192.168.50.133/24 brd 192.168.50.255 scope global dynamic noprefixroute ens33 valid_lft 1644sec preferred_lft 1644sec inet6 fe80::c367:c7cc:3ad4:23b3/64 scope link valid_lft forever preferred_lft forever 可以找到 ens33，其中 inet 192.168.50.133/24 表示 IP 地址为 192.168.50.133，子网掩码为 /24（等于 255.255.255.0）。 这个 IP 地址是 DHCP 动态分配的，说明宿主机分配给虚拟机的 IP 范围就在 192.168.50.xxx，所以我们会将静态 IP 配置在这个范围内。 获取网关地址 ip route | grep default 输出： hedon@ubuntu:~$ ip route | grep defaultdefault via 192.168.50.2 dev ens33 proto dhcp metric 100 说明默认网关是 192.168.50.2， 编辑 /etc/network/interfaces 文件，配置静态 IP 地址，内容如下： auto ens33iface ens33 inet static\taddress 192.168.50.131\tnetmask 255.255.255.0\tgateway 192.168.50.2\tdns-nameservers 8.8.8.8 8.8.4.4 重启 su reboot 再次查看 ip 地址 ip addr 有以下输出便说明静态 IP 配置成功了。 1: lo: LOOPBACK,UP,LOWER_UP mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: BROADCAST,MULTICAST,UP,LOWER_UP mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:82:9e:69 brd ff:ff:ff:ff:ff:ff inet 192.168.50.131/24 brd 192.168.50.255 scope global ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe82:9e69/64 scope link valid_lft forever preferred_lft forever 配置域名 sudo vim /etc/hosts 追加内容如下： 192.168.50.131 kafka1.com192.168.50.132 kafka2.com192.168.50.133 kafka3.com ping 一下 hedon@ubuntu:~$ ping kafka1.comPING kafka1.com (192.168.50.131) 56(84) bytes of data.64 bytes from kafka1.com (192.168.50.131): icmp_seq=1 ttl=64 time=0.024 ms64 bytes from kafka1.com (192.168.50.131): icmp_seq=2 ttl=64 time=0.021 ms64 bytes from kafka1.com (192.168.50.131): icmp_seq=3 ttl=64 time=0.029 ms^C--- kafka1.com ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2029msrtt min/avg/max/mdev = 0.021/0.024/0.029/0.006 ms ping 一下百度，看看能不能访问外网 hedon@ubuntu:~$ ping baidu.comping: baidu.com: Name or service not known 如果这里可以访问，则直接跳过进入下一步，不可以的话，需要配置一下域名解析系统。 配置域名解析系统 sudo vim /etc/resolv.conf 追加下面内容： nameserver 8.8.8.8nameserver 8.8.4.4 再尝试 ping 一下百度： hedon@ubuntu:~$ ping www.baidu.comPING www.a.shifen.com (153.3.238.110) 56(84) bytes of data.64 bytes from 153.3.238.110 (153.3.238.110): icmp_seq=1 ttl=128 time=15.9 ms64 bytes from 153.3.238.110 (153.3.238.110): icmp_seq=2 ttl=128 time=15.9 ms64 bytes from 153.3.238.110 (153.3.238.110): icmp_seq=3 ttl=128 time=16.1 ms64 bytes from 153.3.238.110 (153.3.238.110): icmp_seq=4 ttl=128 time=15.3 ms^C--- www.a.shifen.com ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 14104msrtt min/avg/max/mdev = 15.368/15.850/16.145/0.291 ms 补充说明：/etc/network/interfaces 文件的配置 这是一个用于配置 Linux 系统上网络接口的文件。在这个示例中，我们为名为ens33 的网络接口配置了静态 IP地址和相关的网络设置。下面是各行的解释：auto ens33: 这一行表示在系统启动时自动激活ens33 网络接口。auto关键字后面跟着接口名称。iface ens33 inet static: 这一行定义了ens33 网络接口的配置。iface关键字后面跟着接口名称，inet 表示我们正在配置 IPv4地址，static 表示我们要为接口分配一个静态 IP地址（而不是通过 DHCP 获得）。address 192.168.50.131: 这一行设置了网络接口的静态IP 地址。在这个例子中，我们为 ens33 接口分配了192.168.50.131 IP 地址。IP 地址是 Internet协议（IP）用于在网络中唯一标识设备的数字标签。每个连接到网络的设备都需要一个唯一的IP 地址，以便其他设备可以找到并与之通信。IP 地址通常分为两种版本：IPv4和 IPv6。在此示例中，我们使用了一个 IPv4 地址。netmask 255.255.255.0:这一行定义了子网掩码。在这个例子中，子网掩码是255.255.255.0，表示前三个字节（24位）是网络地址，最后一个字节（8 位）是主机地址。子网掩码用于划分 IP 地址的网络部分和主机部分。子网掩码与 IP地址进行按位与操作，从而得到网络地址。这有助于确定哪些 IP地址属于同一子网，以便正确地将数据包路由到目的地。子网划分有助于组织网络、提高安全性和管理性。gateway 192.168.50.2:这一行设置了默认网关。在这个例子中，我们将默认网关设置为192.168.50.2。默认网关是用于将数据包发送到其他网络的路由器或设备的IP 地址。网关是一个充当网络中数据包传输的中继点的设备，通常是一个路由器。当一个设备需要将数据包发送到不同子网的另一个设备时，它会将数据包发送到网关。网关负责将数据包路由到正确的目的地。默认网关是设备用于将数据包发送到其他网络的首选网关。dns-nameservers 8.8.8.8 8.8.4.4: 这一行指定了 DNS服务器的 IP 地址。在这个例子中，我们使用了谷歌的公共 DNS 服务器8.8.8.8 和 8.8.4.4。DNS服务器用于将主机名解析为 IP 地址。域名系统（DNS）是将人类可读的域名（例如 www.baidu.com）IP地址的系统。DNS服务器是负责执行此解析过程的服务器。当您在浏览器中输入一个网址时，计算机会向DNS 服务器查询该域名对应的 IP 地址，然后将请求发送到该 IP地址以获取网页内容。配置文件中的这些设置将在系统启动时生效。要立即应用更改，您可以使用以下命令重启网络服务：sudo systemctl restart networking 安装 jdk sudo apt updatesudo apt install openjdk-8-jdk 验证 java8 是否已经安装成功： java -version 有以下类似输出的话则表明安装成功： openjdk version 1.8.0_362OpenJDK Runtime Environment (build 1.8.0_362-8u372-ga~us1-0ubuntu1~18.04-b09)OpenJDK 64-Bit Server VM (build 25.362-b09, mixed mode) 安装 zookeeper 在 Ubuntu 上，您可以通过以下步骤安装 Apache Zookeeper 3.5.9： 下载 Apache Zookeeper 3.5.9 的二进制文件。使用以下命令下载并解压缩 Zookeeper： wget https://archive.apache.org/dist/zookeeper/zookeeper-3.5.9/apache-zookeeper-3.5.9-bin.tar.gztar -xzf apache-zookeeper-3.5.9-bin.tar.gz 将解压缩后的文件夹移动到 /opt 目录中： sudo mv apache-zookeeper-3.5.9-bin /opt/zookeeper-3.5.9 在 /opt/zookeeper-3.5.9 目录中创建一个名为 data 的文件夹，用于存储 Zookeeper 的数据： sudo mkdir /opt/zookeeper-3.5.9/data 在 /opt/zookeeper-3.5.9/data 下创建 myid 文件并设置内容为 1，其他两台机器则为 2 和 3： echo 1 | sudo tee /opt/zookeeper-3.5.9/data/myid 复制 Zookeeper 配置文件样本，并将其命名为 zoo.cfg： sudo cp /opt/zookeeper-3.5.9/conf/zoo_sample.cfg /opt/zookeeper-3.5.9/conf/zoo.cfg 使用文本编辑器（例如 vim）编辑 zoo.cfg 文件： sudo vim /opt/zookeeper-3.5.9/conf/zoo.cfg 修改 zoo.cfg 文件： # The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial# synchronization phase can takeinitLimit=10# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# 设置数据存储目录dataDir=/opt/zookeeper-3.5.9/data# the port at which the clients will connectclientPort=2181# 设置集群信息server.1=kafka1.com:2888:3888server.2=kafka2.com:2888:3888server.3=kafka3.com:2888:3888 在 Zookeeper 的配置文件中，server.x=hostname:port1:port2 这种格式的配置项是用来设置 Zookeeper 集群（集群模式下）的。其中，x 是服务器的 ID，hostname 是服务器的主机名或 IP 地址，port1 和 port2 是用于集群间通信的端口。 具体来说： port1（2888）：这是服务器之间用于相互通信的端口。Zookeeper 服务器使用这个端口进行 leader 选举以及同步 follower 和 leader 之间的状态。 port2（3888）：这个端口用于服务器之间的 leader 选举。在 Zookeeper 集群启动或者在 leader 服务器崩溃后，follower 服务器会通过这个端口进行新一轮的 leader 选举。 这两个端口可以根据你的网络配置进行修改，但必须在所有的 Zookeeper 服务器上保持一致。 三个节点都启动 Zookeeper 服务器： /opt/zookeeper/bin/zkServer.sh start 可以连接到 Zookeeper 的端口上（默认是 2181），通过发送四字命令 srvr 来验证 Zookeeper 是否安装正确（部署集群的话需要把所有 Zookeeper 启动）： hedon@ubuntu:/opt/zookeeper-3.5.9$ telnet localhost 2181Trying 127.0.0.1...Connected to localhost.Escape character is ^].srvrZookeeper version: 3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 19:49 GMTLatency min/avg/max: 0/0/0Received: 1Sent: 0Connections: 1Outstanding: 0Zxid: 0x0Mode: standaloneNode count: 5Connection closed by foreign host. 要停止 Zookeeper 服务器，可以使用以下命令： /opt/zookeeper/bin/zkServer.sh stop 安装 Kafka 下载并解压 Kafka wget https://archive.apache.org/dist/kafka/2.7.0/kafka_2.13-2.7.0.tgztar -zxvf kafka_2.13-2.7.0.tgz 将解压缩后的文件夹移动到 /opt 目录中： sudo mv kafka_2.13-2.7.0 /opt/kafka-2.7.0 创建日志目录 sudo mkdir /opt/kafka-2.7.0/kafka-logs 备份 Kafka 默认配置 sudo cp /opt/kafka-2.7.0/config/server.properties /opt/kafka-2.7.0/config/server.properties.bak 修改 Kafka 配置 sudo vim /opt/kafka-2.7.0/config/server.properties 主要是修改下面几个配置： # 集群中每个 broker 的 id 必须唯一，这里分别为 1，2，3broker.id=1# 日志目录log.dirs=/opt/kafka-2.7.0/kafka-logs# 配置 Zookeeperzookeeper.connect=kafka1.com:2181,kafka2.com:2181,kafka3.com:2181# 定义 Kafka Broker 在哪些网络地址上监听连接，下面配置表示在所有的 IP 地址上监听 9092 端口listeners=PLAINTEXT://:9092# 定义 Kafka Broker 如何向外部公布它的地址。这是 Kafka Broker 通知 Producer 和 Consumer 如何连接到自己的方式。例如，如果你设置 advertised.listeners=PLAINTEXT://my.public.ip:9092，那么 Kafka Broker 将告诉 Producer 和 Consumer 它的公共 IP 地址是 my.public.ip，并且它在 9092 端口上监听连接。advertised.listeners=PLAINTEXT://kafka1.com:9092 三个节点都启动 Kafka /opt/kafka-2.7.0/bin/kafka-server-start.sh -daemon /opt/kafka-2.7.0/config/server.properties 选择任意一个节点创建一个新 topic /opt/kafka-2.7.0/bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic test --replication-factor 1 --partitions=2 输出： Created topic test. 在其他节点获取 test 这个 topic 的信息 /opt/kafka-2.7.0/bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic test 可以看到关于 test 这个 topic 的信息是可以获取到的，说明集群之前信息是互通的，集群搭建完毕。 Topic: test\tPartitionCount: 2\tReplicationFactor: 1\tConfigs: segment.bytes=1073741824\tTopic: test\tPartition: 0\tLeader: 1\tReplicas: 1\tIsr: 1\tTopic: test\tPartition: 1\tLeader: 2\tReplicas: 2\tIsr: 2","tags":["kafka","中间件","消息队列","部署"],"categories":["kafka"]},{"title":"Raft-Extended 论文翻译","path":"/2023/11/18/raft/","content":"原文：https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf 辨析 consensus vs consistency 一致性（consistency）往往指分布式系统中多个副本对外呈现的数据的状态。如顺序一致性、线性一致性，描述了多个节点对数据状态的维护能力。 共识（consensus）则描述了分布式系统中多个节点之间，彼此对某个提案达成一致结果的过程。 因此，一致性描述的是结果，共识则是一种手段。 有的人会说一致性和共识实际上是一个问题的一体两面，某种程度上来说，共识方法确实可以看作是实现强一致性的一种方法。事实上在工业界有许多以共识算法作为核心组件的多副本状态机（Replicated State Machine）实现，本质上利用了共识算法保证了所有副本的操作日志具有完全相同的顺序，从而实现了副本的一致性。但是，即使是在这样的场景下，讨论一个共识算法的一致性也是不合适的，因为整个分布式系统最终的一致性并不单单取决于共识算法，共识算法只是解决了其中一个问题。 参考：https://zhuanlan.zhihu.com/p/68743917 0. 摘要 Raft 是用来管理复制日志（replicated log）的一致性协议。它跟 multi-Paxos 作用相同，效率也相当。但是它的组织结构跟 Paxos 不同，也是因为 Raft 更简单的架构使得它更容易被理解，并且更容易在实际工程中得以实现。 为了让 Raft 更容易被理解，Raft 将共识算法的关键性因素切分成几个部分，比如： leader election（领导者选举） log replication（日志复制） safety（安全性） 并且 Raft 实施了一种更强的共识性以便减少必须要考虑的状态（states）的数量。 用户研究表明，对于学生来说，Raft 相比于 Paxos 是更容易学习的。 Raft 还包括一个用于解决变更集群成员问题的新机制，它使用重写多数来保证安全性。 1. 介绍 共识算法允许多台机器作为一个集群协同工作，并且在其中的某几台机器出故障时集群仍然能正常工作。正因为如此，共识算法在建立可靠的大规模软件系统方面发挥了重要作用。在过去十年中，Paxos [15,16] 主导了关于共识算法的讨论：大多数共识性的实现都是基于 Paxos 或受其影响，Paxos 已经成为教授学生关于共识知识的主要工具。 比较遗憾的是，尽管很多人一直在努力尝试使 Paxos 更易懂，Paxos 还是太难理解了。此外，Paxos 的架构需要复杂的改变来支持实际系统。这导致的结果就是系统开发者和学生在学生和使用 Paxos 过程中都很挣扎。 在我们自己与 Paxos 斗争之后，我们开始着手寻找一个新的共识算法，希望可以为系统开发和教学提供更好的基础。 我们的方法是不寻常的，因为我们的主要目标是可理解性：我们可以设计一个比 Paxos 更适合用于实际工程实现并且更易懂的共识算法吗？ 在该算法的设计中，重要的不仅是如何让算法起作用，还要清晰地知道该算法为什么会起作用。 这项工作的结果是一个称为 Raft 的共识性算法。在设计 Raft 时，我们使用了特定的技术来提高它的可理解性，包括： 分解（Raft 分离出三个关键点：leader election、log replication、safety） 减少状态空间（相比于 Paxos，Raft 降低了不确定性的程度和服务器之间的不一致） 一项针对 2 所大学共 43 名学生的用户研究表明，Raft 比 Paxos 更容易理解：在学习两种算法后，其中 33 名学生能够更好地回答 Raft 的相关问题。 Raft 在许多方面类似于现有的公式算法（尤其是 Oki、Liskov 的 Viewstamped Replication [29,22]），但它有几个新特性： Strong leader（强领导性）：相比于其他算法，Raft 使用了更强的领导形式。比如，日志条目只能从 leader 流向 follower（集群中除 leader 外其他的服务器）。这在使 Raft 更易懂的同时简化了日志复制的管理流程。 Leader election（领导选举）：Raft 使用随机计时器来进行领导选举。任何共识算法都需要心跳机制（heartbeats），Raft 只需要在这个基础上，添加少量机制，就可以简单快速地解决冲突。 Membership changes（成员变更）：Raft 在更改集群中服务器集的机制中使用了一个 联合共识（joint consensus） 的方法。在联合共识（joint consensus）下，在集群配置的转换过程中，新旧两种配置大多数是重叠的，这使得集群在配置更改期间可以继续正常运行。 我们认为 Raft 跟 Paxos 以及其他共识算法相比是更优的，这不仅体现在教学方面，还体现在工程实现方面。 它比其他算法更简单且更易于理解 它被描述得十分详细足以满足实际系统的需要 它有多个开源实现，并被多家公司使用 它的安全性已被正式规定和验证 它的效率与其他算法相当 本文剩余部分： 所在节 内容 第 2 节 复制状态机问题（replicated state machine problem） 第 3 节 Paxos 的优缺点 第 4 节 实现 Raft 易理解性的措施 第 5-8 节 Raft 共识性算法详细阐述 第 9 节 评估 Raft 第 10 节 其他相关工作 2. 复制状态机 共识算法一般都是在复制状态机 [37] 的背景下实现的。在这种方法下，一组服务器在的状态机计算相同状态的相同副本，即使某些服务器崩溃，它们也可以继续运行。 复制状态机是用来解决分布式系统中的各种容错问题。比如说，具有单个 leader 的大规模的系统，如 GFS [8]，HDFS [38] 和 RAMCloud [33] ，他们通常都使用单独的复制状态机来管理 leader election 和保存 leader 崩溃后重新选举所需的配置信息。像 Chubby [2] 和 ZooKeeper [11] 都是复制状态机。 复制状态机通常都是使用日志复制（log replication）来实现。如图 1：每个服务器都保存着一份拥有一系列命令的日志，然后服务器上的状态机会按顺序执行日志中的命令。每一份日志中命令相同并且顺序也相同，因此每个状态机可以处理相同的命令序列。所以状态机是可确定的，每个状态机都执行相同的状态和相同的输出序列。 image-20210719200404010 共识算法的主要工作就是保证复制日志（replicated log）的一致性。每台服务器上的共识模块接收来自客户端的命令，并将这些命令添加到其日志当中。它（指共识模块）与其他服务器上的共识模块进行通信，以确保每台服务器上最终以相同的顺序包含相同的命令，即使部分服务器崩溃了，这个条件也可以满足。一旦命令被正确复制，每台服务器上的状态机就会按日志顺序处理它们，并将输出返回给客户端。这样就形成了高可用的复制状态机。 适用于实际系统的共识算法通常都包含以下几点特征： 它们确保在所有非拜占庭错误下的安全性，也就是从不返回一个错误的结果。（即使是网络延迟、分区、数据包丢失、数据包重复和数据包乱序） 拜占庭错误： 出现故障（crash 或 fail-stop，即不响应）但不会伪造信息的情况称为“非拜占庭错误”。 伪造信息恶意响应的情况称为“拜占庭错误”，对应节点称为拜占庭节点。 只要任何大多数（过半）服务器是可运行的，并且可以互相通信和与客户端通信，那么共识算法就可用。假设服务器崩溃了，一小段时间后，它们很可能会根据已经稳定存储的状态来进行恢复，并重新加入集群。 它们在保证日志一致性上不依赖于时序：错误的时钟和极端消息延迟在最坏的情况下会产生影响可用性的一系列问题。 在通常情况下，只要集群中大部分（过半）服务器已经响应了单轮远程过程调用（RPC），命令就可以被视为完成。少数（一半以下）慢服务器不会影响整个系统的性能。 3. Paxos 存在的问题 在过去的十年间，Leslie Lamport 的 Paxos 协议 [15] 几乎成为共识性（consensus）的同义词。它是课堂上被教授最多的共识协议，大多数共识性的实现也是以它为起点。Paxos 首先定义了能在单个决策问题（例如单个复制日志条目）上达成共识的协议。我们将这个子集称为 signle-degree Paxos。然后 Paxos 组合该协议的多个实例去实现一系列决策，比如日志（mutil-Paxos）。Paxos 保证了安全性和活性，它也支持改变集群中的成员，它的安全性也已经被论证了，并且大多数情况下都是高效的。 美中不足的是，Paxos 有两个严重的缺点： Paxos 非常难理解 众所周知，Paxos 非常晦涩难懂，除非下了很大的功夫，很少有人能够成功理解它。因此，尽管目前已经有几个尝试希望将 Paxos [16,20,21] 解释得通俗易懂一些，而且这些解释都集中在 single-decree Paxos，但是它们还是很难懂。 在对 NSDI 2012 参会者的非正式调查中，我们发现很少人会喜欢 Paxos，即使是经验丰富的研究人员。我们自己也一直在跟 Paxos 作斗争，我们也无法完全理解整个 Paxos 协议，直到阅读了几个更简单的描述和自己设计了替代 Paxos 的协议，我们才对 Paxos 有了比较深刻的理解。但这个过程，花了将近一年。 我们推测 Paxos 这么晦涩难懂，主要是因为作者选择了 Single-decree Paxos 来作为基础。Single-decree Paxso 非常搞人：它分为两个阶段，但是并没有对这两个阶段进行简单直观的说明，而且这两个阶段也不能分开了单独理解，所以使用者将就很难理解为什么该算法能起作用。Multi-Paxos 的合成规则又增加了许多复杂性。我们相信，对多个决定（日志，并非单个日志条目）达成共识的总体问题可以用其他更直接和更明显的方式进行分解。 Paxos 没有为实际实现提供一个良好的基础 其中一个原因是没有广泛认同的针对 Multi-Paxos 的算法。Lamport 的描述主要是针对 signle-decree Paxos 的，他描述了针对 multi-Paxos 的可能方法，但缺少了很多细节。 目前已经有人在尝试具体化和优化 Paxos，比如 [26]，[39] 和 [13]，但是这些尝试都互不相同并且它们跟 Lamport 描述的也不尽相同。虽然像 Chubby [4] 这样的系统已经实现了类 Paxos（Paxos-like）算法，但是他们并没有透露出很多的实现细节。 此外，Paxos 的架构对于构建实际系统来说其实是一个糟糕的设计，这是 single-decree Paxos 分解的另一个结果。举个例子，这对于独立选择地日志条目的集合，然后再将它们合并到顺序日志当中没有任何好处，这只会增加复杂性。围绕日志来设计系统是更加简单和高效的方法，其中新条目按受约束的顺序依次附加。另外一个问题是 Paxos 在其核心使用了对称对等方法（尽管它最终表明了这会被用作一种性能优化的弱领导模式）。这在只有一个决策的情况下是有意义的，但是尽管如此，还是很少有实际系统采用了这种方法。如果有一系列的决策需要制定，更简单和更快速的方法应该是首先选择一个 leader，然后由 leader 去协调这些决策。 因此，按照 Paxos 来实现的实际系统往往跟 Paxos 相差很大。几乎所有的实现都是从 Paxos 开始，然后在实现的过程中发现了一系列的难题，在解决难题的过程中，开发出了跟 Paxos 完全不一样的架构。这样既费时又容易出错，而且 Paxos 本身的晦涩难懂又使得问题变得更加严重。Paxos 公式可能是证明其正确性的一个很好的公式，但真正的实现与 Paxos 又相差很大，这证明了它其实没有什么价值。下面来自 Chubby 作者的评论非常典型： 在 Paxos 算法描述和现实实现系统之间有着巨大的鸿沟... （如果一直按照 Paxos 算法走下去），最终的系统往往会建立在一个还未被证明的协议之上。 综合上述问题，我们觉得 Paxos 在教学端和系统构建端都没有提供一个良好的基础。考虑到共识性在大规模软件系统中的重要性，我们决定去尝试一下看看能不能设计一个替代 Paxos 并且具有更好特性的共识算法。Raft 就是这次实验的结果。 4. 为可理解性而设计 在设计 Raft 算法过程中我们有几个目标： 它必须为系统构建提供一个完整且实际的基础，这样才能大大减少开发者的工作 它必须在任何情况下都是安全的并且在典型的应用条件下是可用的，并且在正常情况下是高效的 但是我们最重要的目标，也是我们遇到的最大的挑战： 它必须具有易理解性，它必须保证能够被大多数人轻松地理解。而且它必须能够让人形成直观的认识，这样系统构建者才能在实现过程中对它进行不可避免的拓展。 在设计 Raft 算法的过程中，很多情况下我们需要在多个备选方案下做出抉择。在这种情况下，我们往往会基于可理解性来进行抉择： 解释各个备选方案的难度有多大？例如，它的状态空间有多复杂？它是否具有难以理解的含义？ 对于一个读者来说，完成理解这个方案和方案中的各种含义是否简单？ 我们意识到这一的分析具有高度的主观性。所以我们采取了两种通用的措施来解决这个问题。 第一个措施就是众所周知的问题分解：只要有可能，我们就将问题划分成几个相对独立地解决、解释和理解的子问题。例如，Raft 算法被我们划分成 leader 选举、日志复制、安全性和成员变更几个部分。 第二个措施是通过减少状态的数量来简化状态空间，尽可能地使系统变得更加连贯和尽可能地消除不确定性。很明显的一个例子就是，所有的日志都是不允许有空挡的，并且 Raft 限制了日志之间可能不一样的方式。尽管在大多数情况下我们都极力去消除不确定性，但是在某些情况下不确定性却可以提高可理解性。一个重要的例子就是随机化方法，它们虽然引入了不确定性，但是它们往往能够通过以类似的方式处理所有可能的选择来减少状态空间（随便选，没关系）。所有我们使用了随机化来简化 Raft 中的 leader election 算法。 5. Raft 共识算法 Raft 是一种用来管理第 2 节中提到的复制日志（replicated log）的算法。图 2 是该算法的浓缩，可以作为参考。图 3 列举了该算法的一些关键特性。这两张图中的内容将会在后面的各个章节中逐一介绍。 Raft 在实现共识算法的过程中，首先选举一个 distinguished leader，然后由该 leader 全权负责复制日志的一致性。Leader 从客户端接收日志条目，然后将这些日志条目复制给其他服务器，并且在保证安全性的情况下通知其他服务器将日志条目应用到他们的状态机中。拥有一个 leader 大大简化了对复制日志的管理流程。例如，leader 可以在不跟其他服务器商议的情况下决定新的日志条目应该存放在日志的什么位置，并且数据都是从 leader 流向其他服务器。当然了，一个 leader 可能会崩溃，也可能与其他服务器断开连接，那么这个时候，Raft 就会选举出一个新的 leader 出来。 通过选举一个 leader 的方式，Raft 将共识问题分解成三个独立的子问题，这些问题将会在接下来的子章节中进行讨论： Leader election（领导选举） 一个 leader 倒下之后，一定会有一个新的 leader 站起来。 Log replication（日志复制） leader 必须接收来自客户端的日志条目然后复制到集群中的其他节点，并且强制其他节点的日志和自己的保持一致。 Safety（安全性） Raft 中安全性的关键是图 3 中状态机的安全性：只要有任何服务器节点将一个特定的日志条目应用到它的状态机中，那么其他服务器节点就不能在同一个日志索引位置上存储另外一条不同的指令。第 5.4 节将会描述 Raft 如何保证这种特性，而且该解决方案在 5.2 节描述的选举机制上还增加了额外的限制。 image-20210709155333989 在展示了 Raft 共识算法后，本章节将讨论可用性的一些问题以及时序在系统中的所用。 5.1 Raft 基础 一个 Raft 集群中包含若干个服务器节点，5 个一个比较典型的数字，5 个服务器的集群可以容忍 2 个节点的失效。在任何一个时刻，集群中的每一个节点都只可能是以下是三种身份之一： leader：它会处理所有来自客户端的请求（如果一个客户端和 follower 通信，follower 会将请求重定向到 leader 上） follower：它们被动的：它们不会发送任何请求，只是简单的响应来自 leader 和 candidate 的请求 candidate：这是用来选举一个新的 leader 的时候出现的一种临时状态，这将在第 5.2 节中详细描述 在正常情况下，集群中只有一个 leader，然后剩下的节点都是 follower。图 4 展示了这些状态和它们之间的转换关系，这些转换关系将会在接下来进行讨论。 image-20210709145034498 如图 5 所示，Raft 将时间划分成任意长度的任期（term）。每一段任期从一次选举开始，在这个时候会有一个或者多个 candidate 尝试去成为 leader。如果某一个 candidate 赢得了选举，那么它就会在任期剩下的时间里承担一个 leader 的角色。在某些情况下，一次选举无法选出 leader，这个时候这个任期会以没有 leader 而结束。同时一个新的任期（包含一次新的选举）会很快重新开始。这是因为 Raft 会保证在任意一个任期内，至多有一个 leader。 image-20210709145441879 集群中不同的服务器观察到的任期转换的次数也许是不同的，在某些情况下，一个节点可能没有观察到 leader 选举过程甚至是整个任期过程。 任期在 Raft 中还扮演着一个逻辑时钟（logical clock）的角色，这使得服务器可以发现一些过期的信息，比如过时的 leader。 每一个节点都存储着一个当前任期号（current term number），该任期号会随着时间单调递增。节点之间通信的时候会交换当前任期号，如果一个节点的当前任期号比其他节点小，那么它就将自己的任期号更新为较大的那个值。如果一个 candidate 或者 leader 发现自己的任期号过期了，它就会立刻回到 follower 状态。如果一个节点接收了一个带着过期的任期号的请求，那么它会拒绝这次请求。 Raft 算法中服务器节点之间采用 RPC 进行通信，一般的共识算法都只需要两种类型的 RPC。 RequestVote RPCs（请求投票）：由 candidate 在选举过程中发出（5.2 节中描述） AppendEntries RPCs（追加条目）：由 leader 发出，用来做日志复制和提供心跳机制（5.3 节中描述）。 在第 7 节中为了在节点之间传输快照（snapshot）增加了第三种 RPC。当节点没有及时的收到 RPC 的响应时，会进行重试，而且节点之间都是以并行（parallel）的方式发送 RPC 请求，以此来获得最佳的性能。 5.2 Leader election Raft 采用一种心跳机制来触发 leader 选举。当服务器启动的时候，他们都会称为 follower。一个服务器节点只要从 candidate 或者 leader 那接收到有效的 RPC 就一直保持 follower 的状态。Leader 会周期性地向所有的 follower 发起心跳来维持自己的 leader 地位，所谓心跳，就是不包含日志条目的 AppendEntries RPC。如果一个 follower 在一段时间内没有收到任何信息（这段时间我们称为选举超时 election timeout），那么它就会假定目前集群中没有一个可用的 leader，然后开启一次选举来选择一个新的 leader。 开始进行选举的时候，一个 follower 会自增当前任期号然后切换为 candidate 状态。然后它会给自己投票，同时以并行的方式发送一个 RequestVote RPCs 给集群中的其他服务器节点（企图得到它们的投票）。一个 candidate 会一直保持当前状态直到以下的三件事之一发生（这些情况都会在下面的章节里分别讨论）： 它赢得选举，成为了 leader 其他节点赢得了选择，那么它会变成 follower 一段时间之后没有任何节点在选举中胜出 当一个 candidate 获取集群中过半服务器节点针对同一任期的投票时，它就赢得了这次选举并成为新的 leader。对于同一个任期，每一个服务器节点会按照 先来先服务原则（first-come-first-served） 只投给一个 candidate（在 5.4 节会在投票上增加额外的限制）。这种要求获得过半投票才能成为 leader 的规则确保了最多只有一个 candidate 赢得此次选举（图 3 中的选举安全性）。只要有一个 candidate 赢得选举，它就会成为 leader。然后它就会向集群中其他节点发送心跳消息来确定自己的地位并阻止新的选举。 一个 candidate 在等待其他节点给它投票的时候，它也有可能接收到另外一个自称为 leader 的节点给它发过来的 AppendEntries RPC。 如果这个 leader 的任期号（这个任期号会在这次 RPC 中携带着）不小于这个 candidate 的当前任期号，那么这个 candidate 就会觉得这个 leader 是合法的，然后将自己转变为 follower 状态。 如果这个 leader 的任期号小于这个 candidate 的当前任期号，那么这个 candidate 就会拒绝这次 RPC，然后继续保持 candidate 状态。 第三种可能的结果是 candidate 既没有赢得选举也没有输。可以设想一下这么一个情况。所有的 follower 同时变成 candidate，然后它们都将票投给自己，那这样就没有 candidate 能得到超过半数的投票了，投票无果。当这种情况发生的时候，每个 candidate 都会进行一次超时响应（time out），然后通过自增任期号来开启一轮新的选举，并启动另一轮的 RequestVote RPCs。然而，如果没有额外的措施，这种无结果的投票可能会无限重复下去。 为了解决上述问题，Raft 采用 随机选举超时时间（randomized election timeouts） 来确保很少发生无果的投票，并且就算发生了也能很快地解决。为了防止选票一开始就被瓜分，选举超时时间是从一个固定的区间（比如，150-300ms）中随机选择。这样可以把服务器分散开来以确保在大多数情况下会只有一个服务器率先结束超时，那么这个时候，它就可以赢得选举并在其他服务器结束超时之前发送心跳（译者注：乘虚而入，不讲武德）。 同样的机制也可以被用来解决选票被瓜分（split votes）的情况。每个 candidate 在开始一轮选举之前会重置一个随机选举超时时间，然后一直等待直到结束超时状态。这样减少了在一次投票无果后再一次投票无果的可能性。9.3 节展示了该方案能够快速地选出一个 leader。 选举的例子可以很好地展现可理解性是如何指导我们在多种备选设计方案中做出抉择的。在一开始，我们本打算使用一种等级系统（rank system）：每一个 candidate 被赋予一个一次的等级（rank），如果一个 candidate 发现另外一个 candidate 有着更高的登记，那么它就会返回 follower 状态，这样可以使高等级的 candidate 更加容易地赢得下一轮选举。但是我们发现这种方法在可用性方面会有一些小问题： 如果等级较高的服务器崩溃了，那么等级较低的服务器可能需要进入超时状态，然后重新成为一个 candidate。如果这种操作出现得太快，那么它可能会重启进程去开启一轮新的选举。 经过我们对该算法做出了多次的调整，我们最终还是认为随机重试的方法更加通俗易懂。 5.3 Log replication Leader 一旦被选举出来，它就要开始为客户端的请求提供服务了。每一个客户端请求都包含一条将被复制状态机执行的命令。leader 会以一个新条目的方式将该命令追加到自己的日志中，并且以同步的方式向集群中的其他节点发起 AppendEntires RPCs，让它们复制该条目。当条目被安全地复制（何为安全复制，后面会介绍）之后，leader 会将该条目应用到自己的状态机中，状态机执行该指令，然后把执行的结果返回给客户端。如果 follower 崩溃了或者运行缓慢，或者网络丢包，leader 会不断地重试 AppendEntiries RPCs（即使已经对客户端作出了响应）直到所有的 follower 都成功存储了所有的日志条目。 日志以图 6 展示的方式组织着。每条日志条目都存储着一条状态机指令和 leader 收到该指定时的任期号。日志条目中的任期号可以用来检测多个日志副本之间是否不一致，以此来保证图 3 中的某些性质。每个日志条目还有一个整数索引值来表明它在日志中的位置。 image-20210709165036190 那么问题就来了，leader 什么时候会觉得把日志条目应用到状态机是安全的呢？ 这种日志条目被称为已提交的日志条目。Raft 保证这种已提交的日志条目都是持久化的并且最终都会被所有可用的状态机执行。 一旦创建该日志条目的 leader 将它复制到过半的节点上时（比如图 6 中的条目 7），该日志条目就会被提交。 同时，leader 日志中该日志条目之前的所有日志条目也都会被提交，包括由之前的其他 leader 创建的日志条目。5.4 节会讨论在 leader 变更之后应用该规则的一些细节，并证明这种提交的规则是安全的。leader 会追踪它所知道的要提交的最高索引，并将该索引包含在未来的 AppendEntries RPC 中（包括心跳），以便其他的节点可以发现这个索引。一旦一个 follower 知道了一个日志条目被提交了。它就会将该日志条目按日志顺序应用到自己的状态机中。 我们设计 Raft 日志机制来使得不同节点上的日志之间可以保持高水平的一致性。这么做不仅简化了系统的行为也使得系统更加可预测，同时该机制也是保证安全性的重要组成部分。Raft 会一直维护着以下的特性，这些特性也同时构成了图 3 中的日志匹配特性（Log Matching Property）： 如果不同日志中的两个条目有着相同的索引和任期值，那么它们就存储着相同的命令 如果不同日志中的两个条目有着相同的索引和任期值，那么他们之前的所有日志条目也都相同 第一条特性源于这样一个事实，在给定的一个任期值和给定的一个日志索引中，一个 leader 最多创建一个日志条目，而且日志条目永远不会改变它们在日志中的位置。 第二条特性是由 AppendEntries RPC 执行的一个简单的一致性检查所保证的。当 leader 发送一个 AppendEntries RPC 的时候，leader 会将前一个日志条目的索引位置和任期号包含在里面（紧邻最新的日志条目）。如果一个 follower 在它的日志中找不到包含相同索引位置和任期号的条目，那么它就会拒绝该新的日志条目。一致性检查就像一个归纳步骤：一开始空的日志状态肯定是满足日志匹配特性（Log Matching Property）的，然后一致性检查保证了日志扩展时的日志匹配特性。因此，当 AppendEntries RPC 返回成功时，leader 就知道 follower 的日志一定和自己相同（从第一个日志条目到最新条目）。 正常操作期间，leader 和 follower 的日志都是保持一致的，所以 AppendEntries 的一致性检查从来不会失败。但是，如果 leader 崩溃了，那么就有可能会造成日志处于不一致的状态，比如说老的 leader 可能还没有完全复制它日志中的所有条目它就崩溃了。这些不一致的情况会在一系列的 leader 和 follower 崩溃的情况下加剧。图 7 解释了什么情况下 follower 的日志可能和新的 leader 的日志不同。follower 可能会确实一些在新 leader 中有的日志条目，也有可能拥有一些新的 leader 没有的日志条目，或者同时存在。缺失或多出日志条目的情况有可能会涉及到多个任期。 image-20210712144330139 在 Raft 算法中，leader 通过强制 follower 复制 leader 日志来解决日志不一致的问题。也就是说，follower 中跟 leader 冲突的日志条目会被 leader 的日志条目所覆盖。5.4 节会证明通过增加一个限制，这种方式就可以保证安全性。 为了使 follower 的日志跟自己（leader）一致，leader 必须找到两者达成一致的最大的日志条目索引，删除 follower 日志中从那个索引之后的所有日志条目，并且将自己那个索引之后的所有日志条目发送给 follower。所有的这些操作都发生在 AppendEntries RPCs 的一致性检查的回复中。leader 维护着一个针对每一个 follower 的 nextIndex，这个 nextIndex 代表的就是 leader 要发送给 follower 的下一个日志条目的索引。当选出一个新的 leader 时，该 leader 将所有的 nextIndex 的值都初始化为自己最后一个日志条目的 index 加 1（图 7 中的 11）。如果一个 follower 的日志跟 leader 的是不一致的，那么下一次的 AppendEntries RPC 的一致性检查就会失败。AppendEntries RPC 在被 follower 拒绝之后，leader 对 nextIndex 进行减 1，然后重试 AppendEntries RPC。最终 nextIndex 会在某个位置满足 leader 和 follower 在该位置及之前的日志是一致的，此时，AppendEntries RPC 就会成功，将 follower 跟 leader 冲突的日志条目全部删除然后追加 leader 中的日志条目（需要的话）。一旦 AppendEntries RPC 成功，follower 的日志就和 leader 的一致了，并且在该任期接下来的时间里都保持一致。 如果需要的话，下面的协议可以用来优化被拒绝的 AppendEntries RPCs 的个数。 比如说，当拒绝一个 AppendEntries RPC 的时候，follower 可以包含冲突条目的任期号和自己存储的那个任期的第一个 index。借助这些信息，leader 可以跳过那个任期内所有的日志条目来减少 indexIndex。这样就变成了每个有冲突日志条目的任期只需要一个 AppendEntries RPC，而不是每一个日志条目都需要一次 AppendEntires RPC。 在实践中，我们认为这种优化是没有必要的，因为失败不经常发生并且也不可能有很多不一致的日志条目。 通过上述机制，leader 在当权之后就不需要任何特殊的操作来使日志恢复到一致状态。leader 只需进行正常的操作，然后日志就能在回复 AppendEntries RPC 一致性检查的时候自动趋于一致。leader 从来不会重写或者删除自己的日志条目（图 3 中的 Leader Append-Only 属性）。 上述这种日志复制机制展现了第 2 节中描述的 Raft 算法的共识特性：只要过半的节点能正常运行，Raft 就能接受、复制并处理新的日志条目。在通常情况下，一个新的条目可以在一轮 RPC 中被复制给集群中过半的节点，并且单个运行缓慢的 follower 并不会影响整个集群的性能。 译者注：总结 Leader 收到 Client 的写请求，向所有 Follower 发起一个日志同步请求，得到集群内过半节点（包括 Leader 自己）的响应，就推进 commitIndex，然后 apply 日志到状态机，再推进 applyIndex，返回 Client 成功。 状态机同步分为两轮 RPC 广播： 第一轮：同步日志 AppendEntries，得到过半节点回复，Leader 状态机推进，返回 Client 成功。 第二轮：在下一次的 AppendEntries 中附带上一次的 commitIndex，Follower 收到后，apply 日志条目到各自的状态机。 5.4 Safety 前面的章节描述了 Raft 如何做 Leader Election 和 Log Replication。然而，到目前为止所讨论的机制并不能充分地保证每一个状态机会按相同的顺序执行相同的指令。比如说，一个 follower 可能会进入不可用状态，在此期间，leader 可能提交了若干的日志条目，然后这个 follower 可能被选举为新的 leader 并且用新的日志条目去覆盖这些日志条目。这样就会造成不同的状态机执行不同的指令的情况。 本节通过对 Leader Election 增加一个限制来完善 Raft 算法。这个限制保证了对于给定的任意任期号，该任期号对应的 leader 都包含了之前各个任期所有被提交的日志条目（图 3 中的 Leader Completeness 性质）。有了这个限制，我们也可以使日志提交规则更加清晰。最后，我们会展示对于 Leader Completeness 性质的简要证明并且说该性质是如何保证状态机执行正确的行为的。 5.4.1 选举限制 在任何基于 leader 的共识算法中，leader 最终都必须存储所有已经提交的日志条目。在某些共识算法中，例如 Viewstamped Replication [22]，即使一个节点它一开始并没有包含所有已经提交的日志条目，它也有可能被选举为 leader。这些算法包含一些额外的机制来识别丢失的日志条目并将它们传送给新的 leader，这个机制要么发生在选举阶段，要么在选举完成之后很快进行。比较遗憾的是，这种方法会增加许多额外的机制，使得算法复杂性大大增加。Raft 使用了一种更加简单的方法，它可以保证新 leader 在当选时就包含了之前所有任期中已经提交的日志条目，根本就不需要再传送这些日志条目给新的 leader。这就意味着日志条目的传送只有一个方向，那就是从 leader 到 follower，leader 从来不会覆盖本地日志中已有的日志。 Raft 采用投票的方式来保证一个 candidate 只有拥有之前所有任期中已经提交的日志条目之后，才有可能赢得选举。一个 candidate 如果想要被选为 leader，那它就必须跟集群中超过半数的节点进行通信，这就意味这些节点中至少一个包含了所有已经提交的日志条目。如果 candidate 的日志至少跟过半的服务器节点一样新，那么它就一定包含了所有以及提交的日志条目，一旦有投票者自己的日志比 candidate 的还新，那么这个投票者就会拒绝该投票，该 candidate 也就不会赢得选举。 所谓 “新” ： Raft 通过比较两份日志中的最后一条日志条目的索引和任期号来定义谁的日志更新。 如果两份日志最后条目的任期号不同，那么任期号大的日志更新 如果两份日志最后条目的任期号相同，那么谁的日志更长，谁就更新 5.4.2 提交之前任期内的日志条目 译者注：注意！这一节「提交之前任期内的日志条目」这种操作 Raft 的不允许的！本小节只是用来举一种错误情况！ 如 5.3 节中提到的那样，一旦当前任期内的某个日志条目以及存储到过半的服务器节点上，leader 就知道该日志可以被提交了。如果这个 leader 在提交某个日志条目之前崩溃了，以后的 leader 会尝试完成该日志条目的复制。然而，如果是之前任期内的某个日志条目已经存储到了过半的服务器节点上了，新任期内的 leader 也无法立即断定该日志条目已经被提交了。图 8 展示了一种情况：一个已经被存储到过半节点的老日志条目，仍然有可能会被未来的 leader 覆盖掉。 image-20210712160506841 译者注：对图 8 的理解的补充。 参考： 知乎 核心： 图 8 用来说明为什么 leader 不能提交之前任期的日志，只能通过提交自己任期的日志，从而间接提交之前任期的日志。 分析： 先按错误的情况，也就是 leader 提交之前任期的日志，那么上述的流程： S1 是任期 2 的 leader，日志已经复制给了 S2，此时还没过半； S1 崩溃，S5 获得了 S3、S4、S5 的投票成为 leader，然后写了一个日志条目（index=2，term=3）； S5 刚写完日志，还没来得及复制，就崩溃了，此时 S1 和 S2 都可能当选，加入 S1 当选（currentTerm=4），此刻还没有新的请求进来，S1 将日志条目（index=2，term=2）复制给了 S3，多数派达成，S1 提交了这个日志条目（index=2，term=2）， 注意，该日志不是当前任期内的日志，我们在讨论错误的情况！ 然后请求进来，S1 写日志条目（index=3，term=4），然后 S1 崩溃。 情况一：(d) S5 重启，因为 S5 最后的日志条目的任期号比 S2、S3 大，所以 S5 可以赢得选举（currentTerm=5），S5 将日志条目（index=2，item=3）复制给其他所有节点并提交， 此时 index=2 的日志条目被提交了两次！一次 term=2，一次 term=3，这是不被允许的，因为已经提交的日志条目是不能被覆盖的！ ✖️ 情况二：(e) S1 在崩溃之前将自己的日志条目（index=3，term=4）复制到了过半节点上，这种情况下，S5 不可能选举成功。这是 S1 不发生故障，这是正确复制的情况。✔️ 所以 「leader 可以提交之前任期的日志」 这种操作是不允许的，我们需要加上约束： 「leader 只能提交自己任期的日志」 。 加了约束之后，前面的 (a) 和 (b) 没有改变，从 (c) 开始： S1 还是将日志条目（index=2，term=2）复制给其他节点，它复制给了 S3，此时已经复制给了过半的节点了，但是由于 currentTerm=4，所以 S1 还是不能提交该日志条目。如果 S1 将日志条目（index=3，term=4）也复制给了过半的节点，S1 是可以提交该日志条目的，那么这个时候，前面的日志条目（index=2，term=2）也会被间接提交，这就是 (e) 所展示的情况。 S1 还是将日志条目（index=2，term=2）复制给其他节点，它复制给了 S3，此时已经复制给了过半的节点了，但是由于 currentTerm=4，所以 S1 还是不能提交该日志条目。但是这个时候，S1 只是日志条目（index=3，term=4）写入自己的日志，还没来得及复制就崩溃了。然后 S5 重启并赢得了选举（currentTerm=5），然后将日志条目（index=2，term=3）复制给其他所有节点，现在 index=2 的日志条目是没有提交过的，S5 能提交该日志吗？ 不能！因为 leader 不能提交之前任期的日志！只有等新的请求进来，超过半数节点复制了 1-3-5 之后，term=3 的日志才能跟着 term=5 的日志一起被提交。 延伸： 加了上述约束后，就不会出现同一个 index 上的日志条目被重复提交的情况了，但是这又多出了另外一个问题了：如果一直没有新的请求进来，那么日志条目（index=2，term=3）岂不是就一直不能提交？那不就阻塞了吗？ 这里如果是 kv 数据库，问题就很明显了。假设 (c) 或 (d) 中的日志条目（index=2）里的 Command 是 Set(\"k\", \"1\")，S5 当选 leader 后，客户端来查询 Get(\"k\")，leader 查到日志有记录但又不能回复 1 给客户端（因为按照约束这条日志未提交），线性一致性要求不能返回陈旧的数据，leader 迫切地需要知道这条日志到底能不能提交。 所以 Raft 论文提高了引入 no-op 日志来解决这个问题，这个在 etcd 中有实现。 no-op 日志： no-op 日志即只有 index 和 term 信息，command 信息为空。也是要写到磁盘存储的。 具体流程是在 leader 刚选举成功的时候，立即追加一条 no-op 日志，并立即复制到其它节点，no-op 日志一经提交，leader 前面那些未提交的日志全部间接提交，问题就解决了。像上面的 kv 数据库，有了 no-op 日志之后，Leader 就能快速响应客户端查询了。 本质上，no-op 日志使 leader 隐式地快速提交之前任期未提交的日志，确认当前 commitIndex，这样系统才会快速对外正常工作。 为了解决图 8 中描述的问题，Raft 永远不会通过计算副本数目的方式来提交之前任期内的日志条目。只有 leader 当期内的日志条目才通过计算副本数目的方式来提交。一旦当前任期内的某个日志条目以这种方式被提交（如图 8 中的 e），那么由于日志匹配特性（Log Matching），之前的所有日志条目也会被间接地提交。在某些情况下，leader 可以安全地断定一个老的日志条目已经被提交（例如，如果该条目已经被存储到每一个节点上了）。但是 Raft 为了简化问题，采取了上述描述的更加保守的方法。 Raft 会在提交规则上增加额外的复杂性是因为当 leader 复制之前任期内的日志条目时，这些日志条目都保留原来的任期号。在其他的共识算法中，如果一个新的 leader 要重新复制之前任期里的日志时，它必须使用当前新的任期号。Raft 的做法使得更加容易推导出日志条目，因为它们自始至终都使用同一个任期号。另外，和其他的算法相比，Raft 中的新 leader 只需要发送更少的日志条目（其他算法中必须在它们被提交之前发送更多的冗余日志条目来给它们重新编号）。 5.4.3 安全性论证 给出了完整的 Raft 算法后，我们现在可以更严格地来论证 leader 完整性特性（Leader Completeness Property）（这一讨论基于 9.2 节的安全性证明）。我们先假设 Leader Completeness Property 是不满足的，然后再推出矛盾来。 假设： 假设任期 T 的 leaderT 在任期内提交了一个日志条目，但是该日志条目没有存在未来某些任期的 leader 中，假设 U 是大于 T 的没有存储该日志条目的最小任期号，处在任期 U 的 leader 称为 leaderU。 论证： 因为 leader 从来不删除或重写自己的日志条目，所以如果一个已提交的日志要做到不存在未来的 leaderU 中的话，那么它只可能在 leaderU 选举的过程中被丢失。 leaderT 将该日志复制给了集群中过半的节点，leaderU 从集群中过半的节点得到了投票。因此，至少有一个节点（这里称它为 voter）同时接收了来自 leaderT 的日志条目并且给 leaderU 投票了。 image-20210713185232381 voter 必然在给 leaderU 投票之前就已经接收了这个已经提交的日志条目了。否则，它就会拒绝来自 leaderT 的 AppendEntries RPC 请求，因为如果它在给 leaderU 投票之后再接收条目的话，那么它的当前任期号会比 T 大。 译者注：因为要举行 Leader election 的话需要开一轮新的任期，这个时候前一轮任期已经结束了。我们这里假设了 T U，上述所说的已提交日志条目是在任期 T 中的，如果 voter 先投票的话，那么就说明它已经进入了任期 U 了，而 U T，voter 是不可能接受 leaderT 的 AppendEntries 请求的。 而且，voter 在给 leaderU 投票的时候，它依旧保有该日志条目，因为任何 U、T 之间的 leader 都包含该日志条目（因为我们前面假设了 U 是大于 T 的没有存储该日志条目的最小任期号），而且 leader 从来不会删除条目，并且 follower 只有再跟 leader 冲突的时候才会删除条目。 该投票者把自己的选票投给 leaderU 的时候，leaderU 的日志至少跟 voter 一样新（可以更新），这就导致了以下的两个矛盾之一了。 第一个矛盾： 如果 voter 和 leaderU 最后一个日志条目的任期号相同的话，那么 leaderU 的日志至少和 voter 的一样长，所以 leaderU 的日志一定包含 voter 日志中的所有日志条目。 这是一个矛盾，因为 voter 包含了该已提交的日志条目，所以 leaderU 必定也包含该日志条目，而前面我们假设了 leaderU 是不包含的，这就产生了矛盾。 第二个矛盾： 如果不是上面描述的情况的话，那么 leaderU 最后一个日志条目的任期号必然需要比 voter 的更大。此外，它还比 T 要大，因为 voter 拥有在任期号为 T 提交的日志条目，所以 voter 最后一个日志条目的任期号至少为 T。创建了 leaderU 的最后一个日志条目的之前的 leader 一定已经包含了该已被提交的日志条目（因为我们上面假设了 leaderU 是第一个没有该日志条目的 leader）。所以，根据日志匹配特性，leaderU 一定也包含了该已被提交的日志条目，这样也产生了矛盾。 上述讨论就证明了假设是不成立的。因此，所有比 T 大的任期的 leader 一定包含了任期 T 中提交的所有日志条目。 日志匹配特性保证了未来的 leader 也会包含被间接提交的日志条目，如图 8 (d) 中的索引 2。 通过 leader 的完整性特性，我们就可以证明图 3 中的状态机安全特性了，即如果某个节点已经将某个给定的索引处的日志条目应用到自己的状态机里了，那么其他的节点就不会在相同的索引处应用一个不同的日志条目。在一个节点应用一个日志条目到自己的状态机中时，它的日志和 leader 的日志从开始到该日志条目都是相同的，并且该日志条目必须被提交。现在考虑一个最小的任期号，在该任期中任意节点应用了一个给定的最小索引上面的日志条目，那么 Log 的完整性特性就会保证该任期之后的所有 leader 将存储相同的日志条目，因此在后面的任期中应用该索引上的日志条目的节点会应用相同的值。所以，状态机安全特性是可以得到保证的。 最后，因为 Raft 要求服务器节点按照日志索引顺序应用日志条目，再加上状态机安全特性，这样就意味着我们可以保证所有的服务器都会按照相同的顺序应用相同的日志条目到自己的状态机中了。 5.5 follower 和 candidate 崩溃 到目前为止，我们只关注了 leader 崩溃的情况。follower 和 candidate 崩溃后的处理方式要比 leader 崩溃简单得多，而且它们的处理方式是相同的。如果一个 follower 或者 candidate 崩溃的话，后面发送给它们的 RequestVote 和 AppendEntries RPCs 都会失败。Raft 通过无限重试来处理这种失败。如果崩溃的节点重启了，那么这些 RPC 就会被成功地完成。如果一个节点在完成了一个 RPC，但是还没来得及响应就崩溃了的话，那么在它重启之后它会再次收到同样的请求。Raft 的 RPCs 都是幂等的，所以重复发送相同的 RPCs 不会对系统造成危害。实际情况下，一个 follower 如果接收了一个 AppendEntries 请求，但是这个请求里面的这些日志条目在它日志中已经有了，它就会直接忽略这个新的请求中的这些日志条目。 译者注：幂等 在编程中一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。幂等函数，或幂等方法，是指可以使用相同参数重复执行，并能获得相同结果的函数。这些函数不会影响系统状态，也不用担心重复执行会对系统造成改变。例如，“setTrue()”函数就是一个幂等函数,无论多次执行，其结果都是一样的.更复杂的操作幂等保证是利用唯一交易号(流水号)实现。 5.6 时序和可用性 Raft 中有一个要求就是 Raft 的安全性不能依赖于时序（timing）：整个系统不能因为某些事件运行得比预期快一点或者慢一点就产生错误的结果。然而，可用性（即系统能够及时响应客户端的请求）不可避免的要依赖于时序。比如说，如果信息交换的时间比一般服务器崩溃所持续的时间还要长的话，那么 candidate 可能等不到赢得选举了，而缺少了一个稳定的 leader，Raft 将无法工作。 Raft 中时序最关键的地方就是 Leader election。只要整个系统满足下面的时间要求，Raft 就可以选举并维持一个稳定的 leader： 广播时间（broadcastTime） 选举超时时间（electionTimeout） 平均故障间隔时间（MTBF） 在这个不等式中，广播时间指的是一个节点并行地发送 RPCs 给集群中其他所有的节点并得到响应的平均时间。选举超时时间就是在 5.2 节中介绍的选举超时时间。平均故障间隔时间就是对于一台服务器而言，两次故障间隔时间的平均值。广播时间必须选举超时时间小一个量级，这样 leader 才能够有效发送心跳信息来组织 follower 进入选举状态。再加上随机化选举超时时间的方法，这个不等式也使得无果选票（split vote）变得几乎不可能。而选举超时时间需要比平均故障间隔时间小上几个数量级，这样整个系统才可以稳定地运行。有了这个限制后，当 leader 崩溃后，整个系统会有一段大约选举超时时间的时长不可用，我们希望该情况在整个系统运行时间里只占一小部分。 广播时间和平均故障间隔时间是由系统决定的，但是选举超时时间是我们可以自定义的。Raft 的 RPCs 需要接收方将信息持久化地保存到稳定存储中，所以广播时间大约是 0.5ms ~ 20ms 之间，取决于存储的技术。因此，选举超时时间可能需要在 10ms ~ 500ms 之间。而大多数的服务器的平均故障间隔时间都在几个月甚至更长，所以很容易满足时间的要求。 6. 集群成员变更 到目前为止，我们都假设集群的配置（参与共识算法的服务器节点集合）是固定不变的。但是在实际情况中，我们有时候是需要去改变集群配置的，比如说在服务器崩溃的时候去更换服务器或者是更改副本的数量。尽管可以通过下线整个集群，更新所有配置，然后重启整个集群的方式来实现这个需求，但是这会导致集群在更改过程中是不可用的。另外，如果这个过程中存在一些操作需要人工干预，那么就会有操作失误的风险。为了避免这些问题，我们决定将配置变更自动化并将其纳入到 Raft 的共识算法中来。 为了使配置变更机制足够安全，在配置变更过程中不能存在任何一个时刻使得同一任期中选出两个 leader。遗憾的是，任何服务器直接从旧的配置转换为新的配置的方案都是不安全的。一次性自动地转换所有服务器的配置的不可能的，所以在转换期间整个集群可能划分为两个独立的大多数（如图 10 所示）。 image-20210714195412552 译者注：图 10 补充 上图中，在中间位置 Server1 可以通过自身和 Server2 的选票成为 leader（满足旧配置下收到大多数选票的原则）；Server3 可以通过自身和 Server4、Server5 的选票成为 leader（满足新配置线，即集群有 5 个节点的情况下的收到大多数选票的原则）；此时整个集群可能在同一任期中出现了两个 leader，这和 Raft 协议是违背的。 为了保证安全性，配置变更必须采取一种两段式方法。目前有很多种两段式的实现。例如，有些系统（如 [22] ）在第一阶段停掉旧的配置，所以在这个阶段不能处理用户的请求，然后在第二阶段启用新的配置。在 Raft 中，集群先切换到一个过渡的配置，我们称之为 联合共识（joint consensus） 。一旦联合共识配置已经被提交了，系统就可以切换到新的配置上了。联合共识配置是新旧配置的并集： 日志条目被复制给集群中处于新、老配置的所有节点 新、旧配置的节点都可能成为 leader 达成一致（针对选举和提交）需要分别得到在两种配置上过半的支持 联合共识允许每一个节点在不妥协安全性的前提下，在不同的时刻进行配置转换过程。此外，联合共识还允许在集群配置变更期间响应客户端的请求。 集群配置在复制日志中以特殊的日志条目来存储和通信。图 11 展示了配置变更的过程。 image-20210714193852320 当 leader 接收到一个更新配置的请求的时候，它就创建一个联合共识日志条目 Cold,new，并以前面描述的方式复制该条目。 一旦某个节点将该配置日志条目增加到自己的日志中。那么这个节点就会用该配置来做出未来的所有决策（一个节点总是使用日志中最新的配置，无论该日志是否已经被提交）。 这就意味着 leader 会使用 Cold,new 的规则来判断 Cold,new 日志条目是什么时候被提交的。如果 leader 崩溃了，新的 leader 有可能处于 Cold 配置，也可能处于 Cold,new 配置，这取决于赢得选举的 candidate 是否已经接收到了 Cold,new 配置。在任何情况下，处于 Cnew 状态的节点在此期间都是不能单独做出决定的。 当 Cold,new 被提交了，那么 Cold 和 Cnew 都不能在没有得到对方认可的情况下做出决定，并且 Leade 完整特性（Leader Completeness Property）保证了只有拥有 Cold,new 日志的 candidate 有可能被选为 leader。所以现在 leader 就可以安全地创建一个描述 Cnew 的日志条目并将其复制给集群中的其他节点了。一样的，新的配置被节点收到后就会立刻生效。当新的配置在 Cnew 的规则下被提交了之后，旧配置就变得无关紧要了，处于旧配置的节点也可以关闭了。如图 11 所示，没有任何一个时刻 Cold 和 Cnew 是可以单独做决定的，这保证了安全性。 关于配置变更有三个问题需要解决： 第一个问题：新的节点可能在一开始并没有存储任何的日志条目。当这些节点以这种状态加入到集群中的时候，它们需要一段时间来更新自己的日志，以便赶上其他节点，在这个时间段里面它们是不可能提交一个新的日志条目的。 为了避免因此造成的系统短时间的不可用，Raft 在配置变更前引入了一个额外的阶段。在该阶段中，新的节点以没有投票权身份加入到集群中来（leader 会把日志复制给它们，但是考虑过半的时候不需要考虑它们）。 一旦新节点的日志已经赶上了集群中的其他节点，那么配置变更就可以按照之前描述的方式进行了。 第二个问题：leader 有可能不是新配置中的一员（译者注：也就是说这个 leader 后面是需要被下线的）。在这种情况下，leader 一旦提交了 Cnew 日志条目，它就会退位为 follower（译者注：Cold,new 状态下依旧可用）。这就意味着有这样一段时间（leader 提交 Cnew 期间）：leader 管理着一个不包括自己的集群，它会复制日志给其他节点，但是算副本数量的时候不会算上自己。leader 转换发生在 Cnew 被提交的时候，因为这是新配置可以独立运行的最早时刻（在这个时刻之后，一定是从 Cnew 中选出新的 leader）。在这个时间点之前，有可能只能从 Cold 中选出 leader。 第三个问题：那么被移除的节点（不处于 Cnew 状态的节点）有可能会扰乱集群。这些节点将不会收到心跳信息，所以当选举超时时，它们就会进行新的选举过程。它们会发送带有新任期号的 RequestVote RPCs，这样会导致当前的 leader 回到 follower 状态，然后选出一个新的 leader。但是这些被移除的节点还是会收不到心跳，然后再次超时，再次循环这个过程，导致系统的可用性很差。 为了避免这个问题，当节点认为当前有 leader 存在时，节点会忽略 RequestVote RPCs。具体来说，当一个节点在最小选举超时时间内收到一个 RequestVote RPC，它不会更新它的任期或授予它的投票。这不会影响正常的选举，每个节点在开启一轮选举之前，它会至少等待一次最小选举超时时间。相反，这有利于避免被移除的节点的扰乱：如果一个 leader 能够发送心跳给集群，那它就不会被更大的任期号废黜。 译者注：对配置变更的归纳 配置变更过程 leader 在本地生成一个新的日志条目，其内容是 Cold ∪ Cnew，代表当前时刻新旧成员配置共存，写入本地日志，称为 Cold,new。后面 leader 就以该日志作为自己的配置了。同时将该日志条目复制集群中是所有节点中。在此之后新的日志同步需要保证得到 Cold 和 Cnew 两个多数派的确认。 follower 收到 Cold.new 的日志后更新本地日志，并且此时就以该配置作为自己的成员配置。 如果 Cold 和 Cnew 中的两个多数派确认了 Cold.new 这个日志条目，leader 就提交它。 接下来 leader 生成一条新的日志条目，其内容是新成员配置 Cnew，同样将该日志条目写入本地日志，同时复制给集群中其他节点。 follower 收到新成员配置 Cnew 后，将其写入日志，并且从此刻起，就以该配置作为自己的成员配置，并且如果发现自己不在 Cnew 这个成员配置中会自动退出。 leader 收到 Cnew 的多数派确认后，表示成员变更成功，后续的日志只要得到 Cnew 多数派确认即可。 完成上述两阶段后，leader 就可以给客户端回复配置变更执行成功。 如果当前的 leader 不在 Cnew 的配置中会怎么样？ 因为当前 leader 不在 Cnew 配置中，所以当 Cnew 日志条目被提交的时候，leader 其实是要被下线的（比如说集群节点数从 5 缩容为 3，且刚好下线的节点中包含当前 leader）。那这样的话，在 Cold,new 状态下，leader 还是可用的，但是一旦 Cnew 日志条目被提交了，leader 就需要下线了，这个时候不用当心，因为 Cnew 已经被复制过半了，重新选 leader 也一定是选有 Cnew 的。 如果在配置分发过程中 leader 崩溃了怎么办？ 分两种情况： Cnew 已经分发过半 集群开始重新选举，此时在 Cnew 的规则下，不存在新配置中的节点不会赢得选举（因为他们要在 Cold,new 的情况下决定，但是拿不到 Cnew 的选票），只有拿到 Cnew 的节点可能成为 leader 并继续下发 Cnew 配置，流程恢复。 Cnew 没有分发过半 这种情况下，Cold,new 和 Cnew 的节点都可以成为 leader，但是无所谓，因为无论谁成为 leader，都能根据当前的配置继续完成后续流程（如果是 Cnew 那么相当与完成了最终的配置，不在 Cnew 的节点会因为没有心跳数据而失效）。 旧配置节点下线造成的问题 Raft 的处理方式：当节点确信有 leader 存在时，不会进行投票（在 leader 超时之前收到新的投票请求时不会提升任期号和做出投票）。且开始选举之前等待一个选举超时时间，这样在新 leader 正常工作的情况下，不会受到旧节点的影响。 旧配置节点在发起选举前需要等待一段时间，那么这段时间新 leader 可以发送心跳，这样就减少了影响。 对正常流程的影响不大。（leader 失效后要等一段时间，没有及时触发，然而本身这里就有一个判断失效的时间，好像影响不大；比如原先超时时间是 10s，那么如果设置成 5s，原策略下 10s 超时就是 10s 后开始选举，新策略下 5s 超时就是超时后再等 5s 再开始选举，影响就是超时时间变短） 无数据的新节点加入集群中的问题 新加入的节点需要时间复制数据，在这个过程完成之前，Raft 采用以下机制来保证可用性： 新加入节点没有投票权（ leader 复制日志给他们，但计算已复制日志条目的副本数的时候不考虑它们），直到这些节点的日志追上其他节点。 如果在配置变更过程中接收到用户请求的话，是用旧配置响应还是用新配置响应？ 按照笔者的理解，这个方面，对 Raft 协议的具体实现可以根据自身需求来自定义实现，Raft 的联合共识是为了避免同一时刻出现了 2 个 leader，避免了对客户端的一个请求同时有两个不同的响应出现。而在具体实现中，在某个阶段，究竟是采取新配置响应还是旧配置响应，可以再斟酌。 比如说可以这样： Cold 阶段：使用旧配置，需要过半旧配置节点确认 Cnew 已提交阶段：使用新配置，需要过半新配置节点确认 Cold,new 阶段：配置信息中有节点数量（这样才可能判断是否过半），这个时候新旧配置都需要过半节点确认，而响应新配置执行的结果还是响应旧配置执行的结果，就看 old 多还是 new 多，谁多用谁。 如果 leader 要下线，客户端发来的新的请求如何处理？ 如果是在 leader 复制 Cnew 之后，提交 Cnew 之前的话，leader 工作在新的集群配置下，所以会将日志复制到新集群的节点下，当收到新集群（不包含 leader 本身）超过半数节点确认后，就可以提交日志。 在其他阶段，leader 就是正常可用的。 所谓 Cnew 和 Cold,new 日志条目，里面没有数据，只有指令，里面的指令就是让节点执行对应的配置项。 7. 日志压缩 在正常情况下，Raft 的日志会随着客户端请求的增加而不断增长。但在实际系统中，日志不可能无限制地增长。随着日志越来越长，它会占用越来越多的空间，并且需要花更多的时间来重新执行日志中的日志条目。如果没有一定的机制来清除日志中积累的过期的信息，那么最终一定会影响系统的可用性。 快照技术（snapshotting） 是日志压缩最简单的方法。在快照技术中，某个时间点下的前整个系统的状态都会以快照的形式持久化起来，然后该时间点之前的日志会被全部丢弃。快照技术呗使用在 Chubby 和 ZooKeeper 当中，接下来的章节会介绍 Raft 中的快照技术。 增量压缩方法（Incremental approach to compaction），例如日志清洗（log cleaning）[36] 和日志结构合并树（log-structured merge trees）[30, 5]，都是可行的。这些方法每次只对一小部分数据进行操作，这样就分散了压缩的负载压力。首先，选择一个积累了大量被删除或被覆盖的对象的数据区域，然后重写该区域内还活着的对象，之后释放该区域。和快照技术相比，这需要大量额外的机制，并且增加了更多的复杂性，快照技术通过操作整个数据集来简化问题。虽然日志清理需要对 Raft 进行修改，但是状态机可以使用与快照技术相同的接口来实现 LSM（日志结构合并） 树。 image-20210715195808192 图 12 展示了 Raft 快照技术的基本思想。每一个节点独立地生成快照，快照中只包含自己日志中已经被提交的条目，这个过程主要的工作是状态机将自己的状态写入快照中。Raft 在快照中还保留了少量的元数据： last included index：指的是最后一个被快照取代的日志条目的索引值（状态机最后应用的日志条目） last included term：指的是该条目所处的任期号 保留这些元数据是为了支持快照后第一个条目的 AppendEntries 一致性检查，因为该条目需要一个之前的日志索引和任期号。为了支持集群成员变更（第 6 节中讨论的），快照中还包含日志中到 last included index 为止的最新的配置。一旦节点完成了快照的写入，它可能就会删除 last included index 及之前的所有日志条目，以及之前的快照。 尽管通常情况下，节点都是独立生成快照的，但是 leader 不可避免偶尔需要发送快照给一些落后的 follower。这通常发生在 leader 已经丢弃了需要发给 follower 的下一条日志条目的时候。幸运的是，这种情况在正常操作中是不会出现的：一个与 leader 保持同步的 follower 通常都会拥有该日志条目。不过如果一个 follower 运行比较缓慢，或者是它刚加入集群，那么它就可能会没有该日志条目。这个时候 leader 会通过网络将该快照发送给该 follower，以使得该 follower 可以更新到最新的状态。 image-20210715205247442 这个时候 leader 使用了一种新的 RPC 来发送快照给那些太落后的 followers，如图 13 所示，这种 RPC 叫做 InstallSnapshot。当一个 follower 通过这种 RPC 收到快照的时候，它必须决定如何处理当前已经存在的日志条目。通常情况下，这份快照会包含接受者日志者没有的信息。所以这种情况下 follower 会丢弃它的整个日志，它的日志会全部被快照取代，并且可能有与快照冲突的未提交的条目。相反，如果一个 follower 收到一个描述其日志前缀的快照（可能是由于重传或错误），则被快照覆盖的日志条目将被删除，但是快照之后的条目仍然有效，且必须要保留。 这种快照的方式违反了 Raft 的 strong leader 原则，因为 follower 可能在不知道 leader 的情况下创建快照。但是我们认为这种违背是合乎情理的。leader 的存在，是为了防止在达成共识的时候产生冲突，但是在创建快照的时候，共识已经达成了，因此没有决策会出现冲突。这种情况下，数据还是跟之前一样，只能从 leader 流向 follower，只不过现在允许 follower 可以重新组织它们的数组而已。 我们曾经考虑过一种可替代的方案，那就是只有 leader 可以创建快照，然后由 leader 将这份快照发送给其他所有的 follower。但是，这种方案有两个缺点： 发送快照给每个 follower 会浪费网络带宽和延缓了快照处理过程。实际上每一个 follower 已经拥有了创建自己快照所需要的全部信息了，所以很显然，follower 根据本地的状态创建快照要比通过网络来接收别人发过来的要更加实惠。 这会造成 leader 的实现更加复杂。比如说，leader 发送快照给 follower 的同时要能够做到并行地将新的日志条目发送给它们，这样才不会阻塞新的客户端请求，这就复杂得多了。 还有两个问题会影响快照的性能： 每一个节点必须判断何时去生成快照。如果一个节点生成快照的频率太高，那么就会浪费大量的磁盘带宽和其他资源；如果一个节点生成快照的频率太低，那么就要承担耗尽存储容量的风险，同时也增加了重启时重新执行日志的时间。 一个简单的策略就是当日志大小达到一个固定的阈值的时候就生成一份快照。如果这个阈值设置得显著大于期望的快照的大小，那么快照的磁盘带宽开销将较小。 第二个影响性能的就是写快照需要花费一定的时间，而我们又不希望它会影响到正常的操作。 解决方案就是使用 写时复制的技术（copy-on-write） ，这样新的更新就可以在不影响正在写的快照的情况下被接收。例如，具有泛型函数结构的状态机天然支持这样的功能。另外，操作系统对写时复制技术的支持（如 Linux 上的 fork）可以被用来创建整个状态机的内存快照（我们的实现用的就是这种方法）。 8. 客户端交互 本节介绍客户端如何和 Raft 进行交互，包括客户端如何找到 leader 和 Raft 是如何支持线性化语义的 [10]。这些问题对于所有的基于共识算法的系统都是存在的，Raft 的解决方案也跟其他的系统差不多。 Raft 的客户端们将所有的请求发送给 leader。当客户端第一次启动的时候，它会随机挑选一个节点来进行通信。如果客户端首选的不是 leader，那么被客户端选中的节点就会拒绝客户端的请求并且提供关于它最近收到的 leader 的信息（AppendEntries RPC 包含了 leader 的网络地址）。如果 leader 崩溃了，客户端请求就会超时，这个时候客户端需要随机选择一个节点来重试发送请求。 我们对 Raft 的期许是希望它可以实现线性化语义（即每次操作看起来似乎都是在调用和响应之间的某个点上即时执行一次）。但是，按照上面描述的，Raft 可能会对同一条指令执行多次。例如，如果 leader 在提交了某个日志条目后，在还没来得及响应客户端的时候就崩溃了，那么客户端会和新的 leader 重试该指令，这就造成了同一指令被执行了两次。解决方案是客户端对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每个客户端已经处理的最新的序列号以及相关联的响应。如果状态机接收到了一条已经执行过的指令了，就立即作出响应，并且不会重复执行该指令。 只读操作（Read-Only）可以直接处理而不记录日志。但是，如果不采取任何措施的话，这可能会有返回过期数据（stale data）的风险。因为 leader 响应客户端请求的时候它可能已经被新的 leader 代替了，但是它还不知道自己已经不是最新的 leader 了。 译者补充：为什么一个 leader 好好的会有另外一个 leader 出现？ 参考：https://segmentfault.com/a/1190000039264427 实际上，老的 leader 可能不会马上消失，例如：网络分区将 leader 与集群的其余部分分隔，其余部分选举出了一个新的 leader。然后老的 leader 崩溃后重新连接，可能会不知道新的 leader 已经被选出来了。 线性化的操作肯定不会返回过期的数据。Raft 需要使用两个额外的预防措施来在不适用日志的时候保证这一点。 leader 必须拥有那些已提交的日志条目的最新信息。Leader 完整性特性（Leader Completeness Property）保证了 leader 一定拥有所有已被提交的日志条目，但是在它任期刚开始的时候，它可能还不知道哪些是已经被提交的。为了知道这些信息，它需要在它的任期里提交一个日志条目。 Raft 通过让 leader 在任期开始的时候提交一个空的日志条目到日志中来解决该问题。（译者注：这就是前面 5.4.2 节提到的 no-op 日志） leader 在处理只读请求的时候必须检查自己是否已经被替代了（因为如果一个新 leader 被选出来了，那么这个旧 leader 的数据可能就过时了）。 Raft 通过让 leader 在响应只读请求之前，先和集群中过半的节点交换一次心跳信息来解决该问题。 另一种可选的方案，leader 可以依赖心跳机制来实现一种租约的形式 [9]，但是这种方式的安全性需要依赖于时序（假设时间误差是有界的）。 9. 算法实现与评估 我们已经实现了 Raft 作为复制状态机的一部分，该状态机存储了 RAMCloud [33] 的配置信息，并帮助 RAMCloud 协调器进行故障转移。这个 Raft 实现大概包含了 2000+ 行 C++ 代码，但是这里面没有包含测试、注释和空行。这些代码是开源的 [23]。同时也有大约 25 个其他独立的第三方、针对不同的开发场景、基于这篇论文草稿的开源实现。同时，很多公司已经部署了基于 Raft 算法的系统了。 本节剩下的篇幅将从三个方面来评估 Raft 算法： 可理解性 正确性 性能 9.1 可理解性 为了衡量 Raft 相对于 Paxos 的可理解性，我们针对高层次的本科生和研究生，在斯坦福大学的高级操作系统课程和加州大学伯克利分校的分布式计算课程上，进行了一项实验研究。我们为 Raft 和 Paxos 分别录制了一个视频教程，并且准备了相应的小测验。其中 Raft 课程覆盖了本篇论文除了日志压缩之外的全部内容，而 Paxos 课程涵盖了创建一个与 Raft 等价的复制状态机的全部资料，包括 signle-decree Paxos、multi-decree Paxos、重新配置和一切实际系统需要的性能优化（比如 leader 选举）。这个小测验主要是测试一些对算法的理解和解释一些边缘情况。每个学生都是看完第一个视频，然后做对应的测验，然后再看第二个视频，再做第二份测验。为了解释个人表现与从第一部分研究中获得的经验差异的原因，大约有一半的学生先进行 Paxos 的部分，然后另一半学生先进行 Raft 的部分。我们通过计算参与人员的每一份测验的得分来看参与者是否更加容易理解 Raft 算法。 我们尽可能的使得在比较 Raft 和 Paxos 过程中是公平的。这个实验从两个方面偏向了 Paxos： 43 个参与者中有 15 个人在之前有一些 Paxos 的经验 Paxos 视频教程的时长要长 14% 如表格 1 总结的那样，我们采取了一些措施来减轻这种潜在的偏向。我们所有的材料都可供审查 [28, 31]。 关注点 缓和偏向采取的手段 可供查看的材料 相同的讲课质量 两份教程采用同一个讲师。Paxos 的教程是在现有的一些大学使用的材料基础上改进的。Paxos 的教程要长 14%。 视频 相同的测验难度 问题以难度分组，在两个测验里成对出现。 小测验 公平评分 使用评价量规。随机顺序打分，两个测验交替进行。 评分细则 表格1：考虑到潜在的实验偏向，我们对于每种情况的解决方法，以及相应的材料。 平均上看，参与者在 Raft 测验上的得分要比在 Paxos 测验上的得分高处 4.9 分（在 60 分中，Raft 的平均得分是 25.7 分，Paxos 的平均得分是 20.8 分）。图 14 展示了每个参与者的得分。配对 t 检验（paired t-test）表明，在 95% 的置信度下，Raft 分数的真实分布的平均值至少要比 Paxos 的大 2.5 分。 image-20210716175208106 我们也建立了一个线性回归模型来预测一个新的学生的测验成绩，这个模型基于以下三点： 他们使用的是哪个测验 之前对于 Paxos 的经验 学习算法的顺序 该模型预测，对小测验的选择会产生 12.5 分的有利于 Raft 的差别，这很明显高于观察到的 4.9 分的分差。这是因为实际上许多的学生之前有学习过 Paxos，这对 Paxos 的有很大帮助的，但是对 Raft 的帮助就较小了。但是奇怪的是，模型预测对于先进行 Paxos 小测验的人而言，Raft 的得分低了 6.3 分。虽然我们不知道这是为什么，但是这似乎在统计上是有意义的。 image-20210716183850084 我们同时也在测验之后对参与者进行了调查，调查的内容是他们认为哪个算法更容易去实现或解释。这些调查结果展示在图 15。调查结果是碾压性的，结果表明 Raft 算法更加容易实现和解释（41 人中的 33 个）。然而，这种自我报告的感觉可能没有参与者的测试分数来得可靠，而且参与者可能由于我们假设 Raft 更容易理解而存在偏向。 在参考文献 [33] 中有一个关于 Raft 用户学习的更加详细的讨论。 9.2 正确性 在第 5 节中，我们已经对共识机制制定了正式的规范并且对其安全性做了证明。这份正式的规范使用 TLA+ 规范语言 [17] 使图 2 中对算法的总结的信息非常清晰。它差不多有 400 行并且作为了我们要证明的核心。同时这份规范对于任何想实现 Raft 的人都是十分有用的。我们用 TLA 证明系统 [7] 机械地证明了日志完整性（Log Completeness Property）。然而，这个证明依赖的约束前提还没有被机械证明（例如，我们还没有证明规范中的类型安全）。而且，我们已经编写了状态机安全特性的非正式证明 [31]，它是完整的（它仅依赖于规范）和相对精确的（大约 3500 字长）。 9.3 性能 Raft 的性能跟其他像 Paxos 的共识算法很接近。在性能方面，最重要的关注点就是，当一个 leader 被选举出来后，它要在什么时候复制新的日志条目。Raft 通过很少量的消息包（一轮从 leader 到集群中过半节点的的消息传递）就解决了这个问题。同时，进一步提升 Raft 的性能也是有可能的。比如说，很容易通过支持批量操作和管道操作来提高吞吐量和降低延迟。对于其他共识算法已经提出过很多性能优化方案，其中很多都可以应用到 Raft 上，但是我们暂时把这些工作放到未来的工作中。 我们使用我们自己的 Raft 实现来衡量 Raft 的 leader election 算法的性能并且回答两个问题： leader 选举过程收敛是否足够快？ 在 leader 崩溃之后，最小的系统崩溃时间是多久？ image-20210716194110554 为了衡量 leader election 的性能，我们反复使一个拥有 5 个节点的集群的 leader 宕机，并计算它检测崩溃和重新选一个新的 leader 所需的时间（见图 16）。为了构建一个最坏的情景，我们使各个节点中的日志长度都是不同的，这样某些 candidate 是无法成为 leader 的。而已，为了尽可能出现无结果的投票（split vote）情况，我们的测试脚本在终止 leader 的进程之前从 leader 那触发了一个同步的发送了一次心跳广播（类似于 leader 在崩溃前复制一个日志条目给其他节点）。leader 在其心跳间隔内均匀随机地崩溃，这个心跳间隔也是所有测试中最小选举超时时长的一半。因此，最小宕机时间大约就是最小选举超时时间的一半。 图 16 中上面的图表明，只需要在选举超时时间上使用很小的随机化就可以大大避免出现没有结果的投票的情况。在没有随机化的情况下（译者注：见图 16 中上面的图右边的橙色虚线），由于出现了很多没有结果的投票的情况，leader election 往往都需要花费超过 10s 的时间。仅仅加入 5ms 的随机化时间，就大大改善了选举过程，现在平均的宕机时间只有 287ms。继续增大随机性可以大大改善最坏的情况：通过增加 50ms 的随机化时间，最坏的完成情况（即完成 1000 次实验）只需要 513 ms。 图 16 中下面的图表明，通过减少选举超时时间可以禁烧系统的宕机时间。在选举超时时间为 12~24ms 的情况下，只需要平均 35ms 就可以选举出新的 leader（最长的一次花费了 152ms）。然而，进一步降低选举超时时间可能就会违反 Raft 不等式的要求。 广播时间（broadcastTime） 选举超时时间（electionTimeout） 平均故障间隔时间（MTBF） 因为这会使得在其他节点开启一轮新的选举之前，当前的 leader 要完成发送一次心跳广播变得很难。这会造成不必要的 leader 更换，从而降低了系统的可用性。我们推荐使用一个更为保守的选举超时时间，比如 150~300ms。这样的时间不大可能导致不必要的 leader 更换，同时还能提供不错的可用性。 10. 相关工作 现在已经有很多关于共识算法相关的产物了，其中很多都属于以下类别之一： Lamport 对于 Paxos 的最初的描述 [15]，以及尝试将 Paxos 解释地更清晰的描述 [16, 20, 21 ]。 关于 Paxos 的更详尽的描述，补充遗漏的细节并修改算法，使得可以提供更加容易的实现基础 [26, 39, 13]。 实现共识算法的系统，例如 Chubby [2, 4]，ZooKeeper [11, 12] 和 Spanner [6]。对于 Chubby 和 Spanner 的算法并没有公开发表其技术细节，尽管他们都声称是基于 Paxos 的。ZooKeeper 的算法细节已经发表，但是和 Paxos 着实有着很大的差别。 对于 Paxos 的性能优化 [18, 19, 3, 25, 1, 27]。 Oki 和 Liskov 的 Viewstamped Replication（VR），一种和 Paxos 差不多的替代算法。原始的算法描述 [29] 和分布式传输协议耦合在了一起，但是核心的共识算法在最近更新的版本 [22] 里被分离了出来。VR 使用了一种基于 leader 的方法，和 Raft 有很多相似之处。 Raft 和 Paxos 最大的不同就在于 Raft 的强领导性（strong leadership）。Raft 将 leader election 作为共识协议中非常重要的一环，并且将尽可能多的功能集中到了 leader 身上。这种方法使得算法更加简单和更容易理解。比如说，在 Paxos 中，leader election 和基本的共识协议是正交的：它只是作为一种性能优化，而不是实现共识所必需的。然而，这带来了很多额外的机制： Paxos 中包含了一个两段式的基本共识协议 Paxos 中还包含了一个单独的 leader election 机制 相比之下，Raft 将 leader election 直接纳入了共识算法并且将其作为共识两阶段中的第一个阶段，这使得 Raft 使用的机制要比 Paxos 少得多。 像 Raft 一样，VR 和 ZooKeeper 也是基于 leader 的，因此他们也拥有一些 Raft 的优点。但是，Raft 比 VR 和 ZooKeeper 拥有更少的机制。因为 Raft 尽可能的减少了非 leader 者的功能。例如，Raft 中日志条目都遵循着从 leader 发送给 follower 这一个方向：AppendEntries RPCs 是向外发送的。在 VR 中，日志条目的流动是双向的（leader 人可以在选举过程中接收日志）；这就导致了额外的机制和复杂性。根据 ZooKeeper 公开的资料看，它的日志条目也是双向传输的，但是它的实现更像 Raft。 跟我们上述提到的其他基于共识性的日志复制算法相比，Raft 的消息类型更少。例如，我们计算了一下 VR 和 ZooKeeper 用来实现基本功能和集群成员变更（不包括日志压缩和客户端交互，因为这些都比较独立且和算法关系不大）所需要的消息类型。VR 和 ZooKeeper 都分别定义了 10 种不同的消息类型。相比之下，Raft 只有 4 种消息类型（两种 RPC Request 及其对应的两种 RPC Response）。Raft 的消息的消息量比其他算法的要大一点，但总的来说，它们更加简单。另外，VR 和 ZooKeeper 都在 leader 改变的时候传输了整个日志，所以这些算法为了能在实践中使用，就不得不增加额外的消息类型了。 Raft 的强 leader 模型简化了整个算法，但是同时也排斥了一些性能优化的方法。例如，平等主义 Paxos （EPaxos）在某些没有 leader 的情况下可以达到很高的性能 [27]。平等主义 Paxos 充分发挥了在状态机指令中的交换性。任何服务器都可以在一轮通信下就提交指令，除非其他指令同时被提出了。然而，如果指令都是并发的被提出，并且互相之间不通信沟通，那么 EPaxos 就需要额外的一轮通信。因为任何服务器都可以提交指令，所以 EPaxos 在服务器之间的负载均衡做的很好，并且很容易在 WAN 网络环境下获得很低的延迟。但是，他在 Paxos 上增加了非常明显的复杂性。 一些集群成员变更的方法已经被提出或者在其他的工作中被实现，包括 Lamport 的原始的讨论 [15]，VR [22] 和 SMART [24]。我们选择使用联合共识的方法是因为它利用了共识协议的其余部分，这样我们只需要很少的一些机制就可以实现成员变更。Lamport 的基于 α 的方法之所以没有被 Raft 选择是因为它假设在没有 leader 的情况下也可以达到共识性。和 VR 和 SMART 相比较，Raft 的重新配置算法可以在不限制正常请求处理的情况下进行。相比之下，VR 在配置变更期间需要停止所有正常的处理过程，而 SMART 对未完成请求的数量实施了类似 α 方法的限制。另外，和 VR、SMART 相比，Raft 的方法也只需要增加更少的额外机制来实现。 11. 结论 算法的设计通常以正确性、效率和简洁性为主要目标。虽然这些都是有价值的目标，但我们相信可理解性同样重要。在开发人员将算法转化为实际实现之前，其他任何目标都不能实现，而实际实现将不可避免地偏离和扩展发布的形式。除非开发人员对算法有深刻的理解，并能对算法有直观的认识，否则他们很难在实现中保留算法理想的特性。 在本文中，我们讨论了分布式共识的问题，在这个问题上，一个被广泛接受但难以理解的算法：Paxos，多年来一直让学生和开发人员非常挣扎。我们开发了一种新的算法：Raft，我们已经证明它比 Paxos 更容易理解。我们也相信 Raft 会为系统建设提供更好的基础。将可理解性作为主要设计目标改变了我们处理 Raft 设计的方式。随着设计的进展，我们发现自己反复使用了一些技术，比如分解问题和简化状态空间。这些技术不仅提高了 Raft 的可理解性，而且使我们更容易证实它的正确性。 12. 致谢 这项研究必须感谢以下人员的支持：Ali Ghodsi，David Mazie`res，和伯克利 CS 294-91 课程、斯坦福 CS 240 课程的学生，没有他们的大力支持，这项研究是不可能完成的。Scott Klemmer 帮我们设计了用户调查，Nelson Ray 建议我们进行统计学的分析。在用户调查时使用的关于 Paxos 的幻灯片很大一部分是从 Lorenzo Alvisi 的幻灯片上借鉴过来的。特别的，非常感谢 DavidMazieres 和 Ezra Hoch，他们找到了 Raft 中一些难以发现的漏洞。许多人提供了关于这篇论文十分有用的反馈和用户调查材料，包括 Ed Bugnion，Michael Chan，Hugues Evrard，Daniel Giffin，Arjun Gopalan，Jon Howell，Vimalkumar Jeyakumar，Ankita Kejriwal，Aleksandar Kracun，Amit Levy，Joel Martin，Satoshi Matsushita，Oleg Pesok，David Ramos，Robbert van Renesse，Mendel Rosenblum，Nicolas Schiper，Deian Stefan，Andrew Stone，Ryan Stutsman，David Terei，Stephen Yang，Matei Zaharia 以及 24 位匿名的会议审查人员（可能有重复），并且特别感谢我们的领导人 Eddie Kohler。Werner Vogels 发了一条早期草稿链接的推特，给 Raft 带来了极大的关注。我们的工作由 Gigascale 系统研究中心和 Multiscale 系统研究中心给予支持，这两个研究中心由关注中心研究程序资金支持，一个是半导体研究公司的程序，由 STARnet 支持，一个半导体研究公司的程序由 MARCO 和 DARPA 支持，在国家科学基金会的 0963859 号批准，并且获得了来自 Facebook，Google，Mellanox，NEC，NetApp，SAP 和 Samsung 的支持。Diego Ongaro 由 Junglee 公司，斯坦福的毕业团体支持。 参考文献 [1] BOLOSKY, W. J., BRADSHAW, D., HAAGENS, R. B., KUSTERS, N. P., AND LI, P. Paxos replicated state machines as the basis of a high-performance data store. In Proc. NSDI’11, USENIX Conference on Networked Systems Design and Implementation (2011), USENIX, pp. 141–154. [2] BURROWS, M. The Chubby lock service for loosely- coupled distributed systems. In Proc. OSDI’06, Sympo- sium on Operating Systems Design and Implementation (2006), USENIX, pp. 335–350. [3] CAMARGOS, L. J., SCHMIDT, R. M., AND PEDONE, F. Multicoordinated Paxos. In Proc. PODC’07, ACM Sym- posium on Principles of Distributed Computing (2007), ACM, pp. 316–317. [4] CHANDRA, T. D., GRIESEMER, R., AND REDSTONE, J. Paxos made live: an engineering perspective. In Proc. PODC’07, ACM Symposium on Principles of Distributed Computing (2007), ACM, pp. 398–407. [5] CHANG, F., DEAN, J., GHEMAWAT, S., HSIEH, W. C., WALLACH, D. A., BURROWS, M., CHANDRA, T., FIKES, A., AND GRUBER, R. E. Bigtable: a distributed storage system for structured data. In Proc. OSDI’06, USENIX Symposium on Operating Systems Design and Implementation (2006), USENIX, pp. 205–218. [6] CORBETT, J. C., DEAN, J., EPSTEIN, M., FIKES, A., FROST, C., FURMAN, J. J., GHEMAWAT, S., GUBAREV, A., HEISER, C., HOCHSCHILD, P., HSIEH, W., KAN- THAK, S., KOGAN, E., LI, H., LLOYD, A., MELNIK, S., MWAURA, D., NAGLE, D., QUINLAN, S., RAO, R., ROLIG, L., SAITO, Y., SZYMANIAK, M., TAYLOR, C., WANG, R., AND WOODFORD, D. Spanner: Google’s globally-distributed database. In Proc. OSDI’12, USENIX Conference on Operating Systems Design and Implemen- tation (2012), USENIX, pp. 251–264. [7] COUSINEAU, D., DOLIGEZ, D., LAMPORT, L., MERZ, S., RICKETTS, D., AND VANZETTO, H. TLA+ proofs. In Proc. FM’12, Symposium on Formal Methods (2012), D. Giannakopoulou and D. Me ́ry, Eds., vol. 7436 of Lec- ture Notes in Computer Science, Springer, pp. 147–154. [8] GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google file system. In Proc. SOSP’03, ACM Symposium on Operating Systems Principles (2003), ACM, pp. 29–43. [9] GRAY,C.,ANDCHERITON,D.Leases:Anefficientfault- tolerant mechanism for distributed file cache consistency. In Proceedings of the 12th ACM Ssymposium on Operating Systems Principles (1989), pp. 202–210. [10] HERLIHY, M. P., AND WING, J. M. Linearizability: a correctness condition for concurrent objects. ACM Trans- actions on Programming Languages and Systems 12 (July 1990), 463–492. [11] HUNT, P., KONAR, M., JUNQUEIRA, F. P., AND REED, B . ZooKeeper: wait-free coordination for internet-scale systems. In Proc ATC’10, USENIX Annual Technical Con- ference (2010), USENIX, pp. 145–158. [12] JUNQUEIRA, F. P., REED, B. C., AND SERAFINI, M. Zab: High-performance broadcast for primary-backup sys- tems. In Proc. DSN’11, IEEE/IFIP Int’l Conf. on Depend- able Systems Networks (2011), IEEE Computer Society, pp. 245–256. [13] KIRSCH, J., AND AMIR, Y. Paxos for system builders. Tech. Rep. CNDS-2008-2, Johns Hopkins University, 2008. [14] L A M P O RT, L . Time, clocks, and the ordering of events in a distributed system. Commununications of the ACM 21, 7 (July 1978), 558–565. [15] L A M P O RT, L . The part-time parliament. ACM Transac- tions on Computer Systems 16, 2 (May 1998), 133–169. [16] LAMPORT, L. Paxos made simple. ACM SIGACT News 32, 4 (Dec. 2001), 18–25. [17] L A M P O RT, L . Specifying Systems, The TLA+ Language and Tools for Hardware and Software Engineers. Addison- Wesley, 2002. [18] LAMPORT, L. Generalized consensus and Paxos. Tech. Rep. MSR-TR-2005-33, Microsoft Research, 2005. [19] L A M P O RT, L . Fast paxos. (2006), 79–103. [20] LAMPSON, B. W. How to build a highly available system using consensus. In Distributed Algorithms, O. Baboaglu and K. Marzullo, Eds. Springer-Verlag, 1996, pp. 1–17. [21] LAMPSON, B. W. The ABCD’s of Paxos. In Proc. PODC’01, ACM Symposium on Principles of Distributed Computing (2001), ACM, pp. 13–13. [22] LISKOV, B., AND COWLING, J. Viewstamped replica- tion revisited. Tech. Rep. MIT-CSAIL-TR-2012-021, MIT, July 2012. [23] LogCabin source code. http://github.com/ logcabin/logcabin. [24] LORCH, J. R., ADYA, A., BOLOSKY, W. J., CHAIKEN, R., DOUCEUR, J. R., AND HOWELL, J. The SMART way to migrate replicated stateful services. In Proc. Eu- roSys’06, ACM SIGOPS/EuroSys European Conference on Computer Systems (2006), ACM, pp. 103–115. [25] MAO, Y., JUNQUEIRA, F. P., AND MARZULLO, K. Mencius: building efficient replicated state machines for WANs. In Proc. OSDI’08, USENIX Conference on Operating Systems Design and Implementation (2008), USENIX, pp. 369–384. [26] MAZIE` RES, D. Paxos made practical. http://www.scs.stanford.edu/ ̃dm/home/ papers/paxos.pdf, Jan. 2007. [27] MORARU, I., ANDERSEN, D. G., AND KAMINSKY, M. There is more consensus in egalitarian parliaments. In Proc. SOSP’13, ACM Symposium on Operating System Principles (2013), ACM. [28] Raft user study. http://ramcloud.stanford. edu/ ̃ongaro/userstudy/. [29] OKI, B. M., AND LISKOV, B. H. Viewstamped replication: A new primary copy method to support highly-available distributed systems. In Proc. PODC’88, ACM Symposium on Principles of Distributed Computing (1988), ACM, pp. 8–17. [30] O’NEIL, P., CHENG, E., GAWLICK, D., AND ONEIL, E. The log-structured merge-tree (LSM-tree). Acta Informat- ica 33, 4 (1996), 351–385. [31] ONGARO, D. Consensus: Bridging Theory and Practice. PhD thesis, Stanford University, 2014 (work in progress). [32] ONGARO, D., AND OUSTERHOUT, J. In search of an understandable consensus algorithm. In Proc ATC’14, USENIX Annual Technical Conference (2014), USENIX. [33] OUSTERHOUT, J., AGRAWAL, P., ERICKSON, D., KOZYRAKIS, C., LEVERICH, J., MAZIE`RES, D., MI- TRA, S., NARAYANAN, A., ONGARO, D., PARULKAR, G., ROSENBLUM, M., RUMBLE, S. M., STRATMANN, E., AND STUTSMAN, R. The case for RAMCloud. Com- munications of the ACM 54 (July 2011), 121–130. [34] Raft consensus algorithm website. http://raftconsensus.github.io. [35] REED, B. Personal communications, May 17, 2013. [36] ROSENBLUM, M., AND OUSTERHOUT, J. K. The design and implementation of a log-structured file system. ACM Trans. Comput. Syst. 10 (February 1992), 26–52. [37] S C H N E I D E R , F. B . Implementing fault-tolerant services using the state machine approach: a tutorial. ACM Com- puting Surveys 22, 4 (Dec. 1990), 299–319. [38] SHVACHKO, K., KUANG, H., RADIA, S., AND CHANSLER, R. The Hadoop distributed file system. In Proc. MSST’10, Symposium on Mass Storage Sys- tems and Technologies (2010), IEEE Computer Society, pp. 1–10. [39] VAN RENESSE, R. Paxos made moderately complex. Tech. rep., Cornell University, 2012.","tags":["分布式","raft","原创"],"categories":["论文翻译"]},{"title":"关于我","path":"/about/index.html","content":""}]